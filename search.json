[
  {
    "objectID": "ABCDE.html",
    "href": "ABCDE.html",
    "title": "",
    "section": "",
    "text": "Ea ea velit dolore exercitation reprehenderit reprehenderit aute elit eu proident voluptate commodo adipisicing."
  },
  {
    "objectID": "ABCDE.html#translations",
    "href": "ABCDE.html#translations",
    "title": "",
    "section": "Translations",
    "text": "Translations\nThis guidance is available in language, language, and language"
  },
  {
    "objectID": "ABCDE.html#when-to-use-this-guidance",
    "href": "ABCDE.html#when-to-use-this-guidance",
    "title": "",
    "section": "When to use this guidance",
    "text": "When to use this guidance\n\n\n\n\n\n\nUse this guidance for:\n\nEst qui exercitation est commodo irure\nCulpa ipsum exercitation culpa ea deserunt.\nUse a template for drafting\nUse a checklist to check and demonstrate compliance\nUse a to-do list for planning a manuscript\n\nIt can also be used for:\n\nEnim quis laborum dolor occaecat fugiat (use items #-#).\nQuis nostrud incididunt ipsum nisi eu laboris (use items #-#).\n\n\n\n\n\n\n\n\n\n\nDo not use this guidance for:\n\nwriting incididunt ipsum nisi, use BHUJSD-Cudhjs instead\nwriting incididunt ipsum nisi, use THUYGH instead\nwriting incididunt ipsum nisi\nappraising quality, consider using QWETY instead\n\n\n\n\n\n\n\n\n\n\nFor designing research consider:\n\nresource\nresource\n\nFor appraising research consider:\n\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#download-resources",
    "href": "ABCDE.html#download-resources",
    "title": "",
    "section": "Download Resources",
    "text": "Download Resources\n\n\n\nFor drafting\n\n\nTemplate\n\n\n\n\nFor checking\n\n\nChecklist\n\n\n\n\nFor planning\n\n\nTo-do list"
  },
  {
    "objectID": "ABCDE.html#guidance",
    "href": "ABCDE.html#guidance",
    "title": "",
    "section": "Guidance",
    "text": "Guidance\nApprox. 7 min read\n\nIntroduction\n\n\n1. Occaecat commodo\n\n\n\n\n\n\n\nDescribe irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea exercitation elit Lorem ullamco proident reprehenderit. Ipsum consequat nisi ullamco consectetur anim voluptate.\nIf you did not do irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea.\nIf [applicability criteria not met], describe nisi ullamco consectetur anim.\nYou can report this [options e.g., in the body, a table, a figure, supplement] with pros and cons for each [e.g., word count vs. discoverability].\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nExamples\nWhy readers need this information\nDesign & appraisal resources\nHow to do this item\nTraining\n\n\nExamples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco.\n\n\n\nWhy readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco.\n\n\nDesign and appraisal resouces\n\nresource\nresource\nresource\nresource\n\n\n\nHow to do [something]\n\nresource\nresource\n\n\n\nTraining\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top\n\n\n\n\n\n\n2. Commodo sint\n\n\n\n\n\n\n\nDescribe irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea exercitation elit Lorem ullamco proident reprehenderit. Ipsum consequat nisi ullamco consectetur anim voluptate.\nIf you did not do irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea.\nIf [applicability criteria not met], describe nisi ullamco consectetur anim.\nYou can report this [options e.g., in the body, a table, a figure, supplement] with pros and cons for each [e.g., word count vs. discoverability].\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nExamples\nWhy readers need this information\nDesign & appraisal resources\nHow to do this item\nTraining\n\n\nExamples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco.\n\n\n\nWhy readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco.\n\n\nDesign and appraisal resouces\n\nresource\nresource\nresource\nresource\n\n\n\nHow to do [something]\n\nresource\nresource\n\n\n\nTraining\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top\n\n\n\n\n\n\n3. Sint ea\n\n\n\n\n\n\n\nDescribe irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea exercitation elit Lorem ullamco proident reprehenderit. Ipsum consequat nisi ullamco consectetur anim voluptate.\nIf you did not do irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea.\nIf [applicability criteria not met], describe nisi ullamco consectetur anim.\nYou can report this [options e.g., in the body, a table, a figure, supplement] with pros and cons for each [e.g., word count vs. discoverability].\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nExamples\nWhy readers need this information\nDesign & appraisal resources\nHow to do this item\nTraining\n\n\nExamples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco.\n\n\n\nWhy readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco.\n\n\nDesign and appraisal resouces\n\nresource\nresource\nresource\nresource\n\n\n\nHow to do [something]\n\nresource\nresource\n\n\n\nTraining\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top\n\n\n\n\n\n\n4. Ea laborum\n\n\n\n\n\n\n\nDescribe irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea exercitation elit Lorem ullamco proident reprehenderit. Ipsum consequat nisi ullamco consectetur anim voluptate.\nIf you did not do irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea.\nIf [applicability criteria not met], describe nisi ullamco consectetur anim.\nYou can report this [options e.g., in the body, a table, a figure, supplement] with pros and cons for each [e.g., word count vs. discoverability].\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nExamples\nWhy readers need this information\nDesign & appraisal resources\nHow to do this item\nTraining\n\n\nExamples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco.\n\n\n\nWhy readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco.\n\n\nDesign and appraisal resouces\n\nresource\nresource\nresource\nresource\n\n\n\nHow to do [something]\n\nresource\nresource\n\n\n\nTraining\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top\n\n\n\n\n\n\n5. Laborum voluptate\n\n\n\n\n\n\n\nDescribe irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea exercitation elit Lorem ullamco proident reprehenderit. Ipsum consequat nisi ullamco consectetur anim voluptate.\nIf you did not do irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea.\nIf [applicability criteria not met], describe nisi ullamco consectetur anim.\nYou can report this [options e.g., in the body, a table, a figure, supplement] with pros and cons for each [e.g., word count vs. discoverability].\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nExamples\nWhy readers need this information\nDesign & appraisal resources\nHow to do this item\nTraining\n\n\nExamples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco.\n\n\n\nWhy readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco.\n\n\nDesign and appraisal resouces\n\nresource\nresource\nresource\nresource\n\n\n\nHow to do [something]\n\nresource\nresource\n\n\n\nTraining\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top\n\n\n\n\n\n\nMethods\n\n\n6. Voluptate ut\n\n\n\n\n\n\n\nDescribe irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea exercitation elit Lorem ullamco proident reprehenderit. Ipsum consequat nisi ullamco consectetur anim voluptate.\nIf you did not do irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea.\nIf [applicability criteria not met], describe nisi ullamco consectetur anim.\nYou can report this [options e.g., in the body, a table, a figure, supplement] with pros and cons for each [e.g., word count vs. discoverability].\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nExamples\nWhy readers need this information\nDesign & appraisal resources\nHow to do this item\nTraining\n\n\nExamples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco.\n\n\n\nWhy readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco.\n\n\nDesign and appraisal resouces\n\nresource\nresource\nresource\nresource\n\n\n\nHow to do [something]\n\nresource\nresource\n\n\n\nTraining\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top\n\n\n\n\n\n\n7. Ut veniam\n\n\n\n\n\n\n\nDescribe irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea exercitation elit Lorem ullamco proident reprehenderit. Ipsum consequat nisi ullamco consectetur anim voluptate.\nIf you did not do irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea.\nIf [applicability criteria not met], describe nisi ullamco consectetur anim.\nYou can report this [options e.g., in the body, a table, a figure, supplement] with pros and cons for each [e.g., word count vs. discoverability].\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nExamples\nWhy readers need this information\nDesign & appraisal resources\nHow to do this item\nTraining\n\n\nExamples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco.\n\n\n\nWhy readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco.\n\n\nDesign and appraisal resouces\n\nresource\nresource\nresource\nresource\n\n\n\nHow to do [something]\n\nresource\nresource\n\n\n\nTraining\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top\n\n\n\n\n\n\n8. Veniam voluptate\n\n\n\n\n\n\n\nDescribe irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea exercitation elit Lorem ullamco proident reprehenderit. Ipsum consequat nisi ullamco consectetur anim voluptate.\nIf you did not do irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea.\nIf [applicability criteria not met], describe nisi ullamco consectetur anim.\nYou can report this [options e.g., in the body, a table, a figure, supplement] with pros and cons for each [e.g., word count vs. discoverability].\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nExamples\nWhy readers need this information\nDesign & appraisal resources\nHow to do this item\nTraining\n\n\nExamples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco.\n\n\n\nWhy readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco.\n\n\nDesign and appraisal resouces\n\nresource\nresource\nresource\nresource\n\n\n\nHow to do [something]\n\nresource\nresource\n\n\n\nTraining\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top\n\n\n\n\n\n\n9. Voluptate fugiat\n\n\n\n\n\n\n\nDescribe irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea exercitation elit Lorem ullamco proident reprehenderit. Ipsum consequat nisi ullamco consectetur anim voluptate.\nIf you did not do irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea.\nIf [applicability criteria not met], describe nisi ullamco consectetur anim.\nYou can report this [options e.g., in the body, a table, a figure, supplement] with pros and cons for each [e.g., word count vs. discoverability].\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nExamples\nWhy readers need this information\nDesign & appraisal resources\nHow to do this item\nTraining\n\n\nExamples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco.\n\n\n\nWhy readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco.\n\n\nDesign and appraisal resouces\n\nresource\nresource\nresource\nresource\n\n\n\nHow to do [something]\n\nresource\nresource\n\n\n\nTraining\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top\n\n\n\n\n\n\n10. Fugiat voluptate\n\n\n\n\n\n\n\nDescribe irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea exercitation elit Lorem ullamco proident reprehenderit. Ipsum consequat nisi ullamco consectetur anim voluptate.\nIf you did not do irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea.\nIf [applicability criteria not met], describe nisi ullamco consectetur anim.\nYou can report this [options e.g., in the body, a table, a figure, supplement] with pros and cons for each [e.g., word count vs. discoverability].\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nExamples\nWhy readers need this information\nDesign & appraisal resources\nHow to do this item\nTraining\n\n\nExamples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco.\n\n\n\nWhy readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco.\n\n\nDesign and appraisal resouces\n\nresource\nresource\nresource\nresource\n\n\n\nHow to do [something]\n\nresource\nresource\n\n\n\nTraining\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top\n\n\n\n\n\n\nResults\n\n\n11. Voluptate qui\n\n\n\n\n\n\n\nDescribe irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea exercitation elit Lorem ullamco proident reprehenderit. Ipsum consequat nisi ullamco consectetur anim voluptate.\nIf you did not do irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea.\nIf [applicability criteria not met], describe nisi ullamco consectetur anim.\nYou can report this [options e.g., in the body, a table, a figure, supplement] with pros and cons for each [e.g., word count vs. discoverability].\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nExamples\nWhy readers need this information\nDesign & appraisal resources\nHow to do this item\nTraining\n\n\nExamples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco.\n\n\n\nWhy readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco.\n\n\nDesign and appraisal resouces\n\nresource\nresource\nresource\nresource\n\n\n\nHow to do [something]\n\nresource\nresource\n\n\n\nTraining\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top\n\n\n\n\n\n\n12. Qui minim\n\n\n\n\n\n\n\nDescribe irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea exercitation elit Lorem ullamco proident reprehenderit. Ipsum consequat nisi ullamco consectetur anim voluptate.\nIf you did not do irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea.\nIf [applicability criteria not met], describe nisi ullamco consectetur anim.\nYou can report this [options e.g., in the body, a table, a figure, supplement] with pros and cons for each [e.g., word count vs. discoverability].\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nExamples\nWhy readers need this information\nDesign & appraisal resources\nHow to do this item\nTraining\n\n\nExamples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco.\n\n\n\nWhy readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco.\n\n\nDesign and appraisal resouces\n\nresource\nresource\nresource\nresource\n\n\n\nHow to do [something]\n\nresource\nresource\n\n\n\nTraining\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top\n\n\n\n\n\n\n13. Minim sunt\n\n\n\n\n\n\n\nDescribe irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea exercitation elit Lorem ullamco proident reprehenderit. Ipsum consequat nisi ullamco consectetur anim voluptate.\nIf you did not do irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea.\nIf [applicability criteria not met], describe nisi ullamco consectetur anim.\nYou can report this [options e.g., in the body, a table, a figure, supplement] with pros and cons for each [e.g., word count vs. discoverability].\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nExamples\nWhy readers need this information\nDesign & appraisal resources\nHow to do this item\nTraining\n\n\nExamples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco.\n\n\n\nWhy readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco.\n\n\nDesign and appraisal resouces\n\nresource\nresource\nresource\nresource\n\n\n\nHow to do [something]\n\nresource\nresource\n\n\n\nTraining\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top\n\n\n\n\n\n\n14. Sunt esse\n\n\n\n\n\n\n\nDescribe irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea exercitation elit Lorem ullamco proident reprehenderit. Ipsum consequat nisi ullamco consectetur anim voluptate.\nIf you did not do irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea.\nIf [applicability criteria not met], describe nisi ullamco consectetur anim.\nYou can report this [options e.g., in the body, a table, a figure, supplement] with pros and cons for each [e.g., word count vs. discoverability].\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nExamples\nWhy readers need this information\nDesign & appraisal resources\nHow to do this item\nTraining\n\n\nExamples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco.\n\n\n\nWhy readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco.\n\n\nDesign and appraisal resouces\n\nresource\nresource\nresource\nresource\n\n\n\nHow to do [something]\n\nresource\nresource\n\n\n\nTraining\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top\n\n\n\n\n\n\n15. Esse non\n\n\n\n\n\n\n\nDescribe irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea exercitation elit Lorem ullamco proident reprehenderit. Ipsum consequat nisi ullamco consectetur anim voluptate.\nIf you did not do irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea.\nIf [applicability criteria not met], describe nisi ullamco consectetur anim.\nYou can report this [options e.g., in the body, a table, a figure, supplement] with pros and cons for each [e.g., word count vs. discoverability].\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nExamples\nWhy readers need this information\nDesign & appraisal resources\nHow to do this item\nTraining\n\n\nExamples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco.\n\n\n\nWhy readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco.\n\n\nDesign and appraisal resouces\n\nresource\nresource\nresource\nresource\n\n\n\nHow to do [something]\n\nresource\nresource\n\n\n\nTraining\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top\n\n\n\n\n\n\nDiscussion\n\n\n16. Non et\n\n\n\n\n\n\n\nDescribe irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea exercitation elit Lorem ullamco proident reprehenderit. Ipsum consequat nisi ullamco consectetur anim voluptate.\nIf you did not do irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea.\nIf [applicability criteria not met], describe nisi ullamco consectetur anim.\nYou can report this [options e.g., in the body, a table, a figure, supplement] with pros and cons for each [e.g., word count vs. discoverability].\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nExamples\nWhy readers need this information\nDesign & appraisal resources\nHow to do this item\nTraining\n\n\nExamples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco.\n\n\n\nWhy readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco.\n\n\nDesign and appraisal resouces\n\nresource\nresource\nresource\nresource\n\n\n\nHow to do [something]\n\nresource\nresource\n\n\n\nTraining\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top\n\n\n\n\n\n\n17. Et ea\n\n\n\n\n\n\n\nDescribe irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea exercitation elit Lorem ullamco proident reprehenderit. Ipsum consequat nisi ullamco consectetur anim voluptate.\nIf you did not do irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea.\nIf [applicability criteria not met], describe nisi ullamco consectetur anim.\nYou can report this [options e.g., in the body, a table, a figure, supplement] with pros and cons for each [e.g., word count vs. discoverability].\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nExamples\nWhy readers need this information\nDesign & appraisal resources\nHow to do this item\nTraining\n\n\nExamples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco.\n\n\n\nWhy readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco.\n\n\nDesign and appraisal resouces\n\nresource\nresource\nresource\nresource\n\n\n\nHow to do [something]\n\nresource\nresource\n\n\n\nTraining\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top\n\n\n\n\n\n\n18. Ea pariatur\n\n\n\n\n\n\n\nDescribe irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea exercitation elit Lorem ullamco proident reprehenderit. Ipsum consequat nisi ullamco consectetur anim voluptate.\nIf you did not do irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea.\nIf [applicability criteria not met], describe nisi ullamco consectetur anim.\nYou can report this [options e.g., in the body, a table, a figure, supplement] with pros and cons for each [e.g., word count vs. discoverability].\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nExamples\nWhy readers need this information\nDesign & appraisal resources\nHow to do this item\nTraining\n\n\nExamples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco.\n\n\n\nWhy readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco.\n\n\nDesign and appraisal resouces\n\nresource\nresource\nresource\nresource\n\n\n\nHow to do [something]\n\nresource\nresource\n\n\n\nTraining\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top\n\n\n\n\n\n\n19. Pariatur nisi\n\n\n\n\n\n\n\nDescribe irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea exercitation elit Lorem ullamco proident reprehenderit. Ipsum consequat nisi ullamco consectetur anim voluptate.\nIf you did not do irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea.\nIf [applicability criteria not met], describe nisi ullamco consectetur anim.\nYou can report this [options e.g., in the body, a table, a figure, supplement] with pros and cons for each [e.g., word count vs. discoverability].\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nExamples\nWhy readers need this information\nDesign & appraisal resources\nHow to do this item\nTraining\n\n\nExamples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco.\n\n\n\nWhy readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco.\n\n\nDesign and appraisal resouces\n\nresource\nresource\nresource\nresource\n\n\n\nHow to do [something]\n\nresource\nresource\n\n\n\nTraining\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top\n\n\n\n\n\n\n20. Nisi et\n\n\n\n\n\n\n\nDescribe irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea exercitation elit Lorem ullamco proident reprehenderit. Ipsum consequat nisi ullamco consectetur anim voluptate.\nIf you did not do irure sint ipsum aute eiusmod irure laborum sunt minim. Irure aliquip do dolor do ea ipsum nostrud ea.\nIf [applicability criteria not met], describe nisi ullamco consectetur anim.\nYou can report this [options e.g., in the body, a table, a figure, supplement] with pros and cons for each [e.g., word count vs. discoverability].\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nExamples\nWhy readers need this information\nDesign & appraisal resources\nHow to do this item\nTraining\n\n\nExamples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco.\n\n\n\nWhy readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco.\n\n\nDesign and appraisal resouces\n\nresource\nresource\nresource\nresource\n\n\n\nHow to do [something]\n\nresource\nresource\n\n\n\nTraining\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top"
  },
  {
    "objectID": "ABCDE.html#sec-1-1",
    "href": "ABCDE.html#sec-1-1",
    "title": "",
    "section": "Examples",
    "text": "Examples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-2-1",
    "href": "ABCDE.html#sec-2-1",
    "title": "",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-3-1",
    "href": "ABCDE.html#sec-3-1",
    "title": "",
    "section": "Design and appraisal resouces",
    "text": "Design and appraisal resouces\n\nresource\nresource\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-4-1",
    "href": "ABCDE.html#sec-4-1",
    "title": "",
    "section": "How to do [something]",
    "text": "How to do [something]\n\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-5-1",
    "href": "ABCDE.html#sec-5-1",
    "title": "",
    "section": "Training",
    "text": "Training\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top"
  },
  {
    "objectID": "ABCDE.html#sec-1-2",
    "href": "ABCDE.html#sec-1-2",
    "title": "",
    "section": "Examples",
    "text": "Examples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-2-2",
    "href": "ABCDE.html#sec-2-2",
    "title": "",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-3-2",
    "href": "ABCDE.html#sec-3-2",
    "title": "",
    "section": "Design and appraisal resouces",
    "text": "Design and appraisal resouces\n\nresource\nresource\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-4-2",
    "href": "ABCDE.html#sec-4-2",
    "title": "",
    "section": "How to do [something]",
    "text": "How to do [something]\n\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-5-2",
    "href": "ABCDE.html#sec-5-2",
    "title": "",
    "section": "Training",
    "text": "Training\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top"
  },
  {
    "objectID": "ABCDE.html#sec-1-3",
    "href": "ABCDE.html#sec-1-3",
    "title": "",
    "section": "Examples",
    "text": "Examples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-2-3",
    "href": "ABCDE.html#sec-2-3",
    "title": "",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-3-3",
    "href": "ABCDE.html#sec-3-3",
    "title": "",
    "section": "Design and appraisal resouces",
    "text": "Design and appraisal resouces\n\nresource\nresource\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-4-3",
    "href": "ABCDE.html#sec-4-3",
    "title": "",
    "section": "How to do [something]",
    "text": "How to do [something]\n\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-5-3",
    "href": "ABCDE.html#sec-5-3",
    "title": "",
    "section": "Training",
    "text": "Training\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top"
  },
  {
    "objectID": "ABCDE.html#sec-1-4",
    "href": "ABCDE.html#sec-1-4",
    "title": "",
    "section": "Examples",
    "text": "Examples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-2-4",
    "href": "ABCDE.html#sec-2-4",
    "title": "",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-3-4",
    "href": "ABCDE.html#sec-3-4",
    "title": "",
    "section": "Design and appraisal resouces",
    "text": "Design and appraisal resouces\n\nresource\nresource\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-4-4",
    "href": "ABCDE.html#sec-4-4",
    "title": "",
    "section": "How to do [something]",
    "text": "How to do [something]\n\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-5-4",
    "href": "ABCDE.html#sec-5-4",
    "title": "",
    "section": "Training",
    "text": "Training\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top"
  },
  {
    "objectID": "ABCDE.html#sec-1-5",
    "href": "ABCDE.html#sec-1-5",
    "title": "",
    "section": "Examples",
    "text": "Examples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-2-5",
    "href": "ABCDE.html#sec-2-5",
    "title": "",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-3-5",
    "href": "ABCDE.html#sec-3-5",
    "title": "",
    "section": "Design and appraisal resouces",
    "text": "Design and appraisal resouces\n\nresource\nresource\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-4-5",
    "href": "ABCDE.html#sec-4-5",
    "title": "",
    "section": "How to do [something]",
    "text": "How to do [something]\n\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-5-5",
    "href": "ABCDE.html#sec-5-5",
    "title": "",
    "section": "Training",
    "text": "Training\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top"
  },
  {
    "objectID": "ABCDE.html#sec-1-6",
    "href": "ABCDE.html#sec-1-6",
    "title": "",
    "section": "Examples",
    "text": "Examples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-2-6",
    "href": "ABCDE.html#sec-2-6",
    "title": "",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-3-6",
    "href": "ABCDE.html#sec-3-6",
    "title": "",
    "section": "Design and appraisal resouces",
    "text": "Design and appraisal resouces\n\nresource\nresource\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-4-6",
    "href": "ABCDE.html#sec-4-6",
    "title": "",
    "section": "How to do [something]",
    "text": "How to do [something]\n\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-5-6",
    "href": "ABCDE.html#sec-5-6",
    "title": "",
    "section": "Training",
    "text": "Training\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top"
  },
  {
    "objectID": "ABCDE.html#sec-1-7",
    "href": "ABCDE.html#sec-1-7",
    "title": "",
    "section": "Examples",
    "text": "Examples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-2-7",
    "href": "ABCDE.html#sec-2-7",
    "title": "",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-3-7",
    "href": "ABCDE.html#sec-3-7",
    "title": "",
    "section": "Design and appraisal resouces",
    "text": "Design and appraisal resouces\n\nresource\nresource\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-4-7",
    "href": "ABCDE.html#sec-4-7",
    "title": "",
    "section": "How to do [something]",
    "text": "How to do [something]\n\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-5-7",
    "href": "ABCDE.html#sec-5-7",
    "title": "",
    "section": "Training",
    "text": "Training\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top"
  },
  {
    "objectID": "ABCDE.html#sec-1-8",
    "href": "ABCDE.html#sec-1-8",
    "title": "",
    "section": "Examples",
    "text": "Examples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-2-8",
    "href": "ABCDE.html#sec-2-8",
    "title": "",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-3-8",
    "href": "ABCDE.html#sec-3-8",
    "title": "",
    "section": "Design and appraisal resouces",
    "text": "Design and appraisal resouces\n\nresource\nresource\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-4-8",
    "href": "ABCDE.html#sec-4-8",
    "title": "",
    "section": "How to do [something]",
    "text": "How to do [something]\n\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-5-8",
    "href": "ABCDE.html#sec-5-8",
    "title": "",
    "section": "Training",
    "text": "Training\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top"
  },
  {
    "objectID": "ABCDE.html#sec-1-9",
    "href": "ABCDE.html#sec-1-9",
    "title": "",
    "section": "Examples",
    "text": "Examples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-2-9",
    "href": "ABCDE.html#sec-2-9",
    "title": "",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-3-9",
    "href": "ABCDE.html#sec-3-9",
    "title": "",
    "section": "Design and appraisal resouces",
    "text": "Design and appraisal resouces\n\nresource\nresource\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-4-9",
    "href": "ABCDE.html#sec-4-9",
    "title": "",
    "section": "How to do [something]",
    "text": "How to do [something]\n\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-5-9",
    "href": "ABCDE.html#sec-5-9",
    "title": "",
    "section": "Training",
    "text": "Training\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top"
  },
  {
    "objectID": "ABCDE.html#sec-1-10",
    "href": "ABCDE.html#sec-1-10",
    "title": "",
    "section": "Examples",
    "text": "Examples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-2-10",
    "href": "ABCDE.html#sec-2-10",
    "title": "",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-3-10",
    "href": "ABCDE.html#sec-3-10",
    "title": "",
    "section": "Design and appraisal resouces",
    "text": "Design and appraisal resouces\n\nresource\nresource\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-4-10",
    "href": "ABCDE.html#sec-4-10",
    "title": "",
    "section": "How to do [something]",
    "text": "How to do [something]\n\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-5-10",
    "href": "ABCDE.html#sec-5-10",
    "title": "",
    "section": "Training",
    "text": "Training\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top"
  },
  {
    "objectID": "ABCDE.html#sec-1-11",
    "href": "ABCDE.html#sec-1-11",
    "title": "",
    "section": "Examples",
    "text": "Examples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-2-11",
    "href": "ABCDE.html#sec-2-11",
    "title": "",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-3-11",
    "href": "ABCDE.html#sec-3-11",
    "title": "",
    "section": "Design and appraisal resouces",
    "text": "Design and appraisal resouces\n\nresource\nresource\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-4-11",
    "href": "ABCDE.html#sec-4-11",
    "title": "",
    "section": "How to do [something]",
    "text": "How to do [something]\n\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-5-11",
    "href": "ABCDE.html#sec-5-11",
    "title": "",
    "section": "Training",
    "text": "Training\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top"
  },
  {
    "objectID": "ABCDE.html#sec-1-12",
    "href": "ABCDE.html#sec-1-12",
    "title": "",
    "section": "Examples",
    "text": "Examples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-2-12",
    "href": "ABCDE.html#sec-2-12",
    "title": "",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-3-12",
    "href": "ABCDE.html#sec-3-12",
    "title": "",
    "section": "Design and appraisal resouces",
    "text": "Design and appraisal resouces\n\nresource\nresource\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-4-12",
    "href": "ABCDE.html#sec-4-12",
    "title": "",
    "section": "How to do [something]",
    "text": "How to do [something]\n\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-5-12",
    "href": "ABCDE.html#sec-5-12",
    "title": "",
    "section": "Training",
    "text": "Training\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top"
  },
  {
    "objectID": "ABCDE.html#sec-1-13",
    "href": "ABCDE.html#sec-1-13",
    "title": "",
    "section": "Examples",
    "text": "Examples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-2-13",
    "href": "ABCDE.html#sec-2-13",
    "title": "",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-3-13",
    "href": "ABCDE.html#sec-3-13",
    "title": "",
    "section": "Design and appraisal resouces",
    "text": "Design and appraisal resouces\n\nresource\nresource\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-4-13",
    "href": "ABCDE.html#sec-4-13",
    "title": "",
    "section": "How to do [something]",
    "text": "How to do [something]\n\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-5-13",
    "href": "ABCDE.html#sec-5-13",
    "title": "",
    "section": "Training",
    "text": "Training\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top"
  },
  {
    "objectID": "ABCDE.html#sec-1-14",
    "href": "ABCDE.html#sec-1-14",
    "title": "",
    "section": "Examples",
    "text": "Examples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-2-14",
    "href": "ABCDE.html#sec-2-14",
    "title": "",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-3-14",
    "href": "ABCDE.html#sec-3-14",
    "title": "",
    "section": "Design and appraisal resouces",
    "text": "Design and appraisal resouces\n\nresource\nresource\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-4-14",
    "href": "ABCDE.html#sec-4-14",
    "title": "",
    "section": "How to do [something]",
    "text": "How to do [something]\n\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-5-14",
    "href": "ABCDE.html#sec-5-14",
    "title": "",
    "section": "Training",
    "text": "Training\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top"
  },
  {
    "objectID": "ABCDE.html#sec-1-15",
    "href": "ABCDE.html#sec-1-15",
    "title": "",
    "section": "Examples",
    "text": "Examples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-2-15",
    "href": "ABCDE.html#sec-2-15",
    "title": "",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-3-15",
    "href": "ABCDE.html#sec-3-15",
    "title": "",
    "section": "Design and appraisal resouces",
    "text": "Design and appraisal resouces\n\nresource\nresource\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-4-15",
    "href": "ABCDE.html#sec-4-15",
    "title": "",
    "section": "How to do [something]",
    "text": "How to do [something]\n\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-5-15",
    "href": "ABCDE.html#sec-5-15",
    "title": "",
    "section": "Training",
    "text": "Training\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top"
  },
  {
    "objectID": "ABCDE.html#sec-1-16",
    "href": "ABCDE.html#sec-1-16",
    "title": "",
    "section": "Examples",
    "text": "Examples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-2-16",
    "href": "ABCDE.html#sec-2-16",
    "title": "",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-3-16",
    "href": "ABCDE.html#sec-3-16",
    "title": "",
    "section": "Design and appraisal resouces",
    "text": "Design and appraisal resouces\n\nresource\nresource\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-4-16",
    "href": "ABCDE.html#sec-4-16",
    "title": "",
    "section": "How to do [something]",
    "text": "How to do [something]\n\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-5-16",
    "href": "ABCDE.html#sec-5-16",
    "title": "",
    "section": "Training",
    "text": "Training\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top"
  },
  {
    "objectID": "ABCDE.html#sec-1-17",
    "href": "ABCDE.html#sec-1-17",
    "title": "",
    "section": "Examples",
    "text": "Examples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-2-17",
    "href": "ABCDE.html#sec-2-17",
    "title": "",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-3-17",
    "href": "ABCDE.html#sec-3-17",
    "title": "",
    "section": "Design and appraisal resouces",
    "text": "Design and appraisal resouces\n\nresource\nresource\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-4-17",
    "href": "ABCDE.html#sec-4-17",
    "title": "",
    "section": "How to do [something]",
    "text": "How to do [something]\n\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-5-17",
    "href": "ABCDE.html#sec-5-17",
    "title": "",
    "section": "Training",
    "text": "Training\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top"
  },
  {
    "objectID": "ABCDE.html#sec-1-18",
    "href": "ABCDE.html#sec-1-18",
    "title": "",
    "section": "Examples",
    "text": "Examples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-2-18",
    "href": "ABCDE.html#sec-2-18",
    "title": "",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-3-18",
    "href": "ABCDE.html#sec-3-18",
    "title": "",
    "section": "Design and appraisal resouces",
    "text": "Design and appraisal resouces\n\nresource\nresource\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-4-18",
    "href": "ABCDE.html#sec-4-18",
    "title": "",
    "section": "How to do [something]",
    "text": "How to do [something]\n\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-5-18",
    "href": "ABCDE.html#sec-5-18",
    "title": "",
    "section": "Training",
    "text": "Training\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top"
  },
  {
    "objectID": "ABCDE.html#sec-1-19",
    "href": "ABCDE.html#sec-1-19",
    "title": "",
    "section": "Examples",
    "text": "Examples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-2-19",
    "href": "ABCDE.html#sec-2-19",
    "title": "",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-3-19",
    "href": "ABCDE.html#sec-3-19",
    "title": "",
    "section": "Design and appraisal resouces",
    "text": "Design and appraisal resouces\n\nresource\nresource\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-4-19",
    "href": "ABCDE.html#sec-4-19",
    "title": "",
    "section": "How to do [something]",
    "text": "How to do [something]\n\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-5-19",
    "href": "ABCDE.html#sec-5-19",
    "title": "",
    "section": "Training",
    "text": "Training\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top"
  },
  {
    "objectID": "ABCDE.html#sec-1-20",
    "href": "ABCDE.html#sec-1-20",
    "title": "",
    "section": "Examples",
    "text": "Examples\n✅  Good reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat.\n\nThis example is good because…\n\nEu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu.\n\nThis example is good because…\n❌  Bad reporting\n\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit.\n\nThis example is bad because it is missing X, Y, Z.\n\nEu cillum dolore ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-2-20",
    "href": "ABCDE.html#sec-2-20",
    "title": "",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nEt fugiat velit consequat sunt officia sit veniam officia. Exercitation est nisi ex deserunt laboris reprehenderit ut sit nisi. Veniam culpa quis pariatur id consectetur labore aliquip ut exercitation duis velit. Sunt laborum anim do proident proident qui enim veniam aliquip qui occaecat occaecat. Eu cillum dolore aliquip voluptate et aliquip minim aute. Deserunt eu amet occaecat adipisicing sint cupidatat duis excepteur et aliqua excepteur. Do enim exercitation culpa enim in velit ullamco."
  },
  {
    "objectID": "ABCDE.html#sec-3-20",
    "href": "ABCDE.html#sec-3-20",
    "title": "",
    "section": "Design and appraisal resouces",
    "text": "Design and appraisal resouces\n\nresource\nresource\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-4-20",
    "href": "ABCDE.html#sec-4-20",
    "title": "",
    "section": "How to do [something]",
    "text": "How to do [something]\n\nresource\nresource"
  },
  {
    "objectID": "ABCDE.html#sec-5-20",
    "href": "ABCDE.html#sec-5-20",
    "title": "",
    "section": "Training",
    "text": "Training\n\nLink to relevant EQUATOR training\nLink to other relevant training\n\nBack to top"
  },
  {
    "objectID": "ABCDE.html#faqs",
    "href": "ABCDE.html#faqs",
    "title": "",
    "section": "FAQs",
    "text": "FAQs\n\nHow this guidance was made\nExcepteur deserunt mollit enim fugiat reprehenderit. Fugiat veniam sunt nulla ullamco. Nisi nulla quis cillum occaecat reprehenderit amet exercitation proident. Et et et proident magna fugiat velit do occaecat in nulla. Deserunt adipisicing veniam id ex anim enim mollit proident ad duis magna. Ipsum officia laborum ad eiusmod culpa ullamco. Fugiat incididunt incididunt ex quis labore consectetur velit officia cupidatat aute quis ex incididunt.\n\n\nPublications related to this guidance\nView all related publications in the EQUATOR database.\n\n\nPariatur proident minim\nIncididunt voluptate fugiat aliqua esse. Esse ex voluptate anim aliqua voluptate consectetur minim. Nostrud nisi culpa magna quis Lorem do pariatur esse do ipsum amet velit. Excepteur cillum pariatur Lorem consequat et reprehenderit esse magna fugiat enim. Laboris magna et deserunt pariatur cupidatat culpa eu elit. Cillum consequat non nisi eu. Occaecat proident incididunt proident ea voluptate. Ex velit qui eu cillum.\n\n\nLaborum quis ad\nId quis irure eu officia qui. Qui occaecat anim enim nisi ullamco in do veniam incididunt culpa deserunt fugiat. Ut excepteur consectetur fugiat occaecat sit. Qui irure adipisicing sint nisi minim consectetur commodo. Mollit ad ad ipsum quis exercitation anim anim. Commodo aliquip irure ad consectetur."
  },
  {
    "objectID": "ABCDE.html#training-and-support",
    "href": "ABCDE.html#training-and-support",
    "title": "",
    "section": "Training and Support",
    "text": "Training and Support\nEsse aute consectetur mollit non pariatur deserunt occaecat elit officia. Deserunt amet est eu id qui in aliquip sunt commodo minim dolor. Dolor adipisicing ea sunt sit eiusmod mollit. Quis consequat occaecat nisi enim labore pariatur eu. Ipsum aliqua cillum dolore ipsum consequat. Dolor in ad sit commodo ipsum laboris Lorem qui exercitation amet do nulla. Eu in excepteur ad deserunt.\n\nAute pariatur excepteur\nTempor cupidatat\nElit ea adipisicing dolore."
  },
  {
    "objectID": "ABCDE.html#citation",
    "href": "ABCDE.html#citation",
    "title": "",
    "section": "Citation",
    "text": "Citation\nExcepteur excepteur nulla excepteur Lorem incididunt ex velit quis excepteur et enim. Ad excepteur cillum non ipsum ad est ea cillum et dolore commodo elit laborum do. Lorem magna occaecat laboris nulla sint esse excepteur.\n\n\n\nIrure sint ipsum aute eiusmod\n\n\n\n\n\n\nQui duis consequat nisi anim ullamco officia dolore ut sunt. Consequat ad elit ex fugiat in eiusmod incididunt elit id ullamco. Quis ut minim et elit adipisicing ad cupidatat. Cupidatat dolore voluptate do qui consectetur adipisicing nulla. Ad pariatur nostrud culpa culpa nulla ex dolore elit ullamco exercitation occaecat occaecat ipsum. Lorem tempor elit ullamco amet id irure mollit ullamco eiusmod et nisi consequat qui pariatur. Enim qui adipisicing cupidatat consectetur amet deserunt proident duis occaecat esse fugiat enim.\nLearn more\n\n\n\n\n\n\nDiscussion for [item name]\n\n\n\n\n\n\nThis will be a discussion board for a specific reporting item. We could use something like giscus so that comments are linked to reporting item versions within github.\nLearn more about Giscus"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "guidelines/SRQR-1/index.html#translations",
    "href": "guidelines/SRQR-1/index.html#translations",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Translations",
    "text": "Translations\nThis guidance is also available in French."
  },
  {
    "objectID": "guidelines/SRQR-1/index.html#when-to-use-this-guidance",
    "href": "guidelines/SRQR-1/index.html#when-to-use-this-guidance",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "When to use this guidance",
    "text": "When to use this guidance\n\n\n\n\n\n\nUse this guidance for reporting qualitative research studies.\nThe SRQR items reflect information essential for inclusion in a qualitative research report, but should not be viewed as prescribing a rigid format or standardized content. Consider each item and prioritize elements that are most relevant to your study, findings, context, and readership.\nYou can also use this guideline for:\n\nfor writing protocols (use the items within the Introduction and Method sections).\nreviewing the reporting of an article, but not for appraising its quality."
  },
  {
    "objectID": "guidelines/SRQR-1/index.html#download-resources",
    "href": "guidelines/SRQR-1/index.html#download-resources",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Download Resources",
    "text": "Download Resources\n\n\n\nFor drafting\n\nTemplate\n\n\n\nFor checking\n\nChecklist\n\n\n\nFor planning\n\nTo-do list"
  },
  {
    "objectID": "guidelines/SRQR-1/index.html#guidance",
    "href": "guidelines/SRQR-1/index.html#guidance",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Guidance",
    "text": "Guidance\nApprox. 25 min read\n\n\n1. Title\n\n\n\n\n\n\n\nConcise description of the nature and topic of the study. Identifying the study as qualitative or indicating the approach (e.g., ethnography, grounded theory) or data collection methods (e.g., interview, focus group) is recommended.\nThe authors should provide a title that clearly conveys the topic of the study. We suggest that the title indicate that the study is qualitative or include a commonly used term that identifies the approach (e.g., ethnographic study, grounded theory) or data collection methods (e.g., interviews, focus groups, observations). This allows readers and reviewers to quickly identify the type of study.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nExamples\n\nResidents learning from a narrative experience with dying patients: a qualitative study.\n\n\nMedical students’ perceptions of the factors influencing their academic performance: an exploratory interview study with high-achieving and re-sitting medical students.\n\n\nUndergraduate rural medical education program development: focus group consultation with the NRHA Rural Medical Educators Group.\n\n\n\n\n\n\n2. Abstract\n\n\n\n\n\n\n\nSummary of key elements of the study using the abstract format of the intended publication; typically includes background, purpose, methods, results, and conclusions.\nA reader should be able to read the abstract independent of the manuscript and get a sense of the background, purpose, methods, main results and implications that will be described in greater detail in the manuscript. The information presented in the abstract should be consistent with the information presented in the full text.\nThe abstract’s structure typically needs to conform to journal guidelines, but authors should attempt to include the following:\n\nBackground about the problem or phenomenon of interest\nDescription of the study purpose or research question;\nMethods, including the approach or perspective (e.g., general inductive, grounded theory), context (setting, time period), sample (number and key characteristics of participants, events, documents), data collection strategies (e g., observation, interview, focus group) and data analysis techniques\nDescription of main findings (e.g., themes or inferences) related to the study purpose and/or research question\nStudy implications\n\nIn some cases the journal’s structured abstract format aligns more with positivist paradigms and quantitative approaches than with qualitative traditions, so translation may be necessary. For example, what might be labelled findings in many qualitative research traditions could be reported as results in the abstract.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nExample\nPurpose: Although academic centers rely on assessments from medical trainees regarding the effectiveness of their faculty as teachers, little is known about how trainees conceptualize and approach their role as assessors of their clinical supervisors\nMethod: In 2010, using a constructivist grounded theory approach, five focus group interviews were conducted with 19 residents from an internal medicine residency program. A constant comparative analysis of emergent themes was conducted.\nResults: Residents viewed clinical teaching assessment (CTA) as a time-consuming task with little reward. They reported struggling throughout the academic year to meet their CTA obligations and described several shortcut strategies they used to reduce their burden. Rather than conceptualizing their assessments as a conduit for both formative and summative feedback, residents perceived CTA as useful for the surveillance of clinical supervisors at the extremes of the spectrum of teaching effectiveness. They put the most effort, including the crafting of written comments, into the CTAs of these outliers. Trainees desired greater transparency in the CTA process and were sceptical regarding the anonymity and perceived validity of their faculty appraisals.\nConclusions: Individual and system-based factors conspire to influence postgraduate medical trainees’ motivation for generating high-quality appraisals of clinical teaching. Academic centers need to address these factors if they want to maximize the usefulness of these assessments.\n\n\n\n\nIntroduction\n\n\n3. Problem Formulation\n\n\n\n\n\n\n\nDescription and significance of the problem/phenomenon studied; review of relevant theory and empirical work; problem statement.\nThe problem formulation typically appears in the introduction and describes the theoretical and/or practical issues or concerns that make the study necessary. It should provide an overview of what is known about the problem, highlight gaps in current knowledge (the problem statement), and define the scope of the research problem or phenomena addressed in the study (what will and will not be included). It should include a review of theoretical and/or empirical work directly relevant to the problem or phenomena studied. The problem formulation should be described in a way that suggests the need for a qualitative approach (e.g., to elucidate poorly defined or previously unexplored constructs, to generate theories or to develop causal explanations connecting processes and outcomes, to understand phenomena as they naturally occur and the role of context, to explore problems involving high complexity, to gain insight into participants’ perspectives when such insight is lacking).\n\n\n\n\n\n\nRead more\n\n\n\n\n\nExample\n\nRegulatory focus theory may therefore offer insight into the variability in responses to feedback, but how well do these experimental findings translate to real clinical situations in which the reality of responsiveness to feedback seems frustratingly complex? …. In order to better elaborate a theory to account for this variability in learner response to feedback, the present study was undertaken. We aimed, in this study, to determine how readily clinical learning events could be classified as activating a promotion or a prevention focus, and to explore, through a careful analysis of doctors’ descriptions of their feedback experiences, the predictive value of regulatory focus theory in the context of real clinical learning situations.\n\n\n\n\n\n\n4. Purpose or research question\n\n\n\n\n\n\n\nPurpose of the study and specific objectives or questions\nIn qualitative research, as in all types of research, the authors should include a statement of study intent. This statement can be framed as one or more research questions, purposes, goals, or objectives. Qualitative studies often explore how and why questions related to a social or human problems or phenomenon, and they are designed to enhance readers’ understanding of a problem or phenomenon. By clearly stating the purpose of the study, authors set readers’ expectations for the methods, findings and discussion sections of the manuscript.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nExample\n\nThe purposes of this study were to investigate how medical students recognize, respond to and utilise feedback, and to determine whether there are maturational differences in understandings of the role of feedback across academic years in medical school.\n\n\n\n\n\n\nMethods\n\n\n5. Qualitative approach and research paradigm\n\n\n\n\n\n\n\nQualitative approach (e.g., ethnography, grounded theory, case study, phenomenology, narrative research) and guiding theory if appropriate; identifying the research paradigm (e.g., post-positivist, constructivist/interpretivist) is also recommended; rationale.\nThe research paradigm is the set of beliefs and assumptions that guide the research process. These commonly include positivist, post-positivist, constructivist or interpretivist, and critical theory. Qualitative research generally draws from a post-positivist or constructivist/interpretivist paradigm. We recommend identifying the research paradigm so that readers understand whether the researcher assumes that there is a single, objective reality (positivist or post-positivist) and has thus designed the study to describe this reality or whether the researcher assumes multiple, subjective realities and designed the study to describe these multiple realities, with no attempt to merge or reconcile these realities (constructivist/interpretivist). The paradigm has implications for the study design, approach, methods, and techniques to ensure rigor and trustworthiness.\nQualitative research includes an array of approaches and methodologies, both general (e.g., qualitative content analysis, general inductive approach) and specific (e.g., ethnography, grounded theory, phenomenography). Since the research paradigm does not necessarily dictate particular approaches or methods, the approach should also be clearly defined. Stating the approach provides readers the opportunity to evaluate the fidelity of the research approach to the research question(s) and consider the rationale for modifications and deviations from the selected approach. Qualitative research also includes an array of methods that can be used across paradigms and approaches. (See also Item 10) The researcher should explain why the selected approach is appropriate for the research question, and provide references to theories or traditions that guide the use of the approach as needed.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nExamples\n\nThe study was performed from a constructivist point of view using an interpretative phenomenological epistemology. Based on the notion that social phenomena are constructed by the communal making of meaning about the underlying phenomena, we aimed to construct insightful accounts of lead consultants’ approaches to educational change, rather than to identify the true nature of these approaches. Because the management of change by lead consultants is an under-researched area, we conducted an exploratory qualitative study…\n\n\nGiven the relative dearth of explanatory theories about factors affecting medical students’ emotional reactions, we chose to develop one by applying methods associated with grounded theory, specifically constant comparative analysis, to qualitative data obtained from learning logs and interviews. Our approach was constructivist, deliberately using researchers’ own experiences and acquired knowledge to enhance theoretical sensitivity and enrich theory development.\n\n\n\n\n\n\n6. Researcher characteristics and reflexivity\n\n\n\n\n\n\n\nResearchers’ characteristics that may influence the research, including personal attributes, qualifications/experience, relationship with participants, assumptions, and/or presuppositions; potential or actual interaction between researchers’ characteristics and the research questions, approach, methods, results and/or transferability.\nReflexivity refers to intentional, systematic consideration of the potential or actual effects of the researcher(s) on all aspects of the study process. In positivist and post-positivist paradigms, personal characteristics and perspectives of researchers might be viewed as biases that limit the credibility of study findings, while in constructivist or interpretivist paradigms the characteristics and perspectives of the researchers are important contextual factors that are an accepted part of the study design, data collection, and data analysis. These characteristics and perspectives may explain how the researcher(s) obtained access to the site or participants included in the study or may add valuable insight during data analysis.\nTo demonstrate reflexivity, authors should describe important personal characteristics and perspectives of members of the research team that may influence design, data collection, and data analysis. Relevant personal characteristics might include cultural background, occupation, experience, training, position/ power dynamics, gender, race/ethnicity, and sponsoring institution. Authors should also describe the perspectives, assumptions, prior knowledge, preliminary hypotheses, and/or motives (the stance) of the members of the research team.\nAuthors also should describe the researchers’ relationships to participants in the study and what decisions were made in light of these relationships. For example, were any members of the research team part of the sample of potential participants in the study? Do any members of the team teach, supervise, or have any authority over participants in the study? If so, how do these characteristics influence choices about their roles in data collection and analysis? For observational research (e.g., ethnography), it is also important to identify the role of the researcher along a spectrum from passive observer (no involvement in the activity studied) to participant-observer (ranging from some limited involvement in the activity to full involvement).\nThere is no expectation that the study could be precisely replicated; these characteristics and perspectives of the researcher should not be mentioned in the limitation section. (See also Item 14.)\n\n\n\n\n\n\nRead more\n\n\n\n\n\nExamples\n\nReflexivity was maintained by the research team through the analysis and writing by recording, discussing and challenging established assumptions. In addition both EH and SV kept reflexive diaries.\n\n\nThe first author conducted all interviews and discussion groups. Her own medical undergraduate training took place between 1995 and 2000. She was not known to the participants of this research prior to undertaking the study and deliberately did not undertake any clinical or teaching activities locally alongside this research. Whilst it was useful to know (from her own background) what the students were talking about medically (and in terms of detecting items of significance), as a researcher she made conscious efforts not to accept potentially common assumptions at face value.\n\n\n\n\n\n\n7. Context\n\n\n\n\n\n\n\nSetting/site and salient contextual factors; rationale.\nThe authors should describe the setting/site(s) in which the study was conducted, the reason(s) why the setting/site(s) was selected, and the salient cultural, political, historical, economic and/or other external factors that influence the study. This information helps readers interpret the meaning and significance of the study findings by situating them in social, cultural, temporal and other relevant contexts. Additional context may be reported with findings (i.e., the Results section) to add evidence for interpretations and to enhance discussion of transferability.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nExample\n\nWe conducted the study among hospital-based clinical teachers of students in years 4 to 6 of a six-year undergraduate medical program at Maastricht University Medical School. Years 4 to 6 are devoted to clerkships in the academic hospital and affiliated regional hospitals. Rotations differ in duration depending on the type of rotation and the discipline, and the sequence of rotations differs among students. During rotations, students spend time in the wards, the outpatient clinics, and the accident and emergency department. Clerkships in years 4 and 5 last between 4 and 10 weeks (regular clerkship), whereas students in year 6 undertake an 18-week senior clerkship in a discipline of their choice.\n\n\n\n\n\n\n8. Sampling strategy\n\n\n\n\n\n\n\nHow and why research participants, documents, or events were selected; criteria for deciding when no further sampling was necessary (e.g., sampling saturation); rationale.\nThe authors should describe how and why research participants, documents, and/or events were selected for inclusion in (and, if appropriate, exclusion from) the study, along with a justification for this strategy. We recommend that authors describe the sampling strategy rather than simply labeling the strategy (e.g., purposive or snowball), since such labels do not have a universally accepted definition and, more importantly, since procedures tend to be study- specific. Several sampling strategies are commonly used in qualitative research, although most fall under the umbrella of purposeful (or purposive) sampling. Purposeful sampling means that participants, documents, or events are selected for their relevance to the research question, based on guiding theory or experiences and assumptions of the researchers. Over the course of the research process, researchers may determine that additional or different participants, documents, or events should be included to address the research question. Other sampling techniques, such as theoretical sampling (seeking examples of theoretical constructs), snowball sampling (using study participants to identify additional participants who meet study criteria), and convenience sampling (including any volunteers with no or minimal criteria for inclusion) may be appropriate depending on the question and approach, so long as the authors provide explanation and justification.\nAlthough investigators often do not determine sample size a priori in qualitative research, they should nonetheless describe how they established the final sample size. When appropriate (e.g., when a flexible sampling strategy was used), they should explain the criteria used to decide when no further sampling was necessary. If data collection ends at the point when the researchers determine that saturation has been reached, then the specific criteria used to define saturation should be described.\nProcedures used to recruit participants should also be described, including who was involved in recruitment, what their relationship was to participants, how and when recruitment occurred, and why these procedures were selected. (See also Item 6)\n\n\n\n\n\n\nRead more\n\n\n\n\n\nExamples\n\nAs students’ perceptions were previously shown to be related to gender, age, prior experience and place of attachment,[REF] we purposely selected respondents with different backgrounds. This sampling strategy led to the diversity of gender, age, prior patient experience and place of attachment shown in Table 1.\n\n\nPurposive sampling was directed towards achieving maximum variation in age and specialty, using a snowball approach (a non-probabilistic form of sampling in which persons initially chosen for the sample are used as informants to locate other persons having necessary characteristics making them eligible for the sample).[REF ]\n\n\nPotential participants were all medical students in Years 1 and 2 at the University of Toronto in 2004. Following research ethics board approval, recruitment was conducted via e- mail to class listservs. Participant responses were sent directly to the research assistant, who was unknown to participants, so that the principal investigators did not know who did or did not participate. This process was engaged to protect participants’ anonymity and to avoid any impression of coercion because the lead researcher (SG) was involved in the administration of the undergraduate curriculum at the time. Sample size was estimated to be sufficient based on the principle of theoretical saturation [REF] and our previous experience with this methodology (i.e., with a relatively homogeneous population, we expected to reach saturation with approximately 15 interviews per group). There were no exclusion criteria and we accepted the first 15 students from each class who volunteered.\n\n\n\n\n\n\n9. Ethical issues pertaining to human subjects\n\n\n\n\n\n\n\nDocumentation of approval by an appropriate ethics review board and participant consent, or explanation for lack thereof; other confidentiality and data security issues.\nQualitative research often involves interaction between researchers and research participants. Correspondingly, researchers should ensure that participants are fully aware of their participation in a research study, the risks and benefits associated with the study, the steps and precautions the researchers will take to minimize risks, such as loss of privacy and confidentiality, and how the researchers plan to use the data. Authors should report approval for the study from an appropriate institutional review board for research associated with human subjects. They should describe procedures used to protect participants, including data collection (e.g., recruitment and informed consent) analysis (e.g., data security and integrity) and reporting of findings (e.g., anonymization of excerpts). If researchers provided compensation or offered incentives to facilitate participation, this should be reported.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nExamples\n\nEthical approval was granted by the University of Otago and student participants were invited to attend each focus group discussion by the university representatives of New Zealand Medical Students Association (NZMSA). Usually, a key concern when collecting data from students is that students may feel vulnerable when sharing their experiences with academic staff during a focus group discussion. However, this potential harm was removed as each group discussion was facilitated by a fellow student, the discussions were transcribed by a professional transcribing service, and only the primary researchers [Names] had access to the raw data.\n\n\n\n\n\n\n10. Data collection methods\n\n\n\n\n\n\n\nTypes of data collected; details of data collection procedures including (as appropriate) start and stop dates of data collection and analysis, iterative process, triangulation of sources/methods, and modification of procedures in response to evolving study findings; rationale.\nQualitative research encompasses an array of data collection methods appropriate for various paradigms and approaches, including (but not limited to) interviews, focus groups, observations (direct or indirect via video), and review of written text, photographs, and other documents or materials. Researchers may choose to use information from multiple sources, contexts, and/or time points depending on their approach and research question(s). (See Item 11 for triangulation.) Given this diversity of methods, authors should describe in detail their data collection design and method(s) and justify them in relation to the research question(s), paradigm, approach, and other methods. Methods used to decide when to end data collection should also be described (see Item 8).\nQualitative research often occurs as an iterative process, meaning that researchers begin data analysis before they complete data collection. The data collection and analysis process may occur in phases or stages, and thus authors must clearly describe the iterative process of data collection and analysis. As part of an iterative collection-analysis process, researchers will often alter their data collection methods to explore their preliminary impressions in greater depth and/or actively pursue confirming and disconfirming perspectives. If such changes occur during the research process, authors should describe how and why study procedures changed in response to evolving study findings.\nThe study period (start and end dates for data collection and analysis) should be identified so that readers can place the study in temporal context and identify factors not mentioned by the authors that might affect findings, interpretation, and implications. (See Item 8 for ending data collection.)\nAuthors should describe important characteristics of the individuals conducting interviews, observations or focus groups, and methods used to train these individuals. This information clarifies the relationship between the individuals involved in data collection and the participants in the research and also explains what efforts were made to ensure consistency in the data collection process (See Items 6 and 15.)\n\n\n\n\n\n\nRead more\n\n\n\n\n\nExamples\n\nFurther, it was decided that group interviews, also known as focus group discussions would be the best means of data collection. This is a method of data collection that enables group members to feed off each other’s ideas and an effective moderator will maintain group focus whilst at the same time permitting flexibility in the direction those aspects of the discussion might take. [REF]\n\n\nAdjustments to the interview protocol were made according to early experience and information participants had provided (i.e. redundant questions were eliminated; questions were reworded to improve flow and clarity; additional probes were included).\n\n\nFaculty staff were then interviewed individually by a trained study investigator in a 15-minute, semi-structured interview. This sequence was repeated with other video encounters. Table 2 presents examples of interview questions. Each faculty member was interviewed by at least three interviewers over their various interviews. Interviewers were chosen based on their experience in interviewing. All were trained during a half-day meeting to interpret and deliver the interview guide in the same manner in order to elicit information of a consistent type.\n\n\n\n\n\n\n11. Data collection instruments and technologies\n\n\n\n\n\n\n\nDescription of instruments (e.g., interview guides, questionnaires) and devices (e.g., audio recorders) used for data collection; if/how the instrument(s) changed over the course of the study.\nData collection for qualitative research draws upon a variety of instruments or tools, including (but not limited to) interview or focus group guides, observational protocols and prompts for field notes, and data extraction or coding protocols for selection and analysis of documents, photographs, videos, or other artifacts. The authors should describe all such instruments, guides, and protocols, including their development and cite relevant literatures, theories or conceptual frameworks as appropriate. It is often helpful for authors to provide access to the data collection instrument(s) or a detailed description of them.\nThe authors should also describe the use of equipment for audio or video recording, reproduction of paper documents or computer files, or other processes in data collection. This is relevant so readers understand the full context in which data collection occurred and how this context might have affected data collection (e.g., the influence of recording devices on participants’ behaviors; the nature of inferences drawn from live vs. recorded events).\n\n\n\n\n\n\nRead more\n\n\n\n\n\nExamples\n\nTo facilitate the discussion and to maintain consistency over different sets of discussions, key trigger questions were devised prior to the discussion. The opening trigger question was: Thinking back to some of your best clinical learning placements in 4th and 5th year. What was it about those clinical placements that provided good opportunities for learning?\n\n\nInterviews included discussion of the expectations, processes and consequences of AEE [authentic early experiences]. The interview schedule was derived following identification of questions that could not be fully answered in a systematic review of previous empirical or theoretical literature. It comprised a sequence of topic areas including experiences in action, and areas of frustration in Medical Education such as the learning of content knowledge, achieving functional knowledge, and transfer of knowledge.[REF]… Interested readers can request a copy of the schedules from the corresponding author. Interviews lasted between 20 and 90 min. and discussion groups between 60 and 90 min. All interviews and discussion groups were conducted in private rooms at the participant’s workplace—the medical school for students and faculty, and individual places of work for workplace supervisors (except for one who chose to be interviewed at the medical school). All data were audio-recorded, and transcribed verbatim.\n\n\n\n\n\n\n12. Units of study\n\n\n\n\n\n\n\nNumber and relevant characteristics of participants, documents, or events included in the study; level of participation.\nAuthors should describe the participants, documents, or events included in the study (the units of study). The sampling item (Item 8) describes the target or ideal participants, documents, or events selected for the study. By contrast, Item 12 focuses on description of the actual participants, documents or events included in the study. Authors should describe how the actual participants, documents, or events differ from the targeted sample, why these differences may have occurred, and how this might affect the findings.\nAuthors should describe characteristics of the participants, documents or events that are relevant to the study purpose and research question(s).1 For participants, this might include age, gender, profession, institution, year of training, or relationship to the researcher and/or other participants in the study. For documents, this might include the source, intended audience, date, or type of document. For events, this might include the location, date(s), length, characteristics of attendees or participants in the event, or mood or emotional climate. If the degree of participation varied among individuals (e.g., multiple occasions; interviews and observations), the authors should describe different levels of participation. For example, if some participants were observed and interviewed and others only interviewed, or if some participants completed multiple interviews and others completed a single interview, these variations should be explained. Authors should also explain the reasons for these differences (i.e., the researchers’ choice or the participants’ preferences) and how these different levels of participation were taken into account in the analysis. Authors should also include the dates or timeframe for participation.\nThis information about participants could appear in the Methods section as part of the description of the sample, or at the beginning of the Results section to provide context for the findings presented.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nExample\n\nOf the 70 Mindful Communication program participants, 46 met the eligibility requirements to participate in the in-depth interviews. We randomly chose and then contacted 22 participants, of whom 20 agreed to be interviewed within six months of completing the program: 15 in person and 5 by telephone. Two declined for lack of time. On reaching saturation after 20 interviews, no further attempts to contact the remaining 24 participants were made.\n\n\n\n\n\nThere were 31 nursing handovers covering 137 patients, and 21 resident handovers covering 101 patients included in this study.\n\n\n\n\n\n\n13. Data processing\n\n\n\n\n\n\n\nMethods for processing data prior to and during analysis, including transcription, data entry, data management and security, verification of data integrity, data coding and anonymization / de-identification of excerpts.\nAuthors should describe the ways in which data are prepared for analysis and managed throughout the analysis process. These activities might include transcription, coding, data entry, and organization of data. When processing audio or video recordings, relevant details might include indication of verbatim transcription of dialogue, additional notes to capture non- verbal information (especially for group interviews or focus groups), and annotations to indicate vocal inflections and utterances, as appropriate for the analytic approach. Authors should also describe procedures used to check transcripts for accuracy.\nAuthors should describe the processes used to organize, compile, and categorize data (e.g., field notes, transcripts, documents, photographs, artifacts) for analysis.\nAuthors should describe procedures used to maintain data security and protect the privacy of participants, as specified in the human subjects protocols. For example, if data are anonymized, the authors should explain how and at what point in the process this occurred. Authors may choose to use anonymous labels or identifiers to represent quotes or excerpts from unique participants, documents or events, in order to reflect the variety of sources from which such data were derived.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nExamples\n\nInterviews were anonymised and each participant was given a code number.\n\n\nThe interviewers and another member of the research team (H.B.) reviewed transcripts for accuracy.\n\n\nWe collected data throughout the admission process through direct observation, audio- recording, and chart extraction. We audio-recorded, transcribed, and anonymized both the overnight and morning case review discussions. We also observed the morning case review discussions in person and collected field notes. For each case review discussion, we copied the admission notes from the patient’s record and de-identified all data.\n\n\n\n\n\n\n14. Data analysis\n\n\n\n\n\n\n\nProcess by which inferences, themes, etc. were identified and developed, including the researchers involved in data analysis; usually references a specific paradigm or approach; rationale.\nTechniques used for data analysis will depend on the paradigm, approach, and/or data collection methods selected by the researchers. Correspondingly, authors should be as transparent as possible about the analytic process so that readers can follow the logic of inquiry from the research question(s) to the analysis and findings. For example, this description may involve characterizing the processes and decisions made for initial classification or segmentation of data, pattern identification and description, and/or development of in-depth interpretations.1 If the researchers used an approach that has a well-defined process for data analysis (e.g., grounded theory, discourse analysis, phenomenography), the authors should cite the guiding literature and describe their processes in sufficient detail so readers can judge the extent to which the processes align with the guiding approach. If modification to or deviations from the guiding approach occurred, the authors should explain and justify these modifications.\nAuthors should specify the unit of analysis.1 In qualitative research, the unit of analysis is not necessarily the same as the unit of sampling (e.g., individual participants or events). Instead, some approaches use specific events as the unit of analysis, such as mentions of a particular topic or experience, or observations of a particular behavior or phenomenon, while others use groups rather than individual group participants. This specification has implications for how the data are organized and analyzed as well as the inferences drawn from the data.\nAuthors should explain the rationale underlying different decisions made throughout the data analysis process to provide as much transparency as possible. In some approaches researchers use memoing or bracketing to make their reflections, interpretations, and links among passages explicit and more transparent to others. In some types of analysis, participants’ perspectives or observations that contrast or deviate from the concepts or themes identified by the researchers are an important part of the analysis. In such cases, the authors must describe how these discrepancies were handled during the analysis.\nDuring the analysis process, researchers may draw upon a theoretical perspective or framework, which may have been identified early in the conception of the study or may be identified by the researchers after reviewing some or all of their data. Either way, the authors should describe theoretical or other influences on their analysis scheme or categories if they exist. Sometimes these are referred to as sensitizing concepts to acknowledge that the approach is inductive, but with influence from relevant theory, models, or organizational schemes. Alternatively, themes may be developed from the data with no external influences.\nAuthors should describe which members of the research team are involved in data analysis and what perspective(s) they bring to the analysis. Authors should also indicate if any software was used to assist with data analysis and how it was used (e.g., used to apply codes after the final coding scheme was developed; to extract coded passages for further synthesis and identification of themes; or to identify passages with key words). Simply stating that software was used is insufficient.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nExamples\n\n…we brought sensitizing concepts to the analysis while we conducted an open, inductive analysis.[REF]In this case the sensitizing concepts arose, a priori to analysis, from a framework derived from the literature [REF] (as described above), in which participants’ motivations to act are based on principles of professionalism, internal affect, or potential implications of their actions.[REF]\n\n\nThrough an iterative process of listening, discussing, and relistening, the team identified and consensually validated emerging themes[REF] and appended segments of dialogue supporting the proposed themes. Recruitment stopped when saturation was reached (no new themes were identified). The team systematically reviewed the themes and sorted them into content domains. The team used an analytic matrix to identify patterns and connections amongst the domains. Two of us not involved in the qualitative coding process (R.E., M.K.) audited the analytic matrix, choice of quotes, and thematic analysis.\n\n\nThe analysis started after the first interview. All data were analyzed with the aid of the audio- coding facility of the NVivo 8: QSR International Pty Ltd, Doncaster, Vic, Australia programme. First, [name] and [name] coded independently from one another, making sure to stay semantically close to the participants’ wording. Then we discussed these open codes and defined axial codes.[REF] New insights about the impact of CST were written down in memos.\n\n\nVideotapes were analysed using immersion/crystallisation methods of qualitative data analysis.[REF] With no pre-existing framework developed in advance for analysis, an inductive approach was used to discover patterns of NVB in the data. A team of six researchers met weekly for 18 months to view videos together. Using a consensus-building approach based on a combination of field notes, opportunistic interviews with the participants, and repeated viewing of the same material, sometimes many months apart, we eventually achieved consensus on verbal, non-verbal, and physical themes and patterns observed in the data. Finally, as a test of goodness-of-fit, we carefully reviewed the videotapes for any deviant cases that did not fit the categories we had developed.\n\n\nAll transcripts were coded thematically by four of the five authors, who met regularly to identify areas of convergence until full agreement was reached. One of the interviewers (P.M.) maintained an audit trail to track the team’s developing thinking. A process of dialectical empiricism[REF] was used to categorise the emergent themes into more abstract concepts…\n\n\n\n\n\n\n15. Techniques to enhance trustworthiness\n\n\n\n\n\n\n\nTechniques to enhance trustworthiness and credibility of data analysis,(e.g., member checking, triangulation, audit trail); rationale.\nAuthors should describe methods used to ensure trustworthiness and credibility throughout the data collection and analysis process. Such methods will depend on the paradigm, approach, and/or methods used. Correspondingly, the authors should explain their choice of techniques and why these are appropriate for the particular study.\nCommonly used techniques to enhance trustworthiness include: member checking; triangulation of data sources, methods, and/or researchers; creation of an explicit audit trail; and immersion in the site of data collection for an extended period of time (especially for research in which an observer’s presence is likely to disrupt the phenomenon under investigation). Member checking involves sharing findings, such as descriptions of key phenomena, themes, or an explanatory model, with participants and asking them to verify the accuracy or resonance with their perspectives. Triangulation involves using more than one data source, method, or researcher to add diverse perspectives on the findings of the study and, in some approaches, to test the transferability or generalizability of a model. An audit trail involves careful documentation of all decisions made throughout the study, from initial conceptualization to study design, sampling, analysis, and reporting, to provide transparency and to enable an external researcher to review all the steps involved in the study.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nExamples\nMember checking\n\nMember checks [REF] with an external TBL expert (R.L.) supported the validity of these analyses.\n\nTriangulation of data types and data sources\n\nThe interview data were triangulated with the data of 11 student and supervisor focus groups of a previous study, and more specifically, with those data that concern in particular the influence of CST [Communication Skills Training] on the development of patient-centredness…. Triangulation with the focus group data allowed us to broaden the in-depth information from the interviews in the analysis and to share and compare this with information from students and doctors with varying levels of CST (no, limited, full programme) and from two universities (Universities of Antwerp and Ghent). Moreover, this enabled us to better explore the evolution over time, given that the focus groups included participants at different stages of their study: before clerkships (year 4, undergraduate), during clerkships (year 6, undergraduate), after clerkships (year 7, undergraduate) and postgraduate (general practice trainees, and supervising specialists and GPs; Table 1).\n\n\n\n\n\nFinally, as a test of goodness-of-fit, we carefully reviewed the videotapes for any deviant cases that did not fit the categories we had developed.\n\nTriangulation of Researchers + Audit trail\n\nTo ensure rigor and increase authenticity in our methodology, we used two kinds of triangulation—investigator triangulation and data triangulation.[REF] We sought analytical rigor using an audit trail and multiple coders; our coding team included an experienced clinician (M.G.) as well as a nonclinician with expertise in medical communication and team dynamics (L.L.).\n\n\n\n\n\n\nResults\n\n\n16. Synthesis and interpretation\n\n\n\n\n\n\n\nMain findings (e.g., interpretations, inferences, and themes); might include development of a theory or model, or integration with prior research or theory.\nIn qualitative research the distinction between results and discussion tends to blur because analysis often involves interpretation, inference, and synthesis. Although most journals require separate sections for Results and Discussion, many elements of Items 16-19 could reasonably be reported in either section. As such, we defer to authors and editors to determine where to report these essential elements.\nAuthors should identify the main analytic findings (e.g., interpretations, inferences, narratives, themes, models). The nature of these findings and how they are reported will depend on the approach and methodology selected and thus should be in alignment with the approach and methods.\nIn most cases, the authors should report a synthesis of their data along with specific quotes, examples, or illustrations derived from their data. Authors might also report frequency, variety, representativeness, counter-examples, concrete details, contextualization, conditions, and qualifications related to their findings. Frequency counts (e.g., the frequency of specific themes or codes) play a limited role in qualitative research, and need not be reported unless they play a meaningful role in interpretation of the data.\nFindings might also include integration with prior literature or theory and/or the development of a theory, model or meta-narrative. Judicious use of tables and figures can help communicate such findings.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nExamples\n\nWe identified four patterns of NVB (non-verbal behavior) that relate to handover quality and have dubbed them: (1) joint focus of attention; (2) the poker hand; (3) parallel play; and (4) kerbside consultation. Each pattern constitutes a transfix, or systematic way of participating non-verbally in the care transfer process. And, although there are variations in each pattern, we have been able to code virtually every handover we have observed in nursing, medicine and surgery into one of these four categories.\n\n\nBecause our participants came from similar educational backgrounds, had studied medicine as their tertiary course, were embedded in the culture of medicine, and were associated in meaningful ways with a single medical school, we approached their transcripts with the assumption that they belonged to a loosely formed discourse community. Although their graduation dates ranged over a period of 50 years and their collective sphere of practice included 10 different specialty areas, there were many similarities in their experiences of enculturation during and after medical school.\n\n\nTheir three major (often overlapping) areas of concern were epistemic (acquiring knowledge and skill), interpersonal (relating to patients, families, colleagues and administrators) and personal (achieving work–life balance). In each of these areas, medical enculturation was achieved by two overlapping processes, absorption and assimilation, each of which may have distinct implications for postgraduate medical education.\n\n\n\n\n\n\n17. Links to empirical data\n\n\n\n\n\n\n\nEvidence (e.g., quotes, field notes, text excerpts, photographs) to substantiate analytic findings.\nThe authors should provide evidence (e.g., quotes, field notes, text excerpts, photographs) to substantiate the more general and abstract concepts or inferences they present as findings. Such evidence is typically de-identified to protect the privacy of study participants, settings, and/or institutions. The evidence may be presented in a variety of ways such as in a table or figure, incorporated into a narrative description of findings, as a stand-alone narrative, or in text blocks embedded in the manuscript text. If word limits or media limitations (e.g.. video) limit the authors’ ability to provide sufficient representation of supporting data, an appendix, supplemental material, or web-based repository could be used to provide access to additional data.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nExamples\nSee Frankel et al. for an excellent example of how to use photographs (or snapshots from video) to illustrate and provide supporting evidence for patterns of behavior identified in the analysis. http://qualitysafety.bmj.com/content/21/Suppl_1/i121.\n\nWe identified five interruption types: (1) probing for further data, (2) prompting for expected sequence, (3) teaching around the case, (4) thinking out loud, and (5) providing direction (see Table 1). Several interruption types served both goals of the case review discussions—teaching and patient care. For example, when thinking out loud, supervisors reasoned through problems and taught the team: So that’s the big question, did she have a mechanical fall, or did she have a medicine-related fall? (Case 2). Supervisors prompted for expected sequence, preventing presenters from skipping over information while simultaneously allowing the supervisor to instruct the team on presentation style: So now you can tell me what the rest of his test results are because I haven’t heard those (Case 16).\n\n\n\n\n\n\nDiscussion\n\n\n18. Integration with prior work, implications, transferability, and contribution(s) to the field\n\n\n\n\n\n\n\nShort summary of main findings, explanation of how findings and conclusions connect to, support, elaborate on, or challenge conclusions of earlier scholarship; discussion of scope of application/generalizability; identification of unique contribution(s) to scholarship in a discipline or field.\nThe authors should begin the Discussion with a short summary of the main findings. The short summary reminds readers of the main findings and may help them assess whether the subsequent interpretation and implications formulated by the authors are supported by the findings.\nThe authors’ description of their findings or results should include some interpretation of the data in the context of previous findings, experiences, theory, or a guiding paradigm or approach. The discussion provides authors an opportunity to elaborate on their findings in relation to their research question(s) and study purpose(s); connect their findings to prior empirical work, theories, and/or frameworks; and discuss implications. The authors should explicitly describe how their findings contribute to or advance the field. Implications may include transferability, or specifying the appropriate scope for generalization of the findings beyond the study (e.g., to other settings, populations, time periods, circumstances).\n\n\n\n\n\n\nRead more\n\n\n\n\n\nExamples\n\nThis study contributes to the understanding and discussion of the complexity of involving patients in healthcare education. It shows that integrating patient-led teaching into initiatives that are partly faculty-led influences the way in which students perceive learning from and with PIs. What is not known, however, is whether perceptions are also affected by type of health profession and the students’ different orientation towards logics of care and science, and issues of authority and power relations.\n\nFor complete examples of Discussions, see:\n\nHenriksen & Ringsted, 2013\nWesterman et al., 2013.\n\n\n\n\n\n\n19. Limitations\n\n\n\n\n\n\n\nTrustworthiness and limitations of findings\nAuthors should describe techniques used to ensure trustworthiness in the Methods section of the manuscript. In the Discussion, authors should identify problems or gaps in their efforts to ensure trustworthiness and the potential implications. For example, if researchers intended to interview individuals with certain characteristics, or who might offer different perspectives, but were unsuccessful in recruiting any willing participants, they should explain this issue and describe possible consequences for transferability. (See also Item 18.)\nAll research paradigms and approaches have strengths and weaknesses, and authors should explicitly discuss how the paradigm, approach, and methods they used will influence the situations to which their findings may reasonably apply. (See also Item 18.) In addition, they should explain how specific decisions or events in the conduct of the study strengthen or weaken the rigor of their findings.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nExample\n\nThe study has several limitations. One is that the focus group interview method reveals students’ perceptions rather than their actual behaviors. Observations of the patient-led teaching encounter may have illuminated an understanding of the relationship between patient instructors and medical students. Another limitation is that the PI-led teaching is optional rather than mandatory, which may have influenced students’ attitudes in a positive direction. Moreover, students who are eager to take on extra-curricular activities may not be representative of the whole population. That only 23 out of 39 students signed up for this study might also have influenced results if the missing group of students represented other perceptions than those present in the focus groups. However the received data from the focus groups were rich in information and diverse perceptions were present. Another limitation is the overrepresentation of women over men in our sample. Even though women are also overrepresented in medical school this might potentially have influenced results, but gender differences in perceptions were nevertheless not identified in the data.\n\n\n\n\n\n\nOther\n\n\n20. Conflicts of interest\n\n\n\n\n\n\n\nPotential sources of influence or perceived influence on study conduct and conclusions; how these were managed.\nAuthors should identify any real or potential conflicts of interest that might have influenced or could appear to have influenced the research. Authors should also explain how these conflicts were managed in the conduct of the study, and describe the potential impact on study findings and/or conclusions. Some aspects may be mentioned as part of reflexivity (see Item 6).\n\n\n21. Funding\n\n\n\n\n\n\n\nSources of funding and other support; role of funders in data collection, interpretation, and reporting.\nThe authors should describe any sources of funding and other support for the study and the role of funders in data collection, data analysis, and reporting if applicable.\n\n\n\nEthnography\n\n\n\n\n\n\nOccaecat mollit deserunt excepteur consectetur proident. Anim Lorem ipsum anim nisi velit magna duis Lorem consequat aliquip eu ullamco consequat amet. Magna labore in consequat ad aliquip Lorem elit do in. Esse minim consectetur ut commodo sunt cupidatat voluptate sint voluptate nisi.\nExercitation eu mollit consequat ipsum non ullamco. Velit velit dolor cupidatat proident in laboris eiusmod incididunt mollit nisi laborum minim. Eiusmod et voluptate veniam exercitation sit ex labore.\nLearn more\n\n\n\n\n\n\nGrounded theory\n\n\n\n\n\n\nEt eu labore qui non dolore nulla. Ipsum aliqua laboris commodo deserunt cillum excepteur dolore adipisicing occaecat nostrud anim elit officia amet. Commodo incididunt ad et velit do minim dolore. Ullamco eiusmod Lorem proident tempor laborum nulla incididunt non pariatur irure officia ad. Elit deserunt commodo laboris adipisicing duis amet et dolore nulla nostrud. Nostrud excepteur exercitation elit consequat consequat.\n\n\n\n\n\n\nInterview\n\n\n\n\n\n\nNon exercitation commodo incididunt tempor veniam magna excepteur est reprehenderit Lorem. Exercitation enim et quis velit pariatur mollit fugiat dolore ullamco eu eu. Do ullamco eu culpa sint ad est. Proident veniam sint eu cupidatat minim voluptate officia anim laborum qui aliqua. Enim minim non mollit elit ipsum fugiat dolore cupidatat. Magna adipisicing labore do dolor voluptate non ipsum sint eiusmod aute proident proident ullamco. Excepteur cupidatat occaecat enim culpa cillum laboris ex incididunt officia.\n\n\n\n\n\n\nFocus group\n\n\n\n\n\n\nCommodo commodo labore eu laboris commodo excepteur eu et id deserunt enim id minim cupidatat. Fugiat deserunt eiusmod eiusmod culpa velit dolore ex voluptate anim pariatur. Nostrud cillum cillum culpa exercitation. Labore laboris ex commodo nostrud excepteur eiusmod veniam in esse est exercitation Lorem proident dolor. Labore aliqua fugiat id qui sit qui sit aliquip incididunt labore irure Lorem voluptate. Eiusmod ea ex ea cillum exercitation laboris laborum incididunt labore. Ea officia aliqua id velit nostrud velit amet.\n\n\n\n\n\n\nGeneral inductive\n\n\n\n\n\n\nLaboris ea eu elit irure ex reprehenderit sunt commodo non eiusmod tempor. Eiusmod occaecat mollit consequat dolore voluptate officia exercitation voluptate velit nostrud duis ad consectetur. Magna magna in occaecat nostrud tempor tempor incididunt commodo officia reprehenderit irure aliqua.\n\n\n\n\n\n\nContext\n\n\n\n\n\n\nEiusmod sit cillum sint duis mollit non amet consectetur minim in sunt aliquip laboris ipsum. Culpa ullamco laborum et veniam laborum anim ut aliqua dolore. Aliquip dolore sit tempor velit officia. Aute do nostrud reprehenderit pariatur sint qui. Occaecat consequat ut et sint. Officia aliquip non minim do est minim.\n\n\n\n\n\n\nSetting\n\n\n\n\n\n\nExercitation consectetur ipsum sit sit magna nostrud do commodo aute ad. Elit ea ea aliqua eu commodo. Enim est pariatur tempor non exercitation officia aliquip ut veniam laborum. Eu ea aliqua laborum elit voluptate officia ex ipsum amet.\n\n\n\n\n\n\nSample\n\n\n\n\n\n\nEst irure ipsum laborum ad consectetur. Incididunt dolor duis excepteur proident. Ut ullamco amet aliqua sint fugiat exercitation amet anim magna dolor ullamco. Officia dolore dolore ad laborum. Dolore reprehenderit non deserunt dolore voluptate in Lorem nulla laborum ipsum occaecat eiusmod. Incididunt minim magna sint non veniam incididunt sint consectetur amet qui. Voluptate enim deserunt Lorem proident reprehenderit mollit magna adipisicing reprehenderit laboris nisi qui cupidatat id.\n\n\n\n\n\n\nThemes\n\n\n\n\n\n\nEt et aliquip elit aliquip cillum. Mollit enim ullamco ea ullamco eu ex officia ut culpa cillum laborum ad. Nulla id mollit magna deserunt deserunt eu. Qui esse sint fugiat excepteur consectetur velit incididunt aliquip labore consequat aute Lorem laboris. Cupidatat irure ex minim adipisicing commodo Lorem elit. Sit fugiat dolor dolore pariatur exercitation in duis nostrud cillum id magna non tempor. Duis reprehenderit ex cupidatat reprehenderit duis cillum.\n\n\n\n\n\n\nInferences\n\n\n\n\n\n\nOccaecat qui non aute ut quis aliqua ut sint ipsum enim Lorem. Laborum nostrud esse voluptate dolore exercitation. Excepteur pariatur id pariatur ullamco ea esse do. Aute aute minim veniam reprehenderit id enim aliqua dolor ullamco. Sit tempor nostrud tempor minim. Qui irure laborum consectetur duis do quis laboris eiusmod consequat.\n\n\n\n\n\n\nPositivist\n\n\n\n\n\n\nTempor aliquip minim commodo nisi et aliqua ea mollit esse. Tempor aute consequat exercitation excepteur et. Aliquip dolor duis fugiat incididunt sint ipsum adipisicing eiusmod. Aliqua ea ipsum Lorem amet. Excepteur do dolore commodo veniam Lorem eiusmod cillum cupidatat Lorem minim nulla est ipsum. Eu laboris anim laboris mollit aute quis mollit est duis elit sit voluptate nisi. Amet dolor pariatur dolore culpa.\n\n\n\n\n\n\nPost-positivist\n\n\n\n\n\n\nProident est commodo exercitation ex occaecat ullamco anim ut reprehenderit deserunt dolor voluptate in. Consequat culpa reprehenderit consequat enim dolore officia tempor proident duis eiusmod consequat minim incididunt id. Ad proident qui aute ea.\n\n\n\n\n\n\nConstructs\n\n\n\n\n\n\nId sunt enim dolore nostrud laboris quis quis sint anim ullamco et ut proident. Ipsum pariatur sit eu quis irure eiusmod quis adipisicing velit cillum sit reprehenderit enim. Quis magna sint officia irure cillum irure est. Nostrud nostrud enim commodo magna mollit et eu amet mollit occaecat amet.\n\n\n\n\n\n\nCase study\n\n\n\n\n\n\nLaboris quis ad laboris eu amet. Deserunt enim ad do aliqua qui labore. Anim aute esse fugiat labore reprehenderit quis et eiusmod. Dolore consequat sunt consequat veniam qui nisi magna qui non amet eu. Aute nulla anim culpa enim magna amet adipisicing. Ea eu officia minim minim duis ea reprehenderit Lorem ullamco. Minim et aliqua amet id proident est eiusmod.\n\n\n\n\n\n\nPhenomenology\n\n\n\n\n\n\nEnim ea voluptate ad dolore et commodo in id eiusmod ea. Irure culpa pariatur veniam culpa velit Lorem exercitation ea ut nulla minim non esse reprehenderit. Ipsum do cupidatat culpa ipsum in ea consequat. Dolor sint laborum proident labore nulla fugiat esse sit enim irure anim. Id ex proident et tempor enim est nostrud. Ullamco ex do irure amet proident nulla proident non exercitation esse ullamco est dolor. Exercitation proident mollit nostrud dolore reprehenderit nostrud officia in magna eiusmod.\n\n\n\n\n\n\nNarrative research\n\n\n\n\n\n\nNisi pariatur in sunt est anim sit sint cillum occaecat ut enim commodo amet ullamco. Lorem nulla ut in nulla. Nisi aute cillum ad duis aliqua commodo velit incididunt aliquip.\n\n\n\n\n\n\nConstructivist\n\n\n\n\n\n\nNulla fugiat labore voluptate velit ipsum elit velit nulla laborum cillum minim. Esse laboris adipisicing proident non ipsum adipisicing consequat do do proident ea. Irure minim fugiat proident irure adipisicing. Ea ad consectetur fugiat qui id consectetur nostrud amet elit sit non cupidatat laboris nostrud. Est officia velit sit fugiat ut pariatur consequat incididunt in velit nulla do proident. Reprehenderit pariatur adipisicing mollit mollit consectetur. Lorem consectetur nisi est aute reprehenderit elit amet et deserunt sit pariatur sunt dolore excepteur.\n\n\n\n\n\n\nInterpretivist\n\n\n\n\n\n\nMinim ad fugiat ex magna occaecat nostrud officia est dolor duis tempor quis. Voluptate dolore dolore labore irure ipsum esse do labore excepteur est elit. Excepteur irure veniam cillum sit est aliqua occaecat sunt eu dolor dolor. Adipisicing est mollit aute consequat proident adipisicing minim est ad. Duis cupidatat est enim ex labore sit eu pariatur."
  },
  {
    "objectID": "guidelines/SRQR-1/index.html#sec-title-1",
    "href": "guidelines/SRQR-1/index.html#sec-title-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nResidents learning from a narrative experience with dying patients: a qualitative study.\n\n\nMedical students’ perceptions of the factors influencing their academic performance: an exploratory interview study with high-achieving and re-sitting medical students.\n\n\nUndergraduate rural medical education program development: focus group consultation with the NRHA Rural Medical Educators Group."
  },
  {
    "objectID": "guidelines/SRQR-1/index.html#sec-abstract-1",
    "href": "guidelines/SRQR-1/index.html#sec-abstract-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\nPurpose: Although academic centers rely on assessments from medical trainees regarding the effectiveness of their faculty as teachers, little is known about how trainees conceptualize and approach their role as assessors of their clinical supervisors\nMethod: In 2010, using a constructivist grounded theory approach, five focus group interviews were conducted with 19 residents from an internal medicine residency program. A constant comparative analysis of emergent themes was conducted.\nResults: Residents viewed clinical teaching assessment (CTA) as a time-consuming task with little reward. They reported struggling throughout the academic year to meet their CTA obligations and described several shortcut strategies they used to reduce their burden. Rather than conceptualizing their assessments as a conduit for both formative and summative feedback, residents perceived CTA as useful for the surveillance of clinical supervisors at the extremes of the spectrum of teaching effectiveness. They put the most effort, including the crafting of written comments, into the CTAs of these outliers. Trainees desired greater transparency in the CTA process and were sceptical regarding the anonymity and perceived validity of their faculty appraisals.\nConclusions: Individual and system-based factors conspire to influence postgraduate medical trainees’ motivation for generating high-quality appraisals of clinical teaching. Academic centers need to address these factors if they want to maximize the usefulness of these assessments."
  },
  {
    "objectID": "guidelines/SRQR-1/index.html#sec-problem-formulation-1",
    "href": "guidelines/SRQR-1/index.html#sec-problem-formulation-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nRegulatory focus theory may therefore offer insight into the variability in responses to feedback, but how well do these experimental findings translate to real clinical situations in which the reality of responsiveness to feedback seems frustratingly complex? …. In order to better elaborate a theory to account for this variability in learner response to feedback, the present study was undertaken. We aimed, in this study, to determine how readily clinical learning events could be classified as activating a promotion or a prevention focus, and to explore, through a careful analysis of doctors’ descriptions of their feedback experiences, the predictive value of regulatory focus theory in the context of real clinical learning situations."
  },
  {
    "objectID": "guidelines/SRQR-1/index.html#sec-purpose-1",
    "href": "guidelines/SRQR-1/index.html#sec-purpose-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nThe purposes of this study were to investigate how medical students recognize, respond to and utilise feedback, and to determine whether there are maturational differences in understandings of the role of feedback across academic years in medical school."
  },
  {
    "objectID": "guidelines/SRQR-1/index.html#sec-qualitative-approach-1",
    "href": "guidelines/SRQR-1/index.html#sec-qualitative-approach-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nThe study was performed from a constructivist point of view using an interpretative phenomenological epistemology. Based on the notion that social phenomena are constructed by the communal making of meaning about the underlying phenomena, we aimed to construct insightful accounts of lead consultants’ approaches to educational change, rather than to identify the true nature of these approaches. Because the management of change by lead consultants is an under-researched area, we conducted an exploratory qualitative study…\n\n\nGiven the relative dearth of explanatory theories about factors affecting medical students’ emotional reactions, we chose to develop one by applying methods associated with grounded theory, specifically constant comparative analysis, to qualitative data obtained from learning logs and interviews. Our approach was constructivist, deliberately using researchers’ own experiences and acquired knowledge to enhance theoretical sensitivity and enrich theory development."
  },
  {
    "objectID": "guidelines/SRQR-1/index.html#sec-researcher-characteristics-and-reflexivity-1",
    "href": "guidelines/SRQR-1/index.html#sec-researcher-characteristics-and-reflexivity-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nReflexivity was maintained by the research team through the analysis and writing by recording, discussing and challenging established assumptions. In addition both EH and SV kept reflexive diaries.\n\n\nThe first author conducted all interviews and discussion groups. Her own medical undergraduate training took place between 1995 and 2000. She was not known to the participants of this research prior to undertaking the study and deliberately did not undertake any clinical or teaching activities locally alongside this research. Whilst it was useful to know (from her own background) what the students were talking about medically (and in terms of detecting items of significance), as a researcher she made conscious efforts not to accept potentially common assumptions at face value."
  },
  {
    "objectID": "guidelines/SRQR-1/index.html#sec-context-1",
    "href": "guidelines/SRQR-1/index.html#sec-context-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nWe conducted the study among hospital-based clinical teachers of students in years 4 to 6 of a six-year undergraduate medical program at Maastricht University Medical School. Years 4 to 6 are devoted to clerkships in the academic hospital and affiliated regional hospitals. Rotations differ in duration depending on the type of rotation and the discipline, and the sequence of rotations differs among students. During rotations, students spend time in the wards, the outpatient clinics, and the accident and emergency department. Clerkships in years 4 and 5 last between 4 and 10 weeks (regular clerkship), whereas students in year 6 undertake an 18-week senior clerkship in a discipline of their choice."
  },
  {
    "objectID": "guidelines/SRQR-1/index.html#sec-sampling-strategy-1",
    "href": "guidelines/SRQR-1/index.html#sec-sampling-strategy-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nAs students’ perceptions were previously shown to be related to gender, age, prior experience and place of attachment,[REF] we purposely selected respondents with different backgrounds. This sampling strategy led to the diversity of gender, age, prior patient experience and place of attachment shown in Table 1.\n\n\nPurposive sampling was directed towards achieving maximum variation in age and specialty, using a snowball approach (a non-probabilistic form of sampling in which persons initially chosen for the sample are used as informants to locate other persons having necessary characteristics making them eligible for the sample).[REF ]\n\n\nPotential participants were all medical students in Years 1 and 2 at the University of Toronto in 2004. Following research ethics board approval, recruitment was conducted via e- mail to class listservs. Participant responses were sent directly to the research assistant, who was unknown to participants, so that the principal investigators did not know who did or did not participate. This process was engaged to protect participants’ anonymity and to avoid any impression of coercion because the lead researcher (SG) was involved in the administration of the undergraduate curriculum at the time. Sample size was estimated to be sufficient based on the principle of theoretical saturation [REF] and our previous experience with this methodology (i.e., with a relatively homogeneous population, we expected to reach saturation with approximately 15 interviews per group). There were no exclusion criteria and we accepted the first 15 students from each class who volunteered."
  },
  {
    "objectID": "guidelines/SRQR-1/index.html#sec-ethics-1",
    "href": "guidelines/SRQR-1/index.html#sec-ethics-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nEthical approval was granted by the University of Otago and student participants were invited to attend each focus group discussion by the university representatives of New Zealand Medical Students Association (NZMSA). Usually, a key concern when collecting data from students is that students may feel vulnerable when sharing their experiences with academic staff during a focus group discussion. However, this potential harm was removed as each group discussion was facilitated by a fellow student, the discussions were transcribed by a professional transcribing service, and only the primary researchers [Names] had access to the raw data."
  },
  {
    "objectID": "guidelines/SRQR-1/index.html#sec-data-collection-methods-1",
    "href": "guidelines/SRQR-1/index.html#sec-data-collection-methods-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nFurther, it was decided that group interviews, also known as focus group discussions would be the best means of data collection. This is a method of data collection that enables group members to feed off each other’s ideas and an effective moderator will maintain group focus whilst at the same time permitting flexibility in the direction those aspects of the discussion might take. [REF]\n\n\nAdjustments to the interview protocol were made according to early experience and information participants had provided (i.e. redundant questions were eliminated; questions were reworded to improve flow and clarity; additional probes were included).\n\n\nFaculty staff were then interviewed individually by a trained study investigator in a 15-minute, semi-structured interview. This sequence was repeated with other video encounters. Table 2 presents examples of interview questions. Each faculty member was interviewed by at least three interviewers over their various interviews. Interviewers were chosen based on their experience in interviewing. All were trained during a half-day meeting to interpret and deliver the interview guide in the same manner in order to elicit information of a consistent type."
  },
  {
    "objectID": "guidelines/SRQR-1/index.html#sec-data-collection-instruments-1",
    "href": "guidelines/SRQR-1/index.html#sec-data-collection-instruments-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nTo facilitate the discussion and to maintain consistency over different sets of discussions, key trigger questions were devised prior to the discussion. The opening trigger question was: Thinking back to some of your best clinical learning placements in 4th and 5th year. What was it about those clinical placements that provided good opportunities for learning?\n\n\nInterviews included discussion of the expectations, processes and consequences of AEE [authentic early experiences]. The interview schedule was derived following identification of questions that could not be fully answered in a systematic review of previous empirical or theoretical literature. It comprised a sequence of topic areas including experiences in action, and areas of frustration in Medical Education such as the learning of content knowledge, achieving functional knowledge, and transfer of knowledge.[REF]… Interested readers can request a copy of the schedules from the corresponding author. Interviews lasted between 20 and 90 min. and discussion groups between 60 and 90 min. All interviews and discussion groups were conducted in private rooms at the participant’s workplace—the medical school for students and faculty, and individual places of work for workplace supervisors (except for one who chose to be interviewed at the medical school). All data were audio-recorded, and transcribed verbatim."
  },
  {
    "objectID": "guidelines/SRQR-1/index.html#sec-units-of-study-1",
    "href": "guidelines/SRQR-1/index.html#sec-units-of-study-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nOf the 70 Mindful Communication program participants, 46 met the eligibility requirements to participate in the in-depth interviews. We randomly chose and then contacted 22 participants, of whom 20 agreed to be interviewed within six months of completing the program: 15 in person and 5 by telephone. Two declined for lack of time. On reaching saturation after 20 interviews, no further attempts to contact the remaining 24 participants were made.\n\n\n\n\n\nThere were 31 nursing handovers covering 137 patients, and 21 resident handovers covering 101 patients included in this study."
  },
  {
    "objectID": "guidelines/SRQR-1/index.html#sec-data-processing-1",
    "href": "guidelines/SRQR-1/index.html#sec-data-processing-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nInterviews were anonymised and each participant was given a code number.\n\n\nThe interviewers and another member of the research team (H.B.) reviewed transcripts for accuracy.\n\n\nWe collected data throughout the admission process through direct observation, audio- recording, and chart extraction. We audio-recorded, transcribed, and anonymized both the overnight and morning case review discussions. We also observed the morning case review discussions in person and collected field notes. For each case review discussion, we copied the admission notes from the patient’s record and de-identified all data."
  },
  {
    "objectID": "guidelines/SRQR-1/index.html#sec-data-analysis-1",
    "href": "guidelines/SRQR-1/index.html#sec-data-analysis-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\n…we brought sensitizing concepts to the analysis while we conducted an open, inductive analysis.[REF]In this case the sensitizing concepts arose, a priori to analysis, from a framework derived from the literature [REF] (as described above), in which participants’ motivations to act are based on principles of professionalism, internal affect, or potential implications of their actions.[REF]\n\n\nThrough an iterative process of listening, discussing, and relistening, the team identified and consensually validated emerging themes[REF] and appended segments of dialogue supporting the proposed themes. Recruitment stopped when saturation was reached (no new themes were identified). The team systematically reviewed the themes and sorted them into content domains. The team used an analytic matrix to identify patterns and connections amongst the domains. Two of us not involved in the qualitative coding process (R.E., M.K.) audited the analytic matrix, choice of quotes, and thematic analysis.\n\n\nThe analysis started after the first interview. All data were analyzed with the aid of the audio- coding facility of the NVivo 8: QSR International Pty Ltd, Doncaster, Vic, Australia programme. First, [name] and [name] coded independently from one another, making sure to stay semantically close to the participants’ wording. Then we discussed these open codes and defined axial codes.[REF] New insights about the impact of CST were written down in memos.\n\n\nVideotapes were analysed using immersion/crystallisation methods of qualitative data analysis.[REF] With no pre-existing framework developed in advance for analysis, an inductive approach was used to discover patterns of NVB in the data. A team of six researchers met weekly for 18 months to view videos together. Using a consensus-building approach based on a combination of field notes, opportunistic interviews with the participants, and repeated viewing of the same material, sometimes many months apart, we eventually achieved consensus on verbal, non-verbal, and physical themes and patterns observed in the data. Finally, as a test of goodness-of-fit, we carefully reviewed the videotapes for any deviant cases that did not fit the categories we had developed.\n\n\nAll transcripts were coded thematically by four of the five authors, who met regularly to identify areas of convergence until full agreement was reached. One of the interviewers (P.M.) maintained an audit trail to track the team’s developing thinking. A process of dialectical empiricism[REF] was used to categorise the emergent themes into more abstract concepts…"
  },
  {
    "objectID": "guidelines/SRQR-1/index.html#sec-trustworthiness-1",
    "href": "guidelines/SRQR-1/index.html#sec-trustworthiness-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\nMember checking\n\nMember checks [REF] with an external TBL expert (R.L.) supported the validity of these analyses.\n\nTriangulation of data types and data sources\n\nThe interview data were triangulated with the data of 11 student and supervisor focus groups of a previous study, and more specifically, with those data that concern in particular the influence of CST [Communication Skills Training] on the development of patient-centredness…. Triangulation with the focus group data allowed us to broaden the in-depth information from the interviews in the analysis and to share and compare this with information from students and doctors with varying levels of CST (no, limited, full programme) and from two universities (Universities of Antwerp and Ghent). Moreover, this enabled us to better explore the evolution over time, given that the focus groups included participants at different stages of their study: before clerkships (year 4, undergraduate), during clerkships (year 6, undergraduate), after clerkships (year 7, undergraduate) and postgraduate (general practice trainees, and supervising specialists and GPs; Table 1).\n\n\n\n\n\nFinally, as a test of goodness-of-fit, we carefully reviewed the videotapes for any deviant cases that did not fit the categories we had developed.\n\nTriangulation of Researchers + Audit trail\n\nTo ensure rigor and increase authenticity in our methodology, we used two kinds of triangulation—investigator triangulation and data triangulation.[REF] We sought analytical rigor using an audit trail and multiple coders; our coding team included an experienced clinician (M.G.) as well as a nonclinician with expertise in medical communication and team dynamics (L.L.)."
  },
  {
    "objectID": "guidelines/SRQR-1/index.html#sec-synthesis-and-interpretation-1",
    "href": "guidelines/SRQR-1/index.html#sec-synthesis-and-interpretation-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nWe identified four patterns of NVB (non-verbal behavior) that relate to handover quality and have dubbed them: (1) joint focus of attention; (2) the poker hand; (3) parallel play; and (4) kerbside consultation. Each pattern constitutes a transfix, or systematic way of participating non-verbally in the care transfer process. And, although there are variations in each pattern, we have been able to code virtually every handover we have observed in nursing, medicine and surgery into one of these four categories.\n\n\nBecause our participants came from similar educational backgrounds, had studied medicine as their tertiary course, were embedded in the culture of medicine, and were associated in meaningful ways with a single medical school, we approached their transcripts with the assumption that they belonged to a loosely formed discourse community. Although their graduation dates ranged over a period of 50 years and their collective sphere of practice included 10 different specialty areas, there were many similarities in their experiences of enculturation during and after medical school.\n\n\nTheir three major (often overlapping) areas of concern were epistemic (acquiring knowledge and skill), interpersonal (relating to patients, families, colleagues and administrators) and personal (achieving work–life balance). In each of these areas, medical enculturation was achieved by two overlapping processes, absorption and assimilation, each of which may have distinct implications for postgraduate medical education."
  },
  {
    "objectID": "guidelines/SRQR-1/index.html#sec-links-to-empirical-data-1",
    "href": "guidelines/SRQR-1/index.html#sec-links-to-empirical-data-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\nSee Frankel et al. for an excellent example of how to use photographs (or snapshots from video) to illustrate and provide supporting evidence for patterns of behavior identified in the analysis. http://qualitysafety.bmj.com/content/21/Suppl_1/i121.\n\nWe identified five interruption types: (1) probing for further data, (2) prompting for expected sequence, (3) teaching around the case, (4) thinking out loud, and (5) providing direction (see Table 1). Several interruption types served both goals of the case review discussions—teaching and patient care. For example, when thinking out loud, supervisors reasoned through problems and taught the team: So that’s the big question, did she have a mechanical fall, or did she have a medicine-related fall? (Case 2). Supervisors prompted for expected sequence, preventing presenters from skipping over information while simultaneously allowing the supervisor to instruct the team on presentation style: So now you can tell me what the rest of his test results are because I haven’t heard those (Case 16)."
  },
  {
    "objectID": "guidelines/SRQR-1/index.html#sec-integration-with-prior-work-1",
    "href": "guidelines/SRQR-1/index.html#sec-integration-with-prior-work-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nThis study contributes to the understanding and discussion of the complexity of involving patients in healthcare education. It shows that integrating patient-led teaching into initiatives that are partly faculty-led influences the way in which students perceive learning from and with PIs. What is not known, however, is whether perceptions are also affected by type of health profession and the students’ different orientation towards logics of care and science, and issues of authority and power relations.\n\nFor complete examples of Discussions, see:\n\nHenriksen & Ringsted, 2013\nWesterman et al., 2013."
  },
  {
    "objectID": "guidelines/SRQR-1/index.html#sec-limitations-1",
    "href": "guidelines/SRQR-1/index.html#sec-limitations-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nThe study has several limitations. One is that the focus group interview method reveals students’ perceptions rather than their actual behaviors. Observations of the patient-led teaching encounter may have illuminated an understanding of the relationship between patient instructors and medical students. Another limitation is that the PI-led teaching is optional rather than mandatory, which may have influenced students’ attitudes in a positive direction. Moreover, students who are eager to take on extra-curricular activities may not be representative of the whole population. That only 23 out of 39 students signed up for this study might also have influenced results if the missing group of students represented other perceptions than those present in the focus groups. However the received data from the focus groups were rich in information and diverse perceptions were present. Another limitation is the overrepresentation of women over men in our sample. Even though women are also overrepresented in medical school this might potentially have influenced results, but gender differences in perceptions were nevertheless not identified in the data."
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#translations",
    "href": "guidelines/SRQR-slim/index.html#translations",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Translations",
    "text": "Translations\nThis guidance is also available in French."
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#when-to-use-this-guidance",
    "href": "guidelines/SRQR-slim/index.html#when-to-use-this-guidance",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "When to use this guidance",
    "text": "When to use this guidance\n\n\n\n\n\n\nUse this guidance for reporting qualitative research studies.\nThe SRQR items reflect information essential for inclusion in a qualitative research report, but should not be viewed as prescribing a rigid format or standardized content. Consider each item and prioritize elements that are most relevant to your study, findings, context, and readership.\nYou can also use this guideline for:\n\nfor writing protocols (use the items within the Introduction and Method sections).\nreviewing the reporting of an article, but not for appraising its quality."
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#download-resources",
    "href": "guidelines/SRQR-slim/index.html#download-resources",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Download Resources",
    "text": "Download Resources\n\n\n\nFor drafting\n\nTemplate\n\n\n\nFor checking\n\nChecklist\n\n\n\nFor planning\n\nTo-do list"
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#guidance",
    "href": "guidelines/SRQR-slim/index.html#guidance",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Guidance",
    "text": "Guidance\nApprox. 2 min read\n\n\n1. Title\n\n\n\n\n\n\n\nConcise description of the nature and topic of the study. Identifying the study as qualitative or indicating the approach (e.g., ethnography, grounded theory) or data collection methods (e.g., interview, focus group) is recommended.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead more\nExamples\n\n\nRead more\nThe authors should provide a title that clearly conveys the topic of the study. We suggest that the title indicate that the study is qualitative or include a commonly used term that identifies the approach (e.g., ethnographic study, grounded theory) or data collection methods (e.g., interviews, focus groups, observations). This allows readers and reviewers to quickly identify the type of study.\n\n\nExamples\n\nResidents learning from a narrative experience with dying patients: a qualitative study.\n\n\nMedical students’ perceptions of the factors influencing their academic performance: an exploratory interview study with high-achieving and re-sitting medical students.\n\n\nUndergraduate rural medical education program development: focus group consultation with the NRHA Rural Medical Educators Group.\n\nBack to top”\n\n\n\n\n\n\n2. Abstract\n\n\n\n\n\n\n\nSummary of key elements of the study using the abstract format of the intended publication; typically includes background, purpose, methods, results, and conclusions.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead more\nExample\n\n\nRead more\nA reader should be able to read the abstract independent of the manuscript and get a sense of the background, purpose, methods, main results and implications that will be described in greater detail in the manuscript. The information presented in the abstract should be consistent with the information presented in the full text.\nThe abstract’s structure typically needs to conform to journal guidelines, but authors should attempt to include the following:\n\nBackground about the problem or phenomenon of interest\nDescription of the study purpose or research question;\nMethods, including the approach or perspective (e.g., general inductive, grounded theory), context (setting, time period), sample (number and key characteristics of participants, events, documents), data collection strategies (e g., observation, interview, focus group) and data analysis techniques\nDescription of main findings (e.g., themes or inferences) related to the study purpose and/or research question\nStudy implications\n\nIn some cases the journal’s structured abstract format aligns more with positivist paradigms and quantitative approaches than with qualitative traditions, so translation may be necessary. For example, what might be labelled findings in many qualitative research traditions could be reported as results in the abstract.\n\n\nExample\nPurpose: Although academic centers rely on assessments from medical trainees regarding the effectiveness of their faculty as teachers, little is known about how trainees conceptualize and approach their role as assessors of their clinical supervisors\nMethod: In 2010, using a constructivist grounded theory approach, five focus group interviews were conducted with 19 residents from an internal medicine residency program. A constant comparative analysis of emergent themes was conducted.\nResults: Residents viewed clinical teaching assessment (CTA) as a time-consuming task with little reward. They reported struggling throughout the academic year to meet their CTA obligations and described several shortcut strategies they used to reduce their burden. Rather than conceptualizing their assessments as a conduit for both formative and summative feedback, residents perceived CTA as useful for the surveillance of clinical supervisors at the extremes of the spectrum of teaching effectiveness. They put the most effort, including the crafting of written comments, into the CTAs of these outliers. Trainees desired greater transparency in the CTA process and were sceptical regarding the anonymity and perceived validity of their faculty appraisals.\nConclusions: Individual and system-based factors conspire to influence postgraduate medical trainees’ motivation for generating high-quality appraisals of clinical teaching. Academic centers need to address these factors if they want to maximize the usefulness of these assessments.\nBack to top”\n\n\n\n\n\nIntroduction\n\n\n3. Problem Formulation\n\n\n\n\n\n\n\nDescription and significance of the problem/phenomenon studied; review of relevant theory and empirical work; problem statement.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead more\nExample\n\n\nRead more\nThe problem formulation typically appears in the introduction and describes the theoretical and/or practical issues or concerns that make the study necessary. It should provide an overview of what is known about the problem, highlight gaps in current knowledge (the problem statement), and define the scope of the research problem or phenomena addressed in the study (what will and will not be included). It should include a review of theoretical and/or empirical work directly relevant to the problem or phenomena studied. The problem formulation should be described in a way that suggests the need for a qualitative approach (e.g., to elucidate poorly defined or previously unexplored constructs, to generate theories or to develop causal explanations connecting processes and outcomes, to understand phenomena as they naturally occur and the role of context, to explore problems involving high complexity, to gain insight into participants’ perspectives when such insight is lacking).\n\n\nExample\n\nRegulatory focus theory may therefore offer insight into the variability in responses to feedback, but how well do these experimental findings translate to real clinical situations in which the reality of responsiveness to feedback seems frustratingly complex? …. In order to better elaborate a theory to account for this variability in learner response to feedback, the present study was undertaken. We aimed, in this study, to determine how readily clinical learning events could be classified as activating a promotion or a prevention focus, and to explore, through a careful analysis of doctors’ descriptions of their feedback experiences, the predictive value of regulatory focus theory in the context of real clinical learning situations.\n\nBack to top”\n\n\n\n\n\n\n4. Purpose or research question\n\n\n\n\n\n\n\nPurpose of the study and specific objectives or questions\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead more\nExample\n\n\nRead more\nIn qualitative research, as in all types of research, the authors should include a statement of study intent. This statement can be framed as one or more research questions, purposes, goals, or objectives. Qualitative studies often explore how and why questions related to a social or human problems or phenomenon, and they are designed to enhance readers’ understanding of a problem or phenomenon. By clearly stating the purpose of the study, authors set readers’ expectations for the methods, findings and discussion sections of the manuscript.\n\n\nExample\n\nThe purposes of this study were to investigate how medical students recognize, respond to and utilise feedback, and to determine whether there are maturational differences in understandings of the role of feedback across academic years in medical school.\n\nBack to top”\n\n\n\n\n\n\nMethods\n\n\n5. Qualitative approach and research paradigm\n\n\n\n\n\n\n\nQualitative approach (e.g., ethnography, grounded theory, case study, phenomenology, narrative research) and guiding theory if appropriate; identifying the research paradigm (e.g., post-positivist, constructivist/interpretivist) is also recommended.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead more\nExamples\n\n\nRead more\nThe research paradigm is the set of beliefs and assumptions that guide the research process. These commonly include positivist, post-positivist, constructivist or interpretivist, and critical theory. Qualitative research generally draws from a post-positivist or constructivist/interpretivist paradigm. We recommend identifying the research paradigm so that readers understand whether the researcher assumes that there is a single, objective reality (positivist or post-positivist) and has thus designed the study to describe this reality or whether the researcher assumes multiple, subjective realities and designed the study to describe these multiple realities, with no attempt to merge or reconcile these realities (constructivist/interpretivist). The paradigm has implications for the study design, approach, methods, and techniques to ensure rigor and trustworthiness.\nQualitative research includes an array of approaches and methodologies, both general (e.g., qualitative content analysis, general inductive approach) and specific (e.g., ethnography, grounded theory, phenomenography). Since the research paradigm does not necessarily dictate particular approaches or methods, the approach should also be clearly defined. Stating the approach provides readers the opportunity to evaluate the fidelity of the research approach to the research question(s) and consider the rationale for modifications and deviations from the selected approach. Qualitative research also includes an array of methods that can be used across paradigms and approaches. (See also Item 10) The researcher should explain why the selected approach is appropriate for the research question, and provide references to theories or traditions that guide the use of the approach as needed.\n\n\nExamples\n\nThe study was performed from a constructivist point of view using an interpretative phenomenological epistemology. Based on the notion that social phenomena are constructed by the communal making of meaning about the underlying phenomena, we aimed to construct insightful accounts of lead consultants’ approaches to educational change, rather than to identify the true nature of these approaches. Because the management of change by lead consultants is an under-researched area, we conducted an exploratory qualitative study…\n\n\nGiven the relative dearth of explanatory theories about factors affecting medical students’ emotional reactions, we chose to develop one by applying methods associated with grounded theory, specifically constant comparative analysis, to qualitative data obtained from learning logs and interviews. Our approach was constructivist, deliberately using researchers’ own experiences and acquired knowledge to enhance theoretical sensitivity and enrich theory development.\n\nBack to top”\n\n\n\n\n\n\n6. Researcher characteristics and reflexivity\n\n\n\n\n\n\n\nResearchers’ characteristics that may influence the research, including personal attributes, qualifications/experience, relationship with participants, assumptions, and/or presuppositions; potential or actual interaction between researchers’ characteristics and the research questions, approach, methods, results and/or transferability.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead more\nExamples\n\n\nRead more\nReflexivity refers to intentional, systematic consideration of the potential or actual effects of the researcher(s) on all aspects of the study process. In positivist and post-positivist paradigms, personal characteristics and perspectives of researchers might be viewed as biases that limit the credibility of study findings, while in constructivist or interpretivist paradigms the characteristics and perspectives of the researchers are important contextual factors that are an accepted part of the study design, data collection, and data analysis. These characteristics and perspectives may explain how the researcher(s) obtained access to the site or participants included in the study or may add valuable insight during data analysis.\nTo demonstrate reflexivity, authors should describe important personal characteristics and perspectives of members of the research team that may influence design, data collection, and data analysis. Relevant personal characteristics might include cultural background, occupation, experience, training, position/ power dynamics, gender, race/ethnicity, and sponsoring institution. Authors should also describe the perspectives, assumptions, prior knowledge, preliminary hypotheses, and/or motives (the stance) of the members of the research team.\nAuthors also should describe the researchers’ relationships to participants in the study and what decisions were made in light of these relationships. For example, were any members of the research team part of the sample of potential participants in the study? Do any members of the team teach, supervise, or have any authority over participants in the study? If so, how do these characteristics influence choices about their roles in data collection and analysis? For observational research (e.g., ethnography), it is also important to identify the role of the researcher along a spectrum from passive observer (no involvement in the activity studied) to participant-observer (ranging from some limited involvement in the activity to full involvement).\nThere is no expectation that the study could be precisely replicated; these characteristics and perspectives of the researcher should not be mentioned in the limitation section. (See also Item 14.)\n\n\nExamples\n\nReflexivity was maintained by the research team through the analysis and writing by recording, discussing and challenging established assumptions. In addition both EH and SV kept reflexive diaries.\n\n\nThe first author conducted all interviews and discussion groups. Her own medical undergraduate training took place between 1995 and 2000. She was not known to the participants of this research prior to undertaking the study and deliberately did not undertake any clinical or teaching activities locally alongside this research. Whilst it was useful to know (from her own background) what the students were talking about medically (and in terms of detecting items of significance), as a researcher she made conscious efforts not to accept potentially common assumptions at face value.\n\nBack to top”\n\n\n\n\n\n\n7. Context\n\n\n\n\n\n\n\nSetting/site and salient contextual factors; rationale.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead more\nExample\n\n\nRead more\nThe authors should describe the setting/site(s) in which the study was conducted, the reason(s) why the setting/site(s) was selected, and the salient cultural, political, historical, economic and/or other external factors that influence the study. This information helps readers interpret the meaning and significance of the study findings by situating them in social, cultural, temporal and other relevant contexts. Additional context may be reported with findings (i.e., the Results section) to add evidence for interpretations and to enhance discussion of transferability.\n\n\nExample\n\nWe conducted the study among hospital-based clinical teachers of students in years 4 to 6 of a six-year undergraduate medical program at Maastricht University Medical School. Years 4 to 6 are devoted to clerkships in the academic hospital and affiliated regional hospitals. Rotations differ in duration depending on the type of rotation and the discipline, and the sequence of rotations differs among students. During rotations, students spend time in the wards, the outpatient clinics, and the accident and emergency department. Clerkships in years 4 and 5 last between 4 and 10 weeks (regular clerkship), whereas students in year 6 undertake an 18-week senior clerkship in a discipline of their choice.\n\nBack to top”\n\n\n\n\n\n\n8. Sampling strategy\n\n\n\n\n\n\n\nHow and why research participants, documents, or events were selected; criteria for deciding when no further sampling was necessary (e.g., sampling saturation); rationale.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead more\nExamples\n\n\nRead more\nThe authors should describe how and why research participants, documents, and/or events were selected for inclusion in (and, if appropriate, exclusion from) the study, along with a justification for this strategy. We recommend that authors describe the sampling strategy rather than simply labeling the strategy (e.g., purposive or snowball), since such labels do not have a universally accepted definition and, more importantly, since procedures tend to be study- specific. Several sampling strategies are commonly used in qualitative research, although most fall under the umbrella of purposeful (or purposive) sampling. Purposeful sampling means that participants, documents, or events are selected for their relevance to the research question, based on guiding theory or experiences and assumptions of the researchers. Over the course of the research process, researchers may determine that additional or different participants, documents, or events should be included to address the research question. Other sampling techniques, such as theoretical sampling (seeking examples of theoretical constructs), snowball sampling (using study participants to identify additional participants who meet study criteria), and convenience sampling (including any volunteers with no or minimal criteria for inclusion) may be appropriate depending on the question and approach, so long as the authors provide explanation and justification.\nAlthough investigators often do not determine sample size a priori in qualitative research, they should nonetheless describe how they established the final sample size. When appropriate (e.g., when a flexible sampling strategy was used), they should explain the criteria used to decide when no further sampling was necessary. If data collection ends at the point when the researchers determine that saturation has been reached, then the specific criteria used to define saturation should be described.\nProcedures used to recruit participants should also be described, including who was involved in recruitment, what their relationship was to participants, how and when recruitment occurred, and why these procedures were selected. (See also Item 6)\n\n\nExamples\n\nAs students’ perceptions were previously shown to be related to gender, age, prior experience and place of attachment,[REF] we purposely selected respondents with different backgrounds. This sampling strategy led to the diversity of gender, age, prior patient experience and place of attachment shown in Table 1.\n\n\nPurposive sampling was directed towards achieving maximum variation in age and specialty, using a snowball approach (a non-probabilistic form of sampling in which persons initially chosen for the sample are used as informants to locate other persons having necessary characteristics making them eligible for the sample).[REF ]\n\n\nPotential participants were all medical students in Years 1 and 2 at the University of Toronto in 2004. Following research ethics board approval, recruitment was conducted via e- mail to class listservs. Participant responses were sent directly to the research assistant, who was unknown to participants, so that the principal investigators did not know who did or did not participate. This process was engaged to protect participants’ anonymity and to avoid any impression of coercion because the lead researcher (SG) was involved in the administration of the undergraduate curriculum at the time. Sample size was estimated to be sufficient based on the principle of theoretical saturation [REF] and our previous experience with this methodology (i.e., with a relatively homogeneous population, we expected to reach saturation with approximately 15 interviews per group). There were no exclusion criteria and we accepted the first 15 students from each class who volunteered.\n\nBack to top”\n\n\n\n\n\n\n9. Ethical issues pertaining to human subjects\n\n\n\n\n\n\n\nDocumentation of approval by an appropriate ethics review board and participant consent, or explanation for lack thereof; other confidentiality and data security issues.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead more\nExamples\n\n\nRead more\nQualitative research often involves interaction between researchers and research participants. Correspondingly, researchers should ensure that participants are fully aware of their participation in a research study, the risks and benefits associated with the study, the steps and precautions the researchers will take to minimize risks, such as loss of privacy and confidentiality, and how the researchers plan to use the data. Authors should report approval for the study from an appropriate institutional review board for research associated with human subjects. They should describe procedures used to protect participants, including data collection (e.g., recruitment and informed consent) analysis (e.g., data security and integrity) and reporting of findings (e.g., anonymization of excerpts). If researchers provided compensation or offered incentives to facilitate participation, this should be reported.\n\n\nExamples\n\nEthical approval was granted by the University of Otago and student participants were invited to attend each focus group discussion by the university representatives of New Zealand Medical Students Association (NZMSA). Usually, a key concern when collecting data from students is that students may feel vulnerable when sharing their experiences with academic staff during a focus group discussion. However, this potential harm was removed as each group discussion was facilitated by a fellow student, the discussions were transcribed by a professional transcribing service, and only the primary researchers [Names] had access to the raw data.\n\nBack to top”\n\n\n\n\n\n\n10. Data collection methods\n\n\n\n\n\n\n\nTypes of data collected; details of data collection procedures including (as appropriate) start and stop dates of data collection and analysis, iterative process, triangulation of sources/methods, and modification of procedures in response to evolving study findings.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead more\nExamples\n\n\nRead more\nQualitative research encompasses an array of data collection methods appropriate for various paradigms and approaches, including (but not limited to) interviews, focus groups, observations (direct or indirect via video), and review of written text, photographs, and other documents or materials. Researchers may choose to use information from multiple sources, contexts, and/or time points depending on their approach and research question(s). (See Item 11 for triangulation.) Given this diversity of methods, authors should describe in detail their data collection design and method(s) and justify them in relation to the research question(s), paradigm, approach, and other methods. Methods used to decide when to end data collection should also be described (see Item 8).\nQualitative research often occurs as an iterative process, meaning that researchers begin data analysis before they complete data collection. The data collection and analysis process may occur in phases or stages, and thus authors must clearly describe the iterative process of data collection and analysis. As part of an iterative collection-analysis process, researchers will often alter their data collection methods to explore their preliminary impressions in greater depth and/or actively pursue confirming and disconfirming perspectives. If such changes occur during the research process, authors should describe how and why study procedures changed in response to evolving study findings.\nThe study period (start and end dates for data collection and analysis) should be identified so that readers can place the study in temporal context and identify factors not mentioned by the authors that might affect findings, interpretation, and implications. (See Item 8 for ending data collection.)\nAuthors should describe important characteristics of the individuals conducting interviews, observations or focus groups, and methods used to train these individuals. This information clarifies the relationship between the individuals involved in data collection and the participants in the research and also explains what efforts were made to ensure consistency in the data collection process (See Items 6 and 15.)\n\n\nExamples\n\nFurther, it was decided that group interviews, also known as focus group discussions would be the best means of data collection. This is a method of data collection that enables group members to feed off each other’s ideas and an effective moderator will maintain group focus whilst at the same time permitting flexibility in the direction those aspects of the discussion might take. [REF]\n\n\nAdjustments to the interview protocol were made according to early experience and information participants had provided (i.e. redundant questions were eliminated; questions were reworded to improve flow and clarity; additional probes were included).\n\n\nFaculty staff were then interviewed individually by a trained study investigator in a 15-minute, semi-structured interview. This sequence was repeated with other video encounters. Table 2 presents examples of interview questions. Each faculty member was interviewed by at least three interviewers over their various interviews. Interviewers were chosen based on their experience in interviewing. All were trained during a half-day meeting to interpret and deliver the interview guide in the same manner in order to elicit information of a consistent type.\n\nBack to top”\n\n\n\n\n\n\n11. Data collection instruments and technologies\n\n\n\n\n\n\n\nDescription of instruments (e.g., interview guides, questionnaires) and devices (e.g., audio recorders) used for data collection; if/how the instrument(s) changed over the course of the study.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead more\nExamples\n\n\nRead more\nData collection for qualitative research draws upon a variety of instruments or tools, including (but not limited to) interview or focus group guides, observational protocols and prompts for field notes, and data extraction or coding protocols for selection and analysis of documents, photographs, videos, or other artifacts. The authors should describe all such instruments, guides, and protocols, including their development and cite relevant literatures, theories or conceptual frameworks as appropriate. It is often helpful for authors to provide access to the data collection instrument(s) or a detailed description of them.\nThe authors should also describe the use of equipment for audio or video recording, reproduction of paper documents or computer files, or other processes in data collection. This is relevant so readers understand the full context in which data collection occurred and how this context might have affected data collection (e.g., the influence of recording devices on participants’ behaviors; the nature of inferences drawn from live vs. recorded events).\n\n\nExamples\n\nTo facilitate the discussion and to maintain consistency over different sets of discussions, key trigger questions were devised prior to the discussion. The opening trigger question was: Thinking back to some of your best clinical learning placements in 4th and 5th year. What was it about those clinical placements that provided good opportunities for learning?\n\n\nInterviews included discussion of the expectations, processes and consequences of AEE [authentic early experiences]. The interview schedule was derived following identification of questions that could not be fully answered in a systematic review of previous empirical or theoretical literature. It comprised a sequence of topic areas including experiences in action, and areas of frustration in Medical Education such as the learning of content knowledge, achieving functional knowledge, and transfer of knowledge.[REF]… Interested readers can request a copy of the schedules from the corresponding author. Interviews lasted between 20 and 90 min. and discussion groups between 60 and 90 min. All interviews and discussion groups were conducted in private rooms at the participant’s workplace—the medical school for students and faculty, and individual places of work for workplace supervisors (except for one who chose to be interviewed at the medical school). All data were audio-recorded, and transcribed verbatim.\n\nBack to top”\n\n\n\n\n\n\n12. Units of study\n\n\n\n\n\n\n\nNumber and relevant characteristics of participants, documents, or events included in the study; level of participation.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead more\nExample\n\n\nRead more\nAuthors should describe the participants, documents, or events included in the study (the units of study). The sampling item (Item 8) describes the target or ideal participants, documents, or events selected for the study. By contrast, Item 12 focuses on description of the actual participants, documents or events included in the study. Authors should describe how the actual participants, documents, or events differ from the targeted sample, why these differences may have occurred, and how this might affect the findings.\nAuthors should describe characteristics of the participants, documents or events that are relevant to the study purpose and research question(s).1 For participants, this might include age, gender, profession, institution, year of training, or relationship to the researcher and/or other participants in the study. For documents, this might include the source, intended audience, date, or type of document. For events, this might include the location, date(s), length, characteristics of attendees or participants in the event, or mood or emotional climate. If the degree of participation varied among individuals (e.g., multiple occasions; interviews and observations), the authors should describe different levels of participation. For example, if some participants were observed and interviewed and others only interviewed, or if some participants completed multiple interviews and others completed a single interview, these variations should be explained. Authors should also explain the reasons for these differences (i.e., the researchers’ choice or the participants’ preferences) and how these different levels of participation were taken into account in the analysis. Authors should also include the dates or timeframe for participation.\nThis information about participants could appear in the Methods section as part of the description of the sample, or at the beginning of the Results section to provide context for the findings presented.\n\n\nExample\n\nOf the 70 Mindful Communication program participants, 46 met the eligibility requirements to participate in the in-depth interviews. We randomly chose and then contacted 22 participants, of whom 20 agreed to be interviewed within six months of completing the program: 15 in person and 5 by telephone. Two declined for lack of time. On reaching saturation after 20 interviews, no further attempts to contact the remaining 24 participants were made.\n\n\n\n\n\nThere were 31 nursing handovers covering 137 patients, and 21 resident handovers covering 101 patients included in this study.\n\nBack to top”\n\n\n\n\n\n\n13. Data processing\n\n\n\n\n\n\n\nMethods for processing data prior to and during analysis, including transcription, data entry, data management and security, verification of data integrity, data coding and anonymization / de-identification of excerpts.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead more\nExamples\n\n\nRead more\nAuthors should describe the ways in which data are prepared for analysis and managed throughout the analysis process. These activities might include transcription, coding, data entry, and organization of data. When processing audio or video recordings, relevant details might include indication of verbatim transcription of dialogue, additional notes to capture non- verbal information (especially for group interviews or focus groups), and annotations to indicate vocal inflections and utterances, as appropriate for the analytic approach. Authors should also describe procedures used to check transcripts for accuracy.\nAuthors should describe the processes used to organize, compile, and categorize data (e.g., field notes, transcripts, documents, photographs, artifacts) for analysis.\nAuthors should describe procedures used to maintain data security and protect the privacy of participants, as specified in the human subjects protocols. For example, if data are anonymized, the authors should explain how and at what point in the process this occurred. Authors may choose to use anonymous labels or identifiers to represent quotes or excerpts from unique participants, documents or events, in order to reflect the variety of sources from which such data were derived.\n\n\nExamples\n\nInterviews were anonymised and each participant was given a code number.\n\n\nThe interviewers and another member of the research team (H.B.) reviewed transcripts for accuracy.\n\n\nWe collected data throughout the admission process through direct observation, audio- recording, and chart extraction. We audio-recorded, transcribed, and anonymized both the overnight and morning case review discussions. We also observed the morning case review discussions in person and collected field notes. For each case review discussion, we copied the admission notes from the patient’s record and de-identified all data.\n\nBack to top”\n\n\n\n\n\n\n14. Data analysis\n\n\n\n\n\n\n\nProcess by which inferences, themes, etc. were identified and developed, including the researchers involved in data analysis; usually references a specific paradigm or approach.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead more\nExamples\n\n\nRead more\nTechniques used for data analysis will depend on the paradigm, approach, and/or data collection methods selected by the researchers. Correspondingly, authors should be as transparent as possible about the analytic process so that readers can follow the logic of inquiry from the research question(s) to the analysis and findings. For example, this description may involve characterizing the processes and decisions made for initial classification or segmentation of data, pattern identification and description, and/or development of in-depth interpretations.1 If the researchers used an approach that has a well-defined process for data analysis (e.g., grounded theory, discourse analysis, phenomenography), the authors should cite the guiding literature and describe their processes in sufficient detail so readers can judge the extent to which the processes align with the guiding approach. If modification to or deviations from the guiding approach occurred, the authors should explain and justify these modifications.\nAuthors should specify the unit of analysis.1 In qualitative research, the unit of analysis is not necessarily the same as the unit of sampling (e.g., individual participants or events). Instead, some approaches use specific events as the unit of analysis, such as mentions of a particular topic or experience, or observations of a particular behavior or phenomenon, while others use groups rather than individual group participants. This specification has implications for how the data are organized and analyzed as well as the inferences drawn from the data.\nAuthors should explain the rationale underlying different decisions made throughout the data analysis process to provide as much transparency as possible. In some approaches researchers use memoing or bracketing to make their reflections, interpretations, and links among passages explicit and more transparent to others. In some types of analysis, participants’ perspectives or observations that contrast or deviate from the concepts or themes identified by the researchers are an important part of the analysis. In such cases, the authors must describe how these discrepancies were handled during the analysis.\nDuring the analysis process, researchers may draw upon a theoretical perspective or framework, which may have been identified early in the conception of the study or may be identified by the researchers after reviewing some or all of their data. Either way, the authors should describe theoretical or other influences on their analysis scheme or categories if they exist. Sometimes these are referred to as sensitizing concepts to acknowledge that the approach is inductive, but with influence from relevant theory, models, or organizational schemes. Alternatively, themes may be developed from the data with no external influences.\nAuthors should describe which members of the research team are involved in data analysis and what perspective(s) they bring to the analysis. Authors should also indicate if any software was used to assist with data analysis and how it was used (e.g., used to apply codes after the final coding scheme was developed; to extract coded passages for further synthesis and identification of themes; or to identify passages with key words). Simply stating that software was used is insufficient.\n\n\nExamples\n\n…we brought sensitizing concepts to the analysis while we conducted an open, inductive analysis.[REF]In this case the sensitizing concepts arose, a priori to analysis, from a framework derived from the literature [REF] (as described above), in which participants’ motivations to act are based on principles of professionalism, internal affect, or potential implications of their actions.[REF]\n\n\nThrough an iterative process of listening, discussing, and relistening, the team identified and consensually validated emerging themes[REF] and appended segments of dialogue supporting the proposed themes. Recruitment stopped when saturation was reached (no new themes were identified). The team systematically reviewed the themes and sorted them into content domains. The team used an analytic matrix to identify patterns and connections amongst the domains. Two of us not involved in the qualitative coding process (R.E., M.K.) audited the analytic matrix, choice of quotes, and thematic analysis.\n\n\nThe analysis started after the first interview. All data were analyzed with the aid of the audio- coding facility of the NVivo 8: QSR International Pty Ltd, Doncaster, Vic, Australia programme. First, [name] and [name] coded independently from one another, making sure to stay semantically close to the participants’ wording. Then we discussed these open codes and defined axial codes.[REF] New insights about the impact of CST were written down in memos.\n\n\nVideotapes were analysed using immersion/crystallisation methods of qualitative data analysis.[REF] With no pre-existing framework developed in advance for analysis, an inductive approach was used to discover patterns of NVB in the data. A team of six researchers met weekly for 18 months to view videos together. Using a consensus-building approach based on a combination of field notes, opportunistic interviews with the participants, and repeated viewing of the same material, sometimes many months apart, we eventually achieved consensus on verbal, non-verbal, and physical themes and patterns observed in the data. Finally, as a test of goodness-of-fit, we carefully reviewed the videotapes for any deviant cases that did not fit the categories we had developed.\n\n\nAll transcripts were coded thematically by four of the five authors, who met regularly to identify areas of convergence until full agreement was reached. One of the interviewers (P.M.) maintained an audit trail to track the team’s developing thinking. A process of dialectical empiricism[REF] was used to categorise the emergent themes into more abstract concepts…\n\nBack to top”\n\n\n\n\n\n\n15. Techniques to enhance trustworthiness\n\n\n\n\n\n\n\nTechniques to enhance trustworthiness and credibility of data analysis,(e.g., member checking, triangulation, audit trail); rationale.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead more\nExamples\n\n\nRead more\nAuthors should describe methods used to ensure trustworthiness and credibility throughout the data collection and analysis process. Such methods will depend on the paradigm, approach, and/or methods used. Correspondingly, the authors should explain their choice of techniques and why these are appropriate for the particular study.\nCommonly used techniques to enhance trustworthiness include: member checking; triangulation of data sources, methods, and/or researchers; creation of an explicit audit trail; and immersion in the site of data collection for an extended period of time (especially for research in which an observer’s presence is likely to disrupt the phenomenon under investigation). Member checking involves sharing findings, such as descriptions of key phenomena, themes, or an explanatory model, with participants and asking them to verify the accuracy or resonance with their perspectives. Triangulation involves using more than one data source, method, or researcher to add diverse perspectives on the findings of the study and, in some approaches, to test the transferability or generalizability of a model. An audit trail involves careful documentation of all decisions made throughout the study, from initial conceptualization to study design, sampling, analysis, and reporting, to provide transparency and to enable an external researcher to review all the steps involved in the study.\n\n\nExamples\nMember checking\n\nMember checks [REF] with an external TBL expert (R.L.) supported the validity of these analyses.\n\nTriangulation of data types and data sources\n\nThe interview data were triangulated with the data of 11 student and supervisor focus groups of a previous study, and more specifically, with those data that concern in particular the influence of CST [Communication Skills Training] on the development of patient-centredness…. Triangulation with the focus group data allowed us to broaden the in-depth information from the interviews in the analysis and to share and compare this with information from students and doctors with varying levels of CST (no, limited, full programme) and from two universities (Universities of Antwerp and Ghent). Moreover, this enabled us to better explore the evolution over time, given that the focus groups included participants at different stages of their study: before clerkships (year 4, undergraduate), during clerkships (year 6, undergraduate), after clerkships (year 7, undergraduate) and postgraduate (general practice trainees, and supervising specialists and GPs; Table 1).\n\n\n\n\n\nFinally, as a test of goodness-of-fit, we carefully reviewed the videotapes for any deviant cases that did not fit the categories we had developed.\n\nTriangulation of Researchers + Audit trail\n\nTo ensure rigor and increase authenticity in our methodology, we used two kinds of triangulation—investigator triangulation and data triangulation.[REF] We sought analytical rigor using an audit trail and multiple coders; our coding team included an experienced clinician (M.G.) as well as a nonclinician with expertise in medical communication and team dynamics (L.L.).\n\nBack to top”\n\n\n\n\n\n\nResults\n\n\n16. Synthesis and interpretation\n\n\n\n\n\n\n\nMain findings (e.g., interpretations, inferences, and themes); might include development of a theory or model, or integration with prior research or theory.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead more\nExamples\n\n\nRead more\nIn qualitative research the distinction between results and discussion tends to blur because analysis often involves interpretation, inference, and synthesis. Although most journals require separate sections for Results and Discussion, many elements of Items 16-19 could reasonably be reported in either section. As such, we defer to authors and editors to determine where to report these essential elements.\nAuthors should identify the main analytic findings (e.g., interpretations, inferences, narratives, themes, models). The nature of these findings and how they are reported will depend on the approach and methodology selected and thus should be in alignment with the approach and methods.\nIn most cases, the authors should report a synthesis of their data along with specific quotes, examples, or illustrations derived from their data. Authors might also report frequency, variety, representativeness, counter-examples, concrete details, contextualization, conditions, and qualifications related to their findings. Frequency counts (e.g., the frequency of specific themes or codes) play a limited role in qualitative research, and need not be reported unless they play a meaningful role in interpretation of the data.\nFindings might also include integration with prior literature or theory and/or the development of a theory, model or meta-narrative. Judicious use of tables and figures can help communicate such findings.\n\n\nExamples\n\nWe identified four patterns of NVB (non-verbal behavior) that relate to handover quality and have dubbed them: (1) joint focus of attention; (2) the poker hand; (3) parallel play; and (4) kerbside consultation. Each pattern constitutes a transfix, or systematic way of participating non-verbally in the care transfer process. And, although there are variations in each pattern, we have been able to code virtually every handover we have observed in nursing, medicine and surgery into one of these four categories.\n\n\nBecause our participants came from similar educational backgrounds, had studied medicine as their tertiary course, were embedded in the culture of medicine, and were associated in meaningful ways with a single medical school, we approached their transcripts with the assumption that they belonged to a loosely formed discourse community. Although their graduation dates ranged over a period of 50 years and their collective sphere of practice included 10 different specialty areas, there were many similarities in their experiences of enculturation during and after medical school.\n\n\nTheir three major (often overlapping) areas of concern were epistemic (acquiring knowledge and skill), interpersonal (relating to patients, families, colleagues and administrators) and personal (achieving work–life balance). In each of these areas, medical enculturation was achieved by two overlapping processes, absorption and assimilation, each of which may have distinct implications for postgraduate medical education.\n\nBack to top”\n\n\n\n\n\n\n17. Links to empirical data\n\n\n\n\n\n\n\nEvidence (e.g., quotes, field notes, text excerpts, photographs) to substantiate analytic findings.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead more\nExamples\n\n\nRead more\nThe authors should provide evidence (e.g., quotes, field notes, text excerpts, photographs) to substantiate the more general and abstract concepts or inferences they present as findings. Such evidence is typically de-identified to protect the privacy of study participants, settings, and/or institutions. The evidence may be presented in a variety of ways such as in a table or figure, incorporated into a narrative description of findings, as a stand-alone narrative, or in text blocks embedded in the manuscript text. If word limits or media limitations (e.g.. video) limit the authors’ ability to provide sufficient representation of supporting data, an appendix, supplemental material, or web-based repository could be used to provide access to additional data.\n\n\nExamples\nSee Frankel et al. for an excellent example of how to use photographs (or snapshots from video) to illustrate and provide supporting evidence for patterns of behavior identified in the analysis. http://qualitysafety.bmj.com/content/21/Suppl_1/i121.\n\nWe identified five interruption types: (1) probing for further data, (2) prompting for expected sequence, (3) teaching around the case, (4) thinking out loud, and (5) providing direction (see Table 1). Several interruption types served both goals of the case review discussions—teaching and patient care. For example, when thinking out loud, supervisors reasoned through problems and taught the team: So that’s the big question, did she have a mechanical fall, or did she have a medicine-related fall? (Case 2). Supervisors prompted for expected sequence, preventing presenters from skipping over information while simultaneously allowing the supervisor to instruct the team on presentation style: So now you can tell me what the rest of his test results are because I haven’t heard those (Case 16).\n\nBack to top”\n\n\n\n\n\n\nDiscussion\n\n\n18. Integration with prior work, implications, transferability, and contribution(s) to the field\n\n\n\n\n\n\n\nShort summary of main findings, explanation of how findings and conclusions connect to, support, elaborate on, or challenge conclusions of earlier scholarship; discussion of scope of application/generalizability; identification of unique contribution(s) to scholarship in a discipline or field.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead more\nExamples\n\n\nRead more\nThe authors should begin the Discussion with a short summary of the main findings. The short summary reminds readers of the main findings and may help them assess whether the subsequent interpretation and implications formulated by the authors are supported by the findings.\nThe authors’ description of their findings or results should include some interpretation of the data in the context of previous findings, experiences, theory, or a guiding paradigm or approach. The discussion provides authors an opportunity to elaborate on their findings in relation to their research question(s) and study purpose(s); connect their findings to prior empirical work, theories, and/or frameworks; and discuss implications. The authors should explicitly describe how their findings contribute to or advance the field. Implications may include transferability, or specifying the appropriate scope for generalization of the findings beyond the study (e.g., to other settings, populations, time periods, circumstances).\n\n\nExamples\n\nThis study contributes to the understanding and discussion of the complexity of involving patients in healthcare education. It shows that integrating patient-led teaching into initiatives that are partly faculty-led influences the way in which students perceive learning from and with PIs. What is not known, however, is whether perceptions are also affected by type of health profession and the students’ different orientation towards logics of care and science, and issues of authority and power relations.\n\nFor complete examples of Discussions, see:\n\nHenriksen & Ringsted, 2013\nWesterman et al., 2013.\n\nBack to top”\n\n\n\n\n\n\n19. Limitations\n\n\n\n\n\n\n\nTrustworthiness and limitations of findings\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead more\nExample\n\n\nRead more\nAuthors should describe techniques used to ensure trustworthiness in the Methods section of the manuscript. In the Discussion, authors should identify problems or gaps in their efforts to ensure trustworthiness and the potential implications. For example, if researchers intended to interview individuals with certain characteristics, or who might offer different perspectives, but were unsuccessful in recruiting any willing participants, they should explain this issue and describe possible consequences for transferability. (See also Item 18.)\nAll research paradigms and approaches have strengths and weaknesses, and authors should explicitly discuss how the paradigm, approach, and methods they used will influence the situations to which their findings may reasonably apply. (See also Item 18.) In addition, they should explain how specific decisions or events in the conduct of the study strengthen or weaken the rigor of their findings.\n\n\nExample\n\nThe study has several limitations. One is that the focus group interview method reveals students’ perceptions rather than their actual behaviors. Observations of the patient-led teaching encounter may have illuminated an understanding of the relationship between patient instructors and medical students. Another limitation is that the PI-led teaching is optional rather than mandatory, which may have influenced students’ attitudes in a positive direction. Moreover, students who are eager to take on extra-curricular activities may not be representative of the whole population. That only 23 out of 39 students signed up for this study might also have influenced results if the missing group of students represented other perceptions than those present in the focus groups. However the received data from the focus groups were rich in information and diverse perceptions were present. Another limitation is the overrepresentation of women over men in our sample. Even though women are also overrepresented in medical school this might potentially have influenced results, but gender differences in perceptions were nevertheless not identified in the data.\n\nBack to top”\n\n\n\n\n\n\nOther\n\n\n20. Conflicts of interest\n\n\n\n\n\n\n\nPotential sources of influence or perceived influence on study conduct and conclusions; how these were managed.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nRead more\nAuthors should identify any real or potential conflicts of interest that might have influenced or could appear to have influenced the research. Authors should also explain how these conflicts were managed in the conduct of the study, and describe the potential impact on study findings and/or conclusions. Some aspects may be mentioned as part of reflexivity (see Item 6).\n\n\n\n\n\n21. Funding\n\n\n\n\n\n\n\nSources of funding and other support; role of funders in data collection, interpretation, and reporting.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nRead more\nThe authors should describe any sources of funding and other support for the study and the role of funders in data collection, data analysis, and reporting if applicable.\n\n\n\n\n\n\nEthnography\n\n\n\n\n\n\nOccaecat mollit deserunt excepteur consectetur proident. Anim Lorem ipsum anim nisi velit magna duis Lorem consequat aliquip eu ullamco consequat amet. Magna labore in consequat ad aliquip Lorem elit do in. Esse minim consectetur ut commodo sunt cupidatat voluptate sint voluptate nisi.\nExercitation eu mollit consequat ipsum non ullamco. Velit velit dolor cupidatat proident in laboris eiusmod incididunt mollit nisi laborum minim. Eiusmod et voluptate veniam exercitation sit ex labore.\nLearn more\n\n\n\n\n\n\nGrounded theory\n\n\n\n\n\n\nEt eu labore qui non dolore nulla. Ipsum aliqua laboris commodo deserunt cillum excepteur dolore adipisicing occaecat nostrud anim elit officia amet. Commodo incididunt ad et velit do minim dolore. Ullamco eiusmod Lorem proident tempor laborum nulla incididunt non pariatur irure officia ad. Elit deserunt commodo laboris adipisicing duis amet et dolore nulla nostrud. Nostrud excepteur exercitation elit consequat consequat.\n\n\n\n\n\n\nInterview\n\n\n\n\n\n\nNon exercitation commodo incididunt tempor veniam magna excepteur est reprehenderit Lorem. Exercitation enim et quis velit pariatur mollit fugiat dolore ullamco eu eu. Do ullamco eu culpa sint ad est. Proident veniam sint eu cupidatat minim voluptate officia anim laborum qui aliqua. Enim minim non mollit elit ipsum fugiat dolore cupidatat. Magna adipisicing labore do dolor voluptate non ipsum sint eiusmod aute proident proident ullamco. Excepteur cupidatat occaecat enim culpa cillum laboris ex incididunt officia.\n\n\n\n\n\n\nFocus group\n\n\n\n\n\n\nCommodo commodo labore eu laboris commodo excepteur eu et id deserunt enim id minim cupidatat. Fugiat deserunt eiusmod eiusmod culpa velit dolore ex voluptate anim pariatur. Nostrud cillum cillum culpa exercitation. Labore laboris ex commodo nostrud excepteur eiusmod veniam in esse est exercitation Lorem proident dolor. Labore aliqua fugiat id qui sit qui sit aliquip incididunt labore irure Lorem voluptate. Eiusmod ea ex ea cillum exercitation laboris laborum incididunt labore. Ea officia aliqua id velit nostrud velit amet.\n\n\n\n\n\n\nGeneral inductive\n\n\n\n\n\n\nLaboris ea eu elit irure ex reprehenderit sunt commodo non eiusmod tempor. Eiusmod occaecat mollit consequat dolore voluptate officia exercitation voluptate velit nostrud duis ad consectetur. Magna magna in occaecat nostrud tempor tempor incididunt commodo officia reprehenderit irure aliqua.\n\n\n\n\n\n\nContext\n\n\n\n\n\n\nEiusmod sit cillum sint duis mollit non amet consectetur minim in sunt aliquip laboris ipsum. Culpa ullamco laborum et veniam laborum anim ut aliqua dolore. Aliquip dolore sit tempor velit officia. Aute do nostrud reprehenderit pariatur sint qui. Occaecat consequat ut et sint. Officia aliquip non minim do est minim.\n\n\n\n\n\n\nSetting\n\n\n\n\n\n\nExercitation consectetur ipsum sit sit magna nostrud do commodo aute ad. Elit ea ea aliqua eu commodo. Enim est pariatur tempor non exercitation officia aliquip ut veniam laborum. Eu ea aliqua laborum elit voluptate officia ex ipsum amet.\n\n\n\n\n\n\nSample\n\n\n\n\n\n\nEst irure ipsum laborum ad consectetur. Incididunt dolor duis excepteur proident. Ut ullamco amet aliqua sint fugiat exercitation amet anim magna dolor ullamco. Officia dolore dolore ad laborum. Dolore reprehenderit non deserunt dolore voluptate in Lorem nulla laborum ipsum occaecat eiusmod. Incididunt minim magna sint non veniam incididunt sint consectetur amet qui. Voluptate enim deserunt Lorem proident reprehenderit mollit magna adipisicing reprehenderit laboris nisi qui cupidatat id.\n\n\n\n\n\n\nThemes\n\n\n\n\n\n\nEt et aliquip elit aliquip cillum. Mollit enim ullamco ea ullamco eu ex officia ut culpa cillum laborum ad. Nulla id mollit magna deserunt deserunt eu. Qui esse sint fugiat excepteur consectetur velit incididunt aliquip labore consequat aute Lorem laboris. Cupidatat irure ex minim adipisicing commodo Lorem elit. Sit fugiat dolor dolore pariatur exercitation in duis nostrud cillum id magna non tempor. Duis reprehenderit ex cupidatat reprehenderit duis cillum.\n\n\n\n\n\n\nInferences\n\n\n\n\n\n\nOccaecat qui non aute ut quis aliqua ut sint ipsum enim Lorem. Laborum nostrud esse voluptate dolore exercitation. Excepteur pariatur id pariatur ullamco ea esse do. Aute aute minim veniam reprehenderit id enim aliqua dolor ullamco. Sit tempor nostrud tempor minim. Qui irure laborum consectetur duis do quis laboris eiusmod consequat.\n\n\n\n\n\n\nPositivist\n\n\n\n\n\n\nTempor aliquip minim commodo nisi et aliqua ea mollit esse. Tempor aute consequat exercitation excepteur et. Aliquip dolor duis fugiat incididunt sint ipsum adipisicing eiusmod. Aliqua ea ipsum Lorem amet. Excepteur do dolore commodo veniam Lorem eiusmod cillum cupidatat Lorem minim nulla est ipsum. Eu laboris anim laboris mollit aute quis mollit est duis elit sit voluptate nisi. Amet dolor pariatur dolore culpa.\n\n\n\n\n\n\nPost-positivist\n\n\n\n\n\n\nProident est commodo exercitation ex occaecat ullamco anim ut reprehenderit deserunt dolor voluptate in. Consequat culpa reprehenderit consequat enim dolore officia tempor proident duis eiusmod consequat minim incididunt id. Ad proident qui aute ea.\n\n\n\n\n\n\nConstructs\n\n\n\n\n\n\nId sunt enim dolore nostrud laboris quis quis sint anim ullamco et ut proident. Ipsum pariatur sit eu quis irure eiusmod quis adipisicing velit cillum sit reprehenderit enim. Quis magna sint officia irure cillum irure est. Nostrud nostrud enim commodo magna mollit et eu amet mollit occaecat amet.\n\n\n\n\n\n\nCase study\n\n\n\n\n\n\nLaboris quis ad laboris eu amet. Deserunt enim ad do aliqua qui labore. Anim aute esse fugiat labore reprehenderit quis et eiusmod. Dolore consequat sunt consequat veniam qui nisi magna qui non amet eu. Aute nulla anim culpa enim magna amet adipisicing. Ea eu officia minim minim duis ea reprehenderit Lorem ullamco. Minim et aliqua amet id proident est eiusmod.\n\n\n\n\n\n\nPhenomenology\n\n\n\n\n\n\nEnim ea voluptate ad dolore et commodo in id eiusmod ea. Irure culpa pariatur veniam culpa velit Lorem exercitation ea ut nulla minim non esse reprehenderit. Ipsum do cupidatat culpa ipsum in ea consequat. Dolor sint laborum proident labore nulla fugiat esse sit enim irure anim. Id ex proident et tempor enim est nostrud. Ullamco ex do irure amet proident nulla proident non exercitation esse ullamco est dolor. Exercitation proident mollit nostrud dolore reprehenderit nostrud officia in magna eiusmod.\n\n\n\n\n\n\nNarrative research\n\n\n\n\n\n\nNisi pariatur in sunt est anim sit sint cillum occaecat ut enim commodo amet ullamco. Lorem nulla ut in nulla. Nisi aute cillum ad duis aliqua commodo velit incididunt aliquip.\n\n\n\n\n\n\nConstructivist\n\n\n\n\n\n\nNulla fugiat labore voluptate velit ipsum elit velit nulla laborum cillum minim. Esse laboris adipisicing proident non ipsum adipisicing consequat do do proident ea. Irure minim fugiat proident irure adipisicing. Ea ad consectetur fugiat qui id consectetur nostrud amet elit sit non cupidatat laboris nostrud. Est officia velit sit fugiat ut pariatur consequat incididunt in velit nulla do proident. Reprehenderit pariatur adipisicing mollit mollit consectetur. Lorem consectetur nisi est aute reprehenderit elit amet et deserunt sit pariatur sunt dolore excepteur.\n\n\n\n\n\n\nInterpretivist\n\n\n\n\n\n\nMinim ad fugiat ex magna occaecat nostrud officia est dolor duis tempor quis. Voluptate dolore dolore labore irure ipsum esse do labore excepteur est elit. Excepteur irure veniam cillum sit est aliqua occaecat sunt eu dolor dolor. Adipisicing est mollit aute consequat proident adipisicing minim est ad. Duis cupidatat est enim ex labore sit eu pariatur."
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-title-1",
    "href": "guidelines/SRQR-slim/index.html#sec-title-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Read more",
    "text": "Read more\nThe authors should provide a title that clearly conveys the topic of the study. We suggest that the title indicate that the study is qualitative or include a commonly used term that identifies the approach (e.g., ethnographic study, grounded theory) or data collection methods (e.g., interviews, focus groups, observations). This allows readers and reviewers to quickly identify the type of study."
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-title-2",
    "href": "guidelines/SRQR-slim/index.html#sec-title-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nResidents learning from a narrative experience with dying patients: a qualitative study.\n\n\nMedical students’ perceptions of the factors influencing their academic performance: an exploratory interview study with high-achieving and re-sitting medical students.\n\n\nUndergraduate rural medical education program development: focus group consultation with the NRHA Rural Medical Educators Group.\n\nBack to top”"
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-abstract-1",
    "href": "guidelines/SRQR-slim/index.html#sec-abstract-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Read more",
    "text": "Read more\nA reader should be able to read the abstract independent of the manuscript and get a sense of the background, purpose, methods, main results and implications that will be described in greater detail in the manuscript. The information presented in the abstract should be consistent with the information presented in the full text.\nThe abstract’s structure typically needs to conform to journal guidelines, but authors should attempt to include the following:\n\nBackground about the problem or phenomenon of interest\nDescription of the study purpose or research question;\nMethods, including the approach or perspective (e.g., general inductive, grounded theory), context (setting, time period), sample (number and key characteristics of participants, events, documents), data collection strategies (e g., observation, interview, focus group) and data analysis techniques\nDescription of main findings (e.g., themes or inferences) related to the study purpose and/or research question\nStudy implications\n\nIn some cases the journal’s structured abstract format aligns more with positivist paradigms and quantitative approaches than with qualitative traditions, so translation may be necessary. For example, what might be labelled findings in many qualitative research traditions could be reported as results in the abstract."
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-abstract-2",
    "href": "guidelines/SRQR-slim/index.html#sec-abstract-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\nPurpose: Although academic centers rely on assessments from medical trainees regarding the effectiveness of their faculty as teachers, little is known about how trainees conceptualize and approach their role as assessors of their clinical supervisors\nMethod: In 2010, using a constructivist grounded theory approach, five focus group interviews were conducted with 19 residents from an internal medicine residency program. A constant comparative analysis of emergent themes was conducted.\nResults: Residents viewed clinical teaching assessment (CTA) as a time-consuming task with little reward. They reported struggling throughout the academic year to meet their CTA obligations and described several shortcut strategies they used to reduce their burden. Rather than conceptualizing their assessments as a conduit for both formative and summative feedback, residents perceived CTA as useful for the surveillance of clinical supervisors at the extremes of the spectrum of teaching effectiveness. They put the most effort, including the crafting of written comments, into the CTAs of these outliers. Trainees desired greater transparency in the CTA process and were sceptical regarding the anonymity and perceived validity of their faculty appraisals.\nConclusions: Individual and system-based factors conspire to influence postgraduate medical trainees’ motivation for generating high-quality appraisals of clinical teaching. Academic centers need to address these factors if they want to maximize the usefulness of these assessments.\nBack to top”"
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-problem-formulation-1",
    "href": "guidelines/SRQR-slim/index.html#sec-problem-formulation-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Read more",
    "text": "Read more\nThe problem formulation typically appears in the introduction and describes the theoretical and/or practical issues or concerns that make the study necessary. It should provide an overview of what is known about the problem, highlight gaps in current knowledge (the problem statement), and define the scope of the research problem or phenomena addressed in the study (what will and will not be included). It should include a review of theoretical and/or empirical work directly relevant to the problem or phenomena studied. The problem formulation should be described in a way that suggests the need for a qualitative approach (e.g., to elucidate poorly defined or previously unexplored constructs, to generate theories or to develop causal explanations connecting processes and outcomes, to understand phenomena as they naturally occur and the role of context, to explore problems involving high complexity, to gain insight into participants’ perspectives when such insight is lacking)."
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-problem-formulation-2",
    "href": "guidelines/SRQR-slim/index.html#sec-problem-formulation-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nRegulatory focus theory may therefore offer insight into the variability in responses to feedback, but how well do these experimental findings translate to real clinical situations in which the reality of responsiveness to feedback seems frustratingly complex? …. In order to better elaborate a theory to account for this variability in learner response to feedback, the present study was undertaken. We aimed, in this study, to determine how readily clinical learning events could be classified as activating a promotion or a prevention focus, and to explore, through a careful analysis of doctors’ descriptions of their feedback experiences, the predictive value of regulatory focus theory in the context of real clinical learning situations.\n\nBack to top”"
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-purpose-1",
    "href": "guidelines/SRQR-slim/index.html#sec-purpose-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Read more",
    "text": "Read more\nIn qualitative research, as in all types of research, the authors should include a statement of study intent. This statement can be framed as one or more research questions, purposes, goals, or objectives. Qualitative studies often explore how and why questions related to a social or human problems or phenomenon, and they are designed to enhance readers’ understanding of a problem or phenomenon. By clearly stating the purpose of the study, authors set readers’ expectations for the methods, findings and discussion sections of the manuscript."
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-purpose-2",
    "href": "guidelines/SRQR-slim/index.html#sec-purpose-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nThe purposes of this study were to investigate how medical students recognize, respond to and utilise feedback, and to determine whether there are maturational differences in understandings of the role of feedback across academic years in medical school.\n\nBack to top”"
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-qualitative-approach-1",
    "href": "guidelines/SRQR-slim/index.html#sec-qualitative-approach-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Read more",
    "text": "Read more\nThe research paradigm is the set of beliefs and assumptions that guide the research process. These commonly include positivist, post-positivist, constructivist or interpretivist, and critical theory. Qualitative research generally draws from a post-positivist or constructivist/interpretivist paradigm. We recommend identifying the research paradigm so that readers understand whether the researcher assumes that there is a single, objective reality (positivist or post-positivist) and has thus designed the study to describe this reality or whether the researcher assumes multiple, subjective realities and designed the study to describe these multiple realities, with no attempt to merge or reconcile these realities (constructivist/interpretivist). The paradigm has implications for the study design, approach, methods, and techniques to ensure rigor and trustworthiness.\nQualitative research includes an array of approaches and methodologies, both general (e.g., qualitative content analysis, general inductive approach) and specific (e.g., ethnography, grounded theory, phenomenography). Since the research paradigm does not necessarily dictate particular approaches or methods, the approach should also be clearly defined. Stating the approach provides readers the opportunity to evaluate the fidelity of the research approach to the research question(s) and consider the rationale for modifications and deviations from the selected approach. Qualitative research also includes an array of methods that can be used across paradigms and approaches. (See also Item 10) The researcher should explain why the selected approach is appropriate for the research question, and provide references to theories or traditions that guide the use of the approach as needed."
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-qualitative-approach-2",
    "href": "guidelines/SRQR-slim/index.html#sec-qualitative-approach-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nThe study was performed from a constructivist point of view using an interpretative phenomenological epistemology. Based on the notion that social phenomena are constructed by the communal making of meaning about the underlying phenomena, we aimed to construct insightful accounts of lead consultants’ approaches to educational change, rather than to identify the true nature of these approaches. Because the management of change by lead consultants is an under-researched area, we conducted an exploratory qualitative study…\n\n\nGiven the relative dearth of explanatory theories about factors affecting medical students’ emotional reactions, we chose to develop one by applying methods associated with grounded theory, specifically constant comparative analysis, to qualitative data obtained from learning logs and interviews. Our approach was constructivist, deliberately using researchers’ own experiences and acquired knowledge to enhance theoretical sensitivity and enrich theory development.\n\nBack to top”"
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-researcher-characteristics-and-reflexivity-1",
    "href": "guidelines/SRQR-slim/index.html#sec-researcher-characteristics-and-reflexivity-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Read more",
    "text": "Read more\nReflexivity refers to intentional, systematic consideration of the potential or actual effects of the researcher(s) on all aspects of the study process. In positivist and post-positivist paradigms, personal characteristics and perspectives of researchers might be viewed as biases that limit the credibility of study findings, while in constructivist or interpretivist paradigms the characteristics and perspectives of the researchers are important contextual factors that are an accepted part of the study design, data collection, and data analysis. These characteristics and perspectives may explain how the researcher(s) obtained access to the site or participants included in the study or may add valuable insight during data analysis.\nTo demonstrate reflexivity, authors should describe important personal characteristics and perspectives of members of the research team that may influence design, data collection, and data analysis. Relevant personal characteristics might include cultural background, occupation, experience, training, position/ power dynamics, gender, race/ethnicity, and sponsoring institution. Authors should also describe the perspectives, assumptions, prior knowledge, preliminary hypotheses, and/or motives (the stance) of the members of the research team.\nAuthors also should describe the researchers’ relationships to participants in the study and what decisions were made in light of these relationships. For example, were any members of the research team part of the sample of potential participants in the study? Do any members of the team teach, supervise, or have any authority over participants in the study? If so, how do these characteristics influence choices about their roles in data collection and analysis? For observational research (e.g., ethnography), it is also important to identify the role of the researcher along a spectrum from passive observer (no involvement in the activity studied) to participant-observer (ranging from some limited involvement in the activity to full involvement).\nThere is no expectation that the study could be precisely replicated; these characteristics and perspectives of the researcher should not be mentioned in the limitation section. (See also Item 14.)"
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-researcher-characteristics-and-reflexivity-2",
    "href": "guidelines/SRQR-slim/index.html#sec-researcher-characteristics-and-reflexivity-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nReflexivity was maintained by the research team through the analysis and writing by recording, discussing and challenging established assumptions. In addition both EH and SV kept reflexive diaries.\n\n\nThe first author conducted all interviews and discussion groups. Her own medical undergraduate training took place between 1995 and 2000. She was not known to the participants of this research prior to undertaking the study and deliberately did not undertake any clinical or teaching activities locally alongside this research. Whilst it was useful to know (from her own background) what the students were talking about medically (and in terms of detecting items of significance), as a researcher she made conscious efforts not to accept potentially common assumptions at face value.\n\nBack to top”"
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-context-1",
    "href": "guidelines/SRQR-slim/index.html#sec-context-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Read more",
    "text": "Read more\nThe authors should describe the setting/site(s) in which the study was conducted, the reason(s) why the setting/site(s) was selected, and the salient cultural, political, historical, economic and/or other external factors that influence the study. This information helps readers interpret the meaning and significance of the study findings by situating them in social, cultural, temporal and other relevant contexts. Additional context may be reported with findings (i.e., the Results section) to add evidence for interpretations and to enhance discussion of transferability."
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-context-2",
    "href": "guidelines/SRQR-slim/index.html#sec-context-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nWe conducted the study among hospital-based clinical teachers of students in years 4 to 6 of a six-year undergraduate medical program at Maastricht University Medical School. Years 4 to 6 are devoted to clerkships in the academic hospital and affiliated regional hospitals. Rotations differ in duration depending on the type of rotation and the discipline, and the sequence of rotations differs among students. During rotations, students spend time in the wards, the outpatient clinics, and the accident and emergency department. Clerkships in years 4 and 5 last between 4 and 10 weeks (regular clerkship), whereas students in year 6 undertake an 18-week senior clerkship in a discipline of their choice.\n\nBack to top”"
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-sampling-strategy-1",
    "href": "guidelines/SRQR-slim/index.html#sec-sampling-strategy-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Read more",
    "text": "Read more\nThe authors should describe how and why research participants, documents, and/or events were selected for inclusion in (and, if appropriate, exclusion from) the study, along with a justification for this strategy. We recommend that authors describe the sampling strategy rather than simply labeling the strategy (e.g., purposive or snowball), since such labels do not have a universally accepted definition and, more importantly, since procedures tend to be study- specific. Several sampling strategies are commonly used in qualitative research, although most fall under the umbrella of purposeful (or purposive) sampling. Purposeful sampling means that participants, documents, or events are selected for their relevance to the research question, based on guiding theory or experiences and assumptions of the researchers. Over the course of the research process, researchers may determine that additional or different participants, documents, or events should be included to address the research question. Other sampling techniques, such as theoretical sampling (seeking examples of theoretical constructs), snowball sampling (using study participants to identify additional participants who meet study criteria), and convenience sampling (including any volunteers with no or minimal criteria for inclusion) may be appropriate depending on the question and approach, so long as the authors provide explanation and justification.\nAlthough investigators often do not determine sample size a priori in qualitative research, they should nonetheless describe how they established the final sample size. When appropriate (e.g., when a flexible sampling strategy was used), they should explain the criteria used to decide when no further sampling was necessary. If data collection ends at the point when the researchers determine that saturation has been reached, then the specific criteria used to define saturation should be described.\nProcedures used to recruit participants should also be described, including who was involved in recruitment, what their relationship was to participants, how and when recruitment occurred, and why these procedures were selected. (See also Item 6)"
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-sampling-strategy-2",
    "href": "guidelines/SRQR-slim/index.html#sec-sampling-strategy-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nAs students’ perceptions were previously shown to be related to gender, age, prior experience and place of attachment,[REF] we purposely selected respondents with different backgrounds. This sampling strategy led to the diversity of gender, age, prior patient experience and place of attachment shown in Table 1.\n\n\nPurposive sampling was directed towards achieving maximum variation in age and specialty, using a snowball approach (a non-probabilistic form of sampling in which persons initially chosen for the sample are used as informants to locate other persons having necessary characteristics making them eligible for the sample).[REF ]\n\n\nPotential participants were all medical students in Years 1 and 2 at the University of Toronto in 2004. Following research ethics board approval, recruitment was conducted via e- mail to class listservs. Participant responses were sent directly to the research assistant, who was unknown to participants, so that the principal investigators did not know who did or did not participate. This process was engaged to protect participants’ anonymity and to avoid any impression of coercion because the lead researcher (SG) was involved in the administration of the undergraduate curriculum at the time. Sample size was estimated to be sufficient based on the principle of theoretical saturation [REF] and our previous experience with this methodology (i.e., with a relatively homogeneous population, we expected to reach saturation with approximately 15 interviews per group). There were no exclusion criteria and we accepted the first 15 students from each class who volunteered.\n\nBack to top”"
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-ethics-1",
    "href": "guidelines/SRQR-slim/index.html#sec-ethics-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Read more",
    "text": "Read more\nQualitative research often involves interaction between researchers and research participants. Correspondingly, researchers should ensure that participants are fully aware of their participation in a research study, the risks and benefits associated with the study, the steps and precautions the researchers will take to minimize risks, such as loss of privacy and confidentiality, and how the researchers plan to use the data. Authors should report approval for the study from an appropriate institutional review board for research associated with human subjects. They should describe procedures used to protect participants, including data collection (e.g., recruitment and informed consent) analysis (e.g., data security and integrity) and reporting of findings (e.g., anonymization of excerpts). If researchers provided compensation or offered incentives to facilitate participation, this should be reported."
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-ethics-2",
    "href": "guidelines/SRQR-slim/index.html#sec-ethics-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nEthical approval was granted by the University of Otago and student participants were invited to attend each focus group discussion by the university representatives of New Zealand Medical Students Association (NZMSA). Usually, a key concern when collecting data from students is that students may feel vulnerable when sharing their experiences with academic staff during a focus group discussion. However, this potential harm was removed as each group discussion was facilitated by a fellow student, the discussions were transcribed by a professional transcribing service, and only the primary researchers [Names] had access to the raw data.\n\nBack to top”"
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-data-collection-methods-1",
    "href": "guidelines/SRQR-slim/index.html#sec-data-collection-methods-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Read more",
    "text": "Read more\nQualitative research encompasses an array of data collection methods appropriate for various paradigms and approaches, including (but not limited to) interviews, focus groups, observations (direct or indirect via video), and review of written text, photographs, and other documents or materials. Researchers may choose to use information from multiple sources, contexts, and/or time points depending on their approach and research question(s). (See Item 11 for triangulation.) Given this diversity of methods, authors should describe in detail their data collection design and method(s) and justify them in relation to the research question(s), paradigm, approach, and other methods. Methods used to decide when to end data collection should also be described (see Item 8).\nQualitative research often occurs as an iterative process, meaning that researchers begin data analysis before they complete data collection. The data collection and analysis process may occur in phases or stages, and thus authors must clearly describe the iterative process of data collection and analysis. As part of an iterative collection-analysis process, researchers will often alter their data collection methods to explore their preliminary impressions in greater depth and/or actively pursue confirming and disconfirming perspectives. If such changes occur during the research process, authors should describe how and why study procedures changed in response to evolving study findings.\nThe study period (start and end dates for data collection and analysis) should be identified so that readers can place the study in temporal context and identify factors not mentioned by the authors that might affect findings, interpretation, and implications. (See Item 8 for ending data collection.)\nAuthors should describe important characteristics of the individuals conducting interviews, observations or focus groups, and methods used to train these individuals. This information clarifies the relationship between the individuals involved in data collection and the participants in the research and also explains what efforts were made to ensure consistency in the data collection process (See Items 6 and 15.)"
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-data-collection-methods-2",
    "href": "guidelines/SRQR-slim/index.html#sec-data-collection-methods-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nFurther, it was decided that group interviews, also known as focus group discussions would be the best means of data collection. This is a method of data collection that enables group members to feed off each other’s ideas and an effective moderator will maintain group focus whilst at the same time permitting flexibility in the direction those aspects of the discussion might take. [REF]\n\n\nAdjustments to the interview protocol were made according to early experience and information participants had provided (i.e. redundant questions were eliminated; questions were reworded to improve flow and clarity; additional probes were included).\n\n\nFaculty staff were then interviewed individually by a trained study investigator in a 15-minute, semi-structured interview. This sequence was repeated with other video encounters. Table 2 presents examples of interview questions. Each faculty member was interviewed by at least three interviewers over their various interviews. Interviewers were chosen based on their experience in interviewing. All were trained during a half-day meeting to interpret and deliver the interview guide in the same manner in order to elicit information of a consistent type.\n\nBack to top”"
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-data-collection-instruments-1",
    "href": "guidelines/SRQR-slim/index.html#sec-data-collection-instruments-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Read more",
    "text": "Read more\nData collection for qualitative research draws upon a variety of instruments or tools, including (but not limited to) interview or focus group guides, observational protocols and prompts for field notes, and data extraction or coding protocols for selection and analysis of documents, photographs, videos, or other artifacts. The authors should describe all such instruments, guides, and protocols, including their development and cite relevant literatures, theories or conceptual frameworks as appropriate. It is often helpful for authors to provide access to the data collection instrument(s) or a detailed description of them.\nThe authors should also describe the use of equipment for audio or video recording, reproduction of paper documents or computer files, or other processes in data collection. This is relevant so readers understand the full context in which data collection occurred and how this context might have affected data collection (e.g., the influence of recording devices on participants’ behaviors; the nature of inferences drawn from live vs. recorded events)."
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-data-collection-instruments-2",
    "href": "guidelines/SRQR-slim/index.html#sec-data-collection-instruments-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nTo facilitate the discussion and to maintain consistency over different sets of discussions, key trigger questions were devised prior to the discussion. The opening trigger question was: Thinking back to some of your best clinical learning placements in 4th and 5th year. What was it about those clinical placements that provided good opportunities for learning?\n\n\nInterviews included discussion of the expectations, processes and consequences of AEE [authentic early experiences]. The interview schedule was derived following identification of questions that could not be fully answered in a systematic review of previous empirical or theoretical literature. It comprised a sequence of topic areas including experiences in action, and areas of frustration in Medical Education such as the learning of content knowledge, achieving functional knowledge, and transfer of knowledge.[REF]… Interested readers can request a copy of the schedules from the corresponding author. Interviews lasted between 20 and 90 min. and discussion groups between 60 and 90 min. All interviews and discussion groups were conducted in private rooms at the participant’s workplace—the medical school for students and faculty, and individual places of work for workplace supervisors (except for one who chose to be interviewed at the medical school). All data were audio-recorded, and transcribed verbatim.\n\nBack to top”"
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-units-of-study-1",
    "href": "guidelines/SRQR-slim/index.html#sec-units-of-study-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Read more",
    "text": "Read more\nAuthors should describe the participants, documents, or events included in the study (the units of study). The sampling item (Item 8) describes the target or ideal participants, documents, or events selected for the study. By contrast, Item 12 focuses on description of the actual participants, documents or events included in the study. Authors should describe how the actual participants, documents, or events differ from the targeted sample, why these differences may have occurred, and how this might affect the findings.\nAuthors should describe characteristics of the participants, documents or events that are relevant to the study purpose and research question(s).1 For participants, this might include age, gender, profession, institution, year of training, or relationship to the researcher and/or other participants in the study. For documents, this might include the source, intended audience, date, or type of document. For events, this might include the location, date(s), length, characteristics of attendees or participants in the event, or mood or emotional climate. If the degree of participation varied among individuals (e.g., multiple occasions; interviews and observations), the authors should describe different levels of participation. For example, if some participants were observed and interviewed and others only interviewed, or if some participants completed multiple interviews and others completed a single interview, these variations should be explained. Authors should also explain the reasons for these differences (i.e., the researchers’ choice or the participants’ preferences) and how these different levels of participation were taken into account in the analysis. Authors should also include the dates or timeframe for participation.\nThis information about participants could appear in the Methods section as part of the description of the sample, or at the beginning of the Results section to provide context for the findings presented."
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-units-of-study-2",
    "href": "guidelines/SRQR-slim/index.html#sec-units-of-study-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nOf the 70 Mindful Communication program participants, 46 met the eligibility requirements to participate in the in-depth interviews. We randomly chose and then contacted 22 participants, of whom 20 agreed to be interviewed within six months of completing the program: 15 in person and 5 by telephone. Two declined for lack of time. On reaching saturation after 20 interviews, no further attempts to contact the remaining 24 participants were made.\n\n\n\n\n\nThere were 31 nursing handovers covering 137 patients, and 21 resident handovers covering 101 patients included in this study.\n\nBack to top”"
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-data-processing-1",
    "href": "guidelines/SRQR-slim/index.html#sec-data-processing-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Read more",
    "text": "Read more\nAuthors should describe the ways in which data are prepared for analysis and managed throughout the analysis process. These activities might include transcription, coding, data entry, and organization of data. When processing audio or video recordings, relevant details might include indication of verbatim transcription of dialogue, additional notes to capture non- verbal information (especially for group interviews or focus groups), and annotations to indicate vocal inflections and utterances, as appropriate for the analytic approach. Authors should also describe procedures used to check transcripts for accuracy.\nAuthors should describe the processes used to organize, compile, and categorize data (e.g., field notes, transcripts, documents, photographs, artifacts) for analysis.\nAuthors should describe procedures used to maintain data security and protect the privacy of participants, as specified in the human subjects protocols. For example, if data are anonymized, the authors should explain how and at what point in the process this occurred. Authors may choose to use anonymous labels or identifiers to represent quotes or excerpts from unique participants, documents or events, in order to reflect the variety of sources from which such data were derived."
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-data-processing-2",
    "href": "guidelines/SRQR-slim/index.html#sec-data-processing-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nInterviews were anonymised and each participant was given a code number.\n\n\nThe interviewers and another member of the research team (H.B.) reviewed transcripts for accuracy.\n\n\nWe collected data throughout the admission process through direct observation, audio- recording, and chart extraction. We audio-recorded, transcribed, and anonymized both the overnight and morning case review discussions. We also observed the morning case review discussions in person and collected field notes. For each case review discussion, we copied the admission notes from the patient’s record and de-identified all data.\n\nBack to top”"
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-data-analysis-1",
    "href": "guidelines/SRQR-slim/index.html#sec-data-analysis-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Read more",
    "text": "Read more\nTechniques used for data analysis will depend on the paradigm, approach, and/or data collection methods selected by the researchers. Correspondingly, authors should be as transparent as possible about the analytic process so that readers can follow the logic of inquiry from the research question(s) to the analysis and findings. For example, this description may involve characterizing the processes and decisions made for initial classification or segmentation of data, pattern identification and description, and/or development of in-depth interpretations.1 If the researchers used an approach that has a well-defined process for data analysis (e.g., grounded theory, discourse analysis, phenomenography), the authors should cite the guiding literature and describe their processes in sufficient detail so readers can judge the extent to which the processes align with the guiding approach. If modification to or deviations from the guiding approach occurred, the authors should explain and justify these modifications.\nAuthors should specify the unit of analysis.1 In qualitative research, the unit of analysis is not necessarily the same as the unit of sampling (e.g., individual participants or events). Instead, some approaches use specific events as the unit of analysis, such as mentions of a particular topic or experience, or observations of a particular behavior or phenomenon, while others use groups rather than individual group participants. This specification has implications for how the data are organized and analyzed as well as the inferences drawn from the data.\nAuthors should explain the rationale underlying different decisions made throughout the data analysis process to provide as much transparency as possible. In some approaches researchers use memoing or bracketing to make their reflections, interpretations, and links among passages explicit and more transparent to others. In some types of analysis, participants’ perspectives or observations that contrast or deviate from the concepts or themes identified by the researchers are an important part of the analysis. In such cases, the authors must describe how these discrepancies were handled during the analysis.\nDuring the analysis process, researchers may draw upon a theoretical perspective or framework, which may have been identified early in the conception of the study or may be identified by the researchers after reviewing some or all of their data. Either way, the authors should describe theoretical or other influences on their analysis scheme or categories if they exist. Sometimes these are referred to as sensitizing concepts to acknowledge that the approach is inductive, but with influence from relevant theory, models, or organizational schemes. Alternatively, themes may be developed from the data with no external influences.\nAuthors should describe which members of the research team are involved in data analysis and what perspective(s) they bring to the analysis. Authors should also indicate if any software was used to assist with data analysis and how it was used (e.g., used to apply codes after the final coding scheme was developed; to extract coded passages for further synthesis and identification of themes; or to identify passages with key words). Simply stating that software was used is insufficient."
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-data-analysis-2",
    "href": "guidelines/SRQR-slim/index.html#sec-data-analysis-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\n…we brought sensitizing concepts to the analysis while we conducted an open, inductive analysis.[REF]In this case the sensitizing concepts arose, a priori to analysis, from a framework derived from the literature [REF] (as described above), in which participants’ motivations to act are based on principles of professionalism, internal affect, or potential implications of their actions.[REF]\n\n\nThrough an iterative process of listening, discussing, and relistening, the team identified and consensually validated emerging themes[REF] and appended segments of dialogue supporting the proposed themes. Recruitment stopped when saturation was reached (no new themes were identified). The team systematically reviewed the themes and sorted them into content domains. The team used an analytic matrix to identify patterns and connections amongst the domains. Two of us not involved in the qualitative coding process (R.E., M.K.) audited the analytic matrix, choice of quotes, and thematic analysis.\n\n\nThe analysis started after the first interview. All data were analyzed with the aid of the audio- coding facility of the NVivo 8: QSR International Pty Ltd, Doncaster, Vic, Australia programme. First, [name] and [name] coded independently from one another, making sure to stay semantically close to the participants’ wording. Then we discussed these open codes and defined axial codes.[REF] New insights about the impact of CST were written down in memos.\n\n\nVideotapes were analysed using immersion/crystallisation methods of qualitative data analysis.[REF] With no pre-existing framework developed in advance for analysis, an inductive approach was used to discover patterns of NVB in the data. A team of six researchers met weekly for 18 months to view videos together. Using a consensus-building approach based on a combination of field notes, opportunistic interviews with the participants, and repeated viewing of the same material, sometimes many months apart, we eventually achieved consensus on verbal, non-verbal, and physical themes and patterns observed in the data. Finally, as a test of goodness-of-fit, we carefully reviewed the videotapes for any deviant cases that did not fit the categories we had developed.\n\n\nAll transcripts were coded thematically by four of the five authors, who met regularly to identify areas of convergence until full agreement was reached. One of the interviewers (P.M.) maintained an audit trail to track the team’s developing thinking. A process of dialectical empiricism[REF] was used to categorise the emergent themes into more abstract concepts…\n\nBack to top”"
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-trustworthiness-1",
    "href": "guidelines/SRQR-slim/index.html#sec-trustworthiness-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Read more",
    "text": "Read more\nAuthors should describe methods used to ensure trustworthiness and credibility throughout the data collection and analysis process. Such methods will depend on the paradigm, approach, and/or methods used. Correspondingly, the authors should explain their choice of techniques and why these are appropriate for the particular study.\nCommonly used techniques to enhance trustworthiness include: member checking; triangulation of data sources, methods, and/or researchers; creation of an explicit audit trail; and immersion in the site of data collection for an extended period of time (especially for research in which an observer’s presence is likely to disrupt the phenomenon under investigation). Member checking involves sharing findings, such as descriptions of key phenomena, themes, or an explanatory model, with participants and asking them to verify the accuracy or resonance with their perspectives. Triangulation involves using more than one data source, method, or researcher to add diverse perspectives on the findings of the study and, in some approaches, to test the transferability or generalizability of a model. An audit trail involves careful documentation of all decisions made throughout the study, from initial conceptualization to study design, sampling, analysis, and reporting, to provide transparency and to enable an external researcher to review all the steps involved in the study."
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-trustworthiness-2",
    "href": "guidelines/SRQR-slim/index.html#sec-trustworthiness-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\nMember checking\n\nMember checks [REF] with an external TBL expert (R.L.) supported the validity of these analyses.\n\nTriangulation of data types and data sources\n\nThe interview data were triangulated with the data of 11 student and supervisor focus groups of a previous study, and more specifically, with those data that concern in particular the influence of CST [Communication Skills Training] on the development of patient-centredness…. Triangulation with the focus group data allowed us to broaden the in-depth information from the interviews in the analysis and to share and compare this with information from students and doctors with varying levels of CST (no, limited, full programme) and from two universities (Universities of Antwerp and Ghent). Moreover, this enabled us to better explore the evolution over time, given that the focus groups included participants at different stages of their study: before clerkships (year 4, undergraduate), during clerkships (year 6, undergraduate), after clerkships (year 7, undergraduate) and postgraduate (general practice trainees, and supervising specialists and GPs; Table 1).\n\n\n\n\n\nFinally, as a test of goodness-of-fit, we carefully reviewed the videotapes for any deviant cases that did not fit the categories we had developed.\n\nTriangulation of Researchers + Audit trail\n\nTo ensure rigor and increase authenticity in our methodology, we used two kinds of triangulation—investigator triangulation and data triangulation.[REF] We sought analytical rigor using an audit trail and multiple coders; our coding team included an experienced clinician (M.G.) as well as a nonclinician with expertise in medical communication and team dynamics (L.L.).\n\nBack to top”"
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-synthesis-and-interpretation-1",
    "href": "guidelines/SRQR-slim/index.html#sec-synthesis-and-interpretation-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Read more",
    "text": "Read more\nIn qualitative research the distinction between results and discussion tends to blur because analysis often involves interpretation, inference, and synthesis. Although most journals require separate sections for Results and Discussion, many elements of Items 16-19 could reasonably be reported in either section. As such, we defer to authors and editors to determine where to report these essential elements.\nAuthors should identify the main analytic findings (e.g., interpretations, inferences, narratives, themes, models). The nature of these findings and how they are reported will depend on the approach and methodology selected and thus should be in alignment with the approach and methods.\nIn most cases, the authors should report a synthesis of their data along with specific quotes, examples, or illustrations derived from their data. Authors might also report frequency, variety, representativeness, counter-examples, concrete details, contextualization, conditions, and qualifications related to their findings. Frequency counts (e.g., the frequency of specific themes or codes) play a limited role in qualitative research, and need not be reported unless they play a meaningful role in interpretation of the data.\nFindings might also include integration with prior literature or theory and/or the development of a theory, model or meta-narrative. Judicious use of tables and figures can help communicate such findings."
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-synthesis-and-interpretation-2",
    "href": "guidelines/SRQR-slim/index.html#sec-synthesis-and-interpretation-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nWe identified four patterns of NVB (non-verbal behavior) that relate to handover quality and have dubbed them: (1) joint focus of attention; (2) the poker hand; (3) parallel play; and (4) kerbside consultation. Each pattern constitutes a transfix, or systematic way of participating non-verbally in the care transfer process. And, although there are variations in each pattern, we have been able to code virtually every handover we have observed in nursing, medicine and surgery into one of these four categories.\n\n\nBecause our participants came from similar educational backgrounds, had studied medicine as their tertiary course, were embedded in the culture of medicine, and were associated in meaningful ways with a single medical school, we approached their transcripts with the assumption that they belonged to a loosely formed discourse community. Although their graduation dates ranged over a period of 50 years and their collective sphere of practice included 10 different specialty areas, there were many similarities in their experiences of enculturation during and after medical school.\n\n\nTheir three major (often overlapping) areas of concern were epistemic (acquiring knowledge and skill), interpersonal (relating to patients, families, colleagues and administrators) and personal (achieving work–life balance). In each of these areas, medical enculturation was achieved by two overlapping processes, absorption and assimilation, each of which may have distinct implications for postgraduate medical education.\n\nBack to top”"
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-links-to-empirical-data-1",
    "href": "guidelines/SRQR-slim/index.html#sec-links-to-empirical-data-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Read more",
    "text": "Read more\nThe authors should provide evidence (e.g., quotes, field notes, text excerpts, photographs) to substantiate the more general and abstract concepts or inferences they present as findings. Such evidence is typically de-identified to protect the privacy of study participants, settings, and/or institutions. The evidence may be presented in a variety of ways such as in a table or figure, incorporated into a narrative description of findings, as a stand-alone narrative, or in text blocks embedded in the manuscript text. If word limits or media limitations (e.g.. video) limit the authors’ ability to provide sufficient representation of supporting data, an appendix, supplemental material, or web-based repository could be used to provide access to additional data."
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-links-to-empirical-data-2",
    "href": "guidelines/SRQR-slim/index.html#sec-links-to-empirical-data-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\nSee Frankel et al. for an excellent example of how to use photographs (or snapshots from video) to illustrate and provide supporting evidence for patterns of behavior identified in the analysis. http://qualitysafety.bmj.com/content/21/Suppl_1/i121.\n\nWe identified five interruption types: (1) probing for further data, (2) prompting for expected sequence, (3) teaching around the case, (4) thinking out loud, and (5) providing direction (see Table 1). Several interruption types served both goals of the case review discussions—teaching and patient care. For example, when thinking out loud, supervisors reasoned through problems and taught the team: So that’s the big question, did she have a mechanical fall, or did she have a medicine-related fall? (Case 2). Supervisors prompted for expected sequence, preventing presenters from skipping over information while simultaneously allowing the supervisor to instruct the team on presentation style: So now you can tell me what the rest of his test results are because I haven’t heard those (Case 16).\n\nBack to top”"
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-integration-with-prior-work-1",
    "href": "guidelines/SRQR-slim/index.html#sec-integration-with-prior-work-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Read more",
    "text": "Read more\nThe authors should begin the Discussion with a short summary of the main findings. The short summary reminds readers of the main findings and may help them assess whether the subsequent interpretation and implications formulated by the authors are supported by the findings.\nThe authors’ description of their findings or results should include some interpretation of the data in the context of previous findings, experiences, theory, or a guiding paradigm or approach. The discussion provides authors an opportunity to elaborate on their findings in relation to their research question(s) and study purpose(s); connect their findings to prior empirical work, theories, and/or frameworks; and discuss implications. The authors should explicitly describe how their findings contribute to or advance the field. Implications may include transferability, or specifying the appropriate scope for generalization of the findings beyond the study (e.g., to other settings, populations, time periods, circumstances)."
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-integration-with-prior-work-2",
    "href": "guidelines/SRQR-slim/index.html#sec-integration-with-prior-work-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nThis study contributes to the understanding and discussion of the complexity of involving patients in healthcare education. It shows that integrating patient-led teaching into initiatives that are partly faculty-led influences the way in which students perceive learning from and with PIs. What is not known, however, is whether perceptions are also affected by type of health profession and the students’ different orientation towards logics of care and science, and issues of authority and power relations.\n\nFor complete examples of Discussions, see:\n\nHenriksen & Ringsted, 2013\nWesterman et al., 2013.\n\nBack to top”"
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-limitations-1",
    "href": "guidelines/SRQR-slim/index.html#sec-limitations-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Read more",
    "text": "Read more\nAuthors should describe techniques used to ensure trustworthiness in the Methods section of the manuscript. In the Discussion, authors should identify problems or gaps in their efforts to ensure trustworthiness and the potential implications. For example, if researchers intended to interview individuals with certain characteristics, or who might offer different perspectives, but were unsuccessful in recruiting any willing participants, they should explain this issue and describe possible consequences for transferability. (See also Item 18.)\nAll research paradigms and approaches have strengths and weaknesses, and authors should explicitly discuss how the paradigm, approach, and methods they used will influence the situations to which their findings may reasonably apply. (See also Item 18.) In addition, they should explain how specific decisions or events in the conduct of the study strengthen or weaken the rigor of their findings."
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-limitations-2",
    "href": "guidelines/SRQR-slim/index.html#sec-limitations-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nThe study has several limitations. One is that the focus group interview method reveals students’ perceptions rather than their actual behaviors. Observations of the patient-led teaching encounter may have illuminated an understanding of the relationship between patient instructors and medical students. Another limitation is that the PI-led teaching is optional rather than mandatory, which may have influenced students’ attitudes in a positive direction. Moreover, students who are eager to take on extra-curricular activities may not be representative of the whole population. That only 23 out of 39 students signed up for this study might also have influenced results if the missing group of students represented other perceptions than those present in the focus groups. However the received data from the focus groups were rich in information and diverse perceptions were present. Another limitation is the overrepresentation of women over men in our sample. Even though women are also overrepresented in medical school this might potentially have influenced results, but gender differences in perceptions were nevertheless not identified in the data.\n\nBack to top”"
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-conflicts-of-interest-1",
    "href": "guidelines/SRQR-slim/index.html#sec-conflicts-of-interest-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Read more",
    "text": "Read more\nAuthors should identify any real or potential conflicts of interest that might have influenced or could appear to have influenced the research. Authors should also explain how these conflicts were managed in the conduct of the study, and describe the potential impact on study findings and/or conclusions. Some aspects may be mentioned as part of reflexivity (see Item 6)."
  },
  {
    "objectID": "guidelines/SRQR-slim/index.html#sec-funding-1",
    "href": "guidelines/SRQR-slim/index.html#sec-funding-1",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Read more",
    "text": "Read more\nThe authors should describe any sources of funding and other support for the study and the role of funders in data collection, data analysis, and reporting if applicable."
  },
  {
    "objectID": "guidelines/arrive/index.html#about-this-guideline",
    "href": "guidelines/arrive/index.html#about-this-guideline",
    "title": "The ARRIVE guideline for writing a study using laboratory animals.",
    "section": "About this guideline",
    "text": "About this guideline\n\nThis guideline is relevant to research performed on animals in a laboratory setting,"
  },
  {
    "objectID": "guidelines/arrive/index.html#download-resources",
    "href": "guidelines/arrive/index.html#download-resources",
    "title": "The ARRIVE guideline for writing a study using laboratory animals.",
    "section": "Download Resources",
    "text": "Download Resources\nReporting guidelines are best used early:\n\nUse a template to write quickly and avoid the blank page\nUse a checklist to double-check and reassure others that your work is described transparently\n\n\n\n\nFor drafting\n\nTemplate\n\n\n\nFor checking\n\nChecklist\n\n\n\nFor planning\n\nTo-do list"
  },
  {
    "objectID": "guidelines/arrive/index.html#guidance",
    "href": "guidelines/arrive/index.html#guidance",
    "title": "The ARRIVE guideline for writing a study using laboratory animals.",
    "section": "Guidance",
    "text": "Guidance\nApprox. 4 min read\n\nTitle\n\n\n1. Title\n\n\n\n\n\n\n\n\n\nProvide as accurate and concise a description of the content of the article as possible.\n\n\nAbstract\n\n\n2. Abstract\n\n\n\n\n\n\n\n\n\nProvide an accurate summary of the background, research objectives, including details of the species or strain of animal used, key methods, principal findings and conclusions of the study.\n\n\nIntroduction\n\n\n3a. Background\n\n\n\n\n\n\n\n\n\nInclude sufficient scientific background (including relevant references to previous work) to understand the motivation and context for the study, and explain the experimental approach and rationale.\n\n\n3b. Background\n\n\n\n\n\n\n\n\n\nExplain how and why the animal species and model being used can address the scientific objectives and, where appropriate, the study’s relevance to human biology\n\n\n4. Objectives\n\n\n\n\n\n\n\n\n\nClearly describe the primary and any secondary objectives of the study, or specific hypotheses being tested.\n\n\nMethods\n\n\n5. Ethical statement\n\n\n\n\n\n\n\n\n\nIndicate the nature of the ethical review permissions, relevant licences (e.g. Animal [Scientific Procedures] Act 1986), and national or institutional guidelines for the care and use of animals, that cover the research.\n\n\n6a. Study design\n\n\n\n\n\n\n\n\n\nFor each experiment, give brief details of the study design including the number of experimental and control groups. A time-line diagram or flow chart can be useful to illustrate how complex study designs were carried out.\n\n\n6b. Study design\n\n\n\n\n\n\n\n\n\nReport any steps taken to minimise the effects of subjective bias when allocating animals to treatment (e.g. randomisation procedure) and when assessing results (e.g. if done, describe who was blinded and when).\n\n\n6c. Study design\n\n\n\n\n\n\n\n\n\nDescribe the experimental unit (e.g. a single animal, group or cage of animals). A time-line diagram or flow chart can be useful to illustrate how complex study designs were carried out.\n\n\n7. Experimental procedures\n\n\n\n\n\n\n\n\n\nFor each experiment and each experimental group, including controls, provide precise details of all procedures carried out. For example:\n\nHow (e.g. drug formulation and dose, site and route of administration, anaesthesia and analgesia used [including monitoring], surgical procedure, method of euthanasia). Provide details of any specialist equipment used, including supplier(s).\nWhen (e.g. time of day).\nWhere (e.g. home cage, laboratory, water maze).\nWhy (e.g. rationale for choice of specific anaesthetic, route of administration, drug dose used)\n\n\n\n8a. Experimental animals\n\n\n\n\n\n\n\n\n\nProvide details of the animals used, including species, strain, sex, developmental stage (e.g. mean or median age plus age range) and weight (e.g. mean or median weight plus weight range).\n\n\n8b. Experimental animals\n\n\n\n\n\n\n\n\n\nProvide further relevant information such as the source of animals, international strain nomenclature, genetic modification status (e.g. knock-out or transgenic), genotype, health / immune status, drug or test naïve, previous procedures, etc\n\n\n9a. Housing and husbandry\n\n\n\n\n\n\n\n\n\nProvide details of housing (type of facility e.g. specific pathogen free [SPF]; type of cage or housing; bedding material; number of cage companions; tank shape and material etc. for fish).\n\n\n9b. Housing and husbandry\n\n\n\n\n\n\n\n\n\nReport husbandry conditions (e.g. breeding programme, light / dark cycle, temperature, quality of water etc for fish, type of food, access to food and water, environmental enrichment).\n\n\n9c. Housing and husbandry\n\n\n\n\n\n\n\n\n\nDescribe welfare-related assessments and interventions that were carried out prior to, during, or after the experiment\n\n\n10. Sample size\n\n\n\n\n\n\n\n\n\n\nSpecify the total number of animals used in each experiment, and the number of animals in each experimental group.\nExplain how the number of animals was arrived at. Provide details of any sample\n\nsize calculation used.\n\nIndicate the number of independent replications of each experiment, if relevant.\n\n\n\n11. Allocating animals to experimental groups\n\n\n\n\n\n\n\n\n\n\nGive full details of how animals were allocated to experimental groups, including randomisation or matching if done.\nDescribe the order in which the animals in the different experimental groups were treated and assessed.\n\n\n\n12. Experimental outcomes\n\n\n\n\n\n\n\n\n\nClearly define the primary and secondary experimental outcomes assessed (e.g. cell death, molecular markers, behavioural changes).\n\n\n13. Statistical methods\n\n\n\n\n\n\n\n\n\n\nProvide details of the statistical methods used for each analysis.\nSpecify the unit of analysis for each dataset (e.g. single animal, group of animals,\n\nsingle neuron).\n\nDescribe any methods used to assess whether the data met the assumptions\n\nof the statistical approach\n\n\nResults\n\n\n14. Baseline data\n\n\n\n\n\n\n\n\n\nFor each experimental group, report relevant characteristics and health status of animals (e.g. weight, microbiological status, and drug or test naïve) prior to treatment or testing (this information can often be tabulated).\n\n\n15. Numbers analysed\n\n\n\n\n\n\n\n\n\n\nReport the number of animals in each group included in each analysis. Report absolute numbers (e.g. 10 / 20, not 50%2).\nIf any animals or data were not included in the analysis, explain why.\n\n\n\n16. Outcomes and estimation\n\n\n\n\n\n\n\n\n\nReport the results for each analysis carried out, with a measure of precision (e.g. standard error or confidence interval).\n\n\n17. Adverse events\n\n\n\n\n\n\n\n\n\n\nGive details of all important adverse events in each experimental group.\nDescribe any modifications to the experimental protocols made to reduce adverse events.\n\n\n\nDiscussion\n\n\n18a. Interpretation / scientific implications\n\n\n\n\n\n\n\n\n\nInterpret the results, taking into account the study objectives and hypotheses, current theory and other relevant studies in the literature.\n\n\n18b. Interpretation / scientific implications\n\n\n\n\n\n\n\n\n\nComment on the study limitations including any potential sources of bias, any limitations of the animal model, and the imprecision associated with the results\n\n\n18c. Interpretation / scientific implications\n\n\n\n\n\n\n\n\n\nDescribe any implications of your experimental methods or findings for the\nreplacement, refinement or reduction (the 3Rs) of the use of animals in research.\n\n\n19. Generalisability / translation\n\n\n\n\n\n\n\n\n\nComment on whether, and how, the findings of this study are likely to translate to other species or systems, including any relevance to human biology.\n\n\n20. Funding\n\n\n\n\n\n\n\n\n\nList all funding sources (including grant number) and the role of the funder(s) in the study."
  },
  {
    "objectID": "guidelines/care/index.html#about-this-guideline",
    "href": "guidelines/care/index.html#about-this-guideline",
    "title": "The CARE guideline for writing a case report or case series.",
    "section": "About this guideline",
    "text": "About this guideline\n\nThis guideline is relevant to case reports and case series,"
  },
  {
    "objectID": "guidelines/care/index.html#download-resources",
    "href": "guidelines/care/index.html#download-resources",
    "title": "The CARE guideline for writing a case report or case series.",
    "section": "Download Resources",
    "text": "Download Resources\nReporting guidelines are best used early:\n\nUse a template to write quickly and avoid the blank page\nUse a checklist to double-check and reassure others that your work is described transparently\n\n\n\n\nFor drafting\n\nTemplate\n\n\n\nFor checking\n\nChecklist\n\n\n\nFor planning\n\nTo-do list"
  },
  {
    "objectID": "guidelines/care/index.html#guidance",
    "href": "guidelines/care/index.html#guidance",
    "title": "The CARE guideline for writing a case report or case series.",
    "section": "Guidance",
    "text": "Guidance\nApprox. 1 min read\n\nTitle\n\n\n1. Title\n\n\n\n\n\n\n\nThe area of focus and case report should appear in the title\n\n\nKeywords\n\n\n2. Keywords\n\n\n\n\n\n\n\nTwo to five key words that identify topics in this case report\n\n\nAbstract\n\n\n3a. Introduction\n\n\n\n\n\n\n\nWhat is unique and why is it important?\n\n\n3b. Abstract\n\n\n\n\n\n\n\nThe patient’s main concerns and important clinical findings.\n\n\n3c. Abstract\n\n\n\n\n\n\n\nThe main diagnoses, interventions, and outcomes.\n\n\n3d. Conclusion\n\n\n\n\n\n\n\nWhat are one or more take-away lessons?\n\n\nIntroduction\n\n\n4. Introduction\n\n\n\n\n\n\n\nBriefly summarize why this case is unique with medical literature references.\n\n\nPatient information\n\n\n5a. Patient information\n\n\n\n\n\n\n\nDe-identified demographic and other patient information.\n\n\n5b. Patient information\n\n\n\n\n\n\n\nMain concerns and symptoms of the patient.\n\n\n5c. Patient information\n\n\n\n\n\n\n\nMedical, family, and psychosocial history including genetic information.\n\n\n5d. Patient information\n\n\n\n\n\n\n\nRelevant past interventions and their outcomes.\n\n\nClinical findings\n\n\n6. Clinical findings\n\n\n\n\n\n\n\nRelevant physical examination (PE) and other clinical findings.\n\n\nTimeline\n\n\n7. Timeline\n\n\n\n\n\n\n\nRelevant data from this episode of care organized as a timeline (figure or table).\n\n\nDiagnostic assessment\n\n\n8a. Diagnostic assessment\n\n\n\n\n\n\n\nDiagnostic methods (PE, laboratory testing, imaging, surveys).\n\n\n8b. Diagnostic assessment\n\n\n\n\n\n\n\nDiagnostic challenges.\n\n\n8c. Diagnostic assessment\n\n\n\n\n\n\n\nDiagnostic reasoning including differential diagnosis\n\n\n8d. Diagnostic assessment\n\n\n\n\n\n\n\nPrognostic characteristics when applicable\n\n\nTherapeutic Intervention\n\n\n9a. Therapeutic Intervention\n\n\n\n\n\n\n\nTypes of intervention (pharmacologic, surgical, preventive).\n\n\n9b. Therapeutic Intervention\n\n\n\n\n\n\n\nAdministration of intervention (dosage, strength, duration)\n\n\n9c. Therapeutic Intervention\n\n\n\n\n\n\n\nChanges in the interventions with explanations.\n\n\nFollow up and outcomes\n\n\n10a. Follow up and outcomes\n\n\n\n\n\n\n\nClinician and patient-assessed outcomes when appropriate\n\n\n10b. Follow up and outcomes\n\n\n\n\n\n\n\nImportant follow-up diagnostic and other test results.\n\n\n10c. Follow up and outcomes\n\n\n\n\n\n\n\nIntervention adherence and tolerability (how was this assessed)?\n\n\n10d. Follow up and outcomes\n\n\n\n\n\n\n\nAdverse and unanticipated events.\n\n\nDiscussion\n\n\n11a. Discussion\n\n\n\n\n\n\n\nStrengths and limitations in your approach to this case.\n\n\n11b. Discussion\n\n\n\n\n\n\n\nDiscussion of the relevant medical literature.\n\n\n11c. Discussion\n\n\n\n\n\n\n\nThe rationale for your conclusions.\n\n\n11d. Discussion\n\n\n\n\n\n\n\nThe primary take-away lessons from this case report.\n\n\nPatient perspective\n\n\n12. Patient perspective\n\n\n\n\n\n\n\nThe patient can share their perspective on their case\n\n\nInformed consent\n\n\n13. Informed consent\n\n\n\n\n\n\n\nThe patient should give informed consent."
  },
  {
    "objectID": "guidelines/cheers/index.html#about-this-guideline",
    "href": "guidelines/cheers/index.html#about-this-guideline",
    "title": "The CHEERS guideline for writing a economic evaluation of health interventions",
    "section": "About this guideline",
    "text": "About this guideline\n\nThis guideline is relevant to economic evaluations of health interventions"
  },
  {
    "objectID": "guidelines/cheers/index.html#download-resources",
    "href": "guidelines/cheers/index.html#download-resources",
    "title": "The CHEERS guideline for writing a economic evaluation of health interventions",
    "section": "Download Resources",
    "text": "Download Resources\nReporting guidelines are best used early:\n\nUse a template to write quickly and avoid the blank page\nUse a checklist to double-check and reassure others that your work is described transparently\n\n\n\n\nFor drafting\n\nTemplate\n\n\n\nFor checking\n\nChecklist\n\n\n\nFor planning\n\nTo-do list"
  },
  {
    "objectID": "guidelines/cheers/index.html#guidance",
    "href": "guidelines/cheers/index.html#guidance",
    "title": "The CHEERS guideline for writing a economic evaluation of health interventions",
    "section": "Guidance",
    "text": "Guidance\nApprox. 3 min read\n\nTitle\n\n\n1. Title\n\n\n\n\n\n\n\nIdentify the study as an economic evaluation or use more specific terms such as cost-effectiveness analysis, and describe the interventions compared.\n\n\nAbstract\n\n\n2. Abstract\n\n\n\n\n\n\n\nProvide a structured summary of objectives, perspective, setting, methods (including study design and inputs), results (including base case and uncertainty analyses), and conclusions\n\n\nIntroduction\n\n\n3. Background and objectives\n\n\n\n\n\n\n\nProvide an explicit statement of the broader context for the study. Present the study question and its relevance for health policy or practice decisions\n\n\nMethods\n\n\n4. Target population and subgroups\n\n\n\n\n\n\n\nDescribe characteristics of the base case population and subgroups analysed, including why they were chosen.\n\n\n5. Setting and location\n\n\n\n\n\n\n\nState relevant aspects of the system(s) in which the decision(s) need(s) to be made.\n\n\n6. Study perspective\n\n\n\n\n\n\n\nDescribe the perspective of the study and relate this to the costs being evaluated.\n\n\n7. Comparators\n\n\n\n\n\n\n\nDescribe the interventions or strategies being compared and state why they were chosen.\n\n\n8. Time horizon\n\n\n\n\n\n\n\nState the time horizon(s) over which costs and consequences are being evaluated and say why appropriate.\n\n\n9. Discount rate\n\n\n\n\n\n\n\nReport the choice of discount rate(s) used for costs and outcomes and say why appropriate\n\n\n10. Choice of health outcomes\n\n\n\n\n\n\n\nDescribe what outcomes were used as the measure(s) of benefit in the evaluation and their relevance for the type of analysis performed\n\n\n11a. Meaurement of effectiveness\n\n\n\n\n\n\n\nSingle study-based estimates: Describe fully the design features of the single effectiveness study and why the single study was a sufficient source of clinical effectiveness data\n\n\n11b. Measurement of effectiveness\n\n\n\n\n\n\n\nSynthesis-based estimates: Describe fully the methods used for identification of included studies and synthesis of clinical effectiveness data\n\n\n12. Measurement and valuation of preference based outcomes\n\n\n\n\n\n\n\nIf applicable, describe the population and methods used to elicit preferences for outcomes.\n\n\nEstimating resources\nand costs\n\n\n13a. Estimating resources and costs\n\n\n\n\n\n\n\nSingle study-based economic evaluation: Describe approaches used to estimate resource use associated with the alternative interventions. Describe primary or secondary research methods for valuing each resource item in terms of its unit cost. Describe any adjustments made to approximate to opportunity costs\n\n\nMethods\n\n\n13b. Estimating resources and costs\n\n\n\n\n\n\n\nModel-based economic evaluation: Describe approaches and data sources used to estimate resource use associated with model health states. Describe primary or secondary research methods for valuing each resource item in terms of its unit cost. Describe any adjustments made to approximate to\nopportunity costs.\n\n\n14. Currency, price date, and conversion\n\n\n\n\n\n\n\nReport the dates of the estimated resource quantities and unit costs. Describe methods for adjusting estimated unit costs to the year of reported costs if necessary. Describe methods for converting costs into a common currency base and the exchange rate.\n\n\n15. Choice of model\n\n\n\n\n\n\n\nDescribe and give reasons for the specific type of decision analytical model used. Providing a figure to show model structure is strongly recommended.\n\n\n16. Assumptions\n\n\n\n\n\n\n\nDescribe all structural or other assumptions underpinning the decision-analytical model.\n\n\n17. Analytical methods\n\n\n\n\n\n\n\nDescribe all analytical methods supporting the evaluation. This could include methods for dealing with skewed, missing, or censored data; extrapolation methods; methods for pooling data; approaches to validate or make adjustments (such as half cycle corrections) to a model; and methods for handling population heterogeneity and uncertainty.\n\n\nResults\n\n\n18. Study parameters\n\n\n\n\n\n\n\nReport the values, ranges, references, and, if used, probability distributions for all parameters. Report reasons or sources for distributions used to represent uncertainty where appropriate. Providing a table to show the input values is strongly recommended.\n\n\n19. Incremental costs and outcomes\n\n\n\n\n\n\n\nFor each intervention, report mean values for the main categories of estimated costs and outcomes of interest, as well as mean differences between the comparator groups. If applicable, report incremental cost-effectiveness ratios.\n\n\n20a. Characterising uncertainty\n\n\n\n\n\n\n\nSingle study-based economic evaluation: Describe the effects of sampling uncertainty for the estimated incremental cost and incremental effectiveness parameters, together with the impact of methodological assumptions (such as discount rate, study perspective).\n\n\n20b. Characterising uncertainty\n\n\n\n\n\n\n\nModel-based economic evaluation: Describe the effects on the results of uncertainty for all input parameters, and uncertainty related to the structure of the model and assumptions.\n\n\n21. Characterising heterogeneity\n\n\n\n\n\n\n\nIf applicable, report differences in costs, outcomes, or cost effectiveness that can be explained by variations between subgroups of patients with different baseline characteristics or other observed variability in effects that are not reducible by\nmore information.\n\n\nDiscussion\n\n\n22. Study findings, limitations, generalisability, and current knowledge\n\n\n\n\n\n\n\nSummarise key study findings and describe how they support the conclusions reached. Discuss limitations and the generalisability of the findings and how the findings fit with current knowledge.\n\n\nOther\n\n\n23. Source of funding\n\n\n\n\n\n\n\nDescribe how the study was funded and the role of the funder in the identification, design, conduct, and reporting of the analysis. Describe other non-monetary sources of support\n\n\n24. Conflict of interest\n\n\n\n\n\n\n\nDescribe any potential for conflict of interest of study contributors in accordance with journal policy. In the absence of a journal policy, we recommend authors comply with International Committee of Medical Journal Editors recommendations"
  },
  {
    "objectID": "guidelines/consort/index.html#about-this-guideline",
    "href": "guidelines/consort/index.html#about-this-guideline",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "About this guideline",
    "text": "About this guideline\n\nThis guideline is relevant to randomized trials of medical interventions"
  },
  {
    "objectID": "guidelines/consort/index.html#download-resources",
    "href": "guidelines/consort/index.html#download-resources",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Download Resources",
    "text": "Download Resources\nReporting guidelines are best used early:\n\nUse a template to write quickly and avoid the blank page \nUse a checklist to double-check and reassure others that your article describes all the important details of your work\n\n\n\n\nFor drafting\n\nTemplate\n\n\n\nFor checking\n\nChecklist\n\n\n\nFor planning\n\nTo-do list"
  },
  {
    "objectID": "guidelines/consort/index.html#guidance",
    "href": "guidelines/consort/index.html#guidance",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Guidance",
    "text": "Guidance\nApprox. 2 min read\n\nTitle and Abstract\n\n\n1a. Title\n\n\n\n\n\n\n\nIdentification as a randomized trial in the title.\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe ability to identify a report of a randomised trial in an electronic database depends to a large extent on how it was indexed. Indexers may not classify a report as a randomised trial if the authors do not explicitly report this information.64 To help ensure that a study is appropriately indexed and easily identified, authors should use the word randomised in the title to indicate that the participants were randomly assigned to their comparison groups\n\n\nExamples\n\nSmoking reduction with oral nicotine inhalers: double blind, randomised clinical trial of efficacy and safety\n\nBack to top\n\n\n\n\n\n\n1b. Abstract\n\n\n\n\n\n\n\nStructured summary of trial design, methods, results, and conclusions\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nClear, transparent, and sufficiently detailed abstracts are important because readers often base their assessment of a trial on such information. Some readers use an abstract as a screening tool to decide whether to read the full article. However, as not all trials are freely available and some health professionals do not have access to the full trial reports, healthcare decisions are sometimes made on the basis of abstracts of randomised trials.66\nA journal abstract should contain sufficient information about a trial to serve as an accurate record of its conduct and findings, providing optimal information about the trial within the space constraints and format of a journal. A properly constructed and written abstract helps individuals to assess quickly the relevance of the findings and aids the retrieval of relevant reports from electronic databases.67 The abstract should accurately reflect what is included in the full journal article and should not include information that does not appear in the body of the paper. Studies comparing the accuracy of information reported in a journal abstract with that reported in the text of the full publication have found claims that are inconsistent with, or missing from, the body of the full article.68 69 70 71 Conversely, omitting important harms from the abstract could seriously mislead someone’s interpretation of the trial findings.42 72\nA recent extension to the CONSORT statement provides a list of essential items that authors should include when reporting the main results of a randomised trial in a journal (or conference) abstract (see table 2​2).45 We strongly recommend the use of structured abstracts for reporting randomised trials. They provide readers with information about the trial under a series of headings pertaining to the design, conduct, analysis, and interpretation.73 Some studies have found that structured abstracts are of higher quality than the more traditional descriptive abstracts74 75 and that they allow readers to find information more easily.76 We recognise that many journals have developed their own structure and word limit for reporting abstracts. It is not our intention to suggest changes to these formats, but to recommend what information should be reported\n\n\nExamples\n\nFor specific guidance see CONSORT for abstracts\n\nBack to top\n\n\n\n\n\n\nIntroduction\n\n\n2a. Background and objectives\n\n\n\n\n\n\n\nScientific background and explanation of rationale\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nTypically, the introduction consists of free flowing text, in which authors explain the scientific background and rationale for their trial, and its general outline. It may also be appropriate to include here the objectives of the trial (see item 2b).The rationale may be explanatory (for example, to assess the possible influence of a drug on renal function) or pragmatic (for example, to guide practice by comparing the benefits and harms of two treatments). Authors should report any evidence of the benefits and harms of active interventions included in a trial and should suggest a plausible explanation for how the interventions might work, if this is not obvious.78\nThe Declaration of Helsinki states that biomedical research involving people should be based on a thorough knowledge of the scientific literature.79 That is, it is unethical to expose humans unnecessarily to the risks of research. Some clinical trials have been shown to have been unnecessary because the question they addressed had been or could have been answered by a systematic review of the existing literature.80 81 Thus, the need for a new trial should be justified in the introduction. Ideally, it should include a reference to a systematic review of previous similar trials or a note of the absence of such trials\n\n\nExamples\n\nSurgery is the treatment of choice for patients with disease stage I and II non-small cell lung cancer (NSCLC) … An NSCLC meta-analysis combined the results from eight randomised trials of surgery versus surgery plus adjuvant cisplatin-based chemotherapy and showed a small, but not significant (p=0.08), absolute survival benefit of around 5% at 5 years (from 50% to 55%). At the time the current trial was designed (mid-1990s), adjuvant chemotherapy had not become standard clinical practice … The clinical rationale for neo-adjuvant chemotherapy is three-fold: regression of the primary cancer could be achieved thereby facilitating and simplifying or reducing subsequent surgery; undetected micro-metastases could be dealt with at the start of treatment; and there might be inhibition of the putative stimulus to residual cancer by growth factors released by surgery and by subsequent wound healing … The current trial was therefore set up to compare, in patients with resectable NSCLC, surgery alone versus three cycles of platinum-based chemotherapy followed by surgery in terms of overall survival, quality of life, pathological staging, resectability rates, extent of surgery, and time to and site of relapse\n\nBack to top\n\n\n\n\n\n\n2b. Background and objectives\n\n\n\n\n\n\n\nSpecific objectives or hypothesis\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nObjectives are the questions that the trial was designed to answer. They often relate to the efficacy of a particular therapeutic or preventive intervention. Hypotheses are pre-specified questions being tested to help meet the objectives. Hypotheses are more specific than objectives and are amenable to explicit statistical evaluation. In practice, objectives and hypotheses are not always easily differentiated. Most reports of RCTs provide adequate information about trial objectives and hypotheses\n\n\nExamples\n\nIn the current study we tested the hypothesis that a policy of active management of nulliparous labour would: 1. reduce the rate of caesarean section, 2. reduce the rate of prolonged labour; 3. not influence maternal satisfaction with the birth experience\n\nBack to top\n\n\n\n\n\n\nMethods\n\n\n3a. Trial design\n\n\n\n\n\n\n\nDescription of trial design (such as parallel, factorial) including allocation ratio.\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe word design is often used to refer to all aspects of how a trial is set up, but it also has a narrower interpretation. Many specific aspects of the broader trial design, including details of randomisation and blinding, are addressed elsewhere in the CONSORT checklist. Here we seek information on the type of trial, such as parallel group or factorial, and the conceptual framework, such as superiority or non-inferiority, and other related issues not addressed elsewhere in the checklist.\nThe CONSORT statement focuses mainly on trials with participants individually randomised to one of two parallel groups. In fact, little more than half of published trials have such a design.16 The main alternative designs are multi-arm parallel, crossover, cluster,40 and factorial designs. Also, most trials are set to identify the superiority of a new intervention, if it exists, but others are designed to assess non-inferiority or equivalence.39 It is important that researchers clearly describe these aspects of their trial, including the unit of randomisation (such as patient, GP practice, lesion). It is desirable also to include these details in the abstract (see item 1b).\nIf a less common design is employed, authors are encouraged to explain their choice, especially as such designs may imply the need for a larger sample size or more complex analysis and interpretation.\nAlthough most trials use equal randomisation (such as 1:1 for two groups), it is helpful to provide the allocation ratio explicitly. For drug trials, specifying the phase of the trial (I-IV) may also be relevant\n\n\nExamples\n\nThis was a multicenter, stratified (6 to 11 years and 12 to 17 years of age, with imbalanced randomisation [2:1]), double-blind, placebo-controlled, parallel-group study conducted in the United States (41 sites\n\nBack to top\n\n\n\n\n\n\n3b. Trial design\n\n\n\n\n\n\n\nImportant changes to methods after trial commencement (such as eligibility criteria), with reasons\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nA few trials may start without any fixed plan (that is, are entirely exploratory), but the most will have a protocol that specifies in great detail how the trial will be conducted. There may be deviations from the original protocol, as it is impossible to predict every possible change in circumstances during the course of a trial. Some trials will therefore have important changes to the methods after trial commencement.\nChanges could be due to external information becoming available from other studies, or internal financial difficulties, or could be due to a disappointing recruitment rate. Such protocol changes should be made without breaking the blinding on the accumulating data on participants’ outcomes. In some trials, an independent data monitoring committee will have as part of its remit the possibility of recommending protocol changes based on seeing unblinded data. Such changes might affect the study methods (such as changes to treatment regimens, eligibility criteria, randomisation ratio, or duration of follow-up) or trial conduct (such as dropping a centre with poor data quality).87\nSome trials are set up with a formal adaptive design. There is no universally accepted definition of these designs, but a working definition might be a multistage study design that uses accumulating data to decide how to modify aspects of the study without undermining the validity and integrity of the trial.88 The modifications are usually to the sample sizes and the number of treatment arms and can lead to decisions being made more quickly and with more efficient use of resources. There are, however, important ethical, statistical, and practical issues in considering such a design.89 90\nWhether the modifications are explicitly part of the trial design or in response to changing circumstances, it is essential that they are fully reported to help the reader interpret the results. Changes from protocols are not currently well reported. A review of comparisons with protocols showed that about half of journal articles describing RCTs had an unexplained discrepancy in the primary outcomes.57 Frequent unexplained discrepancies have also been observed for details of randomisation, blinding,91 and statistical analyses\n\n\nExamples\n\nPatients were randomly assigned to one of six parallel groups, initially in 1:1:1:1:1:1 ratio, to receive either one of five otamixaban … regimens … or an active control of unfractionated heparin … an independent Data Monitoring Committee reviewed unblinded data for patient safety; no interim analyses for efficacy or futility were done. During the trial, this committee recommended that the group receiving the lowest dose of otamixaban (0·035 mg/kg/h) be discontinued because of clinical evidence of inadequate anticoagulation. The protocol was immediately amended in accordance with that recommendation, and participants were subsequently randomly assigned in 2:2:2:2:1 ratio to the remaining otamixaban and control groups, respectively\n\nBack to top\n\n\n\n\n\n\n4a. Participants\n\n\n\n\n\n\n\nEligibility criteria for participants\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nA comprehensive description of the eligibility criteria used to select the trial participants is needed to help readers interpret the study. In particular, a clear understanding of these criteria is one of several elements required to judge to whom the results of a trial apply—that is, the trial’s generalisability (applicability) and relevance to clinical or public health practice (see item 21).94 A description of the method of recruitment, such as by referral or self selection (for example, through advertisements), is also important in this context. Because they are applied before randomisation, eligibility criteria do not affect the internal validity of a trial, but they are central to its external validity.\nTypical and widely accepted selection criteria relate to the nature and stage of the disease being studied, the exclusion of persons thought to be particularly vulnerable to harm from the study intervention, and to issues required to ensure that the study satisfies legal and ethical norms. Informed consent by study participants, for example, is typically required in intervention studies. The common distinction between inclusion and exclusion criteria is unnecessary; the same criterion can be phrased to include or exclude participants.95\nDespite their importance, eligibility criteria are often not reported adequately. For example, eight published trials leading to clinical alerts by the National Institutes of Health specified an average of 31 eligibility criteria in their protocols, but only 63% of the criteria were mentioned in the journal articles, and only 19% were mentioned in the clinical alerts.96 Similar deficiencies were found for HIV clinical trials.97 Among 364 reports of RCTs in surgery, 25% did not specify any eligibility criteria\n\n\nExamples\n\nEligible participants were all adults aged 18 or over with HIV who met the eligibility criteria for antiretroviral therapy according to the Malawian national HIV treatment guidelines (WHO clinical stage III or IV or any WHO stage with a CD4 count <250/mm3) and who were starting treatment with a BMI <18.5. Exclusion criteria were pregnancy and lactation or participation in another supplementary feeding programme.\n\nBack to top\n\n\n\n\n\n\n4b. Participants\n\n\n\n\n\n\n\nSettings and locations where the data were collected\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAlong with the eligibility criteria for participants (see item 4a) and the description of the interventions (see item 5), information on the settings and locations is crucial to judge the applicability and generalisability of a trial. Were participants recruited from primary, secondary, or tertiary health care or from the community? Healthcare institutions vary greatly in their organisation, experience, and resources and the baseline risk for the condition under investigation. Other aspects of the setting (including the social, economic, and cultural environment and the climate) may also affect a study’s external validity.\nAuthors should report the number and type of settings and describe the care providers involved. They should report the locations in which the study was carried out, including the country, city if applicable, and immediate environment (for example, community, office practice, hospital clinic, or inpatient unit). In particular, it should be clear whether the trial was carried out in one or several centres (multicentre trials). This description should provide enough information so that readers can judge whether the results of the trial could be relevant to their own setting. The environment in which the trial is conducted may differ considerably from the setting in which the trial’s results are later used to guide practice and policy.94 99 Authors should also report any other information about the settings and locations that could have influenced the observed results, such as problems with transportation that might have affected patient participation or delays in administering interventions\n\n\nExamples\n\nThe study took place at the antiretroviral therapy clinic of Queen Elizabeth Central Hospital in Blantyre, Malawi, from January 2006 to April 2007. Blantyre is the major commercial city of Malawi, with a population of 1 000 000 and an estimated HIV prevalence of 27% in adults in 2004\n\nBack to top\n\n\n\n\n\n\n5. Interventions\n\n\n\n\n\n\n\nThe experimental and control interventions for each group with sufficient details to allow replication, including how and when they were actually administered\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAuthors should describe each intervention thoroughly, including control interventions. The description should allow a clinician wanting to use the intervention to know exactly how to administer the intervention that was evaluated in the trial.102 For a drug intervention, information would include the drug name, dose, method of administration (such as oral, intravenous), timing and duration of administration, conditions under which interventions are withheld, and titration regimen if applicable. If the control group is to receive usual care it is important to describe thoroughly what that constitutes. If the control group or intervention group is to receive a combination of interventions the authors should provide a thorough description of each intervention, an explanation of the order in which the combination of interventions are introduced or withdrawn, and the triggers for their introduction if applicable.\nSpecific extensions of the CONSORT statement address the reporting of non-pharmacologic and herbal interventions and their particular reporting requirements (such as expertise, details of how the interventions were standardised).43 44 We recommend readers consult the statements for non-pharmacologic and herbal interventions as appropriate.\n\n\nExamples\n\nIn POISE, patients received the first dose of the study drug (ie, oral extended-release metoprolol 100 mg or matching placebo) 2-4 h before surgery. Study drug administration required a heart rate of 50 bpm or more and a systolic blood pressure of 100 mm Hg or greater; these haemodynamics were checked before each administration. If, at any time during the first 6 h after surgery, heart rate was 80 bpm or more and systolic blood pressure was 100 mm Hg or higher, patients received their first postoperative dose (extended-release metoprolol 100 mg or matched placebo) orally. If the study drug was not given during the first 6 h, patients received their first postoperative dose at 6 h after surgery. 12 h after the first postoperative dose, patients started taking oral extended-release metoprolol 200 mg or placebo every day for 30 days. If a patient’s heart rate was consistently below 45 bpm or their systolic blood pressure dropped below 100 mm Hg, study drug was withheld until their heart rate or systolic blood pressure recovered; the study drug was then restarted at 100 mg once daily. Patients whose heart rate was consistently 45-49 bpm and systolic blood pressure exceeded 100 mm Hg delayed taking the study drug for 12 h\n\nBack to top\n\n\n\n\n\n\n6a. Outcomes\n\n\n\n\n\n\n\nCompletely defined prespecified primary and secondary outcome measures, including how and when they were assessed\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAll RCTs assess response variables, or outcomes (end points), for which the groups are compared. Most trials have several outcomes, some of which are of more interest than others. The primary outcome measure is the pre-specified outcome considered to be of greatest importance to relevant stakeholders (such a patients, policy makers, clinicians, funders) and is usually the one used in the sample size calculation (see item 7). Some trials may have more than one primary outcome. Having several primary outcomes, however, incurs the problems of interpretation associated with multiplicity of analyses (see items 18 and 20) and is not recommended. Primary outcomes should be explicitly indicated as such in the report of an RCT. Other outcomes of interest are secondary outcomes (additional outcomes). There may be several secondary outcomes, which often include unanticipated or unintended effects of the intervention (see item 19), although harms should always be viewed as important whether they are labelled primary or secondary.\nAll outcome measures, whether primary or secondary, should be identified and completely defined. The principle here is that the information provided should be sufficient to allow others to use the same outcomes.102 When outcomes are assessed at several time points after randomisation, authors should also indicate the pre-specified time point of primary interest. For many non-pharmacological interventions it is helpful to specify who assessed outcomes (for example, if special skills are required to do so) and how many assessors there were.43\nWhere available and appropriate, the use of previously developed and validated scales or consensus guidelines should be reported,104 105 both to enhance quality of measurement and to assist in comparison with similar studies.106 For example, assessment of quality of life is likely to be improved by using a validated instrument.107 Authors should indicate the provenance and properties of scales.\nMore than 70 outcomes were used in 196 RCTs of non-steroidal anti-inflammatory drugs for rheumatoid arthritis,108 and 640 different instruments had been used in 2000 trials in schizophrenia, of which 369 had been used only once.33 Investigation of 149 of those 2000 trials showed that unpublished scales were a source of bias. In non-pharmacological trials, a third of the claims of treatment superiority based on unpublished scales would not have been made if a published scale had been used.109 Similar data have been reported elsewhere.110 111 Only 45% of a cohort of 519 RCTs published in 2000 specified the primary outcome16; this compares with 53% for a similar cohort of 614 RCTs published in 2006\n\n\nExamples\n\nThe primary endpoint with respect to efficacy in psoriasis was the proportion of patients achieving a 75% improvement in psoriasis activity from baseline to 12 weeks as measured by the PASI [psoriasis area and severity index] Additional analyses were done on the percentage change in PASI scores and improvement in target psoriasis lesions.\n\nBack to top\n\n\n\n\n\n\n6b. Outcomes\n\n\n\n\n\n\n\nAny changes to trial outcomes after the trial commenced, with reasons\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThere are many reasons for departures from the initial study protocol (see item 24). Authors should report all major changes to the protocol, including unplanned changes to eligibility criteria, interventions, examinations, data collection, methods of analysis, and outcomes. Such information is not always reported.\nAs indicated earlier (see item 6a), most trials record multiple outcomes, with the risk that results will be reported for only a selected subset (see item 17). Pre-specification and reporting of primary and secondary outcomes (see item 6a) should remove such a risk. In some trials, however, circumstances require a change in the way an outcome is assessed or even, as in the example above, a switch to a different outcome. For example, there may be external evidence from other trials or systematic reviews suggesting the end point might not be appropriate, or recruitment or the overall event rate in the trial may be lower than expected.112 Changing an end point based on unblinded data is much more problematic, although it may be specified in the context of an adaptive trial design.88 Authors should identify and explain any such changes. Likewise, any changes after the trial began of the designation of outcomes as primary or secondary should be reported and explained.\nA comparison of protocols and publications of 102 randomised trials found that 62% of trials reports had at least one primary outcome that was changed, introduced, or omitted compared with the protocol.55 Primary outcomes also differed between protocols and publications for 40% of a cohort of 48 trials funded by the Canadian Institutes of Health Research.113 Not one of the subsequent 150 trial reports mentioned, let alone explained, changes from the protocol. Similar results from other studies have been reported recently in a systematic review of empirical studies examining outcome reporting bias\n\n\nExamples\n\nThe original primary endpoint was all-cause mortality, but, during a masked analysis, the data and safety monitoring board noted that overall mortality was lower than had been predicted and that the study could not be completed with the sample size and power originally planned. The steering committee therefore decided to adopt co-primary endpoints of all-cause mortality (the original primary endpoint), together with all-cause mortality or cardiovascular hospital admissions (the first prespecified secondary endpoint).\n\nBack to top\n\n\n\n\n\n\n7a. Sample size\n\n\n\n\n\n\n\nHow sample size was determined.\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nFor scientific and ethical reasons, the sample size for a trial needs to be planned carefully, with a balance between medical and statistical considerations. Ideally, a study should be large enough to have a high probability (power) of detecting as statistically significant a clinically important difference of a given size if such a difference exists. The size of effect deemed important is inversely related to the sample size necessary to detect it; that is, large samples are necessary to detect small differences. Elements of the sample size calculation are (1) the estimated outcomes in each group (which implies the clinically important target difference between the intervention groups); (2) the α (type I) error level; (3) the statistical power (or the β (type II) error level); and (4), for continuous outcomes, the standard deviation of the measurements.116 The interplay of these elements and their reporting will differ for cluster trials40 and non-inferiority and equivalence trials.39\nAuthors should indicate how the sample size was determined. If a formal power calculation was used, the authors should identify the primary outcome on which the calculation was based (see item 6a), all the quantities used in the calculation, and the resulting target sample size per study group. It is preferable to quote the expected result in the control group and the difference between the groups one would not like to overlook. Alternatively, authors could present the percentage with the event or mean for each group used in their calculations. Details should be given of any allowance made for attrition or non-compliance during the study.\nSome methodologists have written that so called underpowered trials may be acceptable because they could ultimately be combined in a systematic review and meta-analysis,117 118 119 and because some information is better than no information. Of note, important caveats apply—such as the trial should be unbiased, reported properly, and published irrespective of the results, thereby becoming available for meta-analysis.118 On the other hand, many medical researchers worry that underpowered trials with indeterminate results will remain unpublished and insist that all trials should individually have sufficient power. This debate will continue, and members of the CONSORT Group have varying views. Critically however, the debate and those views are immaterial to reporting a trial. Whatever the power of a trial, authors need to properly report their intended size with all their methods and assumptions.118 That transparently reveals the power of the trial to readers and gives them a measure by which to assess whether the trial attained its planned size.\nIn some trials, interim analyses are used to help decide whether to stop early or to continue recruiting sometimes beyond the planned trial end (see item 7b). If the actual sample size differed from the originally intended sample size for some other reason (for example, because of poor recruitment or revision of the target sample size), the explanation should be given.\nReports of studies with small samples frequently include the erroneous conclusion that the intervention groups do not differ, when in fact too few patients were studied to make such a claim.120 Reviews of published trials have consistently found that a high proportion of trials have low power to detect clinically meaningful treatment effects.121 122 123 In reality, small but clinically meaningful true differences are much more likely than large differences to exist, but large trials are required to detect them.124\nIn general, the reported sample sizes in trials seem small. The median sample size was 54 patients in 196 trials in arthritis,108 46 patients in 73 trials in dermatology,8 and 65 patients in 2000 trials in schizophrenia.33 These small sample sizes are consistent with those of a study of 519 trials indexed in PubMed in December 200016 and a similar cohort of trials (n=616) indexed in PubMed in 2006,17 where the median number of patients recruited for parallel group trials was 80 across both years. Moreover, many reviews have found that few authors report how they determined the sample size.8 14 32 33 123\nThere is little merit in a post hoc calculation of statistical power using the results of a trial; the power is then appropriately indicated by confidence intervals (see item 17).\n\n\nExamples\n\nTo detect a reduction in PHS (postoperative hospital stay) of 3 days (SD 5 days), which is in agreement with the study of Lobo et al17 with a two-sided 5% significance level and a power of 80%, a sample size of 50 patients per group was necessary, given an anticipated dropout rate of 10%. To recruit this number of patients a 12-month inclusion period was anticipated.\n\nBack to top\n\n\n\n\n\n\n7b. Sample size\n\n\n\n\n\n\n\nWhen applicable, explanation of any interim analyses and stopping guidelines\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nMany trials recruit participants over a long period. If an intervention is working particularly well or badly, the study may need to be ended early for ethical reasons. This concern can be addressed by examining results as the data accumulate, preferably by an independent data monitoring committee. However, performing multiple statistical examinations of accumulating data without appropriate correction can lead to erroneous results and interpretations.128 If the accumulating data from a trial are examined at five interim analyses that use a P value of 0.05, the overall false positive rate is nearer to 19% than to the nominal 5%.\nSeveral group sequential statistical methods are available to adjust for multiple analyses,129 130 131 and their use should be pre-specified in the trial protocol. With these methods, data are compared at each interim analysis, and a P value less than the critical value specified by the group sequential method indicates statistical significance. Some trialists use group sequential methods as an aid to decision making,132 whereas others treat them as a formal stopping rule (with the intention that the trial will cease if the observed P value is smaller than the critical value).\nAuthors should report whether they or a data monitoring committee took multiple looks at the data and, if so, how many there were, what triggered them, the statistical methods used (including any formal stopping rule), and whether they were planned before the start of the trial, before the data monitoring committee saw any interim data by allocation, or some time thereafter. This information is often not included in published trial reports,133 even in trials that report stopping earlier than planned\n\n\nExamples\n\nTwo interim analyses were performed during the trial. The levels of significance maintained an overall P value of 0.05 and were calculated according to the O’Brien-Fleming stopping boundaries. This final analysis used a Z score of 1.985 with an associated P value of 0.0471.\n\nBack to top\n\n\n\n\n\n\n8a. Randomization - Sequence generation\n\n\n\n\n\n\n\nMethod used to generate the random allocation sequence.\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nParticipants should be assigned to comparison groups in the trial on the basis of a chance (random) process characterised by unpredictability (see box 1). Authors should provide sufficient information that the reader can assess the methods used to generate the random allocation sequence and the likelihood of bias in group assignment. It is important that information on the process of randomisation is included in the body of the main article and not as a separate supplementary file; where it can be missed by the reader.\nThe term random has a precise technical meaning. With random allocation, each participant has a known probability of receiving each intervention before one is assigned, but the assigned intervention is determined by a chance process and cannot be predicted. However, random is often used inappropriately in the literature to describe trials in which non-random, deterministic allocation methods were used, such as alternation, hospital numbers, or date of birth. When investigators use such non-random methods, they should describe them precisely and should not use the term random or any variation of it. Even the term quasi-random is unacceptable for describing such trials. Trials based on non-random methods generally yield biased results.2 3 4 136 Bias presumably arises from the inability to conceal these allocation systems adequately (see item 9).\nMany methods of sequence generation are adequate. However, readers cannot judge adequacy from such terms as random allocation, randomisation, or random without further elaboration. Authors should specify the method of sequence generation, such as a random-number table or a computerised random number generator. The sequence may be generated by the process of minimisation, a non-random but generally acceptable method (see box 2). In some trials, participants are intentionally allocated in unequal numbers to each intervention: for example, to gain more experience with a new procedure or to limit costs of the trial. In such cases, authors should report the randomisation ratio (for example, 2:1 or two treatment participants per each control participant) (see item 3a).\nIn a representative sample of PubMed indexed trials in 2000, only 21% reported an adequate approach to random sequence generation16; this increased to 34% for a similar cohort of PubMed indexed trials in 2006.17 In more than 90% of these cases, researchers used a random number generator on a computer or a random number table.\n\n\nExamples\n\nIndependent pharmacists dispensed either active or placebo inhalers according to a computer generated randomisation list.\n\nBack to top\n\n\n\n\n\n\n8b. Randomization - Sequence generation\n\n\n\n\n\n\n\nType of randomization; details of any restriction (such as blocking and block size)\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nIn trials of several hundred participants or more simple randomisation can usually be trusted to generate similar numbers in the two trial groups139 and to generate groups that are roughly comparable in terms of known and unknown prognostic variables.140 For smaller trials (see item 7a)—and even for trials that are not intended to be small, as they may stop before reaching their target size—some restricted randomisation (procedures to help achieve balance between groups in size or characteristics) may be useful (see box 2).\nIt is important to indicate whether no restriction was used, by stating such or by stating that simple randomisation was done. Otherwise, the methods used to restrict the randomisation, along with the method used for random selection, should be specified. For block randomisation, authors should provide details on how the blocks were generated (for example, by using a permuted block design with a computer random number generator), the block size or sizes, and whether the block size was fixed or randomly varied. If the trialists became aware of the block size(s), that information should also be reported as such knowledge could lead to code breaking. Authors should specify whether stratification was used, and if so, which factors were involved (such as recruitment site, sex, disease stage), the categorisation cut-off values within strata, and the method used for restriction. Although stratification is a useful technique, especially for smaller trials, it is complicated to implement and may be impossible if many stratifying factors are used. If minimisation (see box 2) was used, it should be explicitly identified, as should the variables incorporated into the scheme. If used, a random element should be indicated.\nOnly 9% of 206 reports of trials in specialty journals23 and 39% of 80 trials in general medical journals reported use of stratification.32 In each case, only about half of the reports mentioned the use of restricted randomisation. However, these studies and that of Adetugbo and Williams8 found that the sizes of the treatment groups in many trials were the same or quite similar, yet blocking or stratification had not been mentioned. One possible explanation for the close balance in numbers is underreporting of the use of restricted randomisation.\n\n\nExamples\n\nRandomization sequence was created using Stata 9.0 (StataCorp, College Station, TX) statistical software and was stratified by center with a 1:1 allocation using random block sizes of 2, 4, and 6.\n\nBack to top\n\n\n\n\n\n\n9. Randomization - Allocation concealment mechanism\n\n\n\n\n\n\n\nMechanism used to implement the random allocation sequence (such as sequentially numbered containers), describing any steps taken to conceal the sequence until interventions were assigned\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nItem 8a discussed generation of an unpredictable sequence of assignments. Of considerable importance is how this sequence is applied when participants are enrolled into the trial (see box 1). A generated allocation schedule should be implemented by using allocation concealment,23 a critical mechanism that prevents foreknowledge of treatment assignment and thus shields those who enroll participants from being influenced by this knowledge. The decision to accept or reject a participant should be made, and informed consent should be obtained from the participant, in ignorance of the next assignment in the sequence.148\nThe allocation concealment should not be confused with blinding (see item 11). Allocation concealment seeks to prevent selection bias, protects the assignment sequence until allocation, and can always be successfully implemented.2 In contrast, blinding seeks to prevent performance and ascertainment bias, protects the sequence after allocation, and cannot always be implemented.23 Without adequate allocation concealment, however, even random, unpredictable assignment sequences can be subverted.2 149\nCentralised or third-party assignment is especially desirable. Many good allocation concealment mechanisms incorporate external involvement. Use of a pharmacy or central telephone randomisation system are two common techniques. Automated assignment systems are likely to become more common.150 When external involvement is not feasible, an excellent method of allocation concealment is the use of numbered containers. The interventions (often drugs) are sealed in sequentially numbered identical containers according to the allocation sequence.151 Enclosing assignments in sequentially numbered, opaque, sealed envelopes can be a good allocation concealment mechanism if it is developed and monitored diligently. This method can be corrupted, however, particularly if it is poorly executed. Investigators should ensure that the envelopes are opaque when held to the light, and opened sequentially and only after the participant’s name and other details are written on the appropriate envelope.143\nA number of methodological studies provide empirical evidence to support these precautions.152 153 Trials in which the allocation sequence had been inadequately or unclearly concealed yielded larger estimates of treatment effects than did trials in which authors reported adequate allocation concealment. These findings provide strong empirical evidence that inadequate allocation concealment contributes to bias in estimating treatment effects.\nDespite the importance of the mechanism of allocation concealment, published reports often omit such details. The mechanism used to allocate interventions was omitted in reports of 89% of trials in rheumatoid arthritis,108 48% of trials in obstetrics and gynaecology journals,23 and 44% of trials in general medical journals.32 In a more broadly representative sample of all randomised trials indexed on PubMed, only 18% reported any allocation concealment mechanism, but some of those reported mechanisms were inadequate.\n\n\nExamples\n\nThe doxycycline and placebo were in capsule form and identical in appearance. They were prepacked in bottles and consecutively numbered for each woman according to the randomisation schedule. Each woman was assigned an order number and received the capsules in the corresponding prepacked bottle.\n\nBack to top\n\n\n\n\n\n\n10. Randomization - Implementation\n\n\n\n\n\n\n\nWho generated the allocation sequence, who enrolled participants, and who assigned participants to interventions\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAs noted in item 9, concealment of the allocated intervention at the time of enrolment is especially important. Thus, in addition to knowing the methods used, it is also important to understand how the random sequence was implemented—specifically, who generated the allocation sequence, who enrolled participants, and who assigned participants to trial groups.\nThe process of randomising participants into a trial has three different steps: sequence generation, allocation concealment, and implementation (see box 3). Although the same people may carry out more than one process under each heading, investigators should strive for complete separation of the people involved with generation and allocation concealment from the people involved in the implementation of assignments. Thus, if someone is involved in the sequence generation or allocation concealment steps, ideally they should not be involved in the implementation step.\nEven with flawless sequence generation and allocation concealment, failure to separate creation and concealment of the allocation sequence from assignment to study group may introduce bias. For example, the person who generated an allocation sequence could retain a copy and consult it when interviewing potential participants for a trial. Thus, that person could bias the enrolment or assignment process, regardless of the unpredictability of the assignment sequence. Investigators must then ensure that the assignment schedule is unpredictable and locked away (such as in a safe deposit box in a building rather inaccessible to the enrolment location) from even the person who generated it. The report of the trial should specify where the investigators stored the allocation list\n\n\nExamples\n\nDetermination of whether a patient would be treated by streptomycin and bed-rest (S case) or by bed-rest alone (C case) was made by reference to a statistical series based on random sampling numbers drawn up for each sex at each centre by Professor Bradford Hill; the details of the series were unknown to any of the investigators or to the co-ordinator … After acceptance of a patient by the panel, and before admission to the streptomycin centre, the appropriate numbered envelope was opened at the central office; the card inside told if the patient was to be an S or a C case, and this information was then given to the medical officer of the centre.\n\nBack to top\n\n\n\n\n\n\n11a. Blinding\n\n\n\n\n\n\n\nIf done, who was blinded after assignment to interventions (for example, participants, care providers, those assessing outcomes) and how.\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe term blinding or masking refers to withholding information about the assigned interventions from people involved in the trial who may potentially be influenced by this knowledge. Blinding is an important safeguard against bias, particularly when assessing subjective outcomes.153\nBenjamin Franklin has been credited as being the first to use blinding in a scientific experiment.158 He blindfolded participants so they would not know when he was applying mesmerism (a popular healing fluid of the 18th century) and in so doing showed that mesmerism was a sham. Based on this experiment, the scientific community recognised the power of blinding to reduce bias, and it has remained a commonly used strategy in scientific experiments.\nBox 4, on blinding terminology, defines the groups of individuals (that is, participants, healthcare providers, data collectors, outcome adjudicators, and data analysts) who can potentially introduce bias into a trial through knowledge of the treatment assignments. Participants may respond differently if they are aware of their treatment assignment (such as responding more favourably when they receive the new treatment).153 Lack of blinding may also influence compliance with the intervention, use of co-interventions, and risk of dropping out of the trial.\nUnblinded healthcare providers may introduce similar biases, and unblinded data collectors may differentially assess outcomes (such as frequency or timing), repeat measurements of abnormal findings, or provide encouragement during performance testing. Unblinded outcome adjudicators may differentially assess subjective outcomes, and unblinded data analysts may introduce bias through the choice of analytical strategies, such as the selection of favourable time points or outcomes, and by decisions to remove patients from the analyses. These biases have been well documented.71 153 159 160 161 162\nBlinding, unlike allocation concealment (see item 10), may not always be appropriate or possible. An example is a trial comparing levels of pain associated with sampling blood from the ear or thumb.163 Blinding is particularly important when outcome measures involve some subjectivity, such as assessment of pain. Blinding of data collectors and outcome adjudicators is unlikely to matter for objective outcomes, such as death from any cause. Even then, however, lack of participant or healthcare provider blinding can lead to other problems, such as differential attrition.164 In certain trials, especially surgical trials, blinding of participants and surgeons is often difficult or impossible, but blinding of data collectors and outcome adjudicators is often achievable. For example, lesions can be photographed before and after treatment and assessed by an external observer.165 Regardless of whether blinding is possible, authors can and should always state who was blinded (that is, participants, healthcare providers, data collectors, and outcome adjudicators).\nUnfortunately, authors often do not report whether blinding was used.166 For example, reports of 51% of 506 trials in cystic fibrosis,167 33% of 196 trials in rheumatoid arthritis,108 and 38% of 68 trials in dermatology8 did not state whether blinding was used. Until authors of trials improve their reporting of blinding, readers will have difficulty in judging the validity of the trials that they may wish to use to guide their clinical practice.\nThe term masking is sometimes used in preference to blinding to avoid confusion with the medical condition of being without sight. However, blinding in its methodological sense seems to be understood worldwide and is acceptable for reporting clinical trials.\n\n\nExamples\n\nWhereas patients and physicians allocated to the intervention group were aware of the allocated arm, outcome assessors and data analysts were kept blinded to the allocation.\n\nBack to top\n\n\n\n\n\n\n11b. Blinding\n\n\n\n\n\n\n\nIf relevant, description of the similarity of interventions\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nJust as we seek evidence of concealment to assure us that assignment was truly random, we seek evidence of the method of blinding. In trials with blinding of participants or healthcare providers, authors should state the similarity of the characteristics of the interventions (such as appearance, taste, smell, and method of administration).35 173\nSome people have advocated testing for blinding by asking participants or healthcare providers at the end of a trial whether they think the participant received the experimental or control intervention.174 Because participants and healthcare providers will usually know whether the participant has experienced the primary outcome, this makes it difficult to determine if their responses reflect failure of blinding or accurate assumptions about the efficacy of the intervention.175 Given the uncertainty this type of information provides, we have removed advocating reporting this type of testing for blinding from the CONSORT 2010 Statement. We do, however, advocate that the authors report any known compromises in blinding. For example, authors should report if it was necessary to unblind any participants at any point during the conduct of a trial.\n\n\nExamples\n\nJamieson Laboratories Inc provided 500-mg immediate release niacin in a white, oblong, bisect caplet. We independently confirmed caplet content using high performance liquid chromatography … The placebo was matched to the study drug for taste, color, and size, and contained microcrystalline cellulose, silicon dioxide, dicalcium phosphate, magnesium stearate, and stearic acid.\n\nBack to top\n\n\n\n\n\n\n12a. Statistical methods\n\n\n\n\n\n\n\nStatistical methods used to compare groups for primary and secondary outcomes\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nData can be analysed in many ways, some of which may not be strictly appropriate in a particular situation. It is essential to specify which statistical procedure was used for each analysis, and further clarification may be necessary in the results section of the report. The principle to follow is to, Describe statistical methods with enough detail to enable a knowledgeable reader with access to the original data to verify the reported results (www.icmje.org). It is also important to describe details of the statistical analysis such as intention-to-treat analysis (see box 6).\nAlmost all methods of analysis yield an estimate of the treatment effect, which is a contrast between the outcomes in the comparison groups. Authors should accompany this by a confidence interval for the estimated effect, which indicates a central range of uncertainty for the true treatment effect. The confidence interval may be interpreted as the range of values for the treatment effect that is compatible with the observed data. It is customary to present a 95% confidence interval, which gives the range expected to include the true value in 95 of 100 similar studies.\nStudy findings can also be assessed in terms of their statistical significance. The P value represents the probability that the observed data (or a more extreme result) could have arisen by chance when the interventions did not truly differ. Actual P values (for example, P=0.003) are strongly preferable to imprecise threshold reports such as P<0.05.48 177\nStandard methods of analysis assume that the data are independent. For controlled trials, this usually means that there is one observation per participant. Treating multiple observations from one participant as independent data is a serious error; such data are produced when outcomes can be measured on different parts of the body, as in dentistry or rheumatology. Data analysis should be based on counting each participant once178 179 or should be done by using more complex statistical procedures.180 Incorrect analysis of multiple observations per individual was seen in 123 (63%) of 196 trials in rheumatoid arthritis.\n\n\nExamples\n\nThe primary endpoint was change in bodyweight during the 20 weeks of the study in the intention-to-treat population … Secondary efficacy endpoints included change in waist circumference, systolic and diastolic blood pressure, prevalence of metabolic syndrome … We used an analysis of covariance (ANCOVA) for the primary endpoint and for secondary endpoints waist circumference, blood pressure, and patient-reported outcome scores; this was supplemented by a repeated measures analysis. The ANCOVA model included treatment, country, and sex as fixed effects, and bodyweight at randomisation as covariate. We aimed to assess whether data provided evidence of superiority of each liraglutide dose to placebo (primary objective) and to orlistat (secondary objective).\n\nBack to top\n\n\n\n\n\n\n12b. Statistical methods\n\n\n\n\n\n\n\nMethods for additional analyses, such as subgroup analyses and adjusted analyses\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAs is the case for primary analyses, the method of subgroup analysis should be clearly specified. The strongest analyses are those that look for evidence of a difference in treatment effect in complementary subgroups (for example, older and younger participants), a comparison known as a test of interaction.182 183 A common but misleading approach is to compare P values for separate analyses of the treatment effect in each group. It is incorrect to infer a subgroup effect (interaction) from one significant and one non-significant P value.184 Such inferences have a high false positive rate.\nBecause of the high risk for spurious findings, subgroup analyses are often discouraged.14 185 Post hoc subgroup comparisons (analyses done after looking at the data) are especially likely not to be confirmed by further studies. Such analyses do not have great credibility.\nIn some studies, imbalances in participant characteristics are adjusted for by using some form of multiple regression analysis. Although the need for adjustment is much less in RCTs than in epidemiological studies, an adjusted analysis may be sensible, especially if one or more variables is thought to be prognostic.186 Ideally, adjusted analyses should be specified in the study protocol (see item 24). For example, adjustment is often recommended for any stratification variables (see item 8b) on the principle that the analysis strategy should follow the design. In RCTs, the decision to adjust should not be determined by whether baseline differences are statistically significant (see item 16).183 187 The rationale for any adjusted analyses and the statistical methods used should be specified.\nAuthors should clarify the choice of variables that were adjusted for, indicate how continuous variables were handled, and specify whether the analysis was planned or suggested by the data.188 Reviews of published studies show that reporting of adjusted analyses is inadequate with regard to all of these aspects.188 189 190 191\n\n\nExamples\n\nProportions of patients responding were compared between treatment groups with the Mantel-Haenszel χ2 test, adjusted for the stratification variable, methotrexate use.\n\nBack to top\n\n\n\n\n\n\nResults\n\n\n13a. Participant flow diagram (strongly recommended)\n\n\n\n\n\n\n\nA diagram is strongly recommended. For each group, the numbers of participants who were randomly assigned, received intended treatment, and were analysed for the primary outcome\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe design and conduct of some RCTs is straightforward, and the flow of participants, particularly were there are no losses to follow-up or exclusions, through each phase of the study can be described adequately in a few sentences. In more complex studies, it may be difficult for readers to discern whether and why some participants did not receive the treatment as allocated, were lost to follow-up, or were excluded from the analysis.51 This information is crucial for several reasons. Participants who were excluded after allocation are unlikely to be representative of all participants in the study. For example, patients may not be available for follow-up evaluation because they experienced an acute exacerbation of their illness or harms of treatment.22 192\nAttrition as a result of loss to follow up, which is often unavoidable, needs to be distinguished from investigator-determined exclusion for such reasons as ineligibility, withdrawal from treatment, and poor adherence to the trial protocol. Erroneous conclusions can be reached if participants are excluded from analysis, and imbalances in such omissions between groups may be especially indicative of bias.192 193 194 Information about whether the investigators included in the analysis all participants who underwent randomisation, in the groups to which they were originally allocated (intention-to-treat analysis (see item 16 and box 6)), is therefore of particular importance. Knowing the number of participants who did not receive the intervention as allocated or did not complete treatment permits the reader to assess to what extent the estimated efficacy of therapy might be underestimated in comparison with ideal circumstances.\nIf available, the number of people assessed for eligibility should also be reported. Although this number is relevant to external validity only and is arguably less important than the other counts,195 it is a useful indicator of whether trial participants were likely to be representative of all eligible participants.\nA review of RCTs published in five leading general and internal medicine journals in 1998 found that reporting of the flow of participants was often incomplete, particularly with regard to the number of participants receiving the allocated intervention and the number lost to follow-up.51 Even information as basic as the number of participants who underwent randomisation and the number excluded from analyses was not available in up to 20% of articles.51 Reporting was considerably more thorough in articles that included a diagram of the flow of participants through a trial, as recommended by CONSORT. This study informed the design of the revised flow diagram in the revised CONSORT statement.52 53 54 The suggested template is shown in fig 1​1,, and the counts required are described in detail in table 3​3.\nSome information, such as the number of individuals assessed for eligibility, may not always be known,14 and, depending on the nature of a trial, some counts may be more relevant than others. It will sometimes be useful or necessary to adapt the structure of the flow diagram to a particular trial. In some situations, other information may usefully be added. For example, the flow diagram of a parallel group trial of minimal surgery compared with medical management for chronic gastro-oesophageal reflux also included a parallel non-randomised preference group (see fig 3).196\nThe exact form and content of the flow diagram may be varied according to specific features of a trial. For example, many trials of surgery or vaccination do not include the possibility of discontinuation. Although CONSORT strongly recommends using this graphical device to communicate participant flow throughout the study, there is no specific, prescribed format.\n\n\nExamples\n\nFlow diagram of a multicentre trial of fractional flow reserve versus angiography for guiding percutaneous coronary intervention (PCI) (adapted from Tonino et al313). The diagram includes detailed information on the excluded participants. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2844943/figure/fig2/\n\nBack to top\n\n\n\n\n\n\n13b. Participant flow\n\n\n\n\n\n\n\nFor each group, losses and exclusions after randomization, together with reason\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nSome protocol deviations may be reported in the flow diagram (see item 13a)—for example, participants who did not receive the intended intervention. If participants were excluded after randomisation (contrary to the intention-to-treat principle) because they were found not to meet eligibility criteria (see item 16), they should be included in the flow diagram. Use of the term protocol deviation in published articles is not sufficient to justify exclusion of participants after randomisation. The nature of the protocol deviation and the exact reason for excluding participants after randomisation should always be reported.\n\n\nExamples\n\nThere was only one protocol deviation, in a woman in the study group. She had an abnormal pelvic measurement and was scheduled for elective caesarean section. However, the attending obstetrician judged a trial of labour acceptable; caesarean section was done when there was no progress in the first stage of labour.\n\nBack to top\n\n\n\n\n\n\n14a. Recruitment\n\n\n\n\n\n\n\nDates defining the periods of recruitment and follow-up\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nKnowing when a study took place and over what period participants were recruited places the study in historical context. Medical and surgical therapies, including concurrent therapies, evolve continuously and may affect the routine care given to participants during a trial. Knowing the rate at which participants were recruited may also be useful, especially to other investigators.\nThe length of follow-up is not always a fixed period after randomisation. In many RCTs in which the outcome is time to an event, follow-up of all participants is ended on a specific date. This date should be given, and it is also useful to report the minimum, maximum, and median duration of follow-up.200 201\nA review of reports in oncology journals that used survival analysis, most of which were not RCTs, 201 found that nearly 80% (104 of 132 reports) included the starting and ending dates for accrual of patients, but only 24% (32 of 132 reports) also reported the date on which follow-up ended.\n\n\nExamples\n\nAge-eligible participants were recruited … from February 1993 to September 1994 … Participants attended clinic visits at the time of randomisation (baseline) and at 6-month intervals for 3 years.\n\nBack to top\n\n\n\n\n\n\n14b. Recruitment\n\n\n\n\n\n\n\nWhy the trial ended or was stopped\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nArguably, trialists who arbitrarily conduct unplanned interim analyses after very few events accrue using no statistical guidelines run a high risk of catching the data at a random extreme, which likely represents a large overestimate of treatment benefit.204\nReaders will likely draw weaker inferences from a trial that was truncated in a data-driven manner versus one that reports its findings after reaching a goal independent of results. Thus, RCTs should indicate why the trial came to an end (see box 5). The report should also disclose factors extrinsic to the trial that affected the decision to stop the trial, and who made the decision to stop the trial, including reporting the role the funding agency played in the deliberations and in the decision to stop the trial.134\nA systematic review of 143 RCTs stopped earlier than planned for benefit found that these trials reported stopping after accruing a median of 66 events, estimated a median relative risk of 0.47 and a strong relation between the number of events accrued and the size of the effect, with smaller trials with fewer events yielding the largest treatment effects (odds ratio 31, 95% conﬁdence interval 12 to 82).134 While an increasing number of trials published in high impact medical journals report stopping early, only 0.1% of trials reported stopping early for benefit, which contrasts with estimates arising from simulation studies205 and surveys of data safety and monitoring committees.206 Thus, many trials accruing few participants and reporting large treatment effects may have been stopped earlier than planned but failed to report this action.\n\n\nExamples\n\nAt the time of the interim analysis, the total follow-up included an estimated 63% of the total number of patient-years that would have been collected at the end of the study, leading to a threshold value of 0.0095, as determined by the Lan-DeMets alpha-spending function method … At the interim analysis, the RR was 0.37 in the intervention group, as compared with the control group, with a p value of 0.00073, below the threshold value. The Data and Safety Monitoring Board advised the investigators to interrupt the trial and offer circumcision to the control group, who were then asked to come to the investigation centre, where MC (medical circumcision) was advised and proposed … Because the study was interrupted, some participants did not have a full follow-up on that date, and their visits that were not yet completed are described as planned in this article.\n\nBack to top\n\n\n\n\n\n\n15. Baseline data\n\n\n\n\n\n\n\nA table showing baseline demographic and clinical characteristics for each group\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAlthough the eligibility criteria (see item 4a) indicate who was eligible for the trial, it is also important to know the characteristics of the participants who were actually included. This information allows readers, especially clinicians, to judge how relevant the results of a trial might be to an individual patient.\nRandomised trials aim to compare groups of participants that differ only with respect to the intervention (treatment). Although proper random assignment prevents selection bias, it does not guarantee that the groups are equivalent at baseline. Any differences in baseline characteristics are, however, the result of chance rather than bias.32 The study groups should be compared at baseline for important demographic and clinical characteristics so that readers can assess how similar they were. Baseline data are especially valuable for outcomes that can also be measured at the start of the trial (such as blood pressure).\nBaseline information is most efficiently presented in a table (see table 4​4).). For continuous variables, such as weight or blood pressure, the variability of the data should be reported, along with average values. Continuous variables can be summarised for each group by the mean and standard deviation. When continuous data have an asymmetrical distribution, a preferable approach may be to quote the median and a centile range (such as the 25th and 75th centiles).177 Standard errors and confidence intervals are not appropriate for describing variability—they are inferential rather than descriptive statistics. Variables with a small number of ordered categories (such as stages of disease I to IV) should not be treated as continuous variables; instead, numbers and proportions should be reported for each category.48 177\nUnfortunately significance tests of baseline differences are still common23 32 210; they were reported in half of 50 RCTs trials published in leading general journals in 1997.183 Such significance tests assess the probability that observed baseline differences could have occurred by chance; however, we already know that any differences are caused by chance. Tests of baseline differences are not necessarily wrong, just illogical.211 Such hypothesis testing is superfluous and can mislead investigators and their readers. Rather, comparisons at baseline should be based on consideration of the prognostic strength of the variables measured and the size of any chance imbalances that have occurred\n\n\nExamples\n\nExample of reporting baseline demographic and clinical characteristics.* (Adapted from table 1 of Yusuf et al209) https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2844943/table/tbl4/\n\nBack to top\n\n\n\n\n\n\n16. Numbers analysed\n\n\n\n\n\n\n\nFor each group, number of participants (denominator) included in each analysis and whether the analysis was by original assigned groups\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe number of participants in each group is an essential element of the analyses. Although the flow diagram (see item 13a) may indicate the numbers of participants analysed, these numbers often vary for different outcome measures. The number of participants per group should be given for all analyses. For binary outcomes, (such as risk ratio and risk difference) the denominators or event rates should also be reported. Expressing results as fractions also aids the reader in assessing whether some of the randomly assigned participants were excluded from the analysis. It follows that results should not be presented solely as summary measures, such as relative risks.\nParticipants may sometimes not receive the full intervention, or some ineligible patients may have been randomly allocated in error. One widely recommended way to handle such issues is to analyse all participants according to their original group assignment, regardless of what subsequently occurred (see box 6). This intention-to-treat strategy is not always straightforward to implement. It is common for some patients not to complete a study—they may drop out or be withdrawn from active treatment—and thus are not assessed at the end. If the outcome is mortality, such patients may be included in the analysis based on register information, whereas imputation techniques may need to be used if other outcome data are missing. The term intention-to-treat analysis is often inappropriately used—for example, when those who did not receive the first dose of a trial drug are excluded from the analyses.18\nConversely, analysis can be restricted to only participants who fulfil the protocol in terms of eligibility, interventions, and outcome assessment. This analysis is known as an on-treatment or per protocol analysis. Excluding participants from the analysis can lead to erroneous conclusions. For example, in a trial that compared medical with surgical therapy for carotid stenosis, analysis limited to participants who were available for follow-up showed that surgery reduced the risk for transient ischaemic attack, stroke, and death. However, intention-to-treat analysis based on all participants as originally assigned did not show a superior effect of surgery.214\nIntention-to-treat analysis is generally favoured because it avoids bias associated with non-random loss of participants.215 216 217 Regardless of whether authors use the term intention-to-treat, they should make clear which and how many participants are included in each analysis (see item 13). Non-compliance with assigned therapy may mean that the intention-to-treat analysis underestimates the potential benefit of the treatment, and additional analyses, such as a per protocol analysis, may therefore be considered.218 219 It should be noted, however, that such analyses are often considerably flawed.220\nIn a review of 403 RCTs published in 10 leading medical journals in 2002, 249 (62%) reported the use of intention-to-treat analysis for their primary analysis. This proportion was higher for journals adhering to the CONSORT statement (70% v 48%). Among articles that reported the use of intention-to-treat analysis, only 39% actually analysed all participants as randomised, with more than 60% of articles having missing data in their primary analysis.221 Other studies show similar findings.18 222 223 Trials with no reported exclusions are methodologically weaker in other respects than those that report on some excluded participants,173 strongly indicating that at least some researchers who have excluded participants do not report it. Another study found that reporting an intention-to-treat analysis was associated with other aspects of good study design and reporting, such as describing a sample size calculation.\n\n\nExamples\n\nThe primary analysis was intention-to-treat and involved all patients who were randomly assigned.\n\nBack to top\n\n\n\n\n\n\n17a. Outcomes and estimation\n\n\n\n\n\n\n\nFor each primary and secondary outcome, results for each group, and the estimated effect size and its precision (such as 95% confidence interval)\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nFor each outcome, study results should be reported as a summary of the outcome in each group (for example, the number of participants with or without the event and the denominators, or the mean and standard deviation of measurements), together with the contrast between the groups, known as the effect size. For binary outcomes, the effect size could be the risk ratio (relative risk), odds ratio, or risk difference; for survival time data, it could be the hazard ratio or difference in median survival time; and for continuous data, it is usually the difference in means. Confidence intervals should be presented for the contrast between groups. A common error is the presentation of separate confidence intervals for the outcome in each group rather than for the treatment effect.233 Trial results are often more clearly displayed in a table rather than in the text, as shown in tables 5​5 and 6​6.\nFor all outcomes, authors should provide a confidence interval to indicate the precision (uncertainty) of the estimate.48 235 A 95% confidence interval is conventional, but occasionally other levels are used. Many journals require or strongly encourage the use of confidence intervals.236 They are especially valuable in relation to differences that do not meet conventional statistical significance, for which they often indicate that the result does not rule out an important clinical difference. The use of confidence intervals has increased markedly in recent years, although not in all medical specialties.233 Although P values may be provided in addition to confidence intervals, results should not be reported solely as P values.237 238 Results should be reported for all planned primary and secondary end points, not just for analyses that were statistically significant or interesting. Selective reporting within a study is a widespread and serious problem.55 57 In trials in which interim analyses were performed, interpretation should focus on the final results at the close of the trial, not the interim results.239\nFor both binary and survival time data, expressing the results also as the number needed to treat for benefit or harm can be helpful (see item 21).\n\n\nExamples\n\nExample of reporting of summary results for each study group (binary outcomes).* (Adapted from table 2 of Mease et al103) https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2844943/table/tbl5/\n\nBack to top\n\n\n\n\n\n\n17b. Outcomes and estimation\n\n\n\n\n\n\n\nFor binary outcomes, presentation of both absolute and relative effect sizes is recommended\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nWhen the primary outcome is binary, both the relative effect (risk ratio (relative risk) or odds ratio) and the absolute effect (risk difference) should be reported (with confidence intervals), as neither the relative measure nor the absolute measure alone gives a complete picture of the effect and its implications. Different audiences may prefer either relative or absolute risk, but both doctors and lay people tend to overestimate the effect when it is presented in terms of relative risk.243 244 245 The size of the risk difference is less generalisable to other populations than the relative risk since it depends on the baseline risk in the unexposed group, which tends to vary across populations. For diseases where the outcome is common, a relative risk near unity might indicate clinically important differences in public health terms. In contrast, a large relative risk when the outcome is rare may not be so important for public health (although it may be important to an individual in a high risk category).\n\n\nExamples\n\nThe risk of oxygen dependence or death was reduced by 16% (95% CI 25% to 7%). The absolute difference was −6.3% (95% CI −9.9% to −2.7%); early administration to an estimated 16 babies would therefore prevent 1 baby dying or being long-term dependent on oxygen” Table 7: Example of reporting both absolute and relative effect sizes. (Adapted from table 3 of The OSIRIS Collaborative Group242). https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2844943/table/tbl7/\n\nBack to top\n\n\n\n\n\n\n18. Ancillary analyses\n\n\n\n\n\n\n\nResults of any other analyses performed, including subgroup analyses and adjusted analyses, distinguishing pre-specified from exploratory\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nMultiple analyses of the same data create a risk for false positive findings.246 Authors should resist the temptation to perform many subgroup analyses.183 185 247 Analyses that were prespecified in the trial protocol (see item 24) are much more reliable than those suggested by the data, and therefore authors should report which analyses were prespecified. If subgroup analyses were undertaken, authors should report which subgroups were examined, why, if they were prespecified, and how many were prespecified. Selective reporting of subgroup analyses could lead to bias.248 When evaluating a subgroup the question is not whether the subgroup shows a statistically significant result but whether the subgroup treatment effects are significantly different from each other. To determine this, a test of interaction is helpful, although the power for such tests is typically low. If formal evaluations of interaction are undertaken (see item 12b) they should be reported as the estimated difference in the intervention effect in each subgroup (with a confidence interval), not just as P values.\nIn one survey, 35 of 50 trial reports included subgroup analyses, of which only 42% used tests of interaction.183 It was often difficult to determine whether subgroup analyses had been specified in the protocol. In another survey of surgical trials published in high impact journals, 27 of 72 trials reported 54 subgroup analyses, of which 91% were post hoc and only 6% of subgroup analyses used a test of interaction to assess whether a subgroup effect existed.249\nSimilar recommendations apply to analyses in which adjustment was made for baseline variables. If done, both unadjusted and adjusted analyses should be reported. Authors should indicate whether adjusted analyses, including the choice of variables to adjust for, were planned. Ideally, the trial protocol should state whether adjustment is made for nominated baseline variables by using analysis of covariance.187 Adjustment for variables because they differ significantly at baseline is likely to bias the estimated treatment effect.187 A survey found that unacknowledged discrepancies between protocols and publications were found for all 25 trials reporting subgroup analyses and for 23 of 28 trials reporting adjusted analyses.\n\n\nExamples\n\nOn the basis of a study that suggested perioperative β-blocker efficacy might vary across baseline risk, we prespecified our primary subgroup analysis on the basis of the revised cardiac risk index scoring system. We also did prespecified secondary subgroup analyses based on sex, type of surgery, and use of an epidural or spinal anaesthetic. For all subgroup analyses, we used Cox proportional hazard models that incorporated tests for interactions, designated to be significant at p<0.05 … Figure 3 shows the results of our prespecified subgroup analyses and indicates consistency of effects … Our subgroup analyses were underpowered to detect the modest differences in subgroup effects that one might expect to detect if there was a true subgroup effect.\n\nBack to top\n\n\n\n\n\n\n19. Harms\n\n\n\n\n\n\n\nAll important harms or unintended effects in each group (For specific guidance see CONSORT for harms)\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nReaders need information about the harms as well as the benefits of interventions to make rational and balanced decisions. The existence and nature of adverse effects can have a major impact on whether a particular intervention will be deemed acceptable and useful. Not all reported adverse events observed during a trial are necessarily a consequence of the intervention; some may be a consequence of the condition being treated. Randomised trials offer the best approach for providing safety data as well as efficacy data, although they cannot detect rare harms.\nMany reports of RCTs provide inadequate information on adverse events. A survey of 192 drug trials published from 1967 to 1999 showed that only 39% had adequate reporting of clinical adverse events and 29% had adequate reporting of laboratory defined toxicity.72 More recently, a comparison between the adverse event data submitted to the trials database of the National Cancer Institute, which sponsored the trials, and the information reported in journal articles found that low grade adverse events were underreported in journal articles. High grade events (Common Toxicity Criteria grades 3 to 5) were reported inconsistently in the articles, and the information regarding attribution to investigational drugs was incomplete.251 Moreover, a review of trials published in six general medical journals in 2006 to 2007 found that, although 89% of 133 reports mentioned adverse events, no information on severe adverse events and withdrawal of patients due to an adverse event was given on 27% and 48% of articles, respectively.252\nAn extension of the CONSORT statement has been developed to provide detailed recommendations on the reporting of harms in randomised trials.42 Recommendations and examples of appropriate reporting are freely available from the CONSORT website (www.consort-statement.org). They complement the CONSORT 2010 Statement and should be consulted, particularly if the study of harms was a key objective. Briefly, if data on adverse events were collected, events should be listed and defined, with reference to standardised criteria where appropriate. The methods used for data collection and attribution of events should be described. For each study arm the absolute risk of each adverse event, using appropriate metrics for recurrent events, and the number of participants withdrawn due to harms should be presented. Finally, authors should provide a balanced discussion of benefits and harms.\n\n\nExamples\n\nThe proportion of patients experiencing any adverse event was similar between the rBPI21 [recombinant bactericidal/permeability-increasing protein] and placebo groups: 168 (88.4%) of 190 and 180 (88.7%) of 203, respectively, and it was lower in patients treated with rBPI21 than in those treated with placebo for 11 of 12 body systems … the proportion of patients experiencing a severe adverse event, as judged by the investigators, was numerically lower in the rBPI21 group than the placebo group: 53 (27.9%) of 190 versus 74 (36.5%) of 203 patients, respectively. There were only three serious adverse events reported as drug-related and they all occurred in the placebo group.\n\nBack to top\n\n\n\n\n\n\nDiscussion\n\n\n20. Limitations\n\n\n\n\n\n\n\nTrial limitations, addressing sources of potential bias, imprecision, and, if relevant, multiplicity of analyses\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe discussion sections of scientific reports are often filled with rhetoric supporting the authors’ findings254 and provide little measured argument of the pros and cons of the study and its results. Some journals have attempted to remedy this problem by encouraging more structure to authors’ discussion of their results.255 256 For example, Annals of Internal Medicine recommends that authors structure the discussion section by presenting (1) a brief synopsis of the key findings, (2) consideration of possible mechanisms and explanations, (3) comparison with relevant findings from other published studies (whenever possible including a systematic review combining the results of the current study with the results of all previous relevant studies), (4) limitations of the present study (and methods used to minimise and compensate for those limitations), and (5) a brief section that summarises the clinical and research implications of the work, as appropriate.255 We recommend that authors follow these sensible suggestions, perhaps also using suitable subheadings in the discussion section.\nAlthough discussion of limitations is frequently omitted from research reports,257 identification and discussion of the weaknesses of a study have particular importance.258 For example, a surgical group reported that laparoscopic cholecystectomy, a technically difficult procedure, had significantly lower rates of complications than the more traditional open cholecystectomy for management of acute cholecystitis.259 However, the authors failed to discuss an obvious bias in their results. The study investigators had completed all the laparoscopic cholecystectomies, whereas 80% of the open cholecystectomies had been completed by trainees.\nAuthors should also discuss any imprecision of the results. Imprecision may arise in connection with several aspects of a study, including measurement of a primary outcome (see item 6a) or diagnosis (see item 4a). Perhaps the scale used was validated on an adult population but used in a paediatric one, or the assessor was not trained in how to administer the instrument.\nThe difference between statistical significance and clinical importance should always be borne in mind. Authors should particularly avoid the common error of interpreting a non-significant result as indicating equivalence of interventions. The confidence interval (see item 17a) provides valuable insight into whether the trial result is compatible with a clinically important effect, regardless of the P value.120\nAuthors should exercise special care when evaluating the results of trials with multiple comparisons. Such multiplicity arises from several interventions, outcome measures, time points, subgroup analyses, and other factors. In such circumstances, some statistically significant findings are likely to result from chance alone.\n\n\nExamples\n\nThe preponderance of male patients (85%) is a limitation of our study … We used bare-metal stents, since drug-eluting stents were not available until late during accrual. Although the latter factor may be perceived as a limitation, published data indicate no benefit (either short-term or long-term) with respect to death and myocardial infarction in patients with stable coronary artery disease who receive drug-eluting stents, as compared with those who receive bare-metal stents.\n\nBack to top\n\n\n\n\n\n\n21. Generalisability\n\n\n\n\n\n\n\nGeneralisability (external validity, applicability) of the trial findings\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nExternal validity, also called generalisability or applicability, is the extent to which the results of a study can be generalised to other circumstances.262 Internal validity, the extent to which the design and conduct of the trial eliminate the possibility of bias, is a prerequisite for external validity: the results of a flawed trial are invalid and the question of its external validity becomes irrelevant. There is no absolute external validity; the term is meaningful only with regard to clearly specified conditions that were not directly examined in the trial. Can results be generalised to an individual participant or groups that differ from those enrolled in the trial with regard to age, sex, severity of disease, and comorbid conditions? Are the results applicable to other drugs within a class of similar drugs, to a different dose, timing, and route of administration, and to different concomitant therapies? Can similar results be expected at the primary, secondary, and tertiary levels of care? What about the effect on related outcomes that were not assessed in the trial, and the importance of length of follow-up and duration of treatment, especially with respect to harms?263\nExternal validity is a matter of judgment and depends on the characteristics of the participants included in the trial, the trial setting, the treatment regimens tested, and the outcomes assessed.5 136 It is therefore crucial that adequate information be described about eligibility criteria and the setting and location (see item 4b), the interventions and how they were administered (see item 5), the definition of outcomes (see item 6), and the period of recruitment and follow-up (see item 14). The proportion of control group participants in whom the outcome develops (control group risk) is also important. The proportion of eligible participants who refuse to enter the trial as indicated on the flowchart (see item 13) is relevant for the generalisability of the trial, as it may indicate preferences for or acceptability of an intervention. Similar considerations may apply to clinician preferences.264 265\nSeveral issues are important when results of a trial are applied to an individual patient.266 267 268 Although some variation in treatment response between an individual patient and the patients in a trial or systematic review is to be expected, the differences tend to be in magnitude rather than direction.\nAlthough there are important exceptions,268 therapies (especially drugs 269) found to be beneficial in a narrow range of patients generally have broader application in actual practice. Frameworks for the evaluation of external validity have been proposed, including qualitative studies, such as in integral process evaluations270 and checklists.271 Measures that incorporate baseline risk when calculating therapeutic effects, such as the number needed to treat to obtain one additional favourable outcome and the number needed to treat to produce one adverse effect, are helpful in assessing the benefit-to-risk balance in an individual patient or group with characteristics that differ from the typical trial participant.268 272 273 Finally, after deriving patient centred estimates for the potential benefit and harm from an intervention, the clinician must integrate them with the patient’s values and preferences for therapy. Similar considerations apply when assessing the generalisability of results to different settings and interventions.\n\n\nExamples\n\nAs the intervention was implemented for both sexes, all ages, all types of sports, and at different levels of sports, the results indicate that the entire range of athletes, from young elite to intermediate and recreational senior athletes, would benefit from using the presented training programme for the prevention of recurrences of ankle sprain. By including non-medically treated and medically treated athletes, we covered a broad spectrum of injury severity. This suggests that the present training programme can be implemented in the treatment of all athletes. Furthermore, as it is reasonable to assume that ankle sprains not related to sports are comparable with those in sports, the programme could benefit the general population.\n\nBack to top\n\n\n\n\n\n\n22. Interpretation\n\n\n\n\n\n\n\nInterpretation consistent with results, balancing benefits and harms, and considering other relevant evidence\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nReaders will want to know how the present trial’s results relate to those of other RCTs. This can best be achieved by including a formal systematic review in the results or discussion section of the report.83 275 276 277 Such synthesis may be impractical for trial authors, but it is often possible to quote a systematic review of similar trials. A systematic review may help readers assess whether the results of the RCT are similar to those of other trials in the same topic area and whether participants are similar across studies. Reports of RCTs have often not dealt adequately with these points.277 Bayesian methods can be used to statistically combine the trial data with previous evidence.278\nWe recommend that, at a minimum, the discussion should be as systematic as possible and be based on a comprehensive search, rather than being limited to studies that support the results of the current trial.\n\n\nExamples\n\nStudies published before 1990 suggested that prophylactic immunotherapy also reduced nosocomial infections in very-low-birth-weight infants. However, these studies enrolled small numbers of patients; employed varied designs, preparations, and doses; and included diverse study populations. In this large multicenter, randomised controlled trial, the repeated prophylactic administration of intravenous immune globulin failed to reduce the incidence of nosocomial infections significantly in premature infants weighing 501 to 1500 g at birth.\n\nBack to top\n\n\n\n\n\n\n23. Registration\n\n\n\n\n\n\n\nRegistration number and name of trial registry\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe consequences of non-publication of entire trials,281 282 selective reporting of outcomes within trials, and of per protocol rather than intention-to-treat analysis have been well documented.55 56 283 Covert redundant publication of clinical trials can also cause problems, particularly for authors of systematic reviews when results from the same trial are inadvertently included more than once.284\nTo minimise or avoid these problems there have been repeated calls over the past 25 years to register clinical trials at their inception, to assign unique trial identification numbers, and to record other basic information about the trial so that essential details are made publicly available.285 286 287 288 Provoked by recent serious problems of withholding data,289 there has been a renewed effort to register randomised trials. Indeed, the World Health Organisation states that the registration of all interventional trials is a scientific, ethical and moral responsibility (www.who.int/ictrp/en). By registering a randomised trial, authors typically report a minimal set of information and obtain a unique trial registration number.\nIn September 2004 the International Committee of Medical Journal Editors (ICMJE) changed their policy, saying that they would consider trials for publication only if they had been registered before the enrolment of the first participant.290 This resulted in a dramatic increase in the number of trials being registered.291 The ICMJE gives guidance on acceptable registries (www.icmje.org/faq.pdf).\nIn a recent survey of 165 high impact factor medical journals’ instructions to authors, 44 journals specifically stated that all recent clinical trials must be registered as a requirement of submission to that journal.292\nAuthors should provide the name of the register and the trial’s unique registration number. If authors had not registered their trial they should explicitly state this and give the reason.\n\n\nExamples\n\nThe trial is registered at ClinicalTrials.gov, number NCT00244842.\n\nBack to top\n\n\n\n\n\n\nOther Information\n\n\n24. Protocol\n\n\n\n\n\n\n\nWhere the full trial protocol can be accessed, if available\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nA protocol for the complete trial (rather than a protocol of a specific procedure within a trial) is important because it pre-specifies the methods of the randomised trial, such as the primary outcome (see item 6a). Having a protocol can help to restrict the likelihood of undeclared post hoc changes to the trial methods and selective outcome reporting (see item 6b). Elements that may be important for inclusion in the protocol for a randomised trial are described elsewhere.294\nThere are several options for authors to consider ensuring their trial protocol is accessible to interested readers. As described in the example above, journals reporting a trial’s primary results can make the trial protocol available on their web site. Accessibility to the trial results and protocol is enhanced when the journal is open access. Some journals (such as Trials) publish trial protocols, and such a publication can be referenced when reporting the trial’s principal results. Trial registration (see item 23) will also ensure that many trial protocol details are available, as the minimum trial characteristics included in an approved trial registration database includes several protocol items and results (www.who.int/ictrp/en). Trial investigators may also be able to post their trial protocol on a website through their employer. Whatever mechanism is used, we encourage all trial investigators to make their protocol easily accessible to interested readers.\n\n\nExamples\n\nFull details of the trial protocol can be found in the Supplementary Appendix, available with the full text of this article at www.nejm.org\n\nBack to top\n\n\n\n\n\n\n25. Funding\n\n\n\n\n\n\n\nSources of funding and other support (such as supply of drugs), role of funders\n\n\n\n\n\n\nExplanation, examples, and more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAuthors should report the sources of funding for the trial, as this is important information for readers assessing a trial. Studies have showed that research sponsored by the pharmaceutical industry are more likely to produce results favouring the product made by the company sponsoring the research than studies funded by other sources.297 298 299 300 A systematic review of 30 studies on funding found that research funded by the pharmaceutical industry had four times the odds of having outcomes favouring the sponsor than research funded by other sources (odds ratio 4.05, 95% confidence interval 2.98 to 5.51).297 A large proportion of trial publications do not currently report sources of funding. The degree of underreporting is difficult to quantify. A survey of 370 drug trials found that 29% failed to report sources of funding.301 In another survey, of PubMed indexed randomised trials published in December 2000, source of funding was reported for 66% of the 519 trials.16\nThe level of involvement by a funder and their influence on the design, conduct, analysis, and reporting of a trial varies. It is therefore important that authors describe in detail the role of the funders. If the funder had no such involvement, the authors should state so. Similarly, authors should report any other sources of support, such as supply and preparation of drugs or equipment, or in the analysis of data and writing of the manuscript.\n\n\nExamples\n\nGrant support was received for the intervention from Plan International and for the research from the Wellcome Trust and Joint United Nations Programme on HIV/AIDS (UNAIDS). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-1a_title-1",
    "href": "guidelines/consort/index.html#sec-1a_title-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nThe ability to identify a report of a randomised trial in an electronic database depends to a large extent on how it was indexed. Indexers may not classify a report as a randomised trial if the authors do not explicitly report this information.64 To help ensure that a study is appropriately indexed and easily identified, authors should use the word randomised in the title to indicate that the participants were randomly assigned to their comparison groups"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-1a_title-2",
    "href": "guidelines/consort/index.html#sec-1a_title-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nSmoking reduction with oral nicotine inhalers: double blind, randomised clinical trial of efficacy and safety\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-1b_abstract-1",
    "href": "guidelines/consort/index.html#sec-1b_abstract-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nClear, transparent, and sufficiently detailed abstracts are important because readers often base their assessment of a trial on such information. Some readers use an abstract as a screening tool to decide whether to read the full article. However, as not all trials are freely available and some health professionals do not have access to the full trial reports, healthcare decisions are sometimes made on the basis of abstracts of randomised trials.66\nA journal abstract should contain sufficient information about a trial to serve as an accurate record of its conduct and findings, providing optimal information about the trial within the space constraints and format of a journal. A properly constructed and written abstract helps individuals to assess quickly the relevance of the findings and aids the retrieval of relevant reports from electronic databases.67 The abstract should accurately reflect what is included in the full journal article and should not include information that does not appear in the body of the paper. Studies comparing the accuracy of information reported in a journal abstract with that reported in the text of the full publication have found claims that are inconsistent with, or missing from, the body of the full article.68 69 70 71 Conversely, omitting important harms from the abstract could seriously mislead someone’s interpretation of the trial findings.42 72\nA recent extension to the CONSORT statement provides a list of essential items that authors should include when reporting the main results of a randomised trial in a journal (or conference) abstract (see table 2​2).45 We strongly recommend the use of structured abstracts for reporting randomised trials. They provide readers with information about the trial under a series of headings pertaining to the design, conduct, analysis, and interpretation.73 Some studies have found that structured abstracts are of higher quality than the more traditional descriptive abstracts74 75 and that they allow readers to find information more easily.76 We recognise that many journals have developed their own structure and word limit for reporting abstracts. It is not our intention to suggest changes to these formats, but to recommend what information should be reported"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-1b_abstract-2",
    "href": "guidelines/consort/index.html#sec-1b_abstract-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nFor specific guidance see CONSORT for abstracts\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-2a_background_and_objectives-1",
    "href": "guidelines/consort/index.html#sec-2a_background_and_objectives-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nTypically, the introduction consists of free flowing text, in which authors explain the scientific background and rationale for their trial, and its general outline. It may also be appropriate to include here the objectives of the trial (see item 2b).The rationale may be explanatory (for example, to assess the possible influence of a drug on renal function) or pragmatic (for example, to guide practice by comparing the benefits and harms of two treatments). Authors should report any evidence of the benefits and harms of active interventions included in a trial and should suggest a plausible explanation for how the interventions might work, if this is not obvious.78\nThe Declaration of Helsinki states that biomedical research involving people should be based on a thorough knowledge of the scientific literature.79 That is, it is unethical to expose humans unnecessarily to the risks of research. Some clinical trials have been shown to have been unnecessary because the question they addressed had been or could have been answered by a systematic review of the existing literature.80 81 Thus, the need for a new trial should be justified in the introduction. Ideally, it should include a reference to a systematic review of previous similar trials or a note of the absence of such trials"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-2a_background_and_objectives-2",
    "href": "guidelines/consort/index.html#sec-2a_background_and_objectives-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nSurgery is the treatment of choice for patients with disease stage I and II non-small cell lung cancer (NSCLC) … An NSCLC meta-analysis combined the results from eight randomised trials of surgery versus surgery plus adjuvant cisplatin-based chemotherapy and showed a small, but not significant (p=0.08), absolute survival benefit of around 5% at 5 years (from 50% to 55%). At the time the current trial was designed (mid-1990s), adjuvant chemotherapy had not become standard clinical practice … The clinical rationale for neo-adjuvant chemotherapy is three-fold: regression of the primary cancer could be achieved thereby facilitating and simplifying or reducing subsequent surgery; undetected micro-metastases could be dealt with at the start of treatment; and there might be inhibition of the putative stimulus to residual cancer by growth factors released by surgery and by subsequent wound healing … The current trial was therefore set up to compare, in patients with resectable NSCLC, surgery alone versus three cycles of platinum-based chemotherapy followed by surgery in terms of overall survival, quality of life, pathological staging, resectability rates, extent of surgery, and time to and site of relapse\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-2b_background_and_objectives-1",
    "href": "guidelines/consort/index.html#sec-2b_background_and_objectives-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nObjectives are the questions that the trial was designed to answer. They often relate to the efficacy of a particular therapeutic or preventive intervention. Hypotheses are pre-specified questions being tested to help meet the objectives. Hypotheses are more specific than objectives and are amenable to explicit statistical evaluation. In practice, objectives and hypotheses are not always easily differentiated. Most reports of RCTs provide adequate information about trial objectives and hypotheses"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-2b_background_and_objectives-2",
    "href": "guidelines/consort/index.html#sec-2b_background_and_objectives-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nIn the current study we tested the hypothesis that a policy of active management of nulliparous labour would: 1. reduce the rate of caesarean section, 2. reduce the rate of prolonged labour; 3. not influence maternal satisfaction with the birth experience\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-3a_trial_design-1",
    "href": "guidelines/consort/index.html#sec-3a_trial_design-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nThe word design is often used to refer to all aspects of how a trial is set up, but it also has a narrower interpretation. Many specific aspects of the broader trial design, including details of randomisation and blinding, are addressed elsewhere in the CONSORT checklist. Here we seek information on the type of trial, such as parallel group or factorial, and the conceptual framework, such as superiority or non-inferiority, and other related issues not addressed elsewhere in the checklist.\nThe CONSORT statement focuses mainly on trials with participants individually randomised to one of two parallel groups. In fact, little more than half of published trials have such a design.16 The main alternative designs are multi-arm parallel, crossover, cluster,40 and factorial designs. Also, most trials are set to identify the superiority of a new intervention, if it exists, but others are designed to assess non-inferiority or equivalence.39 It is important that researchers clearly describe these aspects of their trial, including the unit of randomisation (such as patient, GP practice, lesion). It is desirable also to include these details in the abstract (see item 1b).\nIf a less common design is employed, authors are encouraged to explain their choice, especially as such designs may imply the need for a larger sample size or more complex analysis and interpretation.\nAlthough most trials use equal randomisation (such as 1:1 for two groups), it is helpful to provide the allocation ratio explicitly. For drug trials, specifying the phase of the trial (I-IV) may also be relevant"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-3a_trial_design-2",
    "href": "guidelines/consort/index.html#sec-3a_trial_design-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nThis was a multicenter, stratified (6 to 11 years and 12 to 17 years of age, with imbalanced randomisation [2:1]), double-blind, placebo-controlled, parallel-group study conducted in the United States (41 sites\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-3b_trial_design-1",
    "href": "guidelines/consort/index.html#sec-3b_trial_design-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nA few trials may start without any fixed plan (that is, are entirely exploratory), but the most will have a protocol that specifies in great detail how the trial will be conducted. There may be deviations from the original protocol, as it is impossible to predict every possible change in circumstances during the course of a trial. Some trials will therefore have important changes to the methods after trial commencement.\nChanges could be due to external information becoming available from other studies, or internal financial difficulties, or could be due to a disappointing recruitment rate. Such protocol changes should be made without breaking the blinding on the accumulating data on participants’ outcomes. In some trials, an independent data monitoring committee will have as part of its remit the possibility of recommending protocol changes based on seeing unblinded data. Such changes might affect the study methods (such as changes to treatment regimens, eligibility criteria, randomisation ratio, or duration of follow-up) or trial conduct (such as dropping a centre with poor data quality).87\nSome trials are set up with a formal adaptive design. There is no universally accepted definition of these designs, but a working definition might be a multistage study design that uses accumulating data to decide how to modify aspects of the study without undermining the validity and integrity of the trial.88 The modifications are usually to the sample sizes and the number of treatment arms and can lead to decisions being made more quickly and with more efficient use of resources. There are, however, important ethical, statistical, and practical issues in considering such a design.89 90\nWhether the modifications are explicitly part of the trial design or in response to changing circumstances, it is essential that they are fully reported to help the reader interpret the results. Changes from protocols are not currently well reported. A review of comparisons with protocols showed that about half of journal articles describing RCTs had an unexplained discrepancy in the primary outcomes.57 Frequent unexplained discrepancies have also been observed for details of randomisation, blinding,91 and statistical analyses"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-3b_trial_design-2",
    "href": "guidelines/consort/index.html#sec-3b_trial_design-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nPatients were randomly assigned to one of six parallel groups, initially in 1:1:1:1:1:1 ratio, to receive either one of five otamixaban … regimens … or an active control of unfractionated heparin … an independent Data Monitoring Committee reviewed unblinded data for patient safety; no interim analyses for efficacy or futility were done. During the trial, this committee recommended that the group receiving the lowest dose of otamixaban (0·035 mg/kg/h) be discontinued because of clinical evidence of inadequate anticoagulation. The protocol was immediately amended in accordance with that recommendation, and participants were subsequently randomly assigned in 2:2:2:2:1 ratio to the remaining otamixaban and control groups, respectively\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-4a_participants_-1",
    "href": "guidelines/consort/index.html#sec-4a_participants_-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nA comprehensive description of the eligibility criteria used to select the trial participants is needed to help readers interpret the study. In particular, a clear understanding of these criteria is one of several elements required to judge to whom the results of a trial apply—that is, the trial’s generalisability (applicability) and relevance to clinical or public health practice (see item 21).94 A description of the method of recruitment, such as by referral or self selection (for example, through advertisements), is also important in this context. Because they are applied before randomisation, eligibility criteria do not affect the internal validity of a trial, but they are central to its external validity.\nTypical and widely accepted selection criteria relate to the nature and stage of the disease being studied, the exclusion of persons thought to be particularly vulnerable to harm from the study intervention, and to issues required to ensure that the study satisfies legal and ethical norms. Informed consent by study participants, for example, is typically required in intervention studies. The common distinction between inclusion and exclusion criteria is unnecessary; the same criterion can be phrased to include or exclude participants.95\nDespite their importance, eligibility criteria are often not reported adequately. For example, eight published trials leading to clinical alerts by the National Institutes of Health specified an average of 31 eligibility criteria in their protocols, but only 63% of the criteria were mentioned in the journal articles, and only 19% were mentioned in the clinical alerts.96 Similar deficiencies were found for HIV clinical trials.97 Among 364 reports of RCTs in surgery, 25% did not specify any eligibility criteria"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-4a_participants_-2",
    "href": "guidelines/consort/index.html#sec-4a_participants_-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nEligible participants were all adults aged 18 or over with HIV who met the eligibility criteria for antiretroviral therapy according to the Malawian national HIV treatment guidelines (WHO clinical stage III or IV or any WHO stage with a CD4 count <250/mm3) and who were starting treatment with a BMI <18.5. Exclusion criteria were pregnancy and lactation or participation in another supplementary feeding programme.\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-4b_participants_-1",
    "href": "guidelines/consort/index.html#sec-4b_participants_-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nAlong with the eligibility criteria for participants (see item 4a) and the description of the interventions (see item 5), information on the settings and locations is crucial to judge the applicability and generalisability of a trial. Were participants recruited from primary, secondary, or tertiary health care or from the community? Healthcare institutions vary greatly in their organisation, experience, and resources and the baseline risk for the condition under investigation. Other aspects of the setting (including the social, economic, and cultural environment and the climate) may also affect a study’s external validity.\nAuthors should report the number and type of settings and describe the care providers involved. They should report the locations in which the study was carried out, including the country, city if applicable, and immediate environment (for example, community, office practice, hospital clinic, or inpatient unit). In particular, it should be clear whether the trial was carried out in one or several centres (multicentre trials). This description should provide enough information so that readers can judge whether the results of the trial could be relevant to their own setting. The environment in which the trial is conducted may differ considerably from the setting in which the trial’s results are later used to guide practice and policy.94 99 Authors should also report any other information about the settings and locations that could have influenced the observed results, such as problems with transportation that might have affected patient participation or delays in administering interventions"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-4b_participants_-2",
    "href": "guidelines/consort/index.html#sec-4b_participants_-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nThe study took place at the antiretroviral therapy clinic of Queen Elizabeth Central Hospital in Blantyre, Malawi, from January 2006 to April 2007. Blantyre is the major commercial city of Malawi, with a population of 1 000 000 and an estimated HIV prevalence of 27% in adults in 2004\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-5_interventions-1",
    "href": "guidelines/consort/index.html#sec-5_interventions-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nAuthors should describe each intervention thoroughly, including control interventions. The description should allow a clinician wanting to use the intervention to know exactly how to administer the intervention that was evaluated in the trial.102 For a drug intervention, information would include the drug name, dose, method of administration (such as oral, intravenous), timing and duration of administration, conditions under which interventions are withheld, and titration regimen if applicable. If the control group is to receive usual care it is important to describe thoroughly what that constitutes. If the control group or intervention group is to receive a combination of interventions the authors should provide a thorough description of each intervention, an explanation of the order in which the combination of interventions are introduced or withdrawn, and the triggers for their introduction if applicable.\nSpecific extensions of the CONSORT statement address the reporting of non-pharmacologic and herbal interventions and their particular reporting requirements (such as expertise, details of how the interventions were standardised).43 44 We recommend readers consult the statements for non-pharmacologic and herbal interventions as appropriate."
  },
  {
    "objectID": "guidelines/consort/index.html#sec-5_interventions-2",
    "href": "guidelines/consort/index.html#sec-5_interventions-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nIn POISE, patients received the first dose of the study drug (ie, oral extended-release metoprolol 100 mg or matching placebo) 2-4 h before surgery. Study drug administration required a heart rate of 50 bpm or more and a systolic blood pressure of 100 mm Hg or greater; these haemodynamics were checked before each administration. If, at any time during the first 6 h after surgery, heart rate was 80 bpm or more and systolic blood pressure was 100 mm Hg or higher, patients received their first postoperative dose (extended-release metoprolol 100 mg or matched placebo) orally. If the study drug was not given during the first 6 h, patients received their first postoperative dose at 6 h after surgery. 12 h after the first postoperative dose, patients started taking oral extended-release metoprolol 200 mg or placebo every day for 30 days. If a patient’s heart rate was consistently below 45 bpm or their systolic blood pressure dropped below 100 mm Hg, study drug was withheld until their heart rate or systolic blood pressure recovered; the study drug was then restarted at 100 mg once daily. Patients whose heart rate was consistently 45-49 bpm and systolic blood pressure exceeded 100 mm Hg delayed taking the study drug for 12 h\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-6a_outcomes-1",
    "href": "guidelines/consort/index.html#sec-6a_outcomes-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nAll RCTs assess response variables, or outcomes (end points), for which the groups are compared. Most trials have several outcomes, some of which are of more interest than others. The primary outcome measure is the pre-specified outcome considered to be of greatest importance to relevant stakeholders (such a patients, policy makers, clinicians, funders) and is usually the one used in the sample size calculation (see item 7). Some trials may have more than one primary outcome. Having several primary outcomes, however, incurs the problems of interpretation associated with multiplicity of analyses (see items 18 and 20) and is not recommended. Primary outcomes should be explicitly indicated as such in the report of an RCT. Other outcomes of interest are secondary outcomes (additional outcomes). There may be several secondary outcomes, which often include unanticipated or unintended effects of the intervention (see item 19), although harms should always be viewed as important whether they are labelled primary or secondary.\nAll outcome measures, whether primary or secondary, should be identified and completely defined. The principle here is that the information provided should be sufficient to allow others to use the same outcomes.102 When outcomes are assessed at several time points after randomisation, authors should also indicate the pre-specified time point of primary interest. For many non-pharmacological interventions it is helpful to specify who assessed outcomes (for example, if special skills are required to do so) and how many assessors there were.43\nWhere available and appropriate, the use of previously developed and validated scales or consensus guidelines should be reported,104 105 both to enhance quality of measurement and to assist in comparison with similar studies.106 For example, assessment of quality of life is likely to be improved by using a validated instrument.107 Authors should indicate the provenance and properties of scales.\nMore than 70 outcomes were used in 196 RCTs of non-steroidal anti-inflammatory drugs for rheumatoid arthritis,108 and 640 different instruments had been used in 2000 trials in schizophrenia, of which 369 had been used only once.33 Investigation of 149 of those 2000 trials showed that unpublished scales were a source of bias. In non-pharmacological trials, a third of the claims of treatment superiority based on unpublished scales would not have been made if a published scale had been used.109 Similar data have been reported elsewhere.110 111 Only 45% of a cohort of 519 RCTs published in 2000 specified the primary outcome16; this compares with 53% for a similar cohort of 614 RCTs published in 2006"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-6a_outcomes-2",
    "href": "guidelines/consort/index.html#sec-6a_outcomes-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nThe primary endpoint with respect to efficacy in psoriasis was the proportion of patients achieving a 75% improvement in psoriasis activity from baseline to 12 weeks as measured by the PASI [psoriasis area and severity index] Additional analyses were done on the percentage change in PASI scores and improvement in target psoriasis lesions.\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-6b_outcomes-1",
    "href": "guidelines/consort/index.html#sec-6b_outcomes-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nThere are many reasons for departures from the initial study protocol (see item 24). Authors should report all major changes to the protocol, including unplanned changes to eligibility criteria, interventions, examinations, data collection, methods of analysis, and outcomes. Such information is not always reported.\nAs indicated earlier (see item 6a), most trials record multiple outcomes, with the risk that results will be reported for only a selected subset (see item 17). Pre-specification and reporting of primary and secondary outcomes (see item 6a) should remove such a risk. In some trials, however, circumstances require a change in the way an outcome is assessed or even, as in the example above, a switch to a different outcome. For example, there may be external evidence from other trials or systematic reviews suggesting the end point might not be appropriate, or recruitment or the overall event rate in the trial may be lower than expected.112 Changing an end point based on unblinded data is much more problematic, although it may be specified in the context of an adaptive trial design.88 Authors should identify and explain any such changes. Likewise, any changes after the trial began of the designation of outcomes as primary or secondary should be reported and explained.\nA comparison of protocols and publications of 102 randomised trials found that 62% of trials reports had at least one primary outcome that was changed, introduced, or omitted compared with the protocol.55 Primary outcomes also differed between protocols and publications for 40% of a cohort of 48 trials funded by the Canadian Institutes of Health Research.113 Not one of the subsequent 150 trial reports mentioned, let alone explained, changes from the protocol. Similar results from other studies have been reported recently in a systematic review of empirical studies examining outcome reporting bias"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-6b_outcomes-2",
    "href": "guidelines/consort/index.html#sec-6b_outcomes-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nThe original primary endpoint was all-cause mortality, but, during a masked analysis, the data and safety monitoring board noted that overall mortality was lower than had been predicted and that the study could not be completed with the sample size and power originally planned. The steering committee therefore decided to adopt co-primary endpoints of all-cause mortality (the original primary endpoint), together with all-cause mortality or cardiovascular hospital admissions (the first prespecified secondary endpoint).\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-7a_sample_size-1",
    "href": "guidelines/consort/index.html#sec-7a_sample_size-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nFor scientific and ethical reasons, the sample size for a trial needs to be planned carefully, with a balance between medical and statistical considerations. Ideally, a study should be large enough to have a high probability (power) of detecting as statistically significant a clinically important difference of a given size if such a difference exists. The size of effect deemed important is inversely related to the sample size necessary to detect it; that is, large samples are necessary to detect small differences. Elements of the sample size calculation are (1) the estimated outcomes in each group (which implies the clinically important target difference between the intervention groups); (2) the α (type I) error level; (3) the statistical power (or the β (type II) error level); and (4), for continuous outcomes, the standard deviation of the measurements.116 The interplay of these elements and their reporting will differ for cluster trials40 and non-inferiority and equivalence trials.39\nAuthors should indicate how the sample size was determined. If a formal power calculation was used, the authors should identify the primary outcome on which the calculation was based (see item 6a), all the quantities used in the calculation, and the resulting target sample size per study group. It is preferable to quote the expected result in the control group and the difference between the groups one would not like to overlook. Alternatively, authors could present the percentage with the event or mean for each group used in their calculations. Details should be given of any allowance made for attrition or non-compliance during the study.\nSome methodologists have written that so called underpowered trials may be acceptable because they could ultimately be combined in a systematic review and meta-analysis,117 118 119 and because some information is better than no information. Of note, important caveats apply—such as the trial should be unbiased, reported properly, and published irrespective of the results, thereby becoming available for meta-analysis.118 On the other hand, many medical researchers worry that underpowered trials with indeterminate results will remain unpublished and insist that all trials should individually have sufficient power. This debate will continue, and members of the CONSORT Group have varying views. Critically however, the debate and those views are immaterial to reporting a trial. Whatever the power of a trial, authors need to properly report their intended size with all their methods and assumptions.118 That transparently reveals the power of the trial to readers and gives them a measure by which to assess whether the trial attained its planned size.\nIn some trials, interim analyses are used to help decide whether to stop early or to continue recruiting sometimes beyond the planned trial end (see item 7b). If the actual sample size differed from the originally intended sample size for some other reason (for example, because of poor recruitment or revision of the target sample size), the explanation should be given.\nReports of studies with small samples frequently include the erroneous conclusion that the intervention groups do not differ, when in fact too few patients were studied to make such a claim.120 Reviews of published trials have consistently found that a high proportion of trials have low power to detect clinically meaningful treatment effects.121 122 123 In reality, small but clinically meaningful true differences are much more likely than large differences to exist, but large trials are required to detect them.124\nIn general, the reported sample sizes in trials seem small. The median sample size was 54 patients in 196 trials in arthritis,108 46 patients in 73 trials in dermatology,8 and 65 patients in 2000 trials in schizophrenia.33 These small sample sizes are consistent with those of a study of 519 trials indexed in PubMed in December 200016 and a similar cohort of trials (n=616) indexed in PubMed in 2006,17 where the median number of patients recruited for parallel group trials was 80 across both years. Moreover, many reviews have found that few authors report how they determined the sample size.8 14 32 33 123\nThere is little merit in a post hoc calculation of statistical power using the results of a trial; the power is then appropriately indicated by confidence intervals (see item 17)."
  },
  {
    "objectID": "guidelines/consort/index.html#sec-7a_sample_size-2",
    "href": "guidelines/consort/index.html#sec-7a_sample_size-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nTo detect a reduction in PHS (postoperative hospital stay) of 3 days (SD 5 days), which is in agreement with the study of Lobo et al17 with a two-sided 5% significance level and a power of 80%, a sample size of 50 patients per group was necessary, given an anticipated dropout rate of 10%. To recruit this number of patients a 12-month inclusion period was anticipated.\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-7b_sample_size-1",
    "href": "guidelines/consort/index.html#sec-7b_sample_size-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nMany trials recruit participants over a long period. If an intervention is working particularly well or badly, the study may need to be ended early for ethical reasons. This concern can be addressed by examining results as the data accumulate, preferably by an independent data monitoring committee. However, performing multiple statistical examinations of accumulating data without appropriate correction can lead to erroneous results and interpretations.128 If the accumulating data from a trial are examined at five interim analyses that use a P value of 0.05, the overall false positive rate is nearer to 19% than to the nominal 5%.\nSeveral group sequential statistical methods are available to adjust for multiple analyses,129 130 131 and their use should be pre-specified in the trial protocol. With these methods, data are compared at each interim analysis, and a P value less than the critical value specified by the group sequential method indicates statistical significance. Some trialists use group sequential methods as an aid to decision making,132 whereas others treat them as a formal stopping rule (with the intention that the trial will cease if the observed P value is smaller than the critical value).\nAuthors should report whether they or a data monitoring committee took multiple looks at the data and, if so, how many there were, what triggered them, the statistical methods used (including any formal stopping rule), and whether they were planned before the start of the trial, before the data monitoring committee saw any interim data by allocation, or some time thereafter. This information is often not included in published trial reports,133 even in trials that report stopping earlier than planned"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-7b_sample_size-2",
    "href": "guidelines/consort/index.html#sec-7b_sample_size-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nTwo interim analyses were performed during the trial. The levels of significance maintained an overall P value of 0.05 and were calculated according to the O’Brien-Fleming stopping boundaries. This final analysis used a Z score of 1.985 with an associated P value of 0.0471.\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-8a_randomization__sequence_generation-1",
    "href": "guidelines/consort/index.html#sec-8a_randomization__sequence_generation-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nParticipants should be assigned to comparison groups in the trial on the basis of a chance (random) process characterised by unpredictability (see box 1). Authors should provide sufficient information that the reader can assess the methods used to generate the random allocation sequence and the likelihood of bias in group assignment. It is important that information on the process of randomisation is included in the body of the main article and not as a separate supplementary file; where it can be missed by the reader.\nThe term random has a precise technical meaning. With random allocation, each participant has a known probability of receiving each intervention before one is assigned, but the assigned intervention is determined by a chance process and cannot be predicted. However, random is often used inappropriately in the literature to describe trials in which non-random, deterministic allocation methods were used, such as alternation, hospital numbers, or date of birth. When investigators use such non-random methods, they should describe them precisely and should not use the term random or any variation of it. Even the term quasi-random is unacceptable for describing such trials. Trials based on non-random methods generally yield biased results.2 3 4 136 Bias presumably arises from the inability to conceal these allocation systems adequately (see item 9).\nMany methods of sequence generation are adequate. However, readers cannot judge adequacy from such terms as random allocation, randomisation, or random without further elaboration. Authors should specify the method of sequence generation, such as a random-number table or a computerised random number generator. The sequence may be generated by the process of minimisation, a non-random but generally acceptable method (see box 2). In some trials, participants are intentionally allocated in unequal numbers to each intervention: for example, to gain more experience with a new procedure or to limit costs of the trial. In such cases, authors should report the randomisation ratio (for example, 2:1 or two treatment participants per each control participant) (see item 3a).\nIn a representative sample of PubMed indexed trials in 2000, only 21% reported an adequate approach to random sequence generation16; this increased to 34% for a similar cohort of PubMed indexed trials in 2006.17 In more than 90% of these cases, researchers used a random number generator on a computer or a random number table."
  },
  {
    "objectID": "guidelines/consort/index.html#sec-8a_randomization__sequence_generation-2",
    "href": "guidelines/consort/index.html#sec-8a_randomization__sequence_generation-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nIndependent pharmacists dispensed either active or placebo inhalers according to a computer generated randomisation list.\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-8b_randomization__sequence_generation-1",
    "href": "guidelines/consort/index.html#sec-8b_randomization__sequence_generation-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nIn trials of several hundred participants or more simple randomisation can usually be trusted to generate similar numbers in the two trial groups139 and to generate groups that are roughly comparable in terms of known and unknown prognostic variables.140 For smaller trials (see item 7a)—and even for trials that are not intended to be small, as they may stop before reaching their target size—some restricted randomisation (procedures to help achieve balance between groups in size or characteristics) may be useful (see box 2).\nIt is important to indicate whether no restriction was used, by stating such or by stating that simple randomisation was done. Otherwise, the methods used to restrict the randomisation, along with the method used for random selection, should be specified. For block randomisation, authors should provide details on how the blocks were generated (for example, by using a permuted block design with a computer random number generator), the block size or sizes, and whether the block size was fixed or randomly varied. If the trialists became aware of the block size(s), that information should also be reported as such knowledge could lead to code breaking. Authors should specify whether stratification was used, and if so, which factors were involved (such as recruitment site, sex, disease stage), the categorisation cut-off values within strata, and the method used for restriction. Although stratification is a useful technique, especially for smaller trials, it is complicated to implement and may be impossible if many stratifying factors are used. If minimisation (see box 2) was used, it should be explicitly identified, as should the variables incorporated into the scheme. If used, a random element should be indicated.\nOnly 9% of 206 reports of trials in specialty journals23 and 39% of 80 trials in general medical journals reported use of stratification.32 In each case, only about half of the reports mentioned the use of restricted randomisation. However, these studies and that of Adetugbo and Williams8 found that the sizes of the treatment groups in many trials were the same or quite similar, yet blocking or stratification had not been mentioned. One possible explanation for the close balance in numbers is underreporting of the use of restricted randomisation."
  },
  {
    "objectID": "guidelines/consort/index.html#sec-8b_randomization__sequence_generation-2",
    "href": "guidelines/consort/index.html#sec-8b_randomization__sequence_generation-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nRandomization sequence was created using Stata 9.0 (StataCorp, College Station, TX) statistical software and was stratified by center with a 1:1 allocation using random block sizes of 2, 4, and 6.\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-9_randomization__allocation_concealment_mechanism-1",
    "href": "guidelines/consort/index.html#sec-9_randomization__allocation_concealment_mechanism-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nItem 8a discussed generation of an unpredictable sequence of assignments. Of considerable importance is how this sequence is applied when participants are enrolled into the trial (see box 1). A generated allocation schedule should be implemented by using allocation concealment,23 a critical mechanism that prevents foreknowledge of treatment assignment and thus shields those who enroll participants from being influenced by this knowledge. The decision to accept or reject a participant should be made, and informed consent should be obtained from the participant, in ignorance of the next assignment in the sequence.148\nThe allocation concealment should not be confused with blinding (see item 11). Allocation concealment seeks to prevent selection bias, protects the assignment sequence until allocation, and can always be successfully implemented.2 In contrast, blinding seeks to prevent performance and ascertainment bias, protects the sequence after allocation, and cannot always be implemented.23 Without adequate allocation concealment, however, even random, unpredictable assignment sequences can be subverted.2 149\nCentralised or third-party assignment is especially desirable. Many good allocation concealment mechanisms incorporate external involvement. Use of a pharmacy or central telephone randomisation system are two common techniques. Automated assignment systems are likely to become more common.150 When external involvement is not feasible, an excellent method of allocation concealment is the use of numbered containers. The interventions (often drugs) are sealed in sequentially numbered identical containers according to the allocation sequence.151 Enclosing assignments in sequentially numbered, opaque, sealed envelopes can be a good allocation concealment mechanism if it is developed and monitored diligently. This method can be corrupted, however, particularly if it is poorly executed. Investigators should ensure that the envelopes are opaque when held to the light, and opened sequentially and only after the participant’s name and other details are written on the appropriate envelope.143\nA number of methodological studies provide empirical evidence to support these precautions.152 153 Trials in which the allocation sequence had been inadequately or unclearly concealed yielded larger estimates of treatment effects than did trials in which authors reported adequate allocation concealment. These findings provide strong empirical evidence that inadequate allocation concealment contributes to bias in estimating treatment effects.\nDespite the importance of the mechanism of allocation concealment, published reports often omit such details. The mechanism used to allocate interventions was omitted in reports of 89% of trials in rheumatoid arthritis,108 48% of trials in obstetrics and gynaecology journals,23 and 44% of trials in general medical journals.32 In a more broadly representative sample of all randomised trials indexed on PubMed, only 18% reported any allocation concealment mechanism, but some of those reported mechanisms were inadequate."
  },
  {
    "objectID": "guidelines/consort/index.html#sec-9_randomization__allocation_concealment_mechanism-2",
    "href": "guidelines/consort/index.html#sec-9_randomization__allocation_concealment_mechanism-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nThe doxycycline and placebo were in capsule form and identical in appearance. They were prepacked in bottles and consecutively numbered for each woman according to the randomisation schedule. Each woman was assigned an order number and received the capsules in the corresponding prepacked bottle.\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-10_randomization__implementation-1",
    "href": "guidelines/consort/index.html#sec-10_randomization__implementation-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nAs noted in item 9, concealment of the allocated intervention at the time of enrolment is especially important. Thus, in addition to knowing the methods used, it is also important to understand how the random sequence was implemented—specifically, who generated the allocation sequence, who enrolled participants, and who assigned participants to trial groups.\nThe process of randomising participants into a trial has three different steps: sequence generation, allocation concealment, and implementation (see box 3). Although the same people may carry out more than one process under each heading, investigators should strive for complete separation of the people involved with generation and allocation concealment from the people involved in the implementation of assignments. Thus, if someone is involved in the sequence generation or allocation concealment steps, ideally they should not be involved in the implementation step.\nEven with flawless sequence generation and allocation concealment, failure to separate creation and concealment of the allocation sequence from assignment to study group may introduce bias. For example, the person who generated an allocation sequence could retain a copy and consult it when interviewing potential participants for a trial. Thus, that person could bias the enrolment or assignment process, regardless of the unpredictability of the assignment sequence. Investigators must then ensure that the assignment schedule is unpredictable and locked away (such as in a safe deposit box in a building rather inaccessible to the enrolment location) from even the person who generated it. The report of the trial should specify where the investigators stored the allocation list"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-10_randomization__implementation-2",
    "href": "guidelines/consort/index.html#sec-10_randomization__implementation-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nDetermination of whether a patient would be treated by streptomycin and bed-rest (S case) or by bed-rest alone (C case) was made by reference to a statistical series based on random sampling numbers drawn up for each sex at each centre by Professor Bradford Hill; the details of the series were unknown to any of the investigators or to the co-ordinator … After acceptance of a patient by the panel, and before admission to the streptomycin centre, the appropriate numbered envelope was opened at the central office; the card inside told if the patient was to be an S or a C case, and this information was then given to the medical officer of the centre.\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-11a_blinding-1",
    "href": "guidelines/consort/index.html#sec-11a_blinding-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nThe term blinding or masking refers to withholding information about the assigned interventions from people involved in the trial who may potentially be influenced by this knowledge. Blinding is an important safeguard against bias, particularly when assessing subjective outcomes.153\nBenjamin Franklin has been credited as being the first to use blinding in a scientific experiment.158 He blindfolded participants so they would not know when he was applying mesmerism (a popular healing fluid of the 18th century) and in so doing showed that mesmerism was a sham. Based on this experiment, the scientific community recognised the power of blinding to reduce bias, and it has remained a commonly used strategy in scientific experiments.\nBox 4, on blinding terminology, defines the groups of individuals (that is, participants, healthcare providers, data collectors, outcome adjudicators, and data analysts) who can potentially introduce bias into a trial through knowledge of the treatment assignments. Participants may respond differently if they are aware of their treatment assignment (such as responding more favourably when they receive the new treatment).153 Lack of blinding may also influence compliance with the intervention, use of co-interventions, and risk of dropping out of the trial.\nUnblinded healthcare providers may introduce similar biases, and unblinded data collectors may differentially assess outcomes (such as frequency or timing), repeat measurements of abnormal findings, or provide encouragement during performance testing. Unblinded outcome adjudicators may differentially assess subjective outcomes, and unblinded data analysts may introduce bias through the choice of analytical strategies, such as the selection of favourable time points or outcomes, and by decisions to remove patients from the analyses. These biases have been well documented.71 153 159 160 161 162\nBlinding, unlike allocation concealment (see item 10), may not always be appropriate or possible. An example is a trial comparing levels of pain associated with sampling blood from the ear or thumb.163 Blinding is particularly important when outcome measures involve some subjectivity, such as assessment of pain. Blinding of data collectors and outcome adjudicators is unlikely to matter for objective outcomes, such as death from any cause. Even then, however, lack of participant or healthcare provider blinding can lead to other problems, such as differential attrition.164 In certain trials, especially surgical trials, blinding of participants and surgeons is often difficult or impossible, but blinding of data collectors and outcome adjudicators is often achievable. For example, lesions can be photographed before and after treatment and assessed by an external observer.165 Regardless of whether blinding is possible, authors can and should always state who was blinded (that is, participants, healthcare providers, data collectors, and outcome adjudicators).\nUnfortunately, authors often do not report whether blinding was used.166 For example, reports of 51% of 506 trials in cystic fibrosis,167 33% of 196 trials in rheumatoid arthritis,108 and 38% of 68 trials in dermatology8 did not state whether blinding was used. Until authors of trials improve their reporting of blinding, readers will have difficulty in judging the validity of the trials that they may wish to use to guide their clinical practice.\nThe term masking is sometimes used in preference to blinding to avoid confusion with the medical condition of being without sight. However, blinding in its methodological sense seems to be understood worldwide and is acceptable for reporting clinical trials."
  },
  {
    "objectID": "guidelines/consort/index.html#sec-11a_blinding-2",
    "href": "guidelines/consort/index.html#sec-11a_blinding-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nWhereas patients and physicians allocated to the intervention group were aware of the allocated arm, outcome assessors and data analysts were kept blinded to the allocation.\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-11b_blinding-1",
    "href": "guidelines/consort/index.html#sec-11b_blinding-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nJust as we seek evidence of concealment to assure us that assignment was truly random, we seek evidence of the method of blinding. In trials with blinding of participants or healthcare providers, authors should state the similarity of the characteristics of the interventions (such as appearance, taste, smell, and method of administration).35 173\nSome people have advocated testing for blinding by asking participants or healthcare providers at the end of a trial whether they think the participant received the experimental or control intervention.174 Because participants and healthcare providers will usually know whether the participant has experienced the primary outcome, this makes it difficult to determine if their responses reflect failure of blinding or accurate assumptions about the efficacy of the intervention.175 Given the uncertainty this type of information provides, we have removed advocating reporting this type of testing for blinding from the CONSORT 2010 Statement. We do, however, advocate that the authors report any known compromises in blinding. For example, authors should report if it was necessary to unblind any participants at any point during the conduct of a trial."
  },
  {
    "objectID": "guidelines/consort/index.html#sec-11b_blinding-2",
    "href": "guidelines/consort/index.html#sec-11b_blinding-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nJamieson Laboratories Inc provided 500-mg immediate release niacin in a white, oblong, bisect caplet. We independently confirmed caplet content using high performance liquid chromatography … The placebo was matched to the study drug for taste, color, and size, and contained microcrystalline cellulose, silicon dioxide, dicalcium phosphate, magnesium stearate, and stearic acid.\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-12a_statistical_methods-1",
    "href": "guidelines/consort/index.html#sec-12a_statistical_methods-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nData can be analysed in many ways, some of which may not be strictly appropriate in a particular situation. It is essential to specify which statistical procedure was used for each analysis, and further clarification may be necessary in the results section of the report. The principle to follow is to, Describe statistical methods with enough detail to enable a knowledgeable reader with access to the original data to verify the reported results (www.icmje.org). It is also important to describe details of the statistical analysis such as intention-to-treat analysis (see box 6).\nAlmost all methods of analysis yield an estimate of the treatment effect, which is a contrast between the outcomes in the comparison groups. Authors should accompany this by a confidence interval for the estimated effect, which indicates a central range of uncertainty for the true treatment effect. The confidence interval may be interpreted as the range of values for the treatment effect that is compatible with the observed data. It is customary to present a 95% confidence interval, which gives the range expected to include the true value in 95 of 100 similar studies.\nStudy findings can also be assessed in terms of their statistical significance. The P value represents the probability that the observed data (or a more extreme result) could have arisen by chance when the interventions did not truly differ. Actual P values (for example, P=0.003) are strongly preferable to imprecise threshold reports such as P<0.05.48 177\nStandard methods of analysis assume that the data are independent. For controlled trials, this usually means that there is one observation per participant. Treating multiple observations from one participant as independent data is a serious error; such data are produced when outcomes can be measured on different parts of the body, as in dentistry or rheumatology. Data analysis should be based on counting each participant once178 179 or should be done by using more complex statistical procedures.180 Incorrect analysis of multiple observations per individual was seen in 123 (63%) of 196 trials in rheumatoid arthritis."
  },
  {
    "objectID": "guidelines/consort/index.html#sec-12a_statistical_methods-2",
    "href": "guidelines/consort/index.html#sec-12a_statistical_methods-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nThe primary endpoint was change in bodyweight during the 20 weeks of the study in the intention-to-treat population … Secondary efficacy endpoints included change in waist circumference, systolic and diastolic blood pressure, prevalence of metabolic syndrome … We used an analysis of covariance (ANCOVA) for the primary endpoint and for secondary endpoints waist circumference, blood pressure, and patient-reported outcome scores; this was supplemented by a repeated measures analysis. The ANCOVA model included treatment, country, and sex as fixed effects, and bodyweight at randomisation as covariate. We aimed to assess whether data provided evidence of superiority of each liraglutide dose to placebo (primary objective) and to orlistat (secondary objective).\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-12b_statistical_methods-1",
    "href": "guidelines/consort/index.html#sec-12b_statistical_methods-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nAs is the case for primary analyses, the method of subgroup analysis should be clearly specified. The strongest analyses are those that look for evidence of a difference in treatment effect in complementary subgroups (for example, older and younger participants), a comparison known as a test of interaction.182 183 A common but misleading approach is to compare P values for separate analyses of the treatment effect in each group. It is incorrect to infer a subgroup effect (interaction) from one significant and one non-significant P value.184 Such inferences have a high false positive rate.\nBecause of the high risk for spurious findings, subgroup analyses are often discouraged.14 185 Post hoc subgroup comparisons (analyses done after looking at the data) are especially likely not to be confirmed by further studies. Such analyses do not have great credibility.\nIn some studies, imbalances in participant characteristics are adjusted for by using some form of multiple regression analysis. Although the need for adjustment is much less in RCTs than in epidemiological studies, an adjusted analysis may be sensible, especially if one or more variables is thought to be prognostic.186 Ideally, adjusted analyses should be specified in the study protocol (see item 24). For example, adjustment is often recommended for any stratification variables (see item 8b) on the principle that the analysis strategy should follow the design. In RCTs, the decision to adjust should not be determined by whether baseline differences are statistically significant (see item 16).183 187 The rationale for any adjusted analyses and the statistical methods used should be specified.\nAuthors should clarify the choice of variables that were adjusted for, indicate how continuous variables were handled, and specify whether the analysis was planned or suggested by the data.188 Reviews of published studies show that reporting of adjusted analyses is inadequate with regard to all of these aspects.188 189 190 191"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-12b_statistical_methods-2",
    "href": "guidelines/consort/index.html#sec-12b_statistical_methods-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nProportions of patients responding were compared between treatment groups with the Mantel-Haenszel χ2 test, adjusted for the stratification variable, methotrexate use.\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-13a_participant_flow_diagram_strongly_recommended-1",
    "href": "guidelines/consort/index.html#sec-13a_participant_flow_diagram_strongly_recommended-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nThe design and conduct of some RCTs is straightforward, and the flow of participants, particularly were there are no losses to follow-up or exclusions, through each phase of the study can be described adequately in a few sentences. In more complex studies, it may be difficult for readers to discern whether and why some participants did not receive the treatment as allocated, were lost to follow-up, or were excluded from the analysis.51 This information is crucial for several reasons. Participants who were excluded after allocation are unlikely to be representative of all participants in the study. For example, patients may not be available for follow-up evaluation because they experienced an acute exacerbation of their illness or harms of treatment.22 192\nAttrition as a result of loss to follow up, which is often unavoidable, needs to be distinguished from investigator-determined exclusion for such reasons as ineligibility, withdrawal from treatment, and poor adherence to the trial protocol. Erroneous conclusions can be reached if participants are excluded from analysis, and imbalances in such omissions between groups may be especially indicative of bias.192 193 194 Information about whether the investigators included in the analysis all participants who underwent randomisation, in the groups to which they were originally allocated (intention-to-treat analysis (see item 16 and box 6)), is therefore of particular importance. Knowing the number of participants who did not receive the intervention as allocated or did not complete treatment permits the reader to assess to what extent the estimated efficacy of therapy might be underestimated in comparison with ideal circumstances.\nIf available, the number of people assessed for eligibility should also be reported. Although this number is relevant to external validity only and is arguably less important than the other counts,195 it is a useful indicator of whether trial participants were likely to be representative of all eligible participants.\nA review of RCTs published in five leading general and internal medicine journals in 1998 found that reporting of the flow of participants was often incomplete, particularly with regard to the number of participants receiving the allocated intervention and the number lost to follow-up.51 Even information as basic as the number of participants who underwent randomisation and the number excluded from analyses was not available in up to 20% of articles.51 Reporting was considerably more thorough in articles that included a diagram of the flow of participants through a trial, as recommended by CONSORT. This study informed the design of the revised flow diagram in the revised CONSORT statement.52 53 54 The suggested template is shown in fig 1​1,, and the counts required are described in detail in table 3​3.\nSome information, such as the number of individuals assessed for eligibility, may not always be known,14 and, depending on the nature of a trial, some counts may be more relevant than others. It will sometimes be useful or necessary to adapt the structure of the flow diagram to a particular trial. In some situations, other information may usefully be added. For example, the flow diagram of a parallel group trial of minimal surgery compared with medical management for chronic gastro-oesophageal reflux also included a parallel non-randomised preference group (see fig 3).196\nThe exact form and content of the flow diagram may be varied according to specific features of a trial. For example, many trials of surgery or vaccination do not include the possibility of discontinuation. Although CONSORT strongly recommends using this graphical device to communicate participant flow throughout the study, there is no specific, prescribed format."
  },
  {
    "objectID": "guidelines/consort/index.html#sec-13a_participant_flow_diagram_strongly_recommended-2",
    "href": "guidelines/consort/index.html#sec-13a_participant_flow_diagram_strongly_recommended-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nFlow diagram of a multicentre trial of fractional flow reserve versus angiography for guiding percutaneous coronary intervention (PCI) (adapted from Tonino et al313). The diagram includes detailed information on the excluded participants. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2844943/figure/fig2/\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-13b_participant_flow-1",
    "href": "guidelines/consort/index.html#sec-13b_participant_flow-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nSome protocol deviations may be reported in the flow diagram (see item 13a)—for example, participants who did not receive the intended intervention. If participants were excluded after randomisation (contrary to the intention-to-treat principle) because they were found not to meet eligibility criteria (see item 16), they should be included in the flow diagram. Use of the term protocol deviation in published articles is not sufficient to justify exclusion of participants after randomisation. The nature of the protocol deviation and the exact reason for excluding participants after randomisation should always be reported."
  },
  {
    "objectID": "guidelines/consort/index.html#sec-13b_participant_flow-2",
    "href": "guidelines/consort/index.html#sec-13b_participant_flow-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nThere was only one protocol deviation, in a woman in the study group. She had an abnormal pelvic measurement and was scheduled for elective caesarean section. However, the attending obstetrician judged a trial of labour acceptable; caesarean section was done when there was no progress in the first stage of labour.\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-14a_recruitment-1",
    "href": "guidelines/consort/index.html#sec-14a_recruitment-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nKnowing when a study took place and over what period participants were recruited places the study in historical context. Medical and surgical therapies, including concurrent therapies, evolve continuously and may affect the routine care given to participants during a trial. Knowing the rate at which participants were recruited may also be useful, especially to other investigators.\nThe length of follow-up is not always a fixed period after randomisation. In many RCTs in which the outcome is time to an event, follow-up of all participants is ended on a specific date. This date should be given, and it is also useful to report the minimum, maximum, and median duration of follow-up.200 201\nA review of reports in oncology journals that used survival analysis, most of which were not RCTs, 201 found that nearly 80% (104 of 132 reports) included the starting and ending dates for accrual of patients, but only 24% (32 of 132 reports) also reported the date on which follow-up ended."
  },
  {
    "objectID": "guidelines/consort/index.html#sec-14a_recruitment-2",
    "href": "guidelines/consort/index.html#sec-14a_recruitment-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nAge-eligible participants were recruited … from February 1993 to September 1994 … Participants attended clinic visits at the time of randomisation (baseline) and at 6-month intervals for 3 years.\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-14b_recruitment-1",
    "href": "guidelines/consort/index.html#sec-14b_recruitment-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nArguably, trialists who arbitrarily conduct unplanned interim analyses after very few events accrue using no statistical guidelines run a high risk of catching the data at a random extreme, which likely represents a large overestimate of treatment benefit.204\nReaders will likely draw weaker inferences from a trial that was truncated in a data-driven manner versus one that reports its findings after reaching a goal independent of results. Thus, RCTs should indicate why the trial came to an end (see box 5). The report should also disclose factors extrinsic to the trial that affected the decision to stop the trial, and who made the decision to stop the trial, including reporting the role the funding agency played in the deliberations and in the decision to stop the trial.134\nA systematic review of 143 RCTs stopped earlier than planned for benefit found that these trials reported stopping after accruing a median of 66 events, estimated a median relative risk of 0.47 and a strong relation between the number of events accrued and the size of the effect, with smaller trials with fewer events yielding the largest treatment effects (odds ratio 31, 95% conﬁdence interval 12 to 82).134 While an increasing number of trials published in high impact medical journals report stopping early, only 0.1% of trials reported stopping early for benefit, which contrasts with estimates arising from simulation studies205 and surveys of data safety and monitoring committees.206 Thus, many trials accruing few participants and reporting large treatment effects may have been stopped earlier than planned but failed to report this action."
  },
  {
    "objectID": "guidelines/consort/index.html#sec-14b_recruitment-2",
    "href": "guidelines/consort/index.html#sec-14b_recruitment-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nAt the time of the interim analysis, the total follow-up included an estimated 63% of the total number of patient-years that would have been collected at the end of the study, leading to a threshold value of 0.0095, as determined by the Lan-DeMets alpha-spending function method … At the interim analysis, the RR was 0.37 in the intervention group, as compared with the control group, with a p value of 0.00073, below the threshold value. The Data and Safety Monitoring Board advised the investigators to interrupt the trial and offer circumcision to the control group, who were then asked to come to the investigation centre, where MC (medical circumcision) was advised and proposed … Because the study was interrupted, some participants did not have a full follow-up on that date, and their visits that were not yet completed are described as planned in this article.\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-15_baseline_data-1",
    "href": "guidelines/consort/index.html#sec-15_baseline_data-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nAlthough the eligibility criteria (see item 4a) indicate who was eligible for the trial, it is also important to know the characteristics of the participants who were actually included. This information allows readers, especially clinicians, to judge how relevant the results of a trial might be to an individual patient.\nRandomised trials aim to compare groups of participants that differ only with respect to the intervention (treatment). Although proper random assignment prevents selection bias, it does not guarantee that the groups are equivalent at baseline. Any differences in baseline characteristics are, however, the result of chance rather than bias.32 The study groups should be compared at baseline for important demographic and clinical characteristics so that readers can assess how similar they were. Baseline data are especially valuable for outcomes that can also be measured at the start of the trial (such as blood pressure).\nBaseline information is most efficiently presented in a table (see table 4​4).). For continuous variables, such as weight or blood pressure, the variability of the data should be reported, along with average values. Continuous variables can be summarised for each group by the mean and standard deviation. When continuous data have an asymmetrical distribution, a preferable approach may be to quote the median and a centile range (such as the 25th and 75th centiles).177 Standard errors and confidence intervals are not appropriate for describing variability—they are inferential rather than descriptive statistics. Variables with a small number of ordered categories (such as stages of disease I to IV) should not be treated as continuous variables; instead, numbers and proportions should be reported for each category.48 177\nUnfortunately significance tests of baseline differences are still common23 32 210; they were reported in half of 50 RCTs trials published in leading general journals in 1997.183 Such significance tests assess the probability that observed baseline differences could have occurred by chance; however, we already know that any differences are caused by chance. Tests of baseline differences are not necessarily wrong, just illogical.211 Such hypothesis testing is superfluous and can mislead investigators and their readers. Rather, comparisons at baseline should be based on consideration of the prognostic strength of the variables measured and the size of any chance imbalances that have occurred"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-15_baseline_data-2",
    "href": "guidelines/consort/index.html#sec-15_baseline_data-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nExample of reporting baseline demographic and clinical characteristics.* (Adapted from table 1 of Yusuf et al209) https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2844943/table/tbl4/\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-16_numbers_analysed-1",
    "href": "guidelines/consort/index.html#sec-16_numbers_analysed-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nThe number of participants in each group is an essential element of the analyses. Although the flow diagram (see item 13a) may indicate the numbers of participants analysed, these numbers often vary for different outcome measures. The number of participants per group should be given for all analyses. For binary outcomes, (such as risk ratio and risk difference) the denominators or event rates should also be reported. Expressing results as fractions also aids the reader in assessing whether some of the randomly assigned participants were excluded from the analysis. It follows that results should not be presented solely as summary measures, such as relative risks.\nParticipants may sometimes not receive the full intervention, or some ineligible patients may have been randomly allocated in error. One widely recommended way to handle such issues is to analyse all participants according to their original group assignment, regardless of what subsequently occurred (see box 6). This intention-to-treat strategy is not always straightforward to implement. It is common for some patients not to complete a study—they may drop out or be withdrawn from active treatment—and thus are not assessed at the end. If the outcome is mortality, such patients may be included in the analysis based on register information, whereas imputation techniques may need to be used if other outcome data are missing. The term intention-to-treat analysis is often inappropriately used—for example, when those who did not receive the first dose of a trial drug are excluded from the analyses.18\nConversely, analysis can be restricted to only participants who fulfil the protocol in terms of eligibility, interventions, and outcome assessment. This analysis is known as an on-treatment or per protocol analysis. Excluding participants from the analysis can lead to erroneous conclusions. For example, in a trial that compared medical with surgical therapy for carotid stenosis, analysis limited to participants who were available for follow-up showed that surgery reduced the risk for transient ischaemic attack, stroke, and death. However, intention-to-treat analysis based on all participants as originally assigned did not show a superior effect of surgery.214\nIntention-to-treat analysis is generally favoured because it avoids bias associated with non-random loss of participants.215 216 217 Regardless of whether authors use the term intention-to-treat, they should make clear which and how many participants are included in each analysis (see item 13). Non-compliance with assigned therapy may mean that the intention-to-treat analysis underestimates the potential benefit of the treatment, and additional analyses, such as a per protocol analysis, may therefore be considered.218 219 It should be noted, however, that such analyses are often considerably flawed.220\nIn a review of 403 RCTs published in 10 leading medical journals in 2002, 249 (62%) reported the use of intention-to-treat analysis for their primary analysis. This proportion was higher for journals adhering to the CONSORT statement (70% v 48%). Among articles that reported the use of intention-to-treat analysis, only 39% actually analysed all participants as randomised, with more than 60% of articles having missing data in their primary analysis.221 Other studies show similar findings.18 222 223 Trials with no reported exclusions are methodologically weaker in other respects than those that report on some excluded participants,173 strongly indicating that at least some researchers who have excluded participants do not report it. Another study found that reporting an intention-to-treat analysis was associated with other aspects of good study design and reporting, such as describing a sample size calculation."
  },
  {
    "objectID": "guidelines/consort/index.html#sec-16_numbers_analysed-2",
    "href": "guidelines/consort/index.html#sec-16_numbers_analysed-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nThe primary analysis was intention-to-treat and involved all patients who were randomly assigned.\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-17a_outcomes_and_estimation-1",
    "href": "guidelines/consort/index.html#sec-17a_outcomes_and_estimation-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nFor each outcome, study results should be reported as a summary of the outcome in each group (for example, the number of participants with or without the event and the denominators, or the mean and standard deviation of measurements), together with the contrast between the groups, known as the effect size. For binary outcomes, the effect size could be the risk ratio (relative risk), odds ratio, or risk difference; for survival time data, it could be the hazard ratio or difference in median survival time; and for continuous data, it is usually the difference in means. Confidence intervals should be presented for the contrast between groups. A common error is the presentation of separate confidence intervals for the outcome in each group rather than for the treatment effect.233 Trial results are often more clearly displayed in a table rather than in the text, as shown in tables 5​5 and 6​6.\nFor all outcomes, authors should provide a confidence interval to indicate the precision (uncertainty) of the estimate.48 235 A 95% confidence interval is conventional, but occasionally other levels are used. Many journals require or strongly encourage the use of confidence intervals.236 They are especially valuable in relation to differences that do not meet conventional statistical significance, for which they often indicate that the result does not rule out an important clinical difference. The use of confidence intervals has increased markedly in recent years, although not in all medical specialties.233 Although P values may be provided in addition to confidence intervals, results should not be reported solely as P values.237 238 Results should be reported for all planned primary and secondary end points, not just for analyses that were statistically significant or interesting. Selective reporting within a study is a widespread and serious problem.55 57 In trials in which interim analyses were performed, interpretation should focus on the final results at the close of the trial, not the interim results.239\nFor both binary and survival time data, expressing the results also as the number needed to treat for benefit or harm can be helpful (see item 21)."
  },
  {
    "objectID": "guidelines/consort/index.html#sec-17a_outcomes_and_estimation-2",
    "href": "guidelines/consort/index.html#sec-17a_outcomes_and_estimation-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nExample of reporting of summary results for each study group (binary outcomes).* (Adapted from table 2 of Mease et al103) https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2844943/table/tbl5/\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-17b_outcomes_and_estimation-1",
    "href": "guidelines/consort/index.html#sec-17b_outcomes_and_estimation-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nWhen the primary outcome is binary, both the relative effect (risk ratio (relative risk) or odds ratio) and the absolute effect (risk difference) should be reported (with confidence intervals), as neither the relative measure nor the absolute measure alone gives a complete picture of the effect and its implications. Different audiences may prefer either relative or absolute risk, but both doctors and lay people tend to overestimate the effect when it is presented in terms of relative risk.243 244 245 The size of the risk difference is less generalisable to other populations than the relative risk since it depends on the baseline risk in the unexposed group, which tends to vary across populations. For diseases where the outcome is common, a relative risk near unity might indicate clinically important differences in public health terms. In contrast, a large relative risk when the outcome is rare may not be so important for public health (although it may be important to an individual in a high risk category)."
  },
  {
    "objectID": "guidelines/consort/index.html#sec-17b_outcomes_and_estimation-2",
    "href": "guidelines/consort/index.html#sec-17b_outcomes_and_estimation-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nThe risk of oxygen dependence or death was reduced by 16% (95% CI 25% to 7%). The absolute difference was −6.3% (95% CI −9.9% to −2.7%); early administration to an estimated 16 babies would therefore prevent 1 baby dying or being long-term dependent on oxygen” Table 7: Example of reporting both absolute and relative effect sizes. (Adapted from table 3 of The OSIRIS Collaborative Group242). https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2844943/table/tbl7/\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-18_ancillary_analyses-1",
    "href": "guidelines/consort/index.html#sec-18_ancillary_analyses-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nMultiple analyses of the same data create a risk for false positive findings.246 Authors should resist the temptation to perform many subgroup analyses.183 185 247 Analyses that were prespecified in the trial protocol (see item 24) are much more reliable than those suggested by the data, and therefore authors should report which analyses were prespecified. If subgroup analyses were undertaken, authors should report which subgroups were examined, why, if they were prespecified, and how many were prespecified. Selective reporting of subgroup analyses could lead to bias.248 When evaluating a subgroup the question is not whether the subgroup shows a statistically significant result but whether the subgroup treatment effects are significantly different from each other. To determine this, a test of interaction is helpful, although the power for such tests is typically low. If formal evaluations of interaction are undertaken (see item 12b) they should be reported as the estimated difference in the intervention effect in each subgroup (with a confidence interval), not just as P values.\nIn one survey, 35 of 50 trial reports included subgroup analyses, of which only 42% used tests of interaction.183 It was often difficult to determine whether subgroup analyses had been specified in the protocol. In another survey of surgical trials published in high impact journals, 27 of 72 trials reported 54 subgroup analyses, of which 91% were post hoc and only 6% of subgroup analyses used a test of interaction to assess whether a subgroup effect existed.249\nSimilar recommendations apply to analyses in which adjustment was made for baseline variables. If done, both unadjusted and adjusted analyses should be reported. Authors should indicate whether adjusted analyses, including the choice of variables to adjust for, were planned. Ideally, the trial protocol should state whether adjustment is made for nominated baseline variables by using analysis of covariance.187 Adjustment for variables because they differ significantly at baseline is likely to bias the estimated treatment effect.187 A survey found that unacknowledged discrepancies between protocols and publications were found for all 25 trials reporting subgroup analyses and for 23 of 28 trials reporting adjusted analyses."
  },
  {
    "objectID": "guidelines/consort/index.html#sec-18_ancillary_analyses-2",
    "href": "guidelines/consort/index.html#sec-18_ancillary_analyses-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nOn the basis of a study that suggested perioperative β-blocker efficacy might vary across baseline risk, we prespecified our primary subgroup analysis on the basis of the revised cardiac risk index scoring system. We also did prespecified secondary subgroup analyses based on sex, type of surgery, and use of an epidural or spinal anaesthetic. For all subgroup analyses, we used Cox proportional hazard models that incorporated tests for interactions, designated to be significant at p<0.05 … Figure 3 shows the results of our prespecified subgroup analyses and indicates consistency of effects … Our subgroup analyses were underpowered to detect the modest differences in subgroup effects that one might expect to detect if there was a true subgroup effect.\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-19_harms-1",
    "href": "guidelines/consort/index.html#sec-19_harms-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nReaders need information about the harms as well as the benefits of interventions to make rational and balanced decisions. The existence and nature of adverse effects can have a major impact on whether a particular intervention will be deemed acceptable and useful. Not all reported adverse events observed during a trial are necessarily a consequence of the intervention; some may be a consequence of the condition being treated. Randomised trials offer the best approach for providing safety data as well as efficacy data, although they cannot detect rare harms.\nMany reports of RCTs provide inadequate information on adverse events. A survey of 192 drug trials published from 1967 to 1999 showed that only 39% had adequate reporting of clinical adverse events and 29% had adequate reporting of laboratory defined toxicity.72 More recently, a comparison between the adverse event data submitted to the trials database of the National Cancer Institute, which sponsored the trials, and the information reported in journal articles found that low grade adverse events were underreported in journal articles. High grade events (Common Toxicity Criteria grades 3 to 5) were reported inconsistently in the articles, and the information regarding attribution to investigational drugs was incomplete.251 Moreover, a review of trials published in six general medical journals in 2006 to 2007 found that, although 89% of 133 reports mentioned adverse events, no information on severe adverse events and withdrawal of patients due to an adverse event was given on 27% and 48% of articles, respectively.252\nAn extension of the CONSORT statement has been developed to provide detailed recommendations on the reporting of harms in randomised trials.42 Recommendations and examples of appropriate reporting are freely available from the CONSORT website (www.consort-statement.org). They complement the CONSORT 2010 Statement and should be consulted, particularly if the study of harms was a key objective. Briefly, if data on adverse events were collected, events should be listed and defined, with reference to standardised criteria where appropriate. The methods used for data collection and attribution of events should be described. For each study arm the absolute risk of each adverse event, using appropriate metrics for recurrent events, and the number of participants withdrawn due to harms should be presented. Finally, authors should provide a balanced discussion of benefits and harms."
  },
  {
    "objectID": "guidelines/consort/index.html#sec-19_harms-2",
    "href": "guidelines/consort/index.html#sec-19_harms-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nThe proportion of patients experiencing any adverse event was similar between the rBPI21 [recombinant bactericidal/permeability-increasing protein] and placebo groups: 168 (88.4%) of 190 and 180 (88.7%) of 203, respectively, and it was lower in patients treated with rBPI21 than in those treated with placebo for 11 of 12 body systems … the proportion of patients experiencing a severe adverse event, as judged by the investigators, was numerically lower in the rBPI21 group than the placebo group: 53 (27.9%) of 190 versus 74 (36.5%) of 203 patients, respectively. There were only three serious adverse events reported as drug-related and they all occurred in the placebo group.\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-20_limitations-1",
    "href": "guidelines/consort/index.html#sec-20_limitations-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nThe discussion sections of scientific reports are often filled with rhetoric supporting the authors’ findings254 and provide little measured argument of the pros and cons of the study and its results. Some journals have attempted to remedy this problem by encouraging more structure to authors’ discussion of their results.255 256 For example, Annals of Internal Medicine recommends that authors structure the discussion section by presenting (1) a brief synopsis of the key findings, (2) consideration of possible mechanisms and explanations, (3) comparison with relevant findings from other published studies (whenever possible including a systematic review combining the results of the current study with the results of all previous relevant studies), (4) limitations of the present study (and methods used to minimise and compensate for those limitations), and (5) a brief section that summarises the clinical and research implications of the work, as appropriate.255 We recommend that authors follow these sensible suggestions, perhaps also using suitable subheadings in the discussion section.\nAlthough discussion of limitations is frequently omitted from research reports,257 identification and discussion of the weaknesses of a study have particular importance.258 For example, a surgical group reported that laparoscopic cholecystectomy, a technically difficult procedure, had significantly lower rates of complications than the more traditional open cholecystectomy for management of acute cholecystitis.259 However, the authors failed to discuss an obvious bias in their results. The study investigators had completed all the laparoscopic cholecystectomies, whereas 80% of the open cholecystectomies had been completed by trainees.\nAuthors should also discuss any imprecision of the results. Imprecision may arise in connection with several aspects of a study, including measurement of a primary outcome (see item 6a) or diagnosis (see item 4a). Perhaps the scale used was validated on an adult population but used in a paediatric one, or the assessor was not trained in how to administer the instrument.\nThe difference between statistical significance and clinical importance should always be borne in mind. Authors should particularly avoid the common error of interpreting a non-significant result as indicating equivalence of interventions. The confidence interval (see item 17a) provides valuable insight into whether the trial result is compatible with a clinically important effect, regardless of the P value.120\nAuthors should exercise special care when evaluating the results of trials with multiple comparisons. Such multiplicity arises from several interventions, outcome measures, time points, subgroup analyses, and other factors. In such circumstances, some statistically significant findings are likely to result from chance alone."
  },
  {
    "objectID": "guidelines/consort/index.html#sec-20_limitations-2",
    "href": "guidelines/consort/index.html#sec-20_limitations-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nThe preponderance of male patients (85%) is a limitation of our study … We used bare-metal stents, since drug-eluting stents were not available until late during accrual. Although the latter factor may be perceived as a limitation, published data indicate no benefit (either short-term or long-term) with respect to death and myocardial infarction in patients with stable coronary artery disease who receive drug-eluting stents, as compared with those who receive bare-metal stents.\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-21_generalisability-1",
    "href": "guidelines/consort/index.html#sec-21_generalisability-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nExternal validity, also called generalisability or applicability, is the extent to which the results of a study can be generalised to other circumstances.262 Internal validity, the extent to which the design and conduct of the trial eliminate the possibility of bias, is a prerequisite for external validity: the results of a flawed trial are invalid and the question of its external validity becomes irrelevant. There is no absolute external validity; the term is meaningful only with regard to clearly specified conditions that were not directly examined in the trial. Can results be generalised to an individual participant or groups that differ from those enrolled in the trial with regard to age, sex, severity of disease, and comorbid conditions? Are the results applicable to other drugs within a class of similar drugs, to a different dose, timing, and route of administration, and to different concomitant therapies? Can similar results be expected at the primary, secondary, and tertiary levels of care? What about the effect on related outcomes that were not assessed in the trial, and the importance of length of follow-up and duration of treatment, especially with respect to harms?263\nExternal validity is a matter of judgment and depends on the characteristics of the participants included in the trial, the trial setting, the treatment regimens tested, and the outcomes assessed.5 136 It is therefore crucial that adequate information be described about eligibility criteria and the setting and location (see item 4b), the interventions and how they were administered (see item 5), the definition of outcomes (see item 6), and the period of recruitment and follow-up (see item 14). The proportion of control group participants in whom the outcome develops (control group risk) is also important. The proportion of eligible participants who refuse to enter the trial as indicated on the flowchart (see item 13) is relevant for the generalisability of the trial, as it may indicate preferences for or acceptability of an intervention. Similar considerations may apply to clinician preferences.264 265\nSeveral issues are important when results of a trial are applied to an individual patient.266 267 268 Although some variation in treatment response between an individual patient and the patients in a trial or systematic review is to be expected, the differences tend to be in magnitude rather than direction.\nAlthough there are important exceptions,268 therapies (especially drugs 269) found to be beneficial in a narrow range of patients generally have broader application in actual practice. Frameworks for the evaluation of external validity have been proposed, including qualitative studies, such as in integral process evaluations270 and checklists.271 Measures that incorporate baseline risk when calculating therapeutic effects, such as the number needed to treat to obtain one additional favourable outcome and the number needed to treat to produce one adverse effect, are helpful in assessing the benefit-to-risk balance in an individual patient or group with characteristics that differ from the typical trial participant.268 272 273 Finally, after deriving patient centred estimates for the potential benefit and harm from an intervention, the clinician must integrate them with the patient’s values and preferences for therapy. Similar considerations apply when assessing the generalisability of results to different settings and interventions."
  },
  {
    "objectID": "guidelines/consort/index.html#sec-21_generalisability-2",
    "href": "guidelines/consort/index.html#sec-21_generalisability-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nAs the intervention was implemented for both sexes, all ages, all types of sports, and at different levels of sports, the results indicate that the entire range of athletes, from young elite to intermediate and recreational senior athletes, would benefit from using the presented training programme for the prevention of recurrences of ankle sprain. By including non-medically treated and medically treated athletes, we covered a broad spectrum of injury severity. This suggests that the present training programme can be implemented in the treatment of all athletes. Furthermore, as it is reasonable to assume that ankle sprains not related to sports are comparable with those in sports, the programme could benefit the general population.\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-22_interpretation-1",
    "href": "guidelines/consort/index.html#sec-22_interpretation-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nReaders will want to know how the present trial’s results relate to those of other RCTs. This can best be achieved by including a formal systematic review in the results or discussion section of the report.83 275 276 277 Such synthesis may be impractical for trial authors, but it is often possible to quote a systematic review of similar trials. A systematic review may help readers assess whether the results of the RCT are similar to those of other trials in the same topic area and whether participants are similar across studies. Reports of RCTs have often not dealt adequately with these points.277 Bayesian methods can be used to statistically combine the trial data with previous evidence.278\nWe recommend that, at a minimum, the discussion should be as systematic as possible and be based on a comprehensive search, rather than being limited to studies that support the results of the current trial."
  },
  {
    "objectID": "guidelines/consort/index.html#sec-22_interpretation-2",
    "href": "guidelines/consort/index.html#sec-22_interpretation-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nStudies published before 1990 suggested that prophylactic immunotherapy also reduced nosocomial infections in very-low-birth-weight infants. However, these studies enrolled small numbers of patients; employed varied designs, preparations, and doses; and included diverse study populations. In this large multicenter, randomised controlled trial, the repeated prophylactic administration of intravenous immune globulin failed to reduce the incidence of nosocomial infections significantly in premature infants weighing 501 to 1500 g at birth.\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-23_registration-1",
    "href": "guidelines/consort/index.html#sec-23_registration-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nThe consequences of non-publication of entire trials,281 282 selective reporting of outcomes within trials, and of per protocol rather than intention-to-treat analysis have been well documented.55 56 283 Covert redundant publication of clinical trials can also cause problems, particularly for authors of systematic reviews when results from the same trial are inadvertently included more than once.284\nTo minimise or avoid these problems there have been repeated calls over the past 25 years to register clinical trials at their inception, to assign unique trial identification numbers, and to record other basic information about the trial so that essential details are made publicly available.285 286 287 288 Provoked by recent serious problems of withholding data,289 there has been a renewed effort to register randomised trials. Indeed, the World Health Organisation states that the registration of all interventional trials is a scientific, ethical and moral responsibility (www.who.int/ictrp/en). By registering a randomised trial, authors typically report a minimal set of information and obtain a unique trial registration number.\nIn September 2004 the International Committee of Medical Journal Editors (ICMJE) changed their policy, saying that they would consider trials for publication only if they had been registered before the enrolment of the first participant.290 This resulted in a dramatic increase in the number of trials being registered.291 The ICMJE gives guidance on acceptable registries (www.icmje.org/faq.pdf).\nIn a recent survey of 165 high impact factor medical journals’ instructions to authors, 44 journals specifically stated that all recent clinical trials must be registered as a requirement of submission to that journal.292\nAuthors should provide the name of the register and the trial’s unique registration number. If authors had not registered their trial they should explicitly state this and give the reason."
  },
  {
    "objectID": "guidelines/consort/index.html#sec-23_registration-2",
    "href": "guidelines/consort/index.html#sec-23_registration-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nThe trial is registered at ClinicalTrials.gov, number NCT00244842.\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-24_protocol-1",
    "href": "guidelines/consort/index.html#sec-24_protocol-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nA protocol for the complete trial (rather than a protocol of a specific procedure within a trial) is important because it pre-specifies the methods of the randomised trial, such as the primary outcome (see item 6a). Having a protocol can help to restrict the likelihood of undeclared post hoc changes to the trial methods and selective outcome reporting (see item 6b). Elements that may be important for inclusion in the protocol for a randomised trial are described elsewhere.294\nThere are several options for authors to consider ensuring their trial protocol is accessible to interested readers. As described in the example above, journals reporting a trial’s primary results can make the trial protocol available on their web site. Accessibility to the trial results and protocol is enhanced when the journal is open access. Some journals (such as Trials) publish trial protocols, and such a publication can be referenced when reporting the trial’s principal results. Trial registration (see item 23) will also ensure that many trial protocol details are available, as the minimum trial characteristics included in an approved trial registration database includes several protocol items and results (www.who.int/ictrp/en). Trial investigators may also be able to post their trial protocol on a website through their employer. Whatever mechanism is used, we encourage all trial investigators to make their protocol easily accessible to interested readers."
  },
  {
    "objectID": "guidelines/consort/index.html#sec-24_protocol-2",
    "href": "guidelines/consort/index.html#sec-24_protocol-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nFull details of the trial protocol can be found in the Supplementary Appendix, available with the full text of this article at www.nejm.org\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#sec-25_funding-1",
    "href": "guidelines/consort/index.html#sec-25_funding-1",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Read More",
    "text": "Read More\nAuthors should report the sources of funding for the trial, as this is important information for readers assessing a trial. Studies have showed that research sponsored by the pharmaceutical industry are more likely to produce results favouring the product made by the company sponsoring the research than studies funded by other sources.297 298 299 300 A systematic review of 30 studies on funding found that research funded by the pharmaceutical industry had four times the odds of having outcomes favouring the sponsor than research funded by other sources (odds ratio 4.05, 95% confidence interval 2.98 to 5.51).297 A large proportion of trial publications do not currently report sources of funding. The degree of underreporting is difficult to quantify. A survey of 370 drug trials found that 29% failed to report sources of funding.301 In another survey, of PubMed indexed randomised trials published in December 2000, source of funding was reported for 66% of the 519 trials.16\nThe level of involvement by a funder and their influence on the design, conduct, analysis, and reporting of a trial varies. It is therefore important that authors describe in detail the role of the funders. If the funder had no such involvement, the authors should state so. Similarly, authors should report any other sources of support, such as supply and preparation of drugs or equipment, or in the analysis of data and writing of the manuscript."
  },
  {
    "objectID": "guidelines/consort/index.html#sec-25_funding-2",
    "href": "guidelines/consort/index.html#sec-25_funding-2",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Examples",
    "text": "Examples\n\nGrant support was received for the intervention from Plan International and for the research from the Wellcome Trust and Joint United Nations Programme on HIV/AIDS (UNAIDS). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.\n\nBack to top"
  },
  {
    "objectID": "guidelines/consort/index.html#ready-to-get-started",
    "href": "guidelines/consort/index.html#ready-to-get-started",
    "title": "The CONSORT guideline for writing a randomized trial.",
    "section": "Ready to get started?",
    "text": "Ready to get started?\nDownload resources"
  },
  {
    "objectID": "guidelines/coreq/index.html#about-this-guideline",
    "href": "guidelines/coreq/index.html#about-this-guideline",
    "title": "The COREQ guideline for writing a qualitative study.",
    "section": "About this guideline",
    "text": "About this guideline\n\nThis guideline is relevant to qualitative studies, especially those that use interviews, focus groups and surveys"
  },
  {
    "objectID": "guidelines/coreq/index.html#download-resources",
    "href": "guidelines/coreq/index.html#download-resources",
    "title": "The COREQ guideline for writing a qualitative study.",
    "section": "Download Resources",
    "text": "Download Resources\nReporting guidelines are best used early:\n\nUse a template to write quickly and avoid the blank page\nUse a checklist to double-check and reassure others that your work is described transparently\n\n\n\n\nFor drafting\n\nTemplate\n\n\n\nFor checking\n\nChecklist\n\n\n\nFor planning\n\nTo-do list"
  },
  {
    "objectID": "guidelines/coreq/index.html#guidance",
    "href": "guidelines/coreq/index.html#guidance",
    "title": "The COREQ guideline for writing a qualitative study.",
    "section": "Guidance",
    "text": "Guidance\nApprox. 1 min read\n\nResearch team and reflexivity\n\n\n1. Personal Characteristics: Interviewer / facilitator\n\n\n\n\n\n\n\nWhich author / s conducted the interview or focus group?\n\n\n2. Personal Characteristics: Credentials\n\n\n\n\n\n\n\nWhat were the researcher’s credentials? E.g. PhD, MD\n\n\n3. Personal Characteristics: Occupation\n\n\n\n\n\n\n\nWhat was their occupation at the time of the study?\n\n\n4. Personal Characteristics: Gender\n\n\n\n\n\n\n\nWas the researcher male or female?\n\n\n5. Personal Characteristics: Experience and training\n\n\n\n\n\n\n\nWhat experience or training did the researcher have?\n\n\n6. Relationship with participants Relationship established\n\n\n\n\n\n\n\nWas a relationship established prior to study commencement?\n\n\nRelationship with participants\n\n\n7. Participant knowledge of the interviewer\n\n\n\n\n\n\n\nWhat did the participants know about the researcher? e.g. personal goals, reasons for doing the research\n\n\n8. Relationship with participants: Interviewer characteristics\n\n\n\n\n\n\n\nWhat characteristics were reported about the interviewer / facilitator? e.g. Bias, assumptions, reasons and interests in the research topic\n\n\nStudy design\n\n\n9. Theoretical framework: Methodological orientation and Theory\n\n\n\n\n\n\n\nWhat methodological orientation was stated to underpin the study? e.g. grounded theory, discourse analysis, ethnography, phenomenology, content analysis\n\n\n10. Participant selection: sampling\n\n\n\n\n\n\n\nHow were participants selected? e.g. purposive, convenience, consecutive, snowball\n\n\n11. Participant selection: method of approach\n\n\n\n\n\n\n\nHow were participants approached? e.g. face-to-face, telephone, mail, email\n\n\n12. Participant selection: sample size\n\n\n\n\n\n\n\nHow many participants were in the study?\n\n\n13. Participant selection: non-participation\n\n\n\n\n\n\n\nHow many people refused to participate or dropped out? Reasons?\n\n\n14. Setting: setting of data collection\n\n\n\n\n\n\n\nWhere was the data collected? e.g. home, clinic, workplace\n\n\n15. Setting: prescence of non participants\n\n\n\n\n\n\n\nWas anyone else present besides the participants and researchers?\n\n\n16. Setting: description of sample\n\n\n\n\n\n\n\nWhat are the important characteristics of the sample? e.g. demographic data, date\n\n\n17. Data collection: interview guide\n\n\n\n\n\n\n\nWere questions, prompts, guides provided by the authors? Was it pilot tested?\n\n\n18. Data collection: repeat interviews\n\n\n\n\n\n\n\nWere repeat interviews carried out? If yes, how many?\n\n\n19. Data collection: audio / visual recording\n\n\n\n\n\n\n\nDid the research use audio or visual recording to collect the data?\n\n\n20. Data collection: field notes\n\n\n\n\n\n\n\nWere field notes made during and / or after the interview or focus group?\n\n\n21. Data collection: duration\n\n\n\n\n\n\n\nWhat was the duration of the interviews or focus group?\n\n\n22. Data collection: data saturation\n\n\n\n\n\n\n\nWas data saturation discussed?\n\n\n23. Data collection: transcripts returned\n\n\n\n\n\n\n\nWere transcripts returned to participants for comment and / or correction?\n\n\nAnalysis and findings\n\n\n24. Data analysis: number of data coders\n\n\n\n\n\n\n\nHow many data coders coded the data?\n\n\n25. Data analysis: description of the coding tree\n\n\n\n\n\n\n\nDid authors provide a description of the coding tree?\n\n\n26. Data analysis: derivation of themes\n\n\n\n\n\n\n\nWere themes identified in advance or derived from the data?\n\n\n27. Data analysis: software\n\n\n\n\n\n\n\nWhat software, if applicable, was used to manage the data?\n\n\n28. Data analysis: participant checking\n\n\n\n\n\n\n\nDid participants provide feedback on the findings?\n\n\n29. Reporting: quotations presented\n\n\n\n\n\n\n\nWere participant quotations presented to illustrate the themes / findings? Was each quotation identified? e.g. participant number\n\n\n30. Reporting: data and findings consistent\n\n\n\n\n\n\n\nWas there consistency between the data presented and the findings?\n\n\n31. Reporting: clarity of major themes\n\n\n\n\n\n\n\nWere major themes clearly presented in the findings?\n\n\n32. Reporting: clarity of minor themes\n\n\n\n\n\n\n\nIs there a description of diverse cases or discussion of minor themes?"
  },
  {
    "objectID": "guidelines/moose/index.html#about-this-guideline",
    "href": "guidelines/moose/index.html#about-this-guideline",
    "title": "The MOOSE guideline for writing a meta-analysis of observational studies.",
    "section": "About this guideline",
    "text": "About this guideline\n\nThis guideline is relevant to studies reporting meta-analyses of observational studies"
  },
  {
    "objectID": "guidelines/moose/index.html#download-resources",
    "href": "guidelines/moose/index.html#download-resources",
    "title": "The MOOSE guideline for writing a meta-analysis of observational studies.",
    "section": "Download Resources",
    "text": "Download Resources\nReporting guidelines are best used early:\n\nUse a template to write quickly and avoid the blank page\nUse a checklist to double-check and reassure others that your work is described transparently\n\n\n\n\nFor drafting\n\nTemplate\n\n\n\nFor checking\n\nChecklist\n\n\n\nFor planning\n\nTo-do list"
  },
  {
    "objectID": "guidelines/moose/index.html#guidance",
    "href": "guidelines/moose/index.html#guidance",
    "title": "The MOOSE guideline for writing a meta-analysis of observational studies.",
    "section": "Guidance",
    "text": "Guidance\nApprox. 1 min read\n\nTitle\n\n\n1. Title\n\n\n\n\n\n\n\nIdentify the study as a meta-analysis of observational research\n\n\nAbstract\n\n\n2. Abstract\n\n\n\n\n\n\n\nProvide a structured summary including, as applicable: background; objectives; data sources; study eligibility criteria, participants, and interventions; study appraisal and synthesis methods; results; limitations; conclusions and implications of key findings; systematic review registration number (From PRISMA checklist)\n\n\nBackground\n\n\n3a. Background\n\n\n\n\n\n\n\nProblem definition\n\n\n3b. Background\n\n\n\n\n\n\n\nHypothesis statement\n\n\n3c. Background\n\n\n\n\n\n\n\nDescription of study outcomes\n\n\n3d. Background\n\n\n\n\n\n\n\nType of exposure or intervention used\n\n\n3e. Background\n\n\n\n\n\n\n\nType of study designs used\n\n\n3f. Background\n\n\n\n\n\n\n\nStudy population\n\n\nMethods\n\n\n4a. Search strategy\n\n\n\n\n\n\n\nQualifications of searchers (eg, librarians and investigators)\n\n\n4b. Search strategy\n\n\n\n\n\n\n\nSearch strategy, including time period included in the synthesis and keywords\n\n\n4c. Search strategy\n\n\n\n\n\n\n\nEffort to include all available studies, including contact with authors\n\n\n4d. Search strategy\n\n\n\n\n\n\n\nDatabases and registries searched\n\n\n4e. Search strategy\n\n\n\n\n\n\n\nSearch software used, name and version, including special features used (eg, explosion)\n\n\n4f. Search strategy\n\n\n\n\n\n\n\nUse of hand searching (eg, reference lists of obtained articles)\n\n\n4g. Search strategy\n\n\n\n\n\n\n\nList of citations located and those excluded, including justification\n\n\n4h. Search strategy\n\n\n\n\n\n\n\nMethod of addressing articles published in languages other than English\n\n\n4i. Search strategy\n\n\n\n\n\n\n\nMethod of handling abstracts and unpublished studies\n\n\n4j. Search strategy\n\n\n\n\n\n\n\nDescription of any contact with authors\n\n\n5a. Methods\n\n\n\n\n\n\n\nDescription of relevance or appropriateness of studies gathered for assessing the hypothesis to be tested\n\n\n5b. Methods\n\n\n\n\n\n\n\nRationale for the selection and coding of data (eg, sound clinical principles or convenience)\n\n\n5c. Methods\n\n\n\n\n\n\n\nDocumentation of how data were classified and coded (eg, multiple raters, blinding, and interrater reliability)\n\n\n5d. Methods\n\n\n\n\n\n\n\nAssessment of confounding (eg, comparability of cases and controls in studies where appropriate)\n\n\n5e. Methods\n\n\n\n\n\n\n\nAssessment of study quality, including blinding of quality assessors; stratification or regression on possible predictors of study results\n\n\n5f. Methods\n\n\n\n\n\n\n\nAssessment of heterogeneity\n\n\n5g. Methods\n\n\n\n\n\n\n\nDescription of statistical methods (eg, complete description of fixed or random effects models, justification of whether the chosen models account for predictors of study results, dose-response models, or cumulative meta-analysis) in sufficient detail to be replicated\n\n\n5h. Methods\n\n\n\n\n\n\n\nProvision of appropriate tables and graphics\n\n\nResults\n\n\n6a. Results\n\n\n\n\n\n\n\nGraphic summarizing individual study estimates and overall estimate\n\n\n6b. Results\n\n\n\n\n\n\n\nTable giving descriptive information for each study included\n\n\n6c. Results\n\n\n\n\n\n\n\nResults of sensitivity testing (eg, subgroup analysis)\n\n\n6d. Results\n\n\n\n\n\n\n\nIndication of statistical uncertainty of findings\n\n\nDiscussion\n\n\n7a. Discussion\n\n\n\n\n\n\n\nQuantitative assessment of bias (eg. publication bias)\n\n\n7b. Discussion\n\n\n\n\n\n\n\nJustification for exclusion (eg, exclusion of non–English-language citations)\n\n\n7c. Discussion\n\n\n\n\n\n\n\nAssessment of quality of included studies\n\n\nConclusion\n\n\n8a. Conclusion\n\n\n\n\n\n\n\nConsideration of alternative explanations for observed results\n\n\n8b. Conclusion\n\n\n\n\n\n\n\nGeneralization of the conclusions (ie, appropriate for the data presented and within the domain of the literature review)\n\n\n8c. Conclusion\n\n\n\n\n\n\n\nGuidelines for future research\n\n\n8d. Conclusion\n\n\n\n\n\n\n\nDisclosure of funding source"
  },
  {
    "objectID": "guidelines/prisma-p/index.html#about-this-guideline",
    "href": "guidelines/prisma-p/index.html#about-this-guideline",
    "title": "The PRISMA-P guideline for writing a protocol of a systematic review.",
    "section": "About this guideline",
    "text": "About this guideline\n\nThis guideline is relevant to protocols of systematic reviews"
  },
  {
    "objectID": "guidelines/prisma-p/index.html#download-resources",
    "href": "guidelines/prisma-p/index.html#download-resources",
    "title": "The PRISMA-P guideline for writing a protocol of a systematic review.",
    "section": "Download Resources",
    "text": "Download Resources\nReporting guidelines are best used early:\n\nUse a template to write quickly and avoid the blank page\nUse a checklist to double-check and reassure others that your work is described transparently\n\n\n\n\nFor drafting\n\nTemplate\n\n\n\nFor checking\n\nChecklist\n\n\n\nFor planning\n\nTo-do list"
  },
  {
    "objectID": "guidelines/prisma-p/index.html#guidance",
    "href": "guidelines/prisma-p/index.html#guidance",
    "title": "The PRISMA-P guideline for writing a protocol of a systematic review.",
    "section": "Guidance",
    "text": "Guidance\nApprox. 2 min read\n\nTitle\n\n\n1a. Identification\n\n\n\n\n\n\n\nIdentify the report as a protocol of a systematic review\n\n\n1b. Update\n\n\n\n\n\n\n\nIf the protocol is for an update of a previous systematic review, identify as such\n\n\nRegistration\n\n\n2. Registration\n\n\n\n\n\n\n\nIf registered, provide the name of the registry (such as PROSPERO) and registration number\n\n\nAuthors\n\n\n3a. Contact\n\n\n\n\n\n\n\nProvide name, institutional affiliation, e-mail address of all protocol authors; provide physical mailing address of corresponding author\n\n\n3b. Contribution\n\n\n\n\n\n\n\nDescribe contributions of protocol authors and identify the guarantor of the review\n\n\nAmendments\n\n\n4. Amendments\n\n\n\n\n\n\n\nIf the protocol represents an amendment of a previously completed or published protocol, identify as such and list changes; otherwise, state plan for documenting important protocol amendments\n\n\nSupport\n\n\n5a. Sources\n\n\n\n\n\n\n\nIndicate sources of financial or other support for the review\n\n\n5b. Sponsor\n\n\n\n\n\n\n\nProvide name for the review funder and / or sponsor\n\n\n5c. Role of sponsor or funder\n\n\n\n\n\n\n\nDescribe roles of funder(s), sponsor(s), and / or institution(s), if any, in developing the protocol\n\n\nIntroduction\n\n\n6. Rationale\n\n\n\n\n\n\n\nDescribe the rationale for the review in the context of what is already known\n\n\n7. Objectives\n\n\n\n\n\n\n\nProvide an explicit statement of the question(s) the review will address with reference to participants, interventions, comparators, and outcomes (PICO)\n\n\nMethods\n\n\n8. Eligibility criteria\n\n\n\n\n\n\n\nSpecify the study characteristics (such as PICO, study design, setting, time frame) and report characteristics (such as years considered, language, publication status) to be used as criteria for eligibility for the review\n\n\n9. Information sources\n\n\n\n\n\n\n\nDescribe all intended information sources (such as electronic databases, contact with study authors, trial registers or other grey literature sources) with planned dates of coverage\n\n\n10. Search strategy\n\n\n\n\n\n\n\nPresent draft of search strategy to be used for at least one electronic database, including planned limits, such that it could be repeated\n\n\n11a. Study records - data management\n\n\n\n\n\n\n\nDescribe the mechanism(s) that will be used to manage records and data throughout the review\n\n\n11b. Study records - selection process\n\n\n\n\n\n\n\nState the process that will be used for selecting studies (such as two independent reviewers) through each phase of the review (that is, screening, eligibility and inclusion in meta-analysis)\n\n\n11c. Study records - data collection process\n\n\n\n\n\n\n\nDescribe planned method of extracting data from reports (such as piloting forms, done independently, in duplicate), any processes for obtaining and confirming data from investigators\n\n\n12. Data items\n\n\n\n\n\n\n\nList and define all variables for which data will be sought (such as PICO items, funding sources), any pre-planned data assumptions and simplifications\n\n\n13. Outcomes and prioritization\n\n\n\n\n\n\n\nList and define all outcomes for which data will be sought, including prioritization of main and additional outcomes, with rationale\n\n\n14. Risk of bias in individual studies\n\n\n\n\n\n\n\nDescribe anticipated methods for assessing risk of bias of individual studies, including whether this will be done at the outcome or study level, or both; state how this information will be used in data synthesis\n\n\n15a. Data synthesis\n\n\n\n\n\n\n\nDescribe criteria under which study data will be quantitatively synthesised\n\n\n15b. Data synthesis\n\n\n\n\n\n\n\nIf data are appropriate for quantitative synthesis, describe planned summary measures, methods of handling data and methods of combining data from studies, including any planned exploration of consistency (such as I2, Kendall’s τ)\n\n\n15c. Data synthesis\n\n\n\n\n\n\n\nDescribe any proposed additional analyses (such as sensitivity or subgroup analyses, meta-regression)\n\n\n15d. Data synthesis\n\n\n\n\n\n\n\nIf quantitative synthesis is not appropriate, describe the type of summary planned\n\n\n16. Meta-bias(es)\n\n\n\n\n\n\n\nSpecify any planned assessment of meta-bias(es) (such as publication bias across studies, selective reporting within studies)\n\n\n17. Confidence in cumulative evidence\n\n\n\n\n\n\n\nDescribe how the strength of the body of evidence will be assessed (such as GRADE)"
  },
  {
    "objectID": "guidelines/prisma/index.html#about-this-guideline",
    "href": "guidelines/prisma/index.html#about-this-guideline",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "About this guideline",
    "text": "About this guideline\n\nThis guideline is based on the PRISMA statement; an evidence-based minimum set of items for reporting systematic reviews and meta-analyses. Although the guideline focuses on the reporting of reviews evaluating randomized trials, it can also be used for reporting systematic reviews of other types of research, particularly evaluations of interventions."
  },
  {
    "objectID": "guidelines/prisma/index.html#download-resources",
    "href": "guidelines/prisma/index.html#download-resources",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Download Resources",
    "text": "Download Resources\nReporting guidelines are best used early:\n\nUse a template to write quickly and avoid the blank page\nUse a checklist to double-check and reassure others that your work is described transparently\n\n\n\n\nFor drafting\n\nTemplate\n\n\n\nFor checking\n\nChecklist\n\n\n\nFor planning\n\nTo-do list"
  },
  {
    "objectID": "guidelines/prisma/index.html#guidance",
    "href": "guidelines/prisma/index.html#guidance",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Guidance",
    "text": "Guidance\nApprox. 3 min read\n\nTitle\n\n\n1. Title\n\n\n\n\n\n\n\nIdentify the report as a systematic review, meta-analysis, or both.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAuthors should identify their report as a systematic review or meta-analysis. Terms such as review or overview do not describe for readers whether the review was systematic or whether a meta-analysis was performed. A recent survey found that 50% of 300 authors did not mention the terms systematic review or meta-analysis in the title or abstract of their systematic review [3]. Although sensitive search strategies have been developed to identify systematic reviews [22], inclusion of the terms systematic review or meta-analysis in the title may improve indexing and identification.\nWe advise authors to use informative titles that make key information easily accessible to readers. Ideally, a title reflecting the PICOS approach (participants, interventions, comparators, outcomes, and study design) (see Item 11 and Box 2) may help readers as it provides key information about the scope of the review. Specifying the design(s) of the studies included, as shown in the examples, may also help some readers and those searching databases.\nSome journals recommend indicative titles that indicate the topic matter of the review, while others require declarative titles that give the review’s main conclusion. Busy practitioners may prefer to see the conclusion of the review in the title, but declarative titles can oversimplify or exaggerate findings. Thus, many journals and methodologists prefer indicative titles as used in the examples above.\n\n\nExamples\n\nRecurrence rates of video-assisted thoracoscopic versus open surgery in the prevention of recurrent pneumothoraces: a systematic review of randomised and non-randomised trials\n\n\nMortality in randomized trials of antioxidant supplements for primary and secondary prevention: systematic review and meta-analysis\n\nBack to top\n\n\n\n\n\n\nAbstract\n\n\n2. Structured summary\n\n\n\n\n\n\n\nProvide a structured summary including, as applicable: background; objectives; data sources; study eligibility criteria, participants, and interventions; study appraisal and synthesis methods; results; limitations; conclusions and implications of key findings; systematic review registration number\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAbstracts provide key information that enables readers to understand the scope, processes, and findings of a review and to decide whether to read the full report. The abstract may be all that is readily available to a reader, for example, in a bibliographic database. The abstract should present a balanced and realistic assessment of the review’s findings that mirrors, albeit briefly, the main text of the report.\nWe agree with others that the quality of reporting in abstracts presented at conferences and in journal publications needs improvement [24],[25]. While we do not uniformly favor a specific format over another, we generally recommend structured abstracts. Structured abstracts provide readers with a series of headings pertaining to the purpose, conduct, findings, and conclusions of the systematic review being reported [26],[27]. They give readers more complete information and facilitate finding information more easily than unstructured abstracts [28],[29],[30],[31],[32].\nA highly structured abstract of a systematic review could include the following headings: Context (or Background); Objective (or Purpose); Data Sources; Study Selection (or Eligibility Criteria); Study Appraisal and Synthesis Methods (or Data Extraction and Data Synthesis); Results; Limitations; and Conclusions (or Implications). Alternatively, a simpler structure could cover but collapse some of the above headings (e.g., label Study Selection and Study Appraisal as Review Methods) or omit some headings such as Background and Limitations.\nIn the highly structured abstract mentioned above, authors use the Background heading to set the context for readers and explain the importance of the review question. Under the Objectives heading, they ideally use elements of PICOS (see Box 2) to state the primary objective of the review. Under a Data Sources heading, they summarize sources that were searched, any language or publication type restrictions, and the start and end dates of searches. Study Selection statements then ideally describe who selected studies using what inclusion criteria. Data Extraction Methods statements describe appraisal methods during data abstraction and the methods used to integrate or summarize the data. The Data Synthesis section is where the main results of the review are reported. If the review includes meta-analyses, authors should provide numerical results with confidence intervals for the most important outcomes. Ideally, they should specify the amount of evidence in these analyses (numbers of studies and numbers of participants). Under a Limitations heading, authors might describe the most important weaknesses of included studies as well as limitations of the review process. Then authors should provide clear and balanced Conclusions that are closely linked to the objective and findings of the review. Additionally, it would be helpful if authors included some information about funding for the review. Finally, although protocol registration for systematic reviews is still not common practice, if authors have registered their review or received a registration number, we recommend providing the registration information at the end of the abstract.\nTaking all the above considerations into account, the intrinsic tension between the goal of completeness of the abstract and its keeping into the space limit often set by journal editors is recognized as a major challenge.\n\n\nExamples\n\nContext: The role and dose of oral vitamin D supplementation in nonvertebral fracture prevention have not been well established.\n\nObjective: To estimate the effectiveness of vitamin D supplementation in preventing hip and nonvertebral fractures in older persons.\nData Sources: A systematic review of English and non-English articles using MEDLINE and the Cochrane Controlled Trials Register (1960–2005), and EMBASE (1991–2005). Additional studies were identified by contacting clinical experts and searching bibliographies and abstracts presented at the American Society for Bone and Mineral Research (1995–2004). Search terms included randomized controlled trial (RCT), controlled clinical trial, random allocation, double-blind method, cholecalciferol, ergocalciferol, 25-hydroxyvitamin D, fractures, humans, elderly, falls, and bone density.\nStudy Selection: Only double-blind RCTs of oral vitamin D supplementation (cholecalciferol, ergocalciferol) with or without calcium supplementation vs calcium supplementation or placebo in older persons (>60 years) that examined hip or nonvertebral fractures were included.\nData Extraction: Independent extraction of articles by 2 authors using predefined data fields, including study quality indicators.\nData Synthesis: All pooled analyses were based on random-effects models. Five RCTs for hip fracture (n = 9294) and 7 RCTs for nonvertebral fracture risk (n = 9820) met our inclusion criteria. All trials used cholecalciferol. Heterogeneity among studies for both hip and nonvertebral fracture prevention was observed, which disappeared after pooling RCTs with low-dose (400 IU/d) and higher-dose vitamin D (700–800 IU/d), separately. A vitamin D dose of 700 to 800 IU/d reduced the relative risk (RR) of hip fracture by 26% (3 RCTs with 5572 persons; pooled RR, 0.74; 95% confidence interval [CI], 0.61–0.88) and any nonvertebral fracture by 23% (5 RCTs with 6098 persons; pooled RR, 0.77; 95% CI, 0.68–0.87) vs calcium or placebo. No significant benefit was observed for RCTs with 400 IU/d vitamin D (2 RCTs with 3722 persons; pooled RR for hip fracture, 1.15; 95% CI, 0.88–1.50; and pooled RR for any nonvertebral fracture, 1.03; 95% CI, 0.86–1.24).\nConclusions: Oral vitamin D supplementation between 700 to 800 IU/d appears to reduce the risk of hip and any nonvertebral fractures in ambulatory or institutionalized elderly persons. An oral vitamin D dose of 400 IU/d is not sufficient for fracture prevention.\nBack to top\n\n\n\n\n\n\nIntroduction\n\n\n3. Rationale\n\n\n\n\n\n\n\nDescribe the rationale for the review in the context of what is already known.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nReaders need to understand the rationale behind the study and what the systematic review may add to what is already known. Authors should tell readers whether their report is a new systematic review or an update of an existing one. If the review is an update, authors should state reasons for the update, including what has been added to the evidence base since the previous version of the review.\nAn ideal background or introduction that sets context for readers might include the following. First, authors might define the importance of the review question from different perspectives (e.g., public health, individual patient, or health policy). Second, authors might briefly mention the current state of knowledge and its limitations. As in the above example, information about the effects of several different interventions may be available that helps readers understand why potential relative benefits or harms of particular interventions need review. Third, authors might whet readers’ appetites by clearly stating what the review aims to add. They also could discuss the extent to which the limitations of the existing evidence base may be overcome by the review.\n\n\nExamples\n\nReversing the trend of increasing weight for height in children has proven difficult.\n\nIt is widely accepted that increasing energy expenditure and reducing energy intake form the theoretical basis for management. Therefore, interventions\naiming to increase physical activity and improve diet are the foundation of efforts\nto prevent and treat childhood obesity. Such lifestyle interventions have been\nsupported by recent systematic reviews, as well as by the Canadian Paediatric\nSociety, the Royal College of Paediatrics and Child Health, and the American\n\nAcademy of Pediatrics. However, these interventions are fraught with poor\n\nadherence. Thus, school-based interventions are theoretically appealing because\nadherence with interventions can be improved. Consequently, many local\ngovernments have enacted or are considering policies that mandate increased\n\nphysical activity in schools, although the effect of such interventions on body\n\ncomposition has not been assessed\nBack to top\n\n\n\n\n\n\n4. Objectives\n\n\n\n\n\n\n\nProvide an explicit statement of questions being addressed with reference to participants, interventions, comparisons, outcomes, and study design (PICOS).\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe questions being addressed, and the rationale for them, are one of the most critical parts of a systematic review. They should be stated precisely and explicitly so that readers can understand quickly the review’s scope and the potential applicability of the review to their interests [35]. Framing questions so that they include the following five PICOS components may improve the explicitness of review questions: (1) the patient population or disease being addressed (P), (2) the interventions or exposure of interest (I), (3) the comparators (C), (4) the main outcome or endpoint of interest (O), and (5) the study designs chosen (S). For more detail regarding PICOS, see Box 2.\nGood review questions may be narrowly focused or broad, depending on the overall objectives of the review. Sometimes broad questions might increase the applicability of the results and facilitate detection of bias, exploratory analyses, and sensitivity analyses [35],[36]. Whether narrowly focused or broad, precisely stated review objectives are critical as they help define other components of the review process such as the eligibility criteria (Item 6) and the search for relevant literature (Items 7 and 8)\n\n\nExamples\n\nTo examine whether topical or intraluminal antibiotics reduce catheter-related bloodstream infection, we reviewed randomized, controlled trials that assessed the efficacy of these antibiotics for primary prophylaxis against catheter-related bloodstream infection and mortality compared with no antibiotic therapy in adults undergoing hemodialysis\n\nBack to top\n\n\n\n\n\n\nMethods\n\n\n5. Protocol and registration\n\n\n\n\n\n\n\nIndicate if a review protocol exists, if and where it can be accessed (e.g., Web address) and, if available, provide registration information including the registration number.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nA protocol is important because it pre-specifies the objectives and methods of the systematic review. For instance, a protocol specifies outcomes of primary interest, how reviewers will extract information about those outcomes, and methods that reviewers might use to quantitatively summarize the outcome data (see Item 13). Having a protocol can help restrict the likelihood of biased post hoc decisions in review methods, such as selective outcome reporting. Several sources provide guidance about elements to include in the protocol for a systematic review [16],[38],[39]. For meta-analyses of individual patient-level data, we advise authors to describe whether a protocol was explicitly designed and whether, when, and how participating collaborators endorsed it [40],[41].\nAuthors may modify protocols during the research, and readers should not automatically consider such modifications inappropriate. For example, legitimate modifications may extend the period of searches to include older or newer studies, broaden eligibility criteria that proved too narrow, or add analyses if the primary analyses suggest that additional ones are warranted. Authors should, however, describe the modifications and explain their rationale.\nAlthough worthwhile protocol amendments are common, one must consider the effects that protocol modifications may have on the results of a systematic review, especially if the primary outcome is changed. Bias from selective outcome reporting in randomized trials has been well documented [42],[43]. An examination of 47 Cochrane reviews revealed indirect evidence for possible selective reporting bias for systematic reviews. Almost all (n = 43) contained a major change, such as the addition or deletion of outcomes, between the protocol and the full publication [44]. Whether (or to what extent) the changes reflected bias, however, was not clear. For example, it has been rather common not to describe outcomes that were not presented in any of the included studies.\nRegistration of a systematic review, typically with a protocol and registration number, is not yet common, but some opportunities exist [45],[46]. Registration may possibly reduce the risk of multiple reviews addressing the same question [45],[46],[47],[48], reduce publication bias, and provide greater transparency when updating systematic reviews. Of note, a survey of systematic reviews indexed in MEDLINE in November 2004 found that reports of protocol use had increased to about 46% [3] from 8% noted in previous surveys [49]. The improvement was due mostly to Cochrane reviews, which, by requirement, have a published protocol [3].\n\n\nExamples\n\nMethods of the analysis and inclusion criteria were specified in advance and documented in a protocol.\n\nBack to top\n\n\n\n\n\n\n6. Eligibility criteria\n\n\n\n\n\n\n\nSpecify study characteristics (e.g., PICOS, length of follow-up) and report characteristics (e.g., years considered, language, publication status) used as criteria for eligibility, giving rational\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nKnowledge of the eligibility criteria is essential in appraising the validity, applicability, and comprehensiveness of a review. Thus, authors should unambiguously specify eligibility criteria used in the review. Carefully defined eligibility criteria inform various steps of the review methodology. They influence the development of the search strategy and serve to ensure that studies are selected in a systematic and unbiased manner.\nA study may be described in multiple reports, and one report may describe multiple studies. Therefore, we separate eligibility criteria into the following two components: study characteristics and report characteristics. Both need to be reported. Study eligibility criteria are likely to include the populations, interventions, comparators, outcomes, and study designs of interest (PICOS; see Box 2), as well as other study-specific elements, such as specifying a minimum length of follow-up. Authors should state whether studies will be excluded because they do not include (or report) specific outcomes to help readers ascertain whether the systematic review may be biased as a consequence of selective reporting [42],[43].\nReport eligibility criteria are likely to include language of publication, publication status (e.g., inclusion of unpublished material and abstracts), and year of publication. Inclusion or not of non-English language literature [51],[52],[53],[54],[55], unpublished data, or older data can influence the effect estimates in meta-analyses [56],[57],[58],[59]. Caution may need to be exercised in including all identified studies due to potential differences in the risk of bias such as, for example, selective reporting in abstracts [60],[61],[62].\n\n\nExamples\n\nTypes of studies: Randomised clinical trials studying the administration of hepatitis B vaccine to CRF [chronic renal failure] patients, with or without dialysis. No language, publication date, or publication status restrictions were imposed…\n\n\nTypes of participants: Participants of any age with CRF or receiving dialysis (haemodialysis or peritoneal dialysis) were considered. CRF was defined as serum creatinine greater than 200 µmol/L for a period of more than six months or individuals receiving dialysis (haemodialysis or peritoneal dialysis)…Renal transplant patients were excluded from this review as these individuals are immunosuppressed and are receiving immunosuppressant agents to prevent rejection of their transplanted organs, and they have essentially normal renal function…\n\n\nTypes of intervention: Trials comparing the beneficial and harmful effects of hepatitis B vaccines with adjuvant or cytokine co-interventions [and] trials comparing the beneficial and harmful effects of immunoglobulin prophylaxis. This review was limited to studies looking at active immunization. Hepatitis B vaccines (plasma or recombinant (yeast) derived) of all types, dose, and regimens versus placebo, control vaccine, or no vaccine…\n\n\nTypes of outcome measures: Primary outcome measures: Seroconversion, ie, proportion of patients with adequate anti-HBs response (>10 IU/L or Sample Ratio Units). Hepatitis B infections (as measured by hepatitis B core antigen (HBcAg) positivity or persistent HBsAg positivity), both acute and chronic. Acute (primary) HBV [hepatitis B virus] infections were defined as seroconversion to HBsAg positivity or development of IgM anti-HBc. Chronic HBV infections were defined as the persistence of HBsAg for more than six months or HBsAg positivity and liver biopsy compatible with a diagnosis or chronic hepatitis B. Secondary outcome measures: Adverse events of hepatitis B vaccinations…[and]…mortality. [\n\nBack to top\n\n\n\n\n\n\n7. Information sources\n\n\n\n\n\n\n\nDescribe all information sources in the search (e.g., databases with dates of coverage, contact with study authors to identify additional studies) and date last searched.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe National Library of Medicine’s MEDLINE database is one of the most comprehensive sources of health care information in the world. Like any database, however, its coverage is not complete and varies according to the field. Retrieval from any single database, even by an experienced searcher, may be imperfect, which is why detailed reporting is important within the systematic review.\nAt a minimum, for each database searched, authors should report the database, platform, or provider (e.g., Ovid, Dialog, PubMed) and the start and end dates for the search of each database. This information lets readers assess the currency of the review, which is important because the publication time-lag outdates the results of some reviews [64]. This information should also make updating more efficient [65]. Authors should also report who developed and conducted the search [66].\nIn addition to searching databases, authors should report the use of supplementary approaches to identify studies, such as hand searching of journals, checking reference lists, searching trials registries or regulatory agency Web sites [67], contacting manufacturers, or contacting authors. Authors should also report if they attempted to acquire any missing information (e.g., on study methods or results) from investigators or sponsors; it is useful to describe briefly who was contacted and what unpublished information was obtained.\n\n\nExamples\n\nStudies were identified by searching electronic databases, scanning reference lists of articles and consultation with experts in the field and drug companies…No limits were applied for language and foreign papers were translated. This search was applied to Medline (1966–Present), CancerLit (1975–Present), and adapted for Embase (1980–Present), Science Citation Index Expanded (1981–Present) and Pre-Medline electronic databases. Cochrane and DARE (Database of Abstracts of Reviews of Effectiveness) databases were reviewed…The last search was run on 19 June 2001. In addition, we handsearched contents pages of Journal of Clinical Oncology 2001, European Journal of Cancer 2001 and Bone 2001, together with abstracts printed in these journals 1999–2001. A limited update literature search was performed from 19 June 2001 to 31 December 2003.\n\nBack to top\n\n\n\n\n\n\n8. Search\n\n\n\n\n\n\n\nPresent full electronic search strategy for at least one database, including any limits used, such that it could be repeated.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe search strategy is an essential part of the report of any systematic review. Searches may be complicated and iterative, particularly when reviewers search unfamiliar databases or their review is addressing a broad or new topic. Perusing the search strategy allows interested readers to assess the comprehensiveness and completeness of the search, and to replicate it. Thus, we advise authors to report their full electronic search strategy for at least one major database. As an alternative to presenting search strategies for all databases, authors could indicate how the search took into account other databases searched, as index terms vary across databases. If different searches are used for different parts of a wider question (e.g., questions relating to benefits and questions relating to harms), we recommend authors provide at least one example of a strategy for each part of the objective [69]. We also encourage authors to state whether search strategies were peer reviewed as part of the systematic review process [70].\nWe realize that journal restrictions vary and that having the search strategy in the text of the report is not always feasible. We strongly encourage all journals, however, to find ways, such as a Web extra, appendix, or electronic link to an archive, to make search strategies accessible to readers. We also advise all authors to archive their searches so that (1) others may access and review them (e.g., replicate them or understand why their review of a similar topic did not identify the same reports), and (2) future updates of their review are facilitated.\nSeveral sources provide guidance on developing search strategies [71],[72],[73]. Most searches have constraints, for example relating to limited time or financial resources, inaccessible or inadequately indexed reports and databases, unavailability of experts with particular language or database searching skills, or review questions for which pertinent evidence is not easy to find. Authors should be straightforward in describing their search constraints. Apart from the keywords used to identify or exclude records, they should report any additional limitations relevant to the search, such as language and date restrictions (see also eligibility criteria, Item 6) [51].\n\n\nExamples\n\nIn text: “We used the following search terms to search all trials registers and databases: immunoglobulin*; IVIG; sepsis; septic shock; septicaemia; and septicemia…”\n\n\nIn appendix:\n\nSearch strategy: MEDLINE (OVID)\n\nimmunoglobulins/\nimmunoglobulin$.tw.\nivig.tw.\n1 or 2 or 3\nsepsis/\nsepsis.tw.\nseptic shock/\nseptic shock.tw.\nsepticemia/\nsepticaemia.tw.\nsepticemia.tw.\n5 or 6 or 7 or 8 or 9 or 10 or 11\n4 and 12\nrandomized controlled trials/\nrandomized-controlled-trial.pt.\ncontrolled-clinical-trial.pt.\nrandom allocation/\ndouble-blind method/\nsingle-blind method/\n14 or 15 or 16 or 17 or 18 or 19\nexp clinical trials/\nclinical-trial.pt.\n(clin$ adj trial$).ti,ab.\n((singl$ or doubl$ or trebl$ or tripl\\() adj (blind\\))).ti,ab.\nplacebos/\nplacebo$.ti,ab.\nrandom$.ti,ab.\n21 or 22 or 23 or 24 or 25 or 26 or 27\nresearch design/\ncomparative study/\nexp evaluation studies/\nfollow-up studies/\nprospective studies/\n(control$ or prospective$ or volunteer$).ti,ab.\n30 or 31 or 32 or 33 or 34\n20 or 28 or 29 or 35\n13 and 36\n\nBack to top\n\n\n\n\n\n\n9. Study selection\n\n\n\n\n\n\n\nState the process for selecting studies (i.e., for screening, for determining eligibility, for inclusion in the systematic review, and, if applicable, for inclusion in the meta-analysis).\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThere is no standard process for selecting studies to include in a systematic review. Authors usually start with a large number of identified records from their search and sequentially exclude records according to eligibility criteria. We advise authors to report how they screened the retrieved records (typically a title and abstract), how often it was necessary to review the full text publication, and if any types of record (e.g., letters to the editor) were excluded. We also advise using the PRISMA flow diagram to summarize study selection processes (see Item 17; Box 3).\nEfforts to enhance objectivity and avoid mistakes in study selection are important. Thus authors should report whether each stage was carried out by one or several people, who these people were, and, whenever multiple independent investigators performed the selection, what the process was for resolving disagreements. The use of at least two investigators may reduce the possibility of rejecting relevant reports [75]. The benefit may be greatest for topics where selection or rejection of an article requires difficult judgments [76]. For these topics, authors should ideally tell readers the level of inter-rater agreement, how commonly arbitration about selection was required, and what efforts were made to resolve disagreements (e.g., by contact with the authors of the original studies).\n\n\nExamples\n\nEligibility assessment…[was] performed independently in an unblinded standardized manner by 2 reviewers…Disagreements between reviewers were resolved by consensus\n\nBack to top\n\n\n\n\n\n\n10. Data collection process\n\n\n\n\n\n\n\nDescribe the method of data extraction from reports (e.g., piloted forms, independently by two reviewers) and any processes for obtaining and confirming data from investigators.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nReviewers extract information from each included study so that they can critique, present, and summarize evidence in a systematic review. They might also contact authors of included studies for information that has not been, or is unclearly, reported. In meta-analysis of individual patient data, this phase involves collection and scrutiny of detailed raw databases. The authors should describe these methods, including any steps taken to reduce bias and mistakes during data collection and data extraction [78] (Box 3).\nSome systematic reviewers use a data extraction form that could be reported as an appendix or Web extra to their report. These forms could show the reader what information reviewers sought (see Item 11) and how they extracted it. Authors could tell readers if the form was piloted. Regardless, we advise authors to tell readers who extracted what data, whether any extractions were completed in duplicate, and, if so, whether duplicate abstraction was done independently and how disagreements were resolved.\nPublished reports of the included studies may not provide all the information required for the review. Reviewers should describe any actions they took to seek additional information from the original researchers (see Item 7). The description might include how they attempted to contact researchers, what they asked for, and their success in obtaining the necessary information. Authors should also tell readers when individual patient data were sought from the original researchers [41] (see Item 11) and indicate the studies for which such data were used in the analyses. The reviewers ideally should also state whether they confirmed the accuracy of the information included in their review with the original researchers, for example, by sending them a copy of the draft review [79].\nSome studies are published more than once. Duplicate publications may be difficult to ascertain, and their inclusion may introduce bias [80],[81]. We advise authors to describe any steps they used to avoid double counting and piece together data from multiple reports of the same study (e.g., juxtaposing author names, treatment comparisons, sample sizes, or outcomes). We also advise authors to indicate whether all reports on a study were considered, as inconsistencies may reveal important limitations. For example, a review of multiple publications of drug trials showed that reported study characteristics may differ from report to report, including the description of the design, number of patients analyzed, chosen significance level, and outcomes [82]. Authors ideally should present any algorithm that they used to select data from overlapping reports and any efforts they used to solve logical inconsistencies across reports\n\n\nExamples\n\nWe developed a data extraction sheet (based on the Cochrane Consumers and Communication Review Group’s data extraction template), pilot-tested it on ten randomly-selected included studies, and refined it accordingly. One review author extracted the following data from included studies and the second author checked the extracted data…Disagreements were resolved by discussion between the two review authors; if no agreement could be reached, it was planned a third author would decide. We contacted five authors for further information. All responded and one provided numerical data that had only been presented graphically in the published paper.\n\nBack to top\n\n\n\n\n\n\n11. Data items\n\n\n\n\n\n\n\nList and define all variables for which data were sought (e.g., PICOS, funding sources), and any assumptions and simplifications made.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nIt is important for readers to know what information review authors sought, even if some of this information was not available [84]. If the review is limited to reporting only those variables that were obtained, rather than those that were deemed important but could not be obtained, bias might be introduced and the reader might be misled. It is therefore helpful if authors can refer readers to the protocol (see Item 5), and archive their extraction forms (see Item 10), including definitions of variables. The published systematic review should include a description of the processes used with, if relevant, specification of how readers can get access to additional materials.\nWe encourage authors to report whether some variables were added after the review started. Such variables might include those found in the studies that the reviewers identified (e.g., important outcome measures that the reviewers initially overlooked). Authors should describe the reasons for adding any variables to those already pre-specified in the protocol so that readers can understand the review process.\nWe advise authors to report any assumptions they made about missing or unclear information and to explain those processes. For example, in studies of women aged 50 or older it is reasonable to assume that none were pregnant, even if this is not reported. Likewise, review authors might make assumptions about the route of administration of drugs assessed. However, special care should be taken in making assumptions about qualitative information. For example, the upper age limit for children can vary from 15 years to 21 years, intense physiotherapy might mean very different things to different researchers at different times and for different patients, and the volume of blood associated with heavy blood loss might vary widely depending on the setting.\n\n\nExamples\n\nInformation was extracted from each included trial on: (1) characteristics of trial participants (including age, stage and severity of disease, and method of diagnosis), and the trial’s inclusion and exclusion criteria; (2) type of intervention (including type, dose, duration and frequency of the NSAID [non-steroidal anti-inflammatory drug]; versus placebo or versus the type, dose, duration and frequency of another NSAID; or versus another pain management drug; or versus no treatment); (3) type of outcome measure (including the level of pain reduction, improvement in quality of life score (using a validated scale), effect on daily activities, absence from work or school, length of follow up, unintended effects of treatment, number of women requiring more invasive treatment)\n\nBack to top\n\n\n\n\n\n\n12. Risk of bias in individual studies\n\n\n\n\n\n\n\nDescribe methods used for assessing risk of bias in individual studies (including specification of whether this was done at the study or outcome level, or both), and how this information is to be used in any data synthesis.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe likelihood that the treatment effect reported in a systematic review approximates the truth depends on the validity of the included studies, as certain methodological characteristics may be associated with effect sizes [87],[88]. For example, trials without reported adequate allocation concealment exaggerate treatment effects on average compared to those with adequate concealment [88]. Therefore, it is important for authors to describe any methods that they used to gauge the risk of bias in the included studies and how that information was used [89]. Additionally, authors should provide a rationale if no assessment of risk of bias was undertaken. The most popular term to describe the issues relevant to this item is quality, but for the reasons that are elaborated in Box 4 we prefer to name this item as assessment of risk of bias.\nMany methods exist to assess the overall risk of bias in included studies, including scales, checklists, and individual components [90],[91]. As discussed in Box 4, scales that numerically summarize multiple components into a single number are misleading and unhelpful [92],[93]. Rather, authors should specify the methodological components that they assessed. Common markers of validity for randomized trials include the following: appropriate generation of random allocation sequence [94]; concealment of the allocation sequence [93]; blinding of participants, health care providers, data collectors, and outcome adjudicators [95],[96],[97],[98]; proportion of patients lost to follow-up [99],[100]; stopping of trials early for benefit [101]; and whether the analysis followed the intention-to-treat principle [100],[102]. The ultimate decision regarding which methodological features to evaluate requires consideration of the strength of the empiric data, theoretical rationale, and the unique circumstances of the included studies.\nAuthors should report how they assessed risk of bias; whether it was in a blind manner; and if assessments were completed by more than one person, and if so, whether they were completed independently [103],[104]. Similarly, we encourage authors to report any calibration exercises among review team members that were done. Finally, authors need to report how their assessments of risk of bias are used subsequently in the data synthesis (see Item 16). Despite the often difficult task of assessing the risk of bias in included studies, authors are sometimes silent on what they did with the resultant assessments [89]. If authors exclude studies from the review or any subsequent analyses on the basis of the risk of bias, they should tell readers which studies they excluded and explain the reasons for those exclusions (see Item 6). Authors should also describe any planned sensitivity or subgroup analyses related to bias assessments (see Item 16).\n\n\nExamples\n\nTo ascertain the validity of eligible randomized trials, pairs of reviewers working independently and with adequate reliability determined the adequacy of randomization and concealment of allocation, blinding of patients, health care providers, data collectors, and outcome assessors; and extent of loss to follow-up (i.e. proportion of patients in whom the investigators were not able to ascertain outcomes).\n\n\nTo explore variability in study results (heterogeneity) we specified the following hypotheses before conducting the analysis. We hypothesised that effect size may differ according to the methodological quality of the studies.\n\nBack to top\n\n\n\n\n\n\n13. Summary measures\n\n\n\n\n\n\n\nState the principal summary measures (e.g., risk ratio, difference in means).\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nWhen planning a systematic review, it is generally desirable that authors pre-specify the outcomes of primary interest (see Item 5) as well as the intended summary effect measure for each outcome. The chosen summary effect measure may differ from that used in some of the included studies. If possible the choice of effect measures should be explained, though it is not always easy to judge in advance which measure is the most appropriate.\nFor binary outcomes, the most common summary measures are the risk ratio, odds ratio, and risk difference [108]. Relative effects are more consistent across studies than absolute effects [109],[110], although absolute differences are important when interpreting findings (see Item 24).\nFor continuous outcomes, the natural effect measure is the difference in means [108]. Its use is appropriate when outcome measurements in all studies are made on the same scale. The standardized difference in means is used when the studies do not yield directly comparable data. Usually this occurs when all studies assess the same outcome but measure it in a variety of ways (e.g., different scales to measure depression).\nFor time-to-event outcomes, the hazard ratio is the most common summary measure. Reviewers need the log hazard ratio and its standard error for a study to be included in a meta-analysis [111]. This information may not be given for all studies, but methods are available for estimating the desired quantities from other reported information [111]. Risk ratio and odds ratio (in relation to events occurring by a fixed time) are not equivalent to the hazard ratio, and median survival times are not a reliable basis for meta-analysis [112]. If authors have used these measures they should describe their methods in the report.\n\n\nExamples\n\nRelative risk of mortality reduction was the primary measure of treatment effect.\n\n\nThe meta-analyses were performed by computing relative risks (RRs) using random-effects model. Quantitative analyses were performed on an intention-to-treat basis and were confined to data derived from the period of follow-up. RR and 95% confidence intervals for each side effect (and all side effects) were calculated.\n\n\nThe primary outcome measure was the mean difference in log10 HIV-1 viral load comparing zinc supplementation to placebo…\n\nBack to top\n\n\n\n\n\n\n14. Planned methods of analyis\n\n\n\n\n\n\n\nDescribe the methods of handling data and combining results of studies, if done, including measures of consistency (e.g., I2) for each meta-analysis.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe data extracted from the studies in the review may need some transformation (processing) before they are suitable for analysis or for presentation in an evidence table. Although such data handling may facilitate meta-analyses, it is sometimes needed even when meta-analyses are not done. For example, in trials with more than two intervention groups it may be necessary to combine results for two or more groups (e.g., receiving similar but non-identical interventions), or it may be desirable to include only a subset of the data to match the review’s inclusion criteria. When several different scales (e.g., for depression) are used across studies, the sign of some scores may need to be reversed to ensure that all scales are aligned (e.g., so low values represent good health on all scales). Standard deviations may have to be reconstructed from other statistics such as p-values and t statistics [115],[116], or occasionally they may be imputed from the standard deviations observed in other studies [117]. Time-to-event data also usually need careful conversions to a consistent format [111]. Authors should report details of any such data processing.\nStatistical combination of data from two or more separate studies in a meta-analysis may be neither necessary nor desirable (see Box 5 and Item 21). Regardless of the decision to combine individual study results, authors should report how they planned to evaluate between-study variability (heterogeneity or inconsistency) (Box 6). The consistency of results across trials may influence the decision of whether to combine trial results in a meta-analysis.\nWhen meta-analysis is done, authors should specify the effect measure (e.g., relative risk or mean difference) (see Item 13), the statistical method (e.g., inverse variance), and whether a fixed- or random-effects approach, or some other method (e.g., Bayesian) was used (see Box 6). If possible, authors should explain the reasons for those choices.\n\n\nExamples\n\nWe tested for heterogeneity with the Breslow-Day test, and used the method proposed by Higgins et al. to measure inconsistency (the percentage of total variation across studies due to heterogeneity) of effects across lipid-lowering interventions. The advantages of this measure of inconsistency (termed I2) are that it does not inherently depend on the number of studies and is accompanied by an uncertainty interval.\n\n\nIn very few instances, estimates of baseline mean or mean QOL [Quality of life] responses were obtained without corresponding estimates of variance (standard deviation [SD] or standard error). In these instances, an SD was imputed from the mean of the known SDs. In a number of cases, the response data available were the mean and variance in a pre study condition and after therapy. The within-patient variance in these cases could not be calculated directly and was approximated by assuming independence.\n\nBack to top\n\n\n\n\n\n\n15. Risk of bias across studies\n\n\n\n\n\n\n\nSpecify any assessment of risk of bias that may affect the cumulative evidence (e.g., publication bias, selective reporting within studies).\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nReviewers should explore the possibility that the available data are biased. They may examine results from the available studies for clues that suggest there may be missing studies (publication bias) or missing data from the included studies (selective reporting bias) (see Box 7). Authors should report in detail any methods used to investigate possible bias across studies.\nIt is difficult to assess whether within-study selective reporting is present in a systematic review. If a protocol of an individual study is available, the outcomes in the protocol and the published report can be compared. Even in the absence of a protocol, outcomes listed in the methods section of the published report can be compared with those for which results are presented [120]. In only half of 196 trial reports describing comparisons of two drugs in arthritis were all the effect variables in the methods and results sections the same [82]. In other cases, knowledge of the clinical area may suggest that it is likely that the outcome was measured even if it was not reported. For example, in a particular disease, if one of two linked outcomes is reported but the other is not, then one should question whether the latter has been selectively omitted [121],[122].\nOnly 36% (76 of 212) of therapeutic systematic reviews published in November 2004 reported that study publication bias was considered, and only a quarter of those intended to carry out a formal assessment for that bias [3]. Of 60 meta-analyses in 24 articles published in 2005 in which formal assessments were reported, most were based on fewer than ten studies; most displayed statistically significant heterogeneity; and many reviewers misinterpreted the results of the tests employed [123]. A review of trials of antidepressants found that meta-analysis of only the published trials gave effect estimates 32% larger on average than when all trials sent to the drug agency were analyzed [67].\n\n\nExamples\n\nFor each trial we plotted the effect by the inverse of its standard error. The symmetry of such funnel plots was assessed both visually, and formally with Egger’s test, to see if the effect decreased with increasing sample size.\n\n\nWe assessed the possibility of publication bias by evaluating a funnel plot of the trial mean differences for asymmetry, which can result from the non publication of small trials with negative results…Because graphical evaluation can be subjective, we also conducted an adjusted rank correlation test and a regression asymmetry test as formal statistical tests for publication bias…We acknowledge that other factors, such as differences in trial quality or true study heterogeneity, could produce asymmetry in funnel plots.\n\nBack to top\n\n\n\n\n\n\n16. Additional analyses\n\n\n\n\n\n\n\nDescribe methods of additional analyses (e.g., sensitivity or subgroup analyses, meta-regression), if done, indicating which were pre-specified.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAuthors may perform additional analyses to help understand whether the results of their review are robust, all of which should be reported. Such analyses include sensitivity analysis, subgroup analysis, and meta-regression [125].\nSensitivity analyses are used to explore the degree to which the main findings of a systematic review are affected by changes in its methods or in the data used from individual studies (e.g., study inclusion criteria, results of risk of bias assessment). Subgroup analyses address whether the summary effects vary in relation to specific (usually clinical) characteristics of the included studies or their participants. Meta-regression extends the idea of subgroup analysis to the examination of the quantitative influence of study characteristics on the effect size [126]. Meta-regression also allows authors to examine the contribution of different variables to the heterogeneity in study findings. Readers of systematic reviews should be aware that meta-regression has many limitations, including a danger of over-interpretation of findings [127],[128].\nEven with limited data, many additional analyses can be undertaken. The choice of which analysis to undertake will depend on the aims of the review. None of these analyses, however, are exempt from producing potentially misleading results. It is important to inform readers whether these analyses were performed, their rationale, and which were pre-specified.\n\n\nExamples\n\nSensitivity analyses were pre-specified. The treatment effects were examined according to quality components (concealed treatment allocation, blinding of patients and caregivers, blinded outcome assessment), time to initiation of statins, and the type of statin. One post-hoc sensitivity analysis was conducted including unpublished data from a trial using cerivastatin\n\nBack to top\n\n\n\n\n\n\nResults\n\n\n17. Study selection\n\n\n\n\n\n\n\nGive numbers of studies screened, assessed for eligibility, and included in the review, with reasons for exclusions at each stage, ideally with a flow diagram.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAuthors should report, ideally with a flow diagram, the total number of records identified from electronic bibliographic sources (including specialized database or registry searches), hand searches of various sources, reference lists, citation indices, and experts. It is useful if authors delineate for readers the number of selected articles that were identified from the different sources so that they can see, for example, whether most articles were identified through electronic bibliographic sources or from references or experts. Literature identified primarily from references or experts may be prone to citation or publication bias [131],[132].\nThe flow diagram and text should describe clearly the process of report selection throughout the review. Authors should report: unique records identified in searches; records excluded after preliminary screening (e.g., screening of titles and abstracts); reports retrieved for detailed evaluation; potentially eligible reports that were not retrievable; retrieved reports that did not meet inclusion criteria and the primary reasons for exclusion; and the studies included in the review. Indeed, the most appropriate layout may vary for different reviews.\nAuthors should also note the presence of duplicate or supplementary reports so that readers understand the number of individual studies compared to the number of reports that were included in the review. Authors should be consistent in their use of terms, such as whether they are reporting on counts of citations, records, publications, or studies. We believe that reporting the number of studies is the most important.\nA flow diagram can be very useful; it should depict all the studies included based upon fulfilling the eligibility criteria, whether or not data have been combined for statistical analysis. A recent review of 87 systematic reviews found that about half included a QUOROM flow diagram [133]. The authors of this research recommended some important ways that reviewers can improve the use of a flow diagram when describing the flow of information throughout the review process, including a separate flow diagram for each important outcome reported [133].\n\n\nExamples\n\nIn text: A total of 10 studies involving 13 trials were identified for inclusion in the review. The search of Medline, PsycInfo and Cinahl databases provided a total of 584 citations. After adjusting for duplicates 509 remained. Of these, 479 studies were discarded because after reviewing the abstracts it appeared that these papers clearly did not meet the criteria. Three additional studies…were discarded because full text of the study was not available or the paper could not be feasibly translated into English. The full text of the remaining 27 citations was examined in more detail. It appeared that 22 studies did not meet the inclusion criteria as described. Five studies…met the inclusion criteria and were included in the systematic review. An additional five studies…that met the criteria for inclusion were identified by checking the references of located, relevant papers and searching for studies that have cited these papers. No unpublished relevant studies were obtained.\n\n\nExample flow diagram of study selection.\n\nDDW, Digestive Disease Week; UEGW, United European Gastroenterology Week. Reproduced with permission from https://doi.org/10.1371/journal.pmed.1000100.g002\nBack to top\n\n\n\n\n\n\n18. Study characteristics\n\n\n\n\n\n\n\nFor each study, present characteristics for which data were extracted (e.g., study size, PICOS, follow-up period) and provide the citation.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nFor readers to gauge the validity and applicability of a systematic review’s results, they need to know something about the included studies. Such information includes PICOS (Box 2) and specific information relevant to the review question. For example, if the review is examining the long-term effects of antidepressants for moderate depressive disorder, authors should report the follow-up periods of the included studies. For each included study, authors should provide a citation for the source of their information regardless of whether or not the study is published. This information makes it easier for interested readers to retrieve the relevant publications or documents.\nReporting study-level data also allows the comparison of the main characteristics of the studies included in the review. Authors should present enough detail to allow readers to make their own judgments about the relevance of included studies. Such information also makes it possible for readers to conduct their own subgroup analyses and interpret subgroups, based on study characteristics.\nAuthors should avoid, whenever possible, assuming information when it is missing from a study report (e.g., sample size, method of randomization). Reviewers may contact the original investigators to try to obtain missing information or confirm the data extracted for the systematic review. If this information is not obtained, this should be noted in the report. If information is imputed, the reader should be told how this was done and for which items. Presenting study-level data makes it possible to clearly identify unpublished information obtained from the original researchers and make it available for the public record.\nTypically, study-level characteristics are presented as a table as in the example in Table 2. Such presentation ensures that all pertinent items are addressed and that missing or unclear information is clearly indicated. Although paper-based journals do not generally allow for the quantity of information available in electronic journals or Cochrane reviews, this should not be accepted as an excuse for omission of important aspects of the methods or results of included studies, since these can, if necessary, be shown on a Web site.\nFollowing the presentation and description of each included study, as discussed above, reviewers usually provide a narrative summary of the studies. Such a summary provides readers with an overview of the included studies. It may for example address the languages of the published papers, years of publication, and geographic origins of the included studies.\nThe PICOS framework is often helpful in reporting the narrative summary indicating, for example, the clinical characteristics and disease severity of the participants and the main features of the intervention and of the comparison group. For non-pharmacological interventions, it may be helpful to specify for each study the key elements of the intervention received by each group. Full details of the interventions in included studies were reported in only three of 25 systematic reviews relevant to general practice [84].\n\n\nExamples\n\nIn text:\n\nCharacteristics of included studies\nMethods: All four studies finally selected for the review were randomised controlled trials published in English. The duration of the intervention was 24 months for the RIO-North America and 12 months for the RIO-Diabetes, RIO-Lipids and RIO-Europe study. Although the last two described a period of 24 months during which they were conducted, only the first 12-months results are provided. All trials had a run-in, as a single blind period before the randomisation.\nParticipants: The included studies involved 6625 participants. The main inclusion criteria entailed adults (18 years or older), with a body mass index greater than 27 kg/m2 and less than 5 kg variation in body weight within the three months before study entry.\nIntervention: All trials were multicentric. The RIO-North America was conducted in the USA and Canada, RIO-Europe in Europe and the USA, RIO-Diabetes in the USA and 10 other different countries not specified, and RIO-Lipids in eight unspecified different countries.\nThe intervention received was placebo, 5 mg of rimonabant or 20 mg of rimonabant once daily in addition to a mild hypocaloric diet (600 kcal/day deficit).\nOutcomes:\nPrimary: In all studies the primary outcome assessed was weight change from baseline after one year of treatment and the RIO-North America study also evaluated the prevention of weight regain between the first and second year. All studies evaluated adverse effects, including those of any kind and serious events. Quality of life was measured in only one study, but the results were not described (RIO-Europe).\nSecondary and additional outcomes: These included prevalence of metabolic syndrome after one year and change in cardiometabolic risk factors such as blood pressure, lipid profile, etc.\nNo study included mortality and costs as outcome.\nThe timing of outcome measures was variable and could include monthly investigations, evaluations every three months or a single final evaluation after one year.\n\nIn table:\n\nExample Table: Summary of included studies evaluating the efficacy of antiemetic agents in acute gastroenteritis.\nhttps://doi.org/10.1371/journal.pmed.1000100.t002\nBack to top\n\n\n\n\n\n\n19. Risk of bias within studies\n\n\n\n\n\n\n\nPresent data on risk of bias of each study and, if available, any outcome-level assessment (see Item 12).\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nWe recommend that reviewers assess the risk of bias in the included studies using a standard approach with defined criteria (see Item 12). They should report the results of any such assessments [89].\nReporting only summary data (e.g., two of eight trials adequately concealed allocation) is inadequate because it fails to inform readers which studies had the particular methodological shortcoming. A more informative approach is to explicitly report the methodological features evaluated for each study. The Cochrane Collaboration’s new tool for assessing the risk of bias also requests that authors substantiate these assessments with any relevant text from the original studies [11]. It is often easiest to provide these data in a tabular format, as in the example. However, a narrative summary describing the tabular data can also be helpful for readers.\n\n\nExamples\n\nIn table:\n\nExample Table: Quality measures of the randomized controlled trials that failed to fulfill any one of six markers of validity.\nhttps://doi.org/10.1371/journal.pmed.1000100.t003\nBack to top\n\n\n\n\n\n\n20. Results of individual studies\n\n\n\n\n\n\n\nFor all outcomes considered (benefits and harms), present, for each study: (a) simple summary data for each intervention group and (b) effect estimates and confidence intervals, ideally with a forest plot.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nPublication of summary data from individual studies allows the analyses to be reproduced and other analyses and graphical displays to be investigated. Others may wish to assess the impact of excluding particular studies or consider subgroup analyses not reported by the review authors. Displaying the results of each treatment group in included studies also enables inspection of individual study features. For example, if only odds ratios are provided, readers cannot assess the variation in event rates across the studies, making the odds ratio impossible to interpret [138]. Additionally, because data extraction errors in meta-analyses are common and can be large [139], the presentation of the results from individual studies makes it easier to identify errors. For continuous outcomes, readers may wish to examine the consistency of standard deviations across studies, for example, to be reassured that standard deviation and standard error have not been confused [138].\nFor each study, the summary data for each intervention group are generally given for binary outcomes as frequencies with and without the event (or as proportions such as 12/45). It is not sufficient to report event rates per intervention group as percentages. The required summary data for continuous outcomes are the mean, standard deviation, and sample size for each group. In reviews that examine time-to-event data, the authors should report the log hazard ratio and its standard error (or confidence interval) for each included study. Sometimes, essential data are missing from the reports of the included studies and cannot be calculated from other data but may need to be imputed by the reviewers. For example, the standard deviation may be imputed using the typical standard deviations in the other trials [116],[117] (see Item 14). Whenever relevant, authors should indicate which results were not reported directly and had to be estimated from other information (see Item 13). In addition, the inclusion of unpublished data should be noted.\nFor all included studies it is important to present the estimated effect with a confidence interval. This information may be incorporated in a table showing study characteristics or may be shown in a forest plot [140]. The key elements of the forest plot are the effect estimates and confidence intervals for each study shown graphically, but it is preferable also to include, for each study, the numerical group-specific summary data, the effect size and confidence interval, and the percentage weight (see second example [Figure 3]). For discussion of the results of meta-analysis, see Item 21.\nIn principle, all the above information should be provided for every outcome considered in the review, including both benefits and harms. When there are too many outcomes for full information to be included, results for the most important outcomes should be included in the main report with other information provided as a Web appendix. The choice of the information to present should be justified in light of what was originally stated in the protocol. Authors should explicitly mention if the planned main outcomes cannot be presented due to lack of information. There is some evidence that information on harms is only rarely reported in systematic reviews, even when it is available in the original studies [141]. Selective omission of harms results biases a systematic review and decreases its ability to contribute to informed decision making.\n\n\nExamples\n\nIn figure:\n\nExample Figure: Overall failure (defined as failure of assigned regimen or relapse) with tetracycline-rifampicin versus tetracycline-streptomycin.\nCI, confidence interval. Reproduced with permission from https://doi.org/10.1371/journal.pmed.1000100.g003\n\nIn table:\n\nExample Table: Heterotopic ossification in trials comparing radiotherapy to non-steroidal anti-inflammatory drugs after major hip procedures and fractures.\nhttps://doi.org/10.1371/journal.pmed.1000100.t004\nBack to top\n\n\n\n\n\n\n21. Synthesis of results\n\n\n\n\n\n\n\nPresent the main results of the review. If meta-analyses are done, include for each, confidence intervals and measures of consistency.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nResults of systematic reviews should be presented in an orderly manner. Initial narrative descriptions of the evidence covered in the review (see Item 18) may tell readers important things about the study populations and the design and conduct of studies. These descriptions can facilitate the examination of patterns across studies. They may also provide important information about applicability of evidence, suggest the likely effects of any major biases, and allow consideration, in a systematic manner, of multiple explanations for possible differences of findings across studies.\nIf authors have conducted one or more meta-analyses, they should present the results as an estimated effect across studies with a confidence interval. It is often simplest to show each meta-analysis summary with the actual results of included studies in a forest plot (see Item 20) [140]. It should always be clear which of the included studies contributed to each meta-analysis. Authors should also provide, for each meta-analysis, a measure of the consistency of the results from the included studies such as I2 (heterogeneity; see Box 6); a confidence interval may also be given for this measure [145]. If no meta-analysis was performed, the qualitative inferences should be presented as systematically as possible with an explanation of why meta-analysis was not done, as in the second example above [143]. Readers may find a forest plot, without a summary estimate, helpful in such cases.\nAuthors should in general report syntheses for all the outcome measures they set out to investigate (i.e., those described in the protocol; see Item 4) to allow readers to draw their own conclusions about the implications of the results. Readers should be made aware of any deviations from the planned analysis. Authors should tell readers if the planned meta-analysis was not thought appropriate or possible for some of the outcomes and the reasons for that decision.\nIt may not always be sensible to give meta-analysis results and forest plots for each outcome. If the review addresses a broad question, there may be a very large number of outcomes. Also, some outcomes may have been reported in only one or two studies, in which case forest plots are of little value and may be seriously biased.\nOf 300 systematic reviews indexed in MEDLINE in 2004, a little more than half (54%) included meta-analyses, of which the majority (91%) reported assessing for inconsistency in results.\n\n\nExamples\n\nMortality data were available for all six trials, randomizing 311 patients and reporting data for 305 patients. There were no deaths reported in the three respiratory syncytial virus/severe bronchiolitis trials; thus our estimate is based on three trials randomizing 232 patients, 64 of whom died. In the pooled analysis, surfactant was associated with significantly lower mortality (relative risk = 0.7, 95% confidence interval = 0.4–0.97, P = 0.04). There was no evidence of heterogeneity (I2 = 0%).\n\n\nBecause the study designs, participants, interventions, and reported outcome measures varied markedly, we focused on describing the studies, their results, their applicability, and their limitations and on qualitative synthesis rather than meta-analysis.\n\n\nWe detected significant heterogeneity within this comparison (I2 = 46.6%; χ2 = 13.11, df = 7; P = 0.07). Retrospective exploration of the heterogeneity identified one trial that seemed to differ from the others. It included only small ulcers (wound area less than 5 cm2). Exclusion of this trial removed the statistical heterogeneity and did not affect the finding of no evidence of a difference in healing rate between hydrocolloids and simple low adherent dressings (relative risk = 0.98, [95% confidence interval] 0.85 to 1.12; I2 = 0%).\n\nBack to top\n\n\n\n\n\n\n22. Risk of bias across studies\n\n\n\n\n\n\n\nPresent results of any assessment of risk of bias across studies (see Item 15).\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAuthors should present the results of any assessments of risk of bias across studies. If a funnel plot is reported, authors should specify the effect estimate and measure of precision used, presented typically on the x-axis and y-axis, respectively. Authors should describe if and how they have tested the statistical significance of any possible asymmetry (see Item 15). Results of any investigations of selective reporting of outcomes within studies (as discussed in Item 15) should also be reported. Also, we advise authors to tell readers if any pre-specified analyses for assessing risk of bias across studies were not completed and the reasons (e.g., too few included studies).\n\n\nExamples\n\nStrong evidence of heterogeneity (I2 = 79%, P<0.001) was observed. To explore this heterogeneity, a funnel plot was drawn. The funnel plot in Figure 4 shows evidence of considerable asymmetry.\n\nExample Figure 4: Example of a funnel plot showing evidence of considerable asymmetry. SE, standard error. Adapted with permission from. https://doi.org/10.1371/journal.pmed.1000100.g004\n\nSpecifically, four sertraline trials involving 486 participants and one citalopram trial involving 274 participants were reported as having failed to achieve a statistically significant drug effect, without reporting mean HRSD [Hamilton Rating Scale for Depression] scores. We were unable to find data from these trials on pharmaceutical company Web sites or through our search of the published literature. These omissions represent 38% of patients in sertraline trials and 23% of patients in citalopram trials. Analyses with and without inclusion of these trials found no differences in the patterns of results; similarly, the revealed patterns do not interact with drug type. The purpose of using the data obtained from the FDA was to avoid publication bias, by including unpublished as well as published trials. Inclusion of only those sertraline and citalopram trials for which means were reported to the FDA would constitute a form of reporting bias similar to publication bias and would lead to overestimation of drug–placebo differences for these drug types. Therefore, we present analyses only on data for medications for which complete clinical trials’ change was reported.\n\nBack to top\n\n\n\n\n\n\n23. Additional analysis\n\n\n\n\n\n\n\nGive results of additional analyses, if done (e.g., sensitivity or subgroup analyses, meta-regression [see Item 16]).\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAuthors should report any subgroup or sensitivity analyses and whether or not they were pre-specified (see Items 5 and 16). For analyses comparing subgroups of studies (e.g., separating studies of low- and high-dose aspirin), the authors should report any tests for interactions, as well as estimates and confidence intervals from meta-analyses within each subgroup. Similarly, meta-regression results (see Item 16) should not be limited to p-values, but should include effect sizes and confidence intervals [150], as the first example reported above does in a table. The amount of data included in each additional analysis should be specified if different from that considered in the main analyses. This information is especially relevant for sensitivity analyses that exclude some studies; for example, those with high risk of bias.\nImportantly, all additional analyses conducted should be reported, not just those that were statistically significant. This information will help avoid selective outcome reporting bias within the review as has been demonstrated in reports of randomized controlled trials [42],[44],[121],[151],[152]. Results from exploratory subgroup or sensitivity analyses should be interpreted cautiously, bearing in mind the potential for multiple analyses to mislead.\n\n\nExamples\n\n…benefits of chondroitin were smaller in trials with adequate concealment of allocation compared with trials with unclear concealment (P for interaction = 0.050), in trials with an intention-to-treat analysis compared with those that had excluded patients from the analysis (P for interaction = 0.017), and in large compared with small trials (P for interaction = 0.022).\n\n\nSubgroup analyses according to antibody status, antiviral medications, organ transplanted, treatment duration, use of antilymphocyte therapy, time to outcome assessment, study quality and other aspects of study design did not demonstrate any differences in treatment effects. Multivariate meta-regression showed no significant difference in CMV [cytomegalovirus] disease after allowing for potential confounding or effect-modification by prophylactic drug used, organ transplanted or recipient serostatus in CMV positive recipients and CMV negative recipients of CMV positive donors.\n\nBack to top\n\n\n\n\n\n\nDiscussion\n\n\n24. Summary of Evidence\n\n\n\n\n\n\n\nSummarize the main findings, including the strength of evidence for each main outcome; consider their relevance to key groups (e.g., health care providers, users, and policy makers\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAuthors should give a brief and balanced summary of the nature and findings of the review. Sometimes, outcomes for which little or no data were found should be noted due to potential relevance for policy decisions and future research. Applicability of the review’s findings, to different patients, settings, or target audiences, for example, should be mentioned. Although there is no standard way to assess applicability simultaneously to different audiences, some systems do exist [153]. Sometimes, authors formally rate or assess the overall body of evidence addressed in the review and can present the strength of their summary recommendations tied to their assessments of the quality of evidence (e.g., the GRADE system) [10].\nAuthors need to keep in mind that statistical significance of the effects does not always suggest clinical or policy relevance. Likewise, a non-significant result does not demonstrate that a treatment is ineffective. Authors should ideally clarify trade-offs and how the values attached to the main outcomes would lead different people to make different decisions. In addition, adroit authors consider factors that are important in translating the evidence to different settings and that may modify the estimates of effects reported in the review [153]. Patients and health care providers may be primarily interested in which intervention is most likely to provide a benefit with acceptable harms, while policy makers and administrators may value data on organizational impact and resource utilization.\n\n\nExamples\n\nOverall, the evidence is not sufficiently robust to determine the comparative effectiveness of angioplasty (with or without stenting) and medical treatment alone. Only 2 randomized trials with long-term outcomes and a third randomized trial that allowed substantial crossover of treatment after 3 months directly compared angioplasty and medical treatment…the randomized trials did not evaluate enough patients or did not follow patients for a sufficient duration to allow definitive conclusions to be made about clinical outcomes, such as mortality and cardiovascular or kidney failure events.\n\n\nSome acceptable evidence from comparison of medical treatment and angioplasty suggested no difference in long-term kidney function but possibly better blood pressure control after angioplasty, an effect that may be limited to patients with bilateral atherosclerotic renal artery stenosis. The evidence regarding other outcomes is weak. Because the reviewed studies did not explicitly address patients with rapid clinical deterioration who may need acute intervention, our conclusions do not apply to this important subset of patients.\n\nBack to top\n\n\n\n\n\n\n25. Limitations\n\n\n\n\n\n\n\nDiscuss limitations at study and outcome level (e.g., risk of bias), and at review level (e.g., incomplete retrieval of identified research, reporting bias).\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nA discussion of limitations should address the validity (i.e., risk of bias) and reporting (informativeness) of the included studies, limitations of the review process, and generalizability (applicability) of the review. Readers may find it helpful if authors discuss whether studies were threatened by serious risks of bias, whether the estimates of the effect of the intervention are too imprecise, or if there were missing data for many participants or important outcomes.\nLimitations of the review process might include limitations of the search (e.g., restricting to English-language publications), and any difficulties in the study selection, appraisal, and meta-analysis processes. For example, poor or incomplete reporting of study designs, patient populations, and interventions may hamper interpretation and synthesis of the included studies [84]. Applicability of the review may be affected if there are limited data for certain populations or subgroups where the intervention might perform differently or few studies assessing the most important outcomes of interest; or if there is a substantial amount of data relating to an outdated intervention or comparator or heavy reliance on imputation of missing values for summary estimates (Item 14).\n\n\nExamples\n\nOutcome level:\n\nThe meta-analysis reported here combines data across studies in order to estimate treatment effects with more precision than is possible in a single study. The main limitation of this meta-analysis, as with any overview, is that the patient population, the antibiotic regimen and the outcome definitions are not the same across studies.\n\nStudy and review level:\n\nOur study has several limitations. The quality of the studies varied. Randomization was adequate in all trials; however, 7 of the articles did not explicitly state that analysis of data adhered to the intention-to-treat principle, which could lead to overestimation of treatment effect in these trials, and we could not assess the quality of 4 of the 5 trials reported as abstracts. Analyses did not identify an association between components of quality and re-bleeding risk, and the effect size in favour of combination therapy remained statistically significant when we excluded trials that were reported as abstracts.\n\nPublication bias might account for some of the effect we observed. Smaller trials are, in general, analyzed with less methodological rigor than larger studies, and an asymmetrical funnel plot suggests that selective reporting may have led to an overestimation of effect sizes in small trials.\n\nBack to top\n\n\n\n\n\n\n26. Conclusions\n\n\n\n\n\n\n\nProvide a general interpretation of the results in the context of other evidence, and implications for future research.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nSystematic reviewers sometimes draw conclusions that are too optimistic [157] or do not consider the harms equally as carefully as the benefits, although some evidence suggests these problems are decreasing [158]. If conclusions cannot be drawn because there are too few reliable studies, or too much uncertainty, this should be stated. Such a finding can be as important as finding consistent effects from several large studies.\nAuthors should try to relate the results of the review to other evidence, as this helps readers to better interpret the results. For example, there may be other systematic reviews about the same general topic that have used different methods or have addressed related but slightly different questions [159],[160]. Similarly, there may be additional information relevant to decision makers, such as the cost-effectiveness of the intervention (e.g., health technology assessment). Authors may discuss the results of their review in the context of existing evidence regarding other interventions.\nWe advise authors also to make explicit recommendations for future research. In a sample of 2,535 Cochrane reviews, 82% included recommendations for research with specific interventions, 30% suggested the appropriate type of participants, and 52% suggested outcome measures for future research [161]. There is no corresponding assessment about systematic reviews published in medical journals, but we believe that such recommendations are much less common in those reviews.\nClinical research should not be planned without a thorough knowledge of similar, existing research [162]. There is evidence that this still does not occur as it should and that authors of primary studies do not consider a systematic review when they design their studies [163]. We believe systematic reviews have great potential for guiding future clinical research.\n\n\nExamples\n\nImplications for practice:\n\nBetween 1995 and 1997 five different meta-analyses of the effect of antibiotic prophylaxis on infection and mortality were published. All confirmed a significant reduction in infections, though the magnitude of the effect varied from one review to another. The estimated impact on overall mortality was less evident and has generated considerable controversy on the cost effectiveness of the treatment. Only one among the five available reviews, however, suggested that a weak association between respiratory tract infections and mortality exists and lack of sufficient statistical power may have accounted for the limited effect on mortality.\n\nImplications for research:\n\nA logical next step for future trials would thus be the comparison of this protocol against a regimen of a systemic antibiotic agent only to see whether the topical component can be dropped. We have already identified six such trials but the total number of patients so far enrolled (n = 1056) is too small for us to be confident that the two treatments are really equally effective. If the hypothesis is therefore considered worth testing more and larger randomised controlled trials are warranted. Trials of this kind, however, would not resolve the relevant issue of treatment induced resistance. To produce a satisfactory answer to this, studies with a different design would be necessary. Though a detailed discussion goes beyond the scope of this paper, studies in which the intensive care unit rather than the individual patient is the unit of randomisation and in which the occurrence of antibiotic resistance is monitored over a long period of time should be undertaken.\nBack to top\n\n\n\n\n\n\nFunding\n\n\n27. Funding\n\n\n\n\n\n\n\nDescribe sources of funding or other support (e.g., supply of data) for the systematic review; role of funders for the systematic review.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAuthors of systematic reviews, like those of any other research study, should disclose any funding they received to carry out the review, or state if the review was not funded. Lexchin and colleagues [166] observed that outcomes of reports of randomized trials and meta-analyses of clinical trials funded by the pharmaceutical industry are more likely to favor the sponsor’s product compared to studies with other sources of funding. Similar results have been reported elsewhere [167],[168]. Analogous data suggest that similar biases may affect the conclusions of systematic reviews [169].\nGiven the potential role of systematic reviews in decision making, we believe authors should be transparent about the funding and the role of funders, if any. Sometimes the funders will provide services, such as those of a librarian to complete the searches for relevant literature or access to commercial databases not available to the reviewers. Any level of funding or services provided to the systematic review team should be reported. Authors should also report whether the funder had any role in the conduct or report of the review. Beyond funding issues, authors should report any real or perceived conflicts of interest related to their role or the role of the funder in the reporting of the systematic review [170].\nIn a survey of 300 systematic reviews published in November 2004, funding sources were not reported in 41% of the reviews [3]. Only a minority of reviews (2%) reported being funded by for-profit sources, but the true proportion may be higher [171].\n\n\nExamples\n\nAuthors of systematic reviews, like those of any other research study, should disclose any funding they received to carry out the review, or state if the review was not funded. Lexchin and colleagues [166] observed that outcomes of reports of randomized trials and meta-analyses of clinical trials funded by the pharmaceutical industry are more likely to favor the sponsor’s product compared to studies with other sources of funding. Similar results have been reported elsewhere [167],[168]. Analogous data suggest that similar biases may affect the conclusions of systematic reviews [169].\n\n\nGiven the potential role of systematic reviews in decision making, we believe authors should be transparent about the funding and the role of funders, if any. Sometimes the funders will provide services, such as those of a librarian to complete the searches for relevant literature or access to commercial databases not available to the reviewers. Any level of funding or services provided to the systematic review team should be reported. Authors should also report whether the funder had any role in the conduct or report of the review. Beyond funding issues, authors should report any real or perceived conflicts of interest related to their role or the role of the funder in the reporting of the systematic review [170].\n\n\nIn a survey of 300 systematic reviews published in November 2004, funding sources were not reported in 41% of the reviews [3]. Only a minority of reviews (2%) reported being funded by for-profit sources, but the true proportion may be higher [171].\n\nBack to top"
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-1-1",
    "href": "guidelines/prisma/index.html#sec-1-1",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Read More",
    "text": "Read More\nAuthors should identify their report as a systematic review or meta-analysis. Terms such as review or overview do not describe for readers whether the review was systematic or whether a meta-analysis was performed. A recent survey found that 50% of 300 authors did not mention the terms systematic review or meta-analysis in the title or abstract of their systematic review [3]. Although sensitive search strategies have been developed to identify systematic reviews [22], inclusion of the terms systematic review or meta-analysis in the title may improve indexing and identification.\nWe advise authors to use informative titles that make key information easily accessible to readers. Ideally, a title reflecting the PICOS approach (participants, interventions, comparators, outcomes, and study design) (see Item 11 and Box 2) may help readers as it provides key information about the scope of the review. Specifying the design(s) of the studies included, as shown in the examples, may also help some readers and those searching databases.\nSome journals recommend indicative titles that indicate the topic matter of the review, while others require declarative titles that give the review’s main conclusion. Busy practitioners may prefer to see the conclusion of the review in the title, but declarative titles can oversimplify or exaggerate findings. Thus, many journals and methodologists prefer indicative titles as used in the examples above."
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-1-2",
    "href": "guidelines/prisma/index.html#sec-1-2",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Examples",
    "text": "Examples\n\nRecurrence rates of video-assisted thoracoscopic versus open surgery in the prevention of recurrent pneumothoraces: a systematic review of randomised and non-randomised trials\n\n\nMortality in randomized trials of antioxidant supplements for primary and secondary prevention: systematic review and meta-analysis\n\nBack to top"
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-2_structured_summary-1",
    "href": "guidelines/prisma/index.html#sec-2_structured_summary-1",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Read More",
    "text": "Read More\nAbstracts provide key information that enables readers to understand the scope, processes, and findings of a review and to decide whether to read the full report. The abstract may be all that is readily available to a reader, for example, in a bibliographic database. The abstract should present a balanced and realistic assessment of the review’s findings that mirrors, albeit briefly, the main text of the report.\nWe agree with others that the quality of reporting in abstracts presented at conferences and in journal publications needs improvement [24],[25]. While we do not uniformly favor a specific format over another, we generally recommend structured abstracts. Structured abstracts provide readers with a series of headings pertaining to the purpose, conduct, findings, and conclusions of the systematic review being reported [26],[27]. They give readers more complete information and facilitate finding information more easily than unstructured abstracts [28],[29],[30],[31],[32].\nA highly structured abstract of a systematic review could include the following headings: Context (or Background); Objective (or Purpose); Data Sources; Study Selection (or Eligibility Criteria); Study Appraisal and Synthesis Methods (or Data Extraction and Data Synthesis); Results; Limitations; and Conclusions (or Implications). Alternatively, a simpler structure could cover but collapse some of the above headings (e.g., label Study Selection and Study Appraisal as Review Methods) or omit some headings such as Background and Limitations.\nIn the highly structured abstract mentioned above, authors use the Background heading to set the context for readers and explain the importance of the review question. Under the Objectives heading, they ideally use elements of PICOS (see Box 2) to state the primary objective of the review. Under a Data Sources heading, they summarize sources that were searched, any language or publication type restrictions, and the start and end dates of searches. Study Selection statements then ideally describe who selected studies using what inclusion criteria. Data Extraction Methods statements describe appraisal methods during data abstraction and the methods used to integrate or summarize the data. The Data Synthesis section is where the main results of the review are reported. If the review includes meta-analyses, authors should provide numerical results with confidence intervals for the most important outcomes. Ideally, they should specify the amount of evidence in these analyses (numbers of studies and numbers of participants). Under a Limitations heading, authors might describe the most important weaknesses of included studies as well as limitations of the review process. Then authors should provide clear and balanced Conclusions that are closely linked to the objective and findings of the review. Additionally, it would be helpful if authors included some information about funding for the review. Finally, although protocol registration for systematic reviews is still not common practice, if authors have registered their review or received a registration number, we recommend providing the registration information at the end of the abstract.\nTaking all the above considerations into account, the intrinsic tension between the goal of completeness of the abstract and its keeping into the space limit often set by journal editors is recognized as a major challenge."
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-2_structured_summary-2",
    "href": "guidelines/prisma/index.html#sec-2_structured_summary-2",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Examples",
    "text": "Examples\n\nContext: The role and dose of oral vitamin D supplementation in nonvertebral fracture prevention have not been well established.\n\nObjective: To estimate the effectiveness of vitamin D supplementation in preventing hip and nonvertebral fractures in older persons.\nData Sources: A systematic review of English and non-English articles using MEDLINE and the Cochrane Controlled Trials Register (1960–2005), and EMBASE (1991–2005). Additional studies were identified by contacting clinical experts and searching bibliographies and abstracts presented at the American Society for Bone and Mineral Research (1995–2004). Search terms included randomized controlled trial (RCT), controlled clinical trial, random allocation, double-blind method, cholecalciferol, ergocalciferol, 25-hydroxyvitamin D, fractures, humans, elderly, falls, and bone density.\nStudy Selection: Only double-blind RCTs of oral vitamin D supplementation (cholecalciferol, ergocalciferol) with or without calcium supplementation vs calcium supplementation or placebo in older persons (>60 years) that examined hip or nonvertebral fractures were included.\nData Extraction: Independent extraction of articles by 2 authors using predefined data fields, including study quality indicators.\nData Synthesis: All pooled analyses were based on random-effects models. Five RCTs for hip fracture (n = 9294) and 7 RCTs for nonvertebral fracture risk (n = 9820) met our inclusion criteria. All trials used cholecalciferol. Heterogeneity among studies for both hip and nonvertebral fracture prevention was observed, which disappeared after pooling RCTs with low-dose (400 IU/d) and higher-dose vitamin D (700–800 IU/d), separately. A vitamin D dose of 700 to 800 IU/d reduced the relative risk (RR) of hip fracture by 26% (3 RCTs with 5572 persons; pooled RR, 0.74; 95% confidence interval [CI], 0.61–0.88) and any nonvertebral fracture by 23% (5 RCTs with 6098 persons; pooled RR, 0.77; 95% CI, 0.68–0.87) vs calcium or placebo. No significant benefit was observed for RCTs with 400 IU/d vitamin D (2 RCTs with 3722 persons; pooled RR for hip fracture, 1.15; 95% CI, 0.88–1.50; and pooled RR for any nonvertebral fracture, 1.03; 95% CI, 0.86–1.24).\nConclusions: Oral vitamin D supplementation between 700 to 800 IU/d appears to reduce the risk of hip and any nonvertebral fractures in ambulatory or institutionalized elderly persons. An oral vitamin D dose of 400 IU/d is not sufficient for fracture prevention.\nBack to top"
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-3_rationale-1",
    "href": "guidelines/prisma/index.html#sec-3_rationale-1",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Read More",
    "text": "Read More\nReaders need to understand the rationale behind the study and what the systematic review may add to what is already known. Authors should tell readers whether their report is a new systematic review or an update of an existing one. If the review is an update, authors should state reasons for the update, including what has been added to the evidence base since the previous version of the review.\nAn ideal background or introduction that sets context for readers might include the following. First, authors might define the importance of the review question from different perspectives (e.g., public health, individual patient, or health policy). Second, authors might briefly mention the current state of knowledge and its limitations. As in the above example, information about the effects of several different interventions may be available that helps readers understand why potential relative benefits or harms of particular interventions need review. Third, authors might whet readers’ appetites by clearly stating what the review aims to add. They also could discuss the extent to which the limitations of the existing evidence base may be overcome by the review."
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-3_rationale-2",
    "href": "guidelines/prisma/index.html#sec-3_rationale-2",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Examples",
    "text": "Examples\n\nReversing the trend of increasing weight for height in children has proven difficult.\n\nIt is widely accepted that increasing energy expenditure and reducing energy intake form the theoretical basis for management. Therefore, interventions\naiming to increase physical activity and improve diet are the foundation of efforts\nto prevent and treat childhood obesity. Such lifestyle interventions have been\nsupported by recent systematic reviews, as well as by the Canadian Paediatric\nSociety, the Royal College of Paediatrics and Child Health, and the American\n\nAcademy of Pediatrics. However, these interventions are fraught with poor\n\nadherence. Thus, school-based interventions are theoretically appealing because\nadherence with interventions can be improved. Consequently, many local\ngovernments have enacted or are considering policies that mandate increased\n\nphysical activity in schools, although the effect of such interventions on body\n\ncomposition has not been assessed\nBack to top"
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-4_objectives-1",
    "href": "guidelines/prisma/index.html#sec-4_objectives-1",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Read More",
    "text": "Read More\nThe questions being addressed, and the rationale for them, are one of the most critical parts of a systematic review. They should be stated precisely and explicitly so that readers can understand quickly the review’s scope and the potential applicability of the review to their interests [35]. Framing questions so that they include the following five PICOS components may improve the explicitness of review questions: (1) the patient population or disease being addressed (P), (2) the interventions or exposure of interest (I), (3) the comparators (C), (4) the main outcome or endpoint of interest (O), and (5) the study designs chosen (S). For more detail regarding PICOS, see Box 2.\nGood review questions may be narrowly focused or broad, depending on the overall objectives of the review. Sometimes broad questions might increase the applicability of the results and facilitate detection of bias, exploratory analyses, and sensitivity analyses [35],[36]. Whether narrowly focused or broad, precisely stated review objectives are critical as they help define other components of the review process such as the eligibility criteria (Item 6) and the search for relevant literature (Items 7 and 8)"
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-4_objectives-2",
    "href": "guidelines/prisma/index.html#sec-4_objectives-2",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Examples",
    "text": "Examples\n\nTo examine whether topical or intraluminal antibiotics reduce catheter-related bloodstream infection, we reviewed randomized, controlled trials that assessed the efficacy of these antibiotics for primary prophylaxis against catheter-related bloodstream infection and mortality compared with no antibiotic therapy in adults undergoing hemodialysis\n\nBack to top"
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-5_protocol_and_registration-1",
    "href": "guidelines/prisma/index.html#sec-5_protocol_and_registration-1",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Read More",
    "text": "Read More\nA protocol is important because it pre-specifies the objectives and methods of the systematic review. For instance, a protocol specifies outcomes of primary interest, how reviewers will extract information about those outcomes, and methods that reviewers might use to quantitatively summarize the outcome data (see Item 13). Having a protocol can help restrict the likelihood of biased post hoc decisions in review methods, such as selective outcome reporting. Several sources provide guidance about elements to include in the protocol for a systematic review [16],[38],[39]. For meta-analyses of individual patient-level data, we advise authors to describe whether a protocol was explicitly designed and whether, when, and how participating collaborators endorsed it [40],[41].\nAuthors may modify protocols during the research, and readers should not automatically consider such modifications inappropriate. For example, legitimate modifications may extend the period of searches to include older or newer studies, broaden eligibility criteria that proved too narrow, or add analyses if the primary analyses suggest that additional ones are warranted. Authors should, however, describe the modifications and explain their rationale.\nAlthough worthwhile protocol amendments are common, one must consider the effects that protocol modifications may have on the results of a systematic review, especially if the primary outcome is changed. Bias from selective outcome reporting in randomized trials has been well documented [42],[43]. An examination of 47 Cochrane reviews revealed indirect evidence for possible selective reporting bias for systematic reviews. Almost all (n = 43) contained a major change, such as the addition or deletion of outcomes, between the protocol and the full publication [44]. Whether (or to what extent) the changes reflected bias, however, was not clear. For example, it has been rather common not to describe outcomes that were not presented in any of the included studies.\nRegistration of a systematic review, typically with a protocol and registration number, is not yet common, but some opportunities exist [45],[46]. Registration may possibly reduce the risk of multiple reviews addressing the same question [45],[46],[47],[48], reduce publication bias, and provide greater transparency when updating systematic reviews. Of note, a survey of systematic reviews indexed in MEDLINE in November 2004 found that reports of protocol use had increased to about 46% [3] from 8% noted in previous surveys [49]. The improvement was due mostly to Cochrane reviews, which, by requirement, have a published protocol [3]."
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-5_protocol_and_registration-2",
    "href": "guidelines/prisma/index.html#sec-5_protocol_and_registration-2",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Examples",
    "text": "Examples\n\nMethods of the analysis and inclusion criteria were specified in advance and documented in a protocol.\n\nBack to top"
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-6_eligibility_criteria-1",
    "href": "guidelines/prisma/index.html#sec-6_eligibility_criteria-1",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Read More",
    "text": "Read More\nKnowledge of the eligibility criteria is essential in appraising the validity, applicability, and comprehensiveness of a review. Thus, authors should unambiguously specify eligibility criteria used in the review. Carefully defined eligibility criteria inform various steps of the review methodology. They influence the development of the search strategy and serve to ensure that studies are selected in a systematic and unbiased manner.\nA study may be described in multiple reports, and one report may describe multiple studies. Therefore, we separate eligibility criteria into the following two components: study characteristics and report characteristics. Both need to be reported. Study eligibility criteria are likely to include the populations, interventions, comparators, outcomes, and study designs of interest (PICOS; see Box 2), as well as other study-specific elements, such as specifying a minimum length of follow-up. Authors should state whether studies will be excluded because they do not include (or report) specific outcomes to help readers ascertain whether the systematic review may be biased as a consequence of selective reporting [42],[43].\nReport eligibility criteria are likely to include language of publication, publication status (e.g., inclusion of unpublished material and abstracts), and year of publication. Inclusion or not of non-English language literature [51],[52],[53],[54],[55], unpublished data, or older data can influence the effect estimates in meta-analyses [56],[57],[58],[59]. Caution may need to be exercised in including all identified studies due to potential differences in the risk of bias such as, for example, selective reporting in abstracts [60],[61],[62]."
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-6_eligibility_criteria-2",
    "href": "guidelines/prisma/index.html#sec-6_eligibility_criteria-2",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Examples",
    "text": "Examples\n\nTypes of studies: Randomised clinical trials studying the administration of hepatitis B vaccine to CRF [chronic renal failure] patients, with or without dialysis. No language, publication date, or publication status restrictions were imposed…\n\n\nTypes of participants: Participants of any age with CRF or receiving dialysis (haemodialysis or peritoneal dialysis) were considered. CRF was defined as serum creatinine greater than 200 µmol/L for a period of more than six months or individuals receiving dialysis (haemodialysis or peritoneal dialysis)…Renal transplant patients were excluded from this review as these individuals are immunosuppressed and are receiving immunosuppressant agents to prevent rejection of their transplanted organs, and they have essentially normal renal function…\n\n\nTypes of intervention: Trials comparing the beneficial and harmful effects of hepatitis B vaccines with adjuvant or cytokine co-interventions [and] trials comparing the beneficial and harmful effects of immunoglobulin prophylaxis. This review was limited to studies looking at active immunization. Hepatitis B vaccines (plasma or recombinant (yeast) derived) of all types, dose, and regimens versus placebo, control vaccine, or no vaccine…\n\n\nTypes of outcome measures: Primary outcome measures: Seroconversion, ie, proportion of patients with adequate anti-HBs response (>10 IU/L or Sample Ratio Units). Hepatitis B infections (as measured by hepatitis B core antigen (HBcAg) positivity or persistent HBsAg positivity), both acute and chronic. Acute (primary) HBV [hepatitis B virus] infections were defined as seroconversion to HBsAg positivity or development of IgM anti-HBc. Chronic HBV infections were defined as the persistence of HBsAg for more than six months or HBsAg positivity and liver biopsy compatible with a diagnosis or chronic hepatitis B. Secondary outcome measures: Adverse events of hepatitis B vaccinations…[and]…mortality. [\n\nBack to top"
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-7_information_sources-1",
    "href": "guidelines/prisma/index.html#sec-7_information_sources-1",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Read More",
    "text": "Read More\nThe National Library of Medicine’s MEDLINE database is one of the most comprehensive sources of health care information in the world. Like any database, however, its coverage is not complete and varies according to the field. Retrieval from any single database, even by an experienced searcher, may be imperfect, which is why detailed reporting is important within the systematic review.\nAt a minimum, for each database searched, authors should report the database, platform, or provider (e.g., Ovid, Dialog, PubMed) and the start and end dates for the search of each database. This information lets readers assess the currency of the review, which is important because the publication time-lag outdates the results of some reviews [64]. This information should also make updating more efficient [65]. Authors should also report who developed and conducted the search [66].\nIn addition to searching databases, authors should report the use of supplementary approaches to identify studies, such as hand searching of journals, checking reference lists, searching trials registries or regulatory agency Web sites [67], contacting manufacturers, or contacting authors. Authors should also report if they attempted to acquire any missing information (e.g., on study methods or results) from investigators or sponsors; it is useful to describe briefly who was contacted and what unpublished information was obtained."
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-7_information_sources-2",
    "href": "guidelines/prisma/index.html#sec-7_information_sources-2",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Examples",
    "text": "Examples\n\nStudies were identified by searching electronic databases, scanning reference lists of articles and consultation with experts in the field and drug companies…No limits were applied for language and foreign papers were translated. This search was applied to Medline (1966–Present), CancerLit (1975–Present), and adapted for Embase (1980–Present), Science Citation Index Expanded (1981–Present) and Pre-Medline electronic databases. Cochrane and DARE (Database of Abstracts of Reviews of Effectiveness) databases were reviewed…The last search was run on 19 June 2001. In addition, we handsearched contents pages of Journal of Clinical Oncology 2001, European Journal of Cancer 2001 and Bone 2001, together with abstracts printed in these journals 1999–2001. A limited update literature search was performed from 19 June 2001 to 31 December 2003.\n\nBack to top"
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-8_search-1",
    "href": "guidelines/prisma/index.html#sec-8_search-1",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Read More",
    "text": "Read More\nThe search strategy is an essential part of the report of any systematic review. Searches may be complicated and iterative, particularly when reviewers search unfamiliar databases or their review is addressing a broad or new topic. Perusing the search strategy allows interested readers to assess the comprehensiveness and completeness of the search, and to replicate it. Thus, we advise authors to report their full electronic search strategy for at least one major database. As an alternative to presenting search strategies for all databases, authors could indicate how the search took into account other databases searched, as index terms vary across databases. If different searches are used for different parts of a wider question (e.g., questions relating to benefits and questions relating to harms), we recommend authors provide at least one example of a strategy for each part of the objective [69]. We also encourage authors to state whether search strategies were peer reviewed as part of the systematic review process [70].\nWe realize that journal restrictions vary and that having the search strategy in the text of the report is not always feasible. We strongly encourage all journals, however, to find ways, such as a Web extra, appendix, or electronic link to an archive, to make search strategies accessible to readers. We also advise all authors to archive their searches so that (1) others may access and review them (e.g., replicate them or understand why their review of a similar topic did not identify the same reports), and (2) future updates of their review are facilitated.\nSeveral sources provide guidance on developing search strategies [71],[72],[73]. Most searches have constraints, for example relating to limited time or financial resources, inaccessible or inadequately indexed reports and databases, unavailability of experts with particular language or database searching skills, or review questions for which pertinent evidence is not easy to find. Authors should be straightforward in describing their search constraints. Apart from the keywords used to identify or exclude records, they should report any additional limitations relevant to the search, such as language and date restrictions (see also eligibility criteria, Item 6) [51]."
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-8_search-2",
    "href": "guidelines/prisma/index.html#sec-8_search-2",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Examples",
    "text": "Examples\n\nIn text: “We used the following search terms to search all trials registers and databases: immunoglobulin*; IVIG; sepsis; septic shock; septicaemia; and septicemia…”\n\n\nIn appendix:\n\nSearch strategy: MEDLINE (OVID)\n\nimmunoglobulins/\nimmunoglobulin$.tw.\nivig.tw.\n1 or 2 or 3\nsepsis/\nsepsis.tw.\nseptic shock/\nseptic shock.tw.\nsepticemia/\nsepticaemia.tw.\nsepticemia.tw.\n5 or 6 or 7 or 8 or 9 or 10 or 11\n4 and 12\nrandomized controlled trials/\nrandomized-controlled-trial.pt.\ncontrolled-clinical-trial.pt.\nrandom allocation/\ndouble-blind method/\nsingle-blind method/\n14 or 15 or 16 or 17 or 18 or 19\nexp clinical trials/\nclinical-trial.pt.\n(clin$ adj trial$).ti,ab.\n((singl$ or doubl$ or trebl$ or tripl\\() adj (blind\\))).ti,ab.\nplacebos/\nplacebo$.ti,ab.\nrandom$.ti,ab.\n21 or 22 or 23 or 24 or 25 or 26 or 27\nresearch design/\ncomparative study/\nexp evaluation studies/\nfollow-up studies/\nprospective studies/\n(control$ or prospective$ or volunteer$).ti,ab.\n30 or 31 or 32 or 33 or 34\n20 or 28 or 29 or 35\n13 and 36\n\nBack to top"
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-9_study_selection-1",
    "href": "guidelines/prisma/index.html#sec-9_study_selection-1",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Read More",
    "text": "Read More\nThere is no standard process for selecting studies to include in a systematic review. Authors usually start with a large number of identified records from their search and sequentially exclude records according to eligibility criteria. We advise authors to report how they screened the retrieved records (typically a title and abstract), how often it was necessary to review the full text publication, and if any types of record (e.g., letters to the editor) were excluded. We also advise using the PRISMA flow diagram to summarize study selection processes (see Item 17; Box 3).\nEfforts to enhance objectivity and avoid mistakes in study selection are important. Thus authors should report whether each stage was carried out by one or several people, who these people were, and, whenever multiple independent investigators performed the selection, what the process was for resolving disagreements. The use of at least two investigators may reduce the possibility of rejecting relevant reports [75]. The benefit may be greatest for topics where selection or rejection of an article requires difficult judgments [76]. For these topics, authors should ideally tell readers the level of inter-rater agreement, how commonly arbitration about selection was required, and what efforts were made to resolve disagreements (e.g., by contact with the authors of the original studies)."
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-9_study_selection-2",
    "href": "guidelines/prisma/index.html#sec-9_study_selection-2",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Examples",
    "text": "Examples\n\nEligibility assessment…[was] performed independently in an unblinded standardized manner by 2 reviewers…Disagreements between reviewers were resolved by consensus\n\nBack to top"
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-10_data_collection_process-1",
    "href": "guidelines/prisma/index.html#sec-10_data_collection_process-1",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Read More",
    "text": "Read More\nReviewers extract information from each included study so that they can critique, present, and summarize evidence in a systematic review. They might also contact authors of included studies for information that has not been, or is unclearly, reported. In meta-analysis of individual patient data, this phase involves collection and scrutiny of detailed raw databases. The authors should describe these methods, including any steps taken to reduce bias and mistakes during data collection and data extraction [78] (Box 3).\nSome systematic reviewers use a data extraction form that could be reported as an appendix or Web extra to their report. These forms could show the reader what information reviewers sought (see Item 11) and how they extracted it. Authors could tell readers if the form was piloted. Regardless, we advise authors to tell readers who extracted what data, whether any extractions were completed in duplicate, and, if so, whether duplicate abstraction was done independently and how disagreements were resolved.\nPublished reports of the included studies may not provide all the information required for the review. Reviewers should describe any actions they took to seek additional information from the original researchers (see Item 7). The description might include how they attempted to contact researchers, what they asked for, and their success in obtaining the necessary information. Authors should also tell readers when individual patient data were sought from the original researchers [41] (see Item 11) and indicate the studies for which such data were used in the analyses. The reviewers ideally should also state whether they confirmed the accuracy of the information included in their review with the original researchers, for example, by sending them a copy of the draft review [79].\nSome studies are published more than once. Duplicate publications may be difficult to ascertain, and their inclusion may introduce bias [80],[81]. We advise authors to describe any steps they used to avoid double counting and piece together data from multiple reports of the same study (e.g., juxtaposing author names, treatment comparisons, sample sizes, or outcomes). We also advise authors to indicate whether all reports on a study were considered, as inconsistencies may reveal important limitations. For example, a review of multiple publications of drug trials showed that reported study characteristics may differ from report to report, including the description of the design, number of patients analyzed, chosen significance level, and outcomes [82]. Authors ideally should present any algorithm that they used to select data from overlapping reports and any efforts they used to solve logical inconsistencies across reports"
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-10_data_collection_process-2",
    "href": "guidelines/prisma/index.html#sec-10_data_collection_process-2",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Examples",
    "text": "Examples\n\nWe developed a data extraction sheet (based on the Cochrane Consumers and Communication Review Group’s data extraction template), pilot-tested it on ten randomly-selected included studies, and refined it accordingly. One review author extracted the following data from included studies and the second author checked the extracted data…Disagreements were resolved by discussion between the two review authors; if no agreement could be reached, it was planned a third author would decide. We contacted five authors for further information. All responded and one provided numerical data that had only been presented graphically in the published paper.\n\nBack to top"
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-11_data_items-1",
    "href": "guidelines/prisma/index.html#sec-11_data_items-1",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Read More",
    "text": "Read More\nIt is important for readers to know what information review authors sought, even if some of this information was not available [84]. If the review is limited to reporting only those variables that were obtained, rather than those that were deemed important but could not be obtained, bias might be introduced and the reader might be misled. It is therefore helpful if authors can refer readers to the protocol (see Item 5), and archive their extraction forms (see Item 10), including definitions of variables. The published systematic review should include a description of the processes used with, if relevant, specification of how readers can get access to additional materials.\nWe encourage authors to report whether some variables were added after the review started. Such variables might include those found in the studies that the reviewers identified (e.g., important outcome measures that the reviewers initially overlooked). Authors should describe the reasons for adding any variables to those already pre-specified in the protocol so that readers can understand the review process.\nWe advise authors to report any assumptions they made about missing or unclear information and to explain those processes. For example, in studies of women aged 50 or older it is reasonable to assume that none were pregnant, even if this is not reported. Likewise, review authors might make assumptions about the route of administration of drugs assessed. However, special care should be taken in making assumptions about qualitative information. For example, the upper age limit for children can vary from 15 years to 21 years, intense physiotherapy might mean very different things to different researchers at different times and for different patients, and the volume of blood associated with heavy blood loss might vary widely depending on the setting."
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-11_data_items-2",
    "href": "guidelines/prisma/index.html#sec-11_data_items-2",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Examples",
    "text": "Examples\n\nInformation was extracted from each included trial on: (1) characteristics of trial participants (including age, stage and severity of disease, and method of diagnosis), and the trial’s inclusion and exclusion criteria; (2) type of intervention (including type, dose, duration and frequency of the NSAID [non-steroidal anti-inflammatory drug]; versus placebo or versus the type, dose, duration and frequency of another NSAID; or versus another pain management drug; or versus no treatment); (3) type of outcome measure (including the level of pain reduction, improvement in quality of life score (using a validated scale), effect on daily activities, absence from work or school, length of follow up, unintended effects of treatment, number of women requiring more invasive treatment)\n\nBack to top"
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-12_risk_of_bias_in_individual_studies-1",
    "href": "guidelines/prisma/index.html#sec-12_risk_of_bias_in_individual_studies-1",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Read More",
    "text": "Read More\nThe likelihood that the treatment effect reported in a systematic review approximates the truth depends on the validity of the included studies, as certain methodological characteristics may be associated with effect sizes [87],[88]. For example, trials without reported adequate allocation concealment exaggerate treatment effects on average compared to those with adequate concealment [88]. Therefore, it is important for authors to describe any methods that they used to gauge the risk of bias in the included studies and how that information was used [89]. Additionally, authors should provide a rationale if no assessment of risk of bias was undertaken. The most popular term to describe the issues relevant to this item is quality, but for the reasons that are elaborated in Box 4 we prefer to name this item as assessment of risk of bias.\nMany methods exist to assess the overall risk of bias in included studies, including scales, checklists, and individual components [90],[91]. As discussed in Box 4, scales that numerically summarize multiple components into a single number are misleading and unhelpful [92],[93]. Rather, authors should specify the methodological components that they assessed. Common markers of validity for randomized trials include the following: appropriate generation of random allocation sequence [94]; concealment of the allocation sequence [93]; blinding of participants, health care providers, data collectors, and outcome adjudicators [95],[96],[97],[98]; proportion of patients lost to follow-up [99],[100]; stopping of trials early for benefit [101]; and whether the analysis followed the intention-to-treat principle [100],[102]. The ultimate decision regarding which methodological features to evaluate requires consideration of the strength of the empiric data, theoretical rationale, and the unique circumstances of the included studies.\nAuthors should report how they assessed risk of bias; whether it was in a blind manner; and if assessments were completed by more than one person, and if so, whether they were completed independently [103],[104]. Similarly, we encourage authors to report any calibration exercises among review team members that were done. Finally, authors need to report how their assessments of risk of bias are used subsequently in the data synthesis (see Item 16). Despite the often difficult task of assessing the risk of bias in included studies, authors are sometimes silent on what they did with the resultant assessments [89]. If authors exclude studies from the review or any subsequent analyses on the basis of the risk of bias, they should tell readers which studies they excluded and explain the reasons for those exclusions (see Item 6). Authors should also describe any planned sensitivity or subgroup analyses related to bias assessments (see Item 16)."
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-12_risk_of_bias_in_individual_studies-2",
    "href": "guidelines/prisma/index.html#sec-12_risk_of_bias_in_individual_studies-2",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Examples",
    "text": "Examples\n\nTo ascertain the validity of eligible randomized trials, pairs of reviewers working independently and with adequate reliability determined the adequacy of randomization and concealment of allocation, blinding of patients, health care providers, data collectors, and outcome assessors; and extent of loss to follow-up (i.e. proportion of patients in whom the investigators were not able to ascertain outcomes).\n\n\nTo explore variability in study results (heterogeneity) we specified the following hypotheses before conducting the analysis. We hypothesised that effect size may differ according to the methodological quality of the studies.\n\nBack to top"
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-13_summary_measures-1",
    "href": "guidelines/prisma/index.html#sec-13_summary_measures-1",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Read More",
    "text": "Read More\nWhen planning a systematic review, it is generally desirable that authors pre-specify the outcomes of primary interest (see Item 5) as well as the intended summary effect measure for each outcome. The chosen summary effect measure may differ from that used in some of the included studies. If possible the choice of effect measures should be explained, though it is not always easy to judge in advance which measure is the most appropriate.\nFor binary outcomes, the most common summary measures are the risk ratio, odds ratio, and risk difference [108]. Relative effects are more consistent across studies than absolute effects [109],[110], although absolute differences are important when interpreting findings (see Item 24).\nFor continuous outcomes, the natural effect measure is the difference in means [108]. Its use is appropriate when outcome measurements in all studies are made on the same scale. The standardized difference in means is used when the studies do not yield directly comparable data. Usually this occurs when all studies assess the same outcome but measure it in a variety of ways (e.g., different scales to measure depression).\nFor time-to-event outcomes, the hazard ratio is the most common summary measure. Reviewers need the log hazard ratio and its standard error for a study to be included in a meta-analysis [111]. This information may not be given for all studies, but methods are available for estimating the desired quantities from other reported information [111]. Risk ratio and odds ratio (in relation to events occurring by a fixed time) are not equivalent to the hazard ratio, and median survival times are not a reliable basis for meta-analysis [112]. If authors have used these measures they should describe their methods in the report."
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-13_summary_measures-2",
    "href": "guidelines/prisma/index.html#sec-13_summary_measures-2",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Examples",
    "text": "Examples\n\nRelative risk of mortality reduction was the primary measure of treatment effect.\n\n\nThe meta-analyses were performed by computing relative risks (RRs) using random-effects model. Quantitative analyses were performed on an intention-to-treat basis and were confined to data derived from the period of follow-up. RR and 95% confidence intervals for each side effect (and all side effects) were calculated.\n\n\nThe primary outcome measure was the mean difference in log10 HIV-1 viral load comparing zinc supplementation to placebo…\n\nBack to top"
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-14_planned_methods_of_analyis-1",
    "href": "guidelines/prisma/index.html#sec-14_planned_methods_of_analyis-1",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Read More",
    "text": "Read More\nThe data extracted from the studies in the review may need some transformation (processing) before they are suitable for analysis or for presentation in an evidence table. Although such data handling may facilitate meta-analyses, it is sometimes needed even when meta-analyses are not done. For example, in trials with more than two intervention groups it may be necessary to combine results for two or more groups (e.g., receiving similar but non-identical interventions), or it may be desirable to include only a subset of the data to match the review’s inclusion criteria. When several different scales (e.g., for depression) are used across studies, the sign of some scores may need to be reversed to ensure that all scales are aligned (e.g., so low values represent good health on all scales). Standard deviations may have to be reconstructed from other statistics such as p-values and t statistics [115],[116], or occasionally they may be imputed from the standard deviations observed in other studies [117]. Time-to-event data also usually need careful conversions to a consistent format [111]. Authors should report details of any such data processing.\nStatistical combination of data from two or more separate studies in a meta-analysis may be neither necessary nor desirable (see Box 5 and Item 21). Regardless of the decision to combine individual study results, authors should report how they planned to evaluate between-study variability (heterogeneity or inconsistency) (Box 6). The consistency of results across trials may influence the decision of whether to combine trial results in a meta-analysis.\nWhen meta-analysis is done, authors should specify the effect measure (e.g., relative risk or mean difference) (see Item 13), the statistical method (e.g., inverse variance), and whether a fixed- or random-effects approach, or some other method (e.g., Bayesian) was used (see Box 6). If possible, authors should explain the reasons for those choices."
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-14_planned_methods_of_analyis-2",
    "href": "guidelines/prisma/index.html#sec-14_planned_methods_of_analyis-2",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Examples",
    "text": "Examples\n\nWe tested for heterogeneity with the Breslow-Day test, and used the method proposed by Higgins et al. to measure inconsistency (the percentage of total variation across studies due to heterogeneity) of effects across lipid-lowering interventions. The advantages of this measure of inconsistency (termed I2) are that it does not inherently depend on the number of studies and is accompanied by an uncertainty interval.\n\n\nIn very few instances, estimates of baseline mean or mean QOL [Quality of life] responses were obtained without corresponding estimates of variance (standard deviation [SD] or standard error). In these instances, an SD was imputed from the mean of the known SDs. In a number of cases, the response data available were the mean and variance in a pre study condition and after therapy. The within-patient variance in these cases could not be calculated directly and was approximated by assuming independence.\n\nBack to top"
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-15_risk_of_bias_across_studies-1",
    "href": "guidelines/prisma/index.html#sec-15_risk_of_bias_across_studies-1",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Read More",
    "text": "Read More\nReviewers should explore the possibility that the available data are biased. They may examine results from the available studies for clues that suggest there may be missing studies (publication bias) or missing data from the included studies (selective reporting bias) (see Box 7). Authors should report in detail any methods used to investigate possible bias across studies.\nIt is difficult to assess whether within-study selective reporting is present in a systematic review. If a protocol of an individual study is available, the outcomes in the protocol and the published report can be compared. Even in the absence of a protocol, outcomes listed in the methods section of the published report can be compared with those for which results are presented [120]. In only half of 196 trial reports describing comparisons of two drugs in arthritis were all the effect variables in the methods and results sections the same [82]. In other cases, knowledge of the clinical area may suggest that it is likely that the outcome was measured even if it was not reported. For example, in a particular disease, if one of two linked outcomes is reported but the other is not, then one should question whether the latter has been selectively omitted [121],[122].\nOnly 36% (76 of 212) of therapeutic systematic reviews published in November 2004 reported that study publication bias was considered, and only a quarter of those intended to carry out a formal assessment for that bias [3]. Of 60 meta-analyses in 24 articles published in 2005 in which formal assessments were reported, most were based on fewer than ten studies; most displayed statistically significant heterogeneity; and many reviewers misinterpreted the results of the tests employed [123]. A review of trials of antidepressants found that meta-analysis of only the published trials gave effect estimates 32% larger on average than when all trials sent to the drug agency were analyzed [67]."
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-15_risk_of_bias_across_studies-2",
    "href": "guidelines/prisma/index.html#sec-15_risk_of_bias_across_studies-2",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Examples",
    "text": "Examples\n\nFor each trial we plotted the effect by the inverse of its standard error. The symmetry of such funnel plots was assessed both visually, and formally with Egger’s test, to see if the effect decreased with increasing sample size.\n\n\nWe assessed the possibility of publication bias by evaluating a funnel plot of the trial mean differences for asymmetry, which can result from the non publication of small trials with negative results…Because graphical evaluation can be subjective, we also conducted an adjusted rank correlation test and a regression asymmetry test as formal statistical tests for publication bias…We acknowledge that other factors, such as differences in trial quality or true study heterogeneity, could produce asymmetry in funnel plots.\n\nBack to top"
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-16_additional_analyses-1",
    "href": "guidelines/prisma/index.html#sec-16_additional_analyses-1",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Read More",
    "text": "Read More\nAuthors may perform additional analyses to help understand whether the results of their review are robust, all of which should be reported. Such analyses include sensitivity analysis, subgroup analysis, and meta-regression [125].\nSensitivity analyses are used to explore the degree to which the main findings of a systematic review are affected by changes in its methods or in the data used from individual studies (e.g., study inclusion criteria, results of risk of bias assessment). Subgroup analyses address whether the summary effects vary in relation to specific (usually clinical) characteristics of the included studies or their participants. Meta-regression extends the idea of subgroup analysis to the examination of the quantitative influence of study characteristics on the effect size [126]. Meta-regression also allows authors to examine the contribution of different variables to the heterogeneity in study findings. Readers of systematic reviews should be aware that meta-regression has many limitations, including a danger of over-interpretation of findings [127],[128].\nEven with limited data, many additional analyses can be undertaken. The choice of which analysis to undertake will depend on the aims of the review. None of these analyses, however, are exempt from producing potentially misleading results. It is important to inform readers whether these analyses were performed, their rationale, and which were pre-specified."
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-16_additional_analyses-2",
    "href": "guidelines/prisma/index.html#sec-16_additional_analyses-2",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Examples",
    "text": "Examples\n\nSensitivity analyses were pre-specified. The treatment effects were examined according to quality components (concealed treatment allocation, blinding of patients and caregivers, blinded outcome assessment), time to initiation of statins, and the type of statin. One post-hoc sensitivity analysis was conducted including unpublished data from a trial using cerivastatin\n\nBack to top"
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-17_study_selection-1",
    "href": "guidelines/prisma/index.html#sec-17_study_selection-1",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Read More",
    "text": "Read More\nAuthors should report, ideally with a flow diagram, the total number of records identified from electronic bibliographic sources (including specialized database or registry searches), hand searches of various sources, reference lists, citation indices, and experts. It is useful if authors delineate for readers the number of selected articles that were identified from the different sources so that they can see, for example, whether most articles were identified through electronic bibliographic sources or from references or experts. Literature identified primarily from references or experts may be prone to citation or publication bias [131],[132].\nThe flow diagram and text should describe clearly the process of report selection throughout the review. Authors should report: unique records identified in searches; records excluded after preliminary screening (e.g., screening of titles and abstracts); reports retrieved for detailed evaluation; potentially eligible reports that were not retrievable; retrieved reports that did not meet inclusion criteria and the primary reasons for exclusion; and the studies included in the review. Indeed, the most appropriate layout may vary for different reviews.\nAuthors should also note the presence of duplicate or supplementary reports so that readers understand the number of individual studies compared to the number of reports that were included in the review. Authors should be consistent in their use of terms, such as whether they are reporting on counts of citations, records, publications, or studies. We believe that reporting the number of studies is the most important.\nA flow diagram can be very useful; it should depict all the studies included based upon fulfilling the eligibility criteria, whether or not data have been combined for statistical analysis. A recent review of 87 systematic reviews found that about half included a QUOROM flow diagram [133]. The authors of this research recommended some important ways that reviewers can improve the use of a flow diagram when describing the flow of information throughout the review process, including a separate flow diagram for each important outcome reported [133]."
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-17_study_selection-2",
    "href": "guidelines/prisma/index.html#sec-17_study_selection-2",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Examples",
    "text": "Examples\n\nIn text: A total of 10 studies involving 13 trials were identified for inclusion in the review. The search of Medline, PsycInfo and Cinahl databases provided a total of 584 citations. After adjusting for duplicates 509 remained. Of these, 479 studies were discarded because after reviewing the abstracts it appeared that these papers clearly did not meet the criteria. Three additional studies…were discarded because full text of the study was not available or the paper could not be feasibly translated into English. The full text of the remaining 27 citations was examined in more detail. It appeared that 22 studies did not meet the inclusion criteria as described. Five studies…met the inclusion criteria and were included in the systematic review. An additional five studies…that met the criteria for inclusion were identified by checking the references of located, relevant papers and searching for studies that have cited these papers. No unpublished relevant studies were obtained.\n\n\nExample flow diagram of study selection.\n\nDDW, Digestive Disease Week; UEGW, United European Gastroenterology Week. Reproduced with permission from https://doi.org/10.1371/journal.pmed.1000100.g002\nBack to top"
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-18_study_characteristics-1",
    "href": "guidelines/prisma/index.html#sec-18_study_characteristics-1",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Read More",
    "text": "Read More\nFor readers to gauge the validity and applicability of a systematic review’s results, they need to know something about the included studies. Such information includes PICOS (Box 2) and specific information relevant to the review question. For example, if the review is examining the long-term effects of antidepressants for moderate depressive disorder, authors should report the follow-up periods of the included studies. For each included study, authors should provide a citation for the source of their information regardless of whether or not the study is published. This information makes it easier for interested readers to retrieve the relevant publications or documents.\nReporting study-level data also allows the comparison of the main characteristics of the studies included in the review. Authors should present enough detail to allow readers to make their own judgments about the relevance of included studies. Such information also makes it possible for readers to conduct their own subgroup analyses and interpret subgroups, based on study characteristics.\nAuthors should avoid, whenever possible, assuming information when it is missing from a study report (e.g., sample size, method of randomization). Reviewers may contact the original investigators to try to obtain missing information or confirm the data extracted for the systematic review. If this information is not obtained, this should be noted in the report. If information is imputed, the reader should be told how this was done and for which items. Presenting study-level data makes it possible to clearly identify unpublished information obtained from the original researchers and make it available for the public record.\nTypically, study-level characteristics are presented as a table as in the example in Table 2. Such presentation ensures that all pertinent items are addressed and that missing or unclear information is clearly indicated. Although paper-based journals do not generally allow for the quantity of information available in electronic journals or Cochrane reviews, this should not be accepted as an excuse for omission of important aspects of the methods or results of included studies, since these can, if necessary, be shown on a Web site.\nFollowing the presentation and description of each included study, as discussed above, reviewers usually provide a narrative summary of the studies. Such a summary provides readers with an overview of the included studies. It may for example address the languages of the published papers, years of publication, and geographic origins of the included studies.\nThe PICOS framework is often helpful in reporting the narrative summary indicating, for example, the clinical characteristics and disease severity of the participants and the main features of the intervention and of the comparison group. For non-pharmacological interventions, it may be helpful to specify for each study the key elements of the intervention received by each group. Full details of the interventions in included studies were reported in only three of 25 systematic reviews relevant to general practice [84]."
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-18_study_characteristics-2",
    "href": "guidelines/prisma/index.html#sec-18_study_characteristics-2",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Examples",
    "text": "Examples\n\nIn text:\n\nCharacteristics of included studies\nMethods: All four studies finally selected for the review were randomised controlled trials published in English. The duration of the intervention was 24 months for the RIO-North America and 12 months for the RIO-Diabetes, RIO-Lipids and RIO-Europe study. Although the last two described a period of 24 months during which they were conducted, only the first 12-months results are provided. All trials had a run-in, as a single blind period before the randomisation.\nParticipants: The included studies involved 6625 participants. The main inclusion criteria entailed adults (18 years or older), with a body mass index greater than 27 kg/m2 and less than 5 kg variation in body weight within the three months before study entry.\nIntervention: All trials were multicentric. The RIO-North America was conducted in the USA and Canada, RIO-Europe in Europe and the USA, RIO-Diabetes in the USA and 10 other different countries not specified, and RIO-Lipids in eight unspecified different countries.\nThe intervention received was placebo, 5 mg of rimonabant or 20 mg of rimonabant once daily in addition to a mild hypocaloric diet (600 kcal/day deficit).\nOutcomes:\nPrimary: In all studies the primary outcome assessed was weight change from baseline after one year of treatment and the RIO-North America study also evaluated the prevention of weight regain between the first and second year. All studies evaluated adverse effects, including those of any kind and serious events. Quality of life was measured in only one study, but the results were not described (RIO-Europe).\nSecondary and additional outcomes: These included prevalence of metabolic syndrome after one year and change in cardiometabolic risk factors such as blood pressure, lipid profile, etc.\nNo study included mortality and costs as outcome.\nThe timing of outcome measures was variable and could include monthly investigations, evaluations every three months or a single final evaluation after one year.\n\nIn table:\n\nExample Table: Summary of included studies evaluating the efficacy of antiemetic agents in acute gastroenteritis.\nhttps://doi.org/10.1371/journal.pmed.1000100.t002\nBack to top"
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-19_risk_of_bias_within_studies-1",
    "href": "guidelines/prisma/index.html#sec-19_risk_of_bias_within_studies-1",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Read More",
    "text": "Read More\nWe recommend that reviewers assess the risk of bias in the included studies using a standard approach with defined criteria (see Item 12). They should report the results of any such assessments [89].\nReporting only summary data (e.g., two of eight trials adequately concealed allocation) is inadequate because it fails to inform readers which studies had the particular methodological shortcoming. A more informative approach is to explicitly report the methodological features evaluated for each study. The Cochrane Collaboration’s new tool for assessing the risk of bias also requests that authors substantiate these assessments with any relevant text from the original studies [11]. It is often easiest to provide these data in a tabular format, as in the example. However, a narrative summary describing the tabular data can also be helpful for readers."
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-19_risk_of_bias_within_studies-2",
    "href": "guidelines/prisma/index.html#sec-19_risk_of_bias_within_studies-2",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Examples",
    "text": "Examples\n\nIn table:\n\nExample Table: Quality measures of the randomized controlled trials that failed to fulfill any one of six markers of validity.\nhttps://doi.org/10.1371/journal.pmed.1000100.t003\nBack to top"
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-20_results_of_individual_studies-1",
    "href": "guidelines/prisma/index.html#sec-20_results_of_individual_studies-1",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Read More",
    "text": "Read More\nPublication of summary data from individual studies allows the analyses to be reproduced and other analyses and graphical displays to be investigated. Others may wish to assess the impact of excluding particular studies or consider subgroup analyses not reported by the review authors. Displaying the results of each treatment group in included studies also enables inspection of individual study features. For example, if only odds ratios are provided, readers cannot assess the variation in event rates across the studies, making the odds ratio impossible to interpret [138]. Additionally, because data extraction errors in meta-analyses are common and can be large [139], the presentation of the results from individual studies makes it easier to identify errors. For continuous outcomes, readers may wish to examine the consistency of standard deviations across studies, for example, to be reassured that standard deviation and standard error have not been confused [138].\nFor each study, the summary data for each intervention group are generally given for binary outcomes as frequencies with and without the event (or as proportions such as 12/45). It is not sufficient to report event rates per intervention group as percentages. The required summary data for continuous outcomes are the mean, standard deviation, and sample size for each group. In reviews that examine time-to-event data, the authors should report the log hazard ratio and its standard error (or confidence interval) for each included study. Sometimes, essential data are missing from the reports of the included studies and cannot be calculated from other data but may need to be imputed by the reviewers. For example, the standard deviation may be imputed using the typical standard deviations in the other trials [116],[117] (see Item 14). Whenever relevant, authors should indicate which results were not reported directly and had to be estimated from other information (see Item 13). In addition, the inclusion of unpublished data should be noted.\nFor all included studies it is important to present the estimated effect with a confidence interval. This information may be incorporated in a table showing study characteristics or may be shown in a forest plot [140]. The key elements of the forest plot are the effect estimates and confidence intervals for each study shown graphically, but it is preferable also to include, for each study, the numerical group-specific summary data, the effect size and confidence interval, and the percentage weight (see second example [Figure 3]). For discussion of the results of meta-analysis, see Item 21.\nIn principle, all the above information should be provided for every outcome considered in the review, including both benefits and harms. When there are too many outcomes for full information to be included, results for the most important outcomes should be included in the main report with other information provided as a Web appendix. The choice of the information to present should be justified in light of what was originally stated in the protocol. Authors should explicitly mention if the planned main outcomes cannot be presented due to lack of information. There is some evidence that information on harms is only rarely reported in systematic reviews, even when it is available in the original studies [141]. Selective omission of harms results biases a systematic review and decreases its ability to contribute to informed decision making."
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-20_results_of_individual_studies-2",
    "href": "guidelines/prisma/index.html#sec-20_results_of_individual_studies-2",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Examples",
    "text": "Examples\n\nIn figure:\n\nExample Figure: Overall failure (defined as failure of assigned regimen or relapse) with tetracycline-rifampicin versus tetracycline-streptomycin.\nCI, confidence interval. Reproduced with permission from https://doi.org/10.1371/journal.pmed.1000100.g003\n\nIn table:\n\nExample Table: Heterotopic ossification in trials comparing radiotherapy to non-steroidal anti-inflammatory drugs after major hip procedures and fractures.\nhttps://doi.org/10.1371/journal.pmed.1000100.t004\nBack to top"
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-21_synthesis_of_results-1",
    "href": "guidelines/prisma/index.html#sec-21_synthesis_of_results-1",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Read More",
    "text": "Read More\nResults of systematic reviews should be presented in an orderly manner. Initial narrative descriptions of the evidence covered in the review (see Item 18) may tell readers important things about the study populations and the design and conduct of studies. These descriptions can facilitate the examination of patterns across studies. They may also provide important information about applicability of evidence, suggest the likely effects of any major biases, and allow consideration, in a systematic manner, of multiple explanations for possible differences of findings across studies.\nIf authors have conducted one or more meta-analyses, they should present the results as an estimated effect across studies with a confidence interval. It is often simplest to show each meta-analysis summary with the actual results of included studies in a forest plot (see Item 20) [140]. It should always be clear which of the included studies contributed to each meta-analysis. Authors should also provide, for each meta-analysis, a measure of the consistency of the results from the included studies such as I2 (heterogeneity; see Box 6); a confidence interval may also be given for this measure [145]. If no meta-analysis was performed, the qualitative inferences should be presented as systematically as possible with an explanation of why meta-analysis was not done, as in the second example above [143]. Readers may find a forest plot, without a summary estimate, helpful in such cases.\nAuthors should in general report syntheses for all the outcome measures they set out to investigate (i.e., those described in the protocol; see Item 4) to allow readers to draw their own conclusions about the implications of the results. Readers should be made aware of any deviations from the planned analysis. Authors should tell readers if the planned meta-analysis was not thought appropriate or possible for some of the outcomes and the reasons for that decision.\nIt may not always be sensible to give meta-analysis results and forest plots for each outcome. If the review addresses a broad question, there may be a very large number of outcomes. Also, some outcomes may have been reported in only one or two studies, in which case forest plots are of little value and may be seriously biased.\nOf 300 systematic reviews indexed in MEDLINE in 2004, a little more than half (54%) included meta-analyses, of which the majority (91%) reported assessing for inconsistency in results."
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-21_synthesis_of_results-2",
    "href": "guidelines/prisma/index.html#sec-21_synthesis_of_results-2",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Examples",
    "text": "Examples\n\nMortality data were available for all six trials, randomizing 311 patients and reporting data for 305 patients. There were no deaths reported in the three respiratory syncytial virus/severe bronchiolitis trials; thus our estimate is based on three trials randomizing 232 patients, 64 of whom died. In the pooled analysis, surfactant was associated with significantly lower mortality (relative risk = 0.7, 95% confidence interval = 0.4–0.97, P = 0.04). There was no evidence of heterogeneity (I2 = 0%).\n\n\nBecause the study designs, participants, interventions, and reported outcome measures varied markedly, we focused on describing the studies, their results, their applicability, and their limitations and on qualitative synthesis rather than meta-analysis.\n\n\nWe detected significant heterogeneity within this comparison (I2 = 46.6%; χ2 = 13.11, df = 7; P = 0.07). Retrospective exploration of the heterogeneity identified one trial that seemed to differ from the others. It included only small ulcers (wound area less than 5 cm2). Exclusion of this trial removed the statistical heterogeneity and did not affect the finding of no evidence of a difference in healing rate between hydrocolloids and simple low adherent dressings (relative risk = 0.98, [95% confidence interval] 0.85 to 1.12; I2 = 0%).\n\nBack to top"
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-22_risk_of_bias_across_studies-1",
    "href": "guidelines/prisma/index.html#sec-22_risk_of_bias_across_studies-1",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Read More",
    "text": "Read More\nAuthors should present the results of any assessments of risk of bias across studies. If a funnel plot is reported, authors should specify the effect estimate and measure of precision used, presented typically on the x-axis and y-axis, respectively. Authors should describe if and how they have tested the statistical significance of any possible asymmetry (see Item 15). Results of any investigations of selective reporting of outcomes within studies (as discussed in Item 15) should also be reported. Also, we advise authors to tell readers if any pre-specified analyses for assessing risk of bias across studies were not completed and the reasons (e.g., too few included studies)."
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-22_risk_of_bias_across_studies-2",
    "href": "guidelines/prisma/index.html#sec-22_risk_of_bias_across_studies-2",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Examples",
    "text": "Examples\n\nStrong evidence of heterogeneity (I2 = 79%, P<0.001) was observed. To explore this heterogeneity, a funnel plot was drawn. The funnel plot in Figure 4 shows evidence of considerable asymmetry.\n\nExample Figure 4: Example of a funnel plot showing evidence of considerable asymmetry. SE, standard error. Adapted with permission from. https://doi.org/10.1371/journal.pmed.1000100.g004\n\nSpecifically, four sertraline trials involving 486 participants and one citalopram trial involving 274 participants were reported as having failed to achieve a statistically significant drug effect, without reporting mean HRSD [Hamilton Rating Scale for Depression] scores. We were unable to find data from these trials on pharmaceutical company Web sites or through our search of the published literature. These omissions represent 38% of patients in sertraline trials and 23% of patients in citalopram trials. Analyses with and without inclusion of these trials found no differences in the patterns of results; similarly, the revealed patterns do not interact with drug type. The purpose of using the data obtained from the FDA was to avoid publication bias, by including unpublished as well as published trials. Inclusion of only those sertraline and citalopram trials for which means were reported to the FDA would constitute a form of reporting bias similar to publication bias and would lead to overestimation of drug–placebo differences for these drug types. Therefore, we present analyses only on data for medications for which complete clinical trials’ change was reported.\n\nBack to top"
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-23_additional_analysis-1",
    "href": "guidelines/prisma/index.html#sec-23_additional_analysis-1",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Read More",
    "text": "Read More\nAuthors should report any subgroup or sensitivity analyses and whether or not they were pre-specified (see Items 5 and 16). For analyses comparing subgroups of studies (e.g., separating studies of low- and high-dose aspirin), the authors should report any tests for interactions, as well as estimates and confidence intervals from meta-analyses within each subgroup. Similarly, meta-regression results (see Item 16) should not be limited to p-values, but should include effect sizes and confidence intervals [150], as the first example reported above does in a table. The amount of data included in each additional analysis should be specified if different from that considered in the main analyses. This information is especially relevant for sensitivity analyses that exclude some studies; for example, those with high risk of bias.\nImportantly, all additional analyses conducted should be reported, not just those that were statistically significant. This information will help avoid selective outcome reporting bias within the review as has been demonstrated in reports of randomized controlled trials [42],[44],[121],[151],[152]. Results from exploratory subgroup or sensitivity analyses should be interpreted cautiously, bearing in mind the potential for multiple analyses to mislead."
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-23_additional_analysis-2",
    "href": "guidelines/prisma/index.html#sec-23_additional_analysis-2",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Examples",
    "text": "Examples\n\n…benefits of chondroitin were smaller in trials with adequate concealment of allocation compared with trials with unclear concealment (P for interaction = 0.050), in trials with an intention-to-treat analysis compared with those that had excluded patients from the analysis (P for interaction = 0.017), and in large compared with small trials (P for interaction = 0.022).\n\n\nSubgroup analyses according to antibody status, antiviral medications, organ transplanted, treatment duration, use of antilymphocyte therapy, time to outcome assessment, study quality and other aspects of study design did not demonstrate any differences in treatment effects. Multivariate meta-regression showed no significant difference in CMV [cytomegalovirus] disease after allowing for potential confounding or effect-modification by prophylactic drug used, organ transplanted or recipient serostatus in CMV positive recipients and CMV negative recipients of CMV positive donors.\n\nBack to top"
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-24_summary_of_evidence-1",
    "href": "guidelines/prisma/index.html#sec-24_summary_of_evidence-1",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Read More",
    "text": "Read More\nAuthors should give a brief and balanced summary of the nature and findings of the review. Sometimes, outcomes for which little or no data were found should be noted due to potential relevance for policy decisions and future research. Applicability of the review’s findings, to different patients, settings, or target audiences, for example, should be mentioned. Although there is no standard way to assess applicability simultaneously to different audiences, some systems do exist [153]. Sometimes, authors formally rate or assess the overall body of evidence addressed in the review and can present the strength of their summary recommendations tied to their assessments of the quality of evidence (e.g., the GRADE system) [10].\nAuthors need to keep in mind that statistical significance of the effects does not always suggest clinical or policy relevance. Likewise, a non-significant result does not demonstrate that a treatment is ineffective. Authors should ideally clarify trade-offs and how the values attached to the main outcomes would lead different people to make different decisions. In addition, adroit authors consider factors that are important in translating the evidence to different settings and that may modify the estimates of effects reported in the review [153]. Patients and health care providers may be primarily interested in which intervention is most likely to provide a benefit with acceptable harms, while policy makers and administrators may value data on organizational impact and resource utilization."
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-24_summary_of_evidence-2",
    "href": "guidelines/prisma/index.html#sec-24_summary_of_evidence-2",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Examples",
    "text": "Examples\n\nOverall, the evidence is not sufficiently robust to determine the comparative effectiveness of angioplasty (with or without stenting) and medical treatment alone. Only 2 randomized trials with long-term outcomes and a third randomized trial that allowed substantial crossover of treatment after 3 months directly compared angioplasty and medical treatment…the randomized trials did not evaluate enough patients or did not follow patients for a sufficient duration to allow definitive conclusions to be made about clinical outcomes, such as mortality and cardiovascular or kidney failure events.\n\n\nSome acceptable evidence from comparison of medical treatment and angioplasty suggested no difference in long-term kidney function but possibly better blood pressure control after angioplasty, an effect that may be limited to patients with bilateral atherosclerotic renal artery stenosis. The evidence regarding other outcomes is weak. Because the reviewed studies did not explicitly address patients with rapid clinical deterioration who may need acute intervention, our conclusions do not apply to this important subset of patients.\n\nBack to top"
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-25_limitations-1",
    "href": "guidelines/prisma/index.html#sec-25_limitations-1",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Read More",
    "text": "Read More\nA discussion of limitations should address the validity (i.e., risk of bias) and reporting (informativeness) of the included studies, limitations of the review process, and generalizability (applicability) of the review. Readers may find it helpful if authors discuss whether studies were threatened by serious risks of bias, whether the estimates of the effect of the intervention are too imprecise, or if there were missing data for many participants or important outcomes.\nLimitations of the review process might include limitations of the search (e.g., restricting to English-language publications), and any difficulties in the study selection, appraisal, and meta-analysis processes. For example, poor or incomplete reporting of study designs, patient populations, and interventions may hamper interpretation and synthesis of the included studies [84]. Applicability of the review may be affected if there are limited data for certain populations or subgroups where the intervention might perform differently or few studies assessing the most important outcomes of interest; or if there is a substantial amount of data relating to an outdated intervention or comparator or heavy reliance on imputation of missing values for summary estimates (Item 14)."
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-25_limitations-2",
    "href": "guidelines/prisma/index.html#sec-25_limitations-2",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Examples",
    "text": "Examples\n\nOutcome level:\n\nThe meta-analysis reported here combines data across studies in order to estimate treatment effects with more precision than is possible in a single study. The main limitation of this meta-analysis, as with any overview, is that the patient population, the antibiotic regimen and the outcome definitions are not the same across studies.\n\nStudy and review level:\n\nOur study has several limitations. The quality of the studies varied. Randomization was adequate in all trials; however, 7 of the articles did not explicitly state that analysis of data adhered to the intention-to-treat principle, which could lead to overestimation of treatment effect in these trials, and we could not assess the quality of 4 of the 5 trials reported as abstracts. Analyses did not identify an association between components of quality and re-bleeding risk, and the effect size in favour of combination therapy remained statistically significant when we excluded trials that were reported as abstracts.\n\nPublication bias might account for some of the effect we observed. Smaller trials are, in general, analyzed with less methodological rigor than larger studies, and an asymmetrical funnel plot suggests that selective reporting may have led to an overestimation of effect sizes in small trials.\n\nBack to top"
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-26_conclusions-1",
    "href": "guidelines/prisma/index.html#sec-26_conclusions-1",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Read More",
    "text": "Read More\nSystematic reviewers sometimes draw conclusions that are too optimistic [157] or do not consider the harms equally as carefully as the benefits, although some evidence suggests these problems are decreasing [158]. If conclusions cannot be drawn because there are too few reliable studies, or too much uncertainty, this should be stated. Such a finding can be as important as finding consistent effects from several large studies.\nAuthors should try to relate the results of the review to other evidence, as this helps readers to better interpret the results. For example, there may be other systematic reviews about the same general topic that have used different methods or have addressed related but slightly different questions [159],[160]. Similarly, there may be additional information relevant to decision makers, such as the cost-effectiveness of the intervention (e.g., health technology assessment). Authors may discuss the results of their review in the context of existing evidence regarding other interventions.\nWe advise authors also to make explicit recommendations for future research. In a sample of 2,535 Cochrane reviews, 82% included recommendations for research with specific interventions, 30% suggested the appropriate type of participants, and 52% suggested outcome measures for future research [161]. There is no corresponding assessment about systematic reviews published in medical journals, but we believe that such recommendations are much less common in those reviews.\nClinical research should not be planned without a thorough knowledge of similar, existing research [162]. There is evidence that this still does not occur as it should and that authors of primary studies do not consider a systematic review when they design their studies [163]. We believe systematic reviews have great potential for guiding future clinical research."
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-26_conclusions-2",
    "href": "guidelines/prisma/index.html#sec-26_conclusions-2",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Examples",
    "text": "Examples\n\nImplications for practice:\n\nBetween 1995 and 1997 five different meta-analyses of the effect of antibiotic prophylaxis on infection and mortality were published. All confirmed a significant reduction in infections, though the magnitude of the effect varied from one review to another. The estimated impact on overall mortality was less evident and has generated considerable controversy on the cost effectiveness of the treatment. Only one among the five available reviews, however, suggested that a weak association between respiratory tract infections and mortality exists and lack of sufficient statistical power may have accounted for the limited effect on mortality.\n\nImplications for research:\n\nA logical next step for future trials would thus be the comparison of this protocol against a regimen of a systemic antibiotic agent only to see whether the topical component can be dropped. We have already identified six such trials but the total number of patients so far enrolled (n = 1056) is too small for us to be confident that the two treatments are really equally effective. If the hypothesis is therefore considered worth testing more and larger randomised controlled trials are warranted. Trials of this kind, however, would not resolve the relevant issue of treatment induced resistance. To produce a satisfactory answer to this, studies with a different design would be necessary. Though a detailed discussion goes beyond the scope of this paper, studies in which the intensive care unit rather than the individual patient is the unit of randomisation and in which the occurrence of antibiotic resistance is monitored over a long period of time should be undertaken.\nBack to top"
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-27_funding-1",
    "href": "guidelines/prisma/index.html#sec-27_funding-1",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Read More",
    "text": "Read More\nAuthors of systematic reviews, like those of any other research study, should disclose any funding they received to carry out the review, or state if the review was not funded. Lexchin and colleagues [166] observed that outcomes of reports of randomized trials and meta-analyses of clinical trials funded by the pharmaceutical industry are more likely to favor the sponsor’s product compared to studies with other sources of funding. Similar results have been reported elsewhere [167],[168]. Analogous data suggest that similar biases may affect the conclusions of systematic reviews [169].\nGiven the potential role of systematic reviews in decision making, we believe authors should be transparent about the funding and the role of funders, if any. Sometimes the funders will provide services, such as those of a librarian to complete the searches for relevant literature or access to commercial databases not available to the reviewers. Any level of funding or services provided to the systematic review team should be reported. Authors should also report whether the funder had any role in the conduct or report of the review. Beyond funding issues, authors should report any real or perceived conflicts of interest related to their role or the role of the funder in the reporting of the systematic review [170].\nIn a survey of 300 systematic reviews published in November 2004, funding sources were not reported in 41% of the reviews [3]. Only a minority of reviews (2%) reported being funded by for-profit sources, but the true proportion may be higher [171]."
  },
  {
    "objectID": "guidelines/prisma/index.html#sec-27_funding-2",
    "href": "guidelines/prisma/index.html#sec-27_funding-2",
    "title": "The PRISMA guideline for writing a systematic review and meta-analysis.",
    "section": "Examples",
    "text": "Examples\n\nAuthors of systematic reviews, like those of any other research study, should disclose any funding they received to carry out the review, or state if the review was not funded. Lexchin and colleagues [166] observed that outcomes of reports of randomized trials and meta-analyses of clinical trials funded by the pharmaceutical industry are more likely to favor the sponsor’s product compared to studies with other sources of funding. Similar results have been reported elsewhere [167],[168]. Analogous data suggest that similar biases may affect the conclusions of systematic reviews [169].\n\n\nGiven the potential role of systematic reviews in decision making, we believe authors should be transparent about the funding and the role of funders, if any. Sometimes the funders will provide services, such as those of a librarian to complete the searches for relevant literature or access to commercial databases not available to the reviewers. Any level of funding or services provided to the systematic review team should be reported. Authors should also report whether the funder had any role in the conduct or report of the review. Beyond funding issues, authors should report any real or perceived conflicts of interest related to their role or the role of the funder in the reporting of the systematic review [170].\n\n\nIn a survey of 300 systematic reviews published in November 2004, funding sources were not reported in 41% of the reviews [3]. Only a minority of reviews (2%) reported being funded by for-profit sources, but the true proportion may be higher [171].\n\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#about-this-guideline",
    "href": "guidelines/spirit/index.html#about-this-guideline",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "About this guideline",
    "text": "About this guideline\n\nThis guideline is relevant to protocols of clinical trials,"
  },
  {
    "objectID": "guidelines/spirit/index.html#download-resources",
    "href": "guidelines/spirit/index.html#download-resources",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Download Resources",
    "text": "Download Resources\nReporting guidelines are best used early:\n\nUse a template to write quickly and avoid the blank page\nUse a checklist to double-check and reassure others that your work is described transparently\n\n\n\n\nFor drafting\n\nTemplate\n\n\n\nFor checking\n\nChecklist\n\n\n\nFor planning\n\nTo-do list"
  },
  {
    "objectID": "guidelines/spirit/index.html#guidance",
    "href": "guidelines/spirit/index.html#guidance",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Guidance",
    "text": "Guidance\nApprox. 6 min read\n\nAdministrative information\n\n\n1. Title\n\n\n\n\n\n\n\nDescriptive title identifying the study design, population, interventions, and, if applicable, trial acronym\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nThe title provides an important means of trial identification. A succinct description that conveys the topic (study population, interventions), acronym (if any), and basic study design—including the method of intervention allocation (eg, parallel group randomised trial; single-group trial)—will facilitate retrieval from literature or internet searches and rapid judgment of relevance.20 It can also be helpful to include the trial framework (eg, superiority, non-inferiority), study objective or primary outcome, and if relevant, the study phase (eg, phase II).\n\n\n\n\n\n\n2a. Trial registration\n\n\n\n\n\n\n\nTrial identifier and registry name. If not yet registered, name of intended registry\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThere are compelling ethical and scientific reasons for trial registration.22 23 24 Documentation of a trial’s existence on a publicly accessible registry can help to increase transparency,24 25 decrease unnecessary duplication of research effort, facilitate identification of ongoing trials for prospective participants, and identify selective reporting of study results.26 27 28 As mandated by the International Committee of Medical Journal Editors (ICMJE) and jurisdictional legislation,29 30 31 registration of clinical trials should occur before recruitment of the first trial participant.\nWe recommend that registry names and trial identifiers assigned by the registries be prominently placed in the protocol, such as on the cover page. If the trial is not yet registered, the intended registry should be indicated and the protocol updated upon registration. When registration in multiple registries is required (eg, to meet local regulation), each identifier should be clearly listed in the protocol and each registry.\n\n\nExamples\n\nEudraCT: 2010-019180-10\n\nClinicalTrials.gov: NCT01066572\nISRCTN: 54540667\nBack to top\n\n\n\n\n\n\n2b. Trial registration: data set\n\n\n\n\n\n\n\nAll items from the World Health Organization Trial Registration Data Set\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nIn addition to a trial registration number, the World Health Organization (WHO) recommends a minimum standard list of items to be included in a trial registry in order for a trial to be considered fully registered (www.who.int/ictrp/network/trds/en/index.html). These standards are supported by ICMJE, other journal editors, and jurisdictional legislation.29 30 31 We recommend that the WHO Trial Registration Data Set be included in the protocol to serve as a brief structured summary of the trial. Its inclusion in the protocol can also signal updates for the registry when associated protocol sections are amended—thereby promoting consistency between information in the protocol and registry.\n\n\nExamples\n\nExample of trial registration data\n\nhttp://www.bmj.com/content/346/bmj.e7586## Table 2\nBack to top\n\n\n\n\n\n\n3. Protocol version\n\n\n\n\n\n\n\nDate and version identifier\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nSequentially labelling and dating each protocol version helps to mitigate potential confusion over which document is the most recent. Explicitly listing the changes made relative to the previous protocol version is also important (see Item 25). Transparent tracking of versions and amendments facilitates trial conduct, review, and oversight.\n\n\nExamples\n\nIssue date: 25 Jul 2005\n\nProtocol amendment number: 05\nAuthors: MD, JH\nRevision chronology:\nUM . . . 00, 2004-Jan-30 Original\nUM . . . 01, 2004-Feb-7 Amendment 01.:\nPrimary reason for amendment: changes in Section 7.1 regarding composition of comparator placebo\nAdditional changes (these changes in and of themselves would not justify a protocol amendment): correction of typographical error in Section 3.3 . . .\nUM . . . 05, 2005-Jul-25 Amendment No.5:\nAt the request of US FDA statements were added to the protocol to better clarify and define the algorithm for determining clinical or microbiological failures prior to the follow-up visit.\nBack to top\n\n\n\n\n\n\n4. Funding\n\n\n\n\n\n\n\nSources and types of financial, material, and other support\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nA description of the sources of financial and non-financial support provides relevant information to assess study feasibility and potential competing interests (Item 28). Although both industry funded and non-industry funded trials are susceptible to bias,4 35 the former are more likely to report trial results and conclusions that favour their own interventions.27 36 37 38 39 This tendency could be due to industry trials being more likely to select effective interventions for evaluation (Item 6a), to use less effective control interventions (Item 6b), or to selectively report outcomes (Item 12), analyses (Item 20) or full studies (Item 31).38 40 41 42 43 Non-financial support (eg, provision of drugs) from industry has not been shown to be associated with biased results, although few studies have examined this issue.44 45\nAt a minimum, the protocol should identify the sources of financial and non-financial support; the specific type (eg, funds, equipment, drugs, services) and time period of support; and any vested interest that the funder may have in the trial. If a trial is not yet funded when the protocol is first written, the proposed sources of support should be listed and updated as funders are confirmed.\nNo clear consensus exists regarding the level of additional funding details that should be provided in the trial protocol as opposed to trial contracts, although full disclosure of funding information in the protocol can help to better identify financial competing interests. Some jurisdictional guidelines require more detailed disclosure, including monetary amounts granted from each funder, the mechanism of providing financial support (eg, paid in fixed sum or per recruited participant), and the specific fund recipient (eg, trial investigator, department/institute).46 Detailed disclosure allows research ethics committees/institutional review boards (REC/IRBs) to assess whether the reimbursement amount is reasonable in relation to the time and expenses incurred for trial conduct.\n\n\nExamples\n\nTranexamic acid will be manufactured by Pharmacia (Pfizer, Sandwich, UK) and placebo by South Devon Healthcare NHS Trust, UK. The treatment packs will be prepared by an independent clinical trial supply company (Brecon Pharmaceuticals Limited, Hereford, UK) . . .\n\n\nLSHTM [London School of Hygiene and Tropical Medicine] is funding the run-in costs for the WOMAN trial and up to 2,000 patients’ recruitment. The main phase is funded by the UK Department of Health and the Wellcome Trust. Funding for this trial covers meetings and central organisational costs only. Pfizer, the manufacturer of tranexamic acid, have provided the funding for the trial drug and placebo used for this trial. An educational grant, equipment and consumables for ROTEM [thromboelastometry procedure] analysis has been provided by Tem Innovations GmbH, M.-Kollar-Str. 13-15, 81829 Munich, Germany for use in the WOMAN-ETAC study. An application for funding to support local organisational costs has been made to University of Ibadan Senate Research Grant. The design, management, analysis and reporting of the study are entirely independent of the manufacturers of tranexamic acid and Tem Innovations GmbH\n\nBack to top\n\n\n\n\n\n\n5a. Roles and responsibilities: contributorship\n\n\n\n\n\n\n\nNames, affiliations, and roles of protocol contributors\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nIndividuals who contribute substantively to protocol development and drafting should have their contributions reported. As with authorship of journal articles,48 listing the protocol contributors, their affiliations, and their roles in the protocol development process provides due recognition, accountability, and transparency. Naming of contributors can also help to identify competing interests and reduce ghost authorship (Items 28 and 31b).9 10 If professional medical writers are employed to draft the protocol, then this should be acknowledged as well.\nNaming of authors and statements of contributorship are standard for protocols published in journals such as Trials49 but are uncommon for unpublished protocols. Only five of 44 industry-initiated protocols approved in 1994-95 by a Danish research ethics committee explicitly identified the protocol authors.\n\n\nExamples\n\nRTL [address], EJM [address], AK [address] . . .\n\nAuthors’ contributions\nRTL conceived of the study. AK, EN, SB, PR, WJ, JH, and MC initiated the study design and JK and LG helped with implementation. RTL, JK, LG, and FP are grant holders. LT and EM provided statistical expertise in clinical trial design and RN is conducting the primary statistical analysis. All authors contributed to refinement of the study protocol and approved the final manuscript\nBack to top\n\n\n\n\n\n\n5b. Roles and responsibilities: sponsor contact information\n\n\n\n\n\n\n\nName and contact information for the trial sponsor\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe sponsor can be defined as the individual, company, institution, or organisation assuming overall responsibility for the initiation and management of the trial, and is not necessarily the main funder.51 52 In general, the company is the sponsor in industry initiated trials, while the funding agency or institution of the principal investigator is often the sponsor for investigator initiated trials. For some investigator initiated trials, the principal investigator can be considered to be a sponsor-investigator who assumes both sponsor and investigator roles.51 53\nIdentification of the trial sponsor provides transparency and accountability. The protocol should identify the name, contact information, and if applicable, the regulatory agency identifying number of the sponsor\n\n\nExamples\n\nTrial Sponsor: University of Nottingham\n\nSponsor’s Reference: RIS 8024 . . .\nContact name: Mr PC\nAddress: King’s Meadow Campus . . .\nTelephone: . . .\nEmail: . . .\nBack to top\n\n\n\n\n\n\n5c. Roles and responsibilities: sponsor and funder\n\n\n\n\n\n\n\nRole of study sponsor and funders, if any, in study design; collection, management, analysis, and interpretation of data; writing of the report; and the decision to submit the report for publication, including whether they will have ultimate authority over any of these activities\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThere is potential for bias when the trial sponsor or funder (sometimes the same entity) has competing interests (Item 28) and substantial influence on the planning, conduct, or reporting of a trial. Empirical research indicates that specific forms of bias tend to be more prevalent in trials funded by industry compared to those funded by non-commercial sources.36 37 38 45 55 56 57 58 59 60 The design, analysis, interpretation, and reporting of most industry-initiated trials are controlled by the sponsor; this authority is often enforced by contractual agreements signed between the sponsor and trial investigators (Item 29).10 61\nThe protocol should explicitly outline the roles and responsibilities of the sponsor and any funders in study design, conduct, data analysis and interpretation, manuscript writing, and dissemination of results. It is also important to state whether the sponsor or funder controls the final decision regarding any of these aspects of the trial.\nDespite the importance of declaring the roles of the trial sponsor and funders, few protocols explicitly do so. Among 44 protocols for industry-initiated trials receiving ethics approval in Denmark from 1994-95, none stated explicitly who had contributed to the design of the trial.\n\n\nExamples\n\nThis funding source had no role in the design of this study and will not have any role during its execution, analyses, interpretation of the data, or decision to submit results.\n\nBack to top\n\n\n\n\n\n\n5d. Roles and responsibilities: committees\n\n\n\n\n\n\n\nComposition, roles, and responsibilities of the coordinating centre, steering committee, endpoint adjudication committee, data management team, and other individuals or groups overseeing the trial, if applicable (see Item 21a for data monitoring committee)\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe protocol should outline the general membership of the various committees or groups involved in trial coordination and conduct; describe the roles and responsibilities of each; and (when known) identify the chairs and members. This information helps to ensure that roles and responsibilities are clearly understood at the trial onset, and facilitates communication from external parties regarding the trial. It also enables readers to understand the mandate and expertise of those responsible for overseeing participant safety, study design, database integrity, and study conduct. For example, empirical evidence supports the pivotal role of an epidemiologist or biostatistician in designing and conducting higher quality trials.63 64\n\n\nExamples\n\nPrincipal investigator and research physician:\n\nDesign and conduct of RITUXVAS\nPreparation of protocol and revisions\nPreparation of investigators brochure (IB) and CRFs [case report forms]\nOrganising steering committee meetings\nManaging CTO [clinical trials office]\nPublication of study reports\nMembers of TMC [Trial Management Committee]\nSteering committee (SC)\n(see title page for members)\nAgreement of final protocol\nAll lead investigators will be steering committee members. One lead investigator per country will be nominated as national coordinator.\nRecruitment of patients and liaising with principle [sic] investigator\nReviewing progress of study and if necessary agreeing changes to the protocol and/or investigators brochure to facilitate the smooth running of the study.\n\nTrial management committee (TMC):\n\n(Principle [sic] investigator, research physician, administrator)\nStudy planning\nOrganisation of steering committee meetings\nProvide annual risk report MHRA [Medicines and Healthcare Products Regulatory Agency] and ethics committee\nSUSAR [Serious unexpected suspected adverse events] reporting to MHRA and Roche\nResponsible for trial master file\nBudget administration and contractual issues with individual centres\nAdvice for lead investigators\nAudit of 6 monthly feedback forms and decide when site visit to occur.\nAssistance with international review, board/independent ethics committee applications\nData verification\nRandomisation\nOrganisation of central serum sample collection\n\nData manager:\n\nMaintenance of trial IT system and data entry\nData verification\n\nLead investigators:\n\nIn each participating centre a lead investigator (senior nephrologist/rheumatologist/ immunologist) will be identified, to be responsible for identification, recruitment, data collection and completion of CRFs, along with follow up of study patients and adherence to study protocol and investigators brochure. . . . Lead investigators will be steering committee members, with one investigator per country being nominated as national coordinator.\nBack to top\n\n\n\n\n\n\nIntroduction\n\n\n6a. Background and rationale\n\n\n\n\n\n\n\nDescription of research question and justification for undertaking the trial, including summary of relevant studies (published and unpublished) examining benefits and harms for each intervention\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe value of a research question, as well as the ethical and scientific justification for a trial, depend to a large degree on the uncertainty of the comparative benefits or harms of the interventions, which depends in turn on the existing body of knowledge on the topic. The background section of a protocol should summarise the importance of the research question, justify the need for the trial in the context of available evidence, and present any available data regarding the potential effects of the interventions (efficacy and harms).66 67 This information is particularly important to the trial participants and personnel, as it provides motivation for contributing to the trial.68 69 It is also relevant to funders, REC/IRBs, and other stakeholders who evaluate the scientific and ethical basis for trial conduct.\nTo place the trial in the context of available evidence, it is strongly recommended that an up-to-date systematic review of relevant studies be summarised and cited in the protocol.70 Several funders request this information in grant applications.71 72 Failure to review the cumulated evidence can lead to unnecessary duplication of research or to trial participants being deprived of effective, or exposed to harmful, interventions.73 74 75 76 A minority of published trial reports cite a systematic review of pre-existing evidence,77 78 and in one survey only half of trial investigators were aware of a relevant existing review when they had designed their trial.79 Given that about half of trials remain unpublished,80 81 82 and that published trials often represent a biased subset of all trials,80 83 it is important that systematic reviews include a search of online resources such as trial registries, results databases, and regulatory agency websites.\n\n\nExamples\n\nBackground\n\nIntroduction: For people at ages 5 to 45 years, trauma is second only to HIV/AIDS as a cause of death. . . .\nMechanisms: The haemostatic system helps to maintain the integrity of the circulatory system after severe vascular injury, whether traumatic or surgical in origin.[reference] Major surgery and trauma trigger similar haemostatic responses . . . Antifibrinolytic agents have been shown to reduce blood loss in patients with both normal and exaggerated fibrinolytic responses to surgery, and do so without apparently increasing the risk of post-operative complications, . . .\nExisting knowledge: Systemic antifibrinolytic agents are widely used in major surgery to prevent fibrinolysis and thus reduce surgical blood loss. A recent systematic review [reference] of randomised controlled trials of antifibrinolytic agents (mainly aprotinin or tranexamic acid) in elective surgical patients identified 89 trials including 8,580 randomised patients (74 trials in cardiac, eight in orthopaedic, four in liver, and three in vascular surgery). The results showed that these treatments reduced the numbers needing transfusion by one third, reduced the volume needed per transfusion by one unit, and halved the need for further surgery to control bleeding. These differences were all highly statistically significant. There was also a statistically non-significant reduction in the risk of death (RR=0.85: 95% CI 0.63 to 1.14) in the antifibrinolytic treated group.\nNeed for a trial: A simple and widely practicable treatment that reduces blood loss following trauma might prevent thousands of premature trauma deaths each year and secondly could reduce exposure to the risks of blood transfusion. Blood is a scarce and expensive resource and major concerns remain about the risk of transfusion-transmitted infection. . . . A large randomised trial is therefore needed of the use of a simple, inexpensive, widely practicable antifibrinolytic treatment such as tranexamic acid . . . in a wide range of trauma patients who, when they reach hospital are thought to be at risk of major haemorrhage that could significantly affect their chances of survival.\nDose selection\nThe systematic review of randomised controlled trials of antifibrinolytic agents in surgery showed that dose regimens of tranexamic acid vary widely.[reference] . . .\nIn this emergency situation, administration of a fixed dose would be more practicable as determining the weight of a patient would be impossible. Therefore a fixed dose within the dose range which has been shown to inhibit fibrinolysis and provide haemostatic benefit is being used for this trial. . . . The planned duration of administration allows for the full effect of tranexamic acid on the immediate risk of haemorrhage without extending too far into the acute phase response seen after surgery and trauma.\nBack to top\n\n\n\n\n\n\n6b. Background and rationale: choice of comparators\n\n\n\n\n\n\n\nExplanation for choice of comparators\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe choice of control interventions has important implications for trial ethics, recruitment, results, and interpretation. In trials comparing an intervention to an active control or usual care, a clear description of the rationale for the comparator intervention will facilitate understanding of its appropriateness.86 87 For example, a trial in which the control group receives an inappropriately low dose of an active drug will overestimate the relative efficacy of the study intervention in clinical practice; conversely, an inappropriately high dose in the control group will lead to an underestimate of the relative harms of the study intervention.87 88\nThe appropriateness of using placebo-only control groups has been the subject of extensive debate and merits careful consideration of the existence of other effective treatments, the potential risks to trial participants, and the need for assay sensitivity—that is, ability to distinguish an effective intervention from less effective or ineffective interventions.89 90 In addition, surveys have demonstrated that a potential barrier to trial participation is the possibility of being allocated a placebo-only or active control intervention that is perceived to be less desirable than the study intervention.68 69 91 92 Evidence also suggests that enrolled participants perceive the effect of a given intervention differently depending on whether the control group consists of an active comparator or only placebo.93 94 95 96\nFinally, studies suggest that some active comparators in head-to-head randomised trials are presumed by trial investigators to be effective despite having never previously been shown to be superior to placebo.74 97 In a systematic review of over 100 head-to-head antibiotic trials for mild to moderate chronic obstructive pulmonary disease,74 cumulative meta-analysis of preceding placebo controlled trials did not show a significant effect of antibiotics over placebo. Such studies again highlight the importance of providing a thorough background and rationale for a trial and the choice of comparators—including data from an up-to-date systematic review—to enable potential participants, physicians, REC/IRBs, and funders to discern the merit of the trial.\n\n\nExamples\n\nChoice of comparator\n\nIn spite of the increasing numbers of resistant strains, chloroquine monotherapy is still recommended as standard blood-stage therapy for patients with P [Plasmodium] vivax malaria in the countries in which this trial will be conducted. Its selection as comparator is therefore justified. The adult dose of chloroquine will be 620 mg for 2 days followed by 310 mg on the third day and for children 10 mg/kg for the first two days and 5 mg/kg for the third day. Total dose is in accordance with the current practice in the countries where the study is conducted. The safety profile of chloroquine is well established and known. Although generally well tolerated, the following side-effects of chloroquine treatment have been described:\n\nGastro-intestinal disturbances, headache, hypotension, convulsions, visual disturbances, depigmentation or loss of hair, skin reactions (rashes, pruritus) and, rarely, bone-marrow suppression and hypersensitivity reactions such as urticaria and angioedema. Their occurrence during the present trial may however be unlikely given the short (3-day) duration of treatment.\n\nBack to top\n\n\n\n\n\n\n7. Objectives\n\n\n\n\n\n\n\nSpecific objectives or hypotheses\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe study objectives reflect the scientific questions to be answered by the trial, and define its purpose and scope. They are closely tied to the trial design (Item 8) and analysis methods (Item 20). For example, the sample size calculation and statistical analyses for superiority trials will differ from those investigating non-inferiority.\nThe objectives are generally phrased using neutral wording (eg, to compare the effect of treatment A versus treatment B on outcome X) rather than in terms of a particular direction of effect.99 A hypothesis states the predicted effect of the interventions on the trial outcomes. For multiarm trials, the objectives should clarify the way in which all the treatment groups will be compared (eg, A versus B; A versus C).\n\n\nExamples\n\n1.1 Research hypothesis\n\nApixaban is noninferior to warfarin for prevention of stroke (hemorrhagic, ischemic or of unspecified type) or systemic embolism in subjects with atrial fibrillation (AF) and additional risk factor(s) for stroke.\n2 Study objectives\n2.1 Primary objective\nTo determine if apixaban is noninferior to warfarin (INR [international normalized ratio] target range 2.0-3.0) in the combined endpoint of stroke (hemorrhagic, ischemic or of unspecified type) and systemic embolism, in subjects with AF and at least one additional risk factor for stroke.\n2.2 Secondary objectives\n2.2.1 Key secondary objectives\nThe key secondary objectives are to determine, in subjects with AF and at least one additional risk factor for stroke, if apixaban is superior to warfarin (INR target range 2.0 - 3.0) for,\n\nthe combined endpoint of stroke (hemorrhagic, ischemic or of unspecified type) and systemic embolism\nmajor bleeding [International Society of Thrombosis and Hemostasis]\nall-cause death\n\n2.2.2 Other secondary objectives\nTo compare, in subjects with AF and at least one additional risk factor for stroke, apixaban and warfarin with respect to:\n\nThe composite endpoint of stroke (ischemic, hemorrhagic, or of unspecified type), systemic embolism and major bleeding, in warfarin naive subjects\nTo assess the safety of apixaban in subjects with AF and at least one additional risk factor for stroke.\n\nBack to top\n\n\n\n\n\n\n8. Trial design\n\n\n\n\n\n\n\nDescription of trial design including type of trial (eg, parallel group, crossover, factorial, single group), allocation ratio, and framework (eg, superiority, equivalence, non-inferiority, exploratory)\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe most common design for published randomised trials is the parallel group, two arm, superiority trial with 1:1 allocation ratio.101 Other trial types include crossover, cluster, factorial, split body, and n of 1 randomised trials, as well as single group trials and non-randomised comparative trials.\nFor trials with more than one study group, the allocation ratio reflects the intended relative number of participants in each group (eg, 1:1 or 2:1). Unequal allocation ratios are used for a variety of reasons, including potential cost savings, allowance for learning curves, and ethical considerations when the balance of existing evidence appears to be in favour of one intervention over the other.102 Evidence also suggests a preference of some participants for enrolling in trials with an allocation ratio that favours allocation to an active treatment.92\nThe framework of a trial refers to its overall objective to test the superiority, non-inferiority, or equivalence of one intervention with another, or in the case of exploratory pilot trials, to gather preliminary information on the intervention (eg, harms, pharmacokinetics) and the feasibility of conducting a full-scale trial.\nIt is important to specify and explain the choice of study design because of its close relation to the trial objectives (Item 7) and its influence on the study methods, conduct, costs,103 results,104 105 106 and interpretation. For example, factorial and non-inferiority trials can involve more complex methods, analyses, and interpretations than parallel group superiority trials.107 108 In addition, the interpretation of trial results in published reports is not always consistent with the pre-specified trial framework,6 109 110 especially among reports claiming post hoc equivalence based on a failure to demonstrate superiority rather than a specific test of equivalence.109\nThere is increasing interest in adaptive designs for clinical trials, defined as the use of accumulating data to decide how to modify aspects of a study as it continues, without undermining the validity and integrity of the trial.111 112 Examples of potential adaptations include stopping the trial early, modifying the allocation ratio, re-estimating the sample size, and changing the eligibility criteria. The most valid adaptive designs are those in which the opportunity to make adaptations is based on prespecified decision rules that are fully documented in the protocol (Item 21b).\n\n\nExamples\n\nThe PROUD trial is designed as a randomised, controlled, observer, surgeon and patient blinded multicenter superiority trial with two parallel groups and a primary endpoint of wound infection during 30 days after surgery . . . randomization will be performed as block randomization with a 1:1 allocation.\n\nBack to top\n\n\n\n\n\n\nMethods: Participants, interventions, and outcomes\n\n\n9. Study setting\n\n\n\n\n\n\n\nDescription of study settings (eg, community clinic, academic hospital) and list of countries where data will be collected. Reference to where list of study sites can be obtained\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nA description of the environment in which a trial will be conducted provides important context in terms of the applicability of the study results; the existence and type of applicable local regulation and ethics oversight; and the type of healthcare and research infrastructure available. These considerations can vary substantially within and between countries.\nAt a minimum, the countries , type of setting (eg, urban versus rural), and the likely number of study sites should be reported in the protocol. These factors have been associated with recruitment success and degree of attrition for some trials,68 91 92 114 115 116 117 but not for others.118 119 Trial location has also been associated with trial outcome,120 aspects of trial quality (eg, authenticity of randomisation121), and generalisability.122\n\n\nExamples\n\nSelection of countries\n\n. . . To detect an intervention-related difference in HIV incidences with the desired power, the baseline incidences at the sites must be sufficiently high. We chose the participating sites so that the average baseline annual incidence across all communities in the study is likely to reach at least 3%. The various sites in sub-Saharan Africa met this criterion, but we also wanted sites in Asia to extend the generalizability of the intervention. The only location in Asia with sufficient incidence at the community level is in ethnic minority communities in Northern Thailand, where HIV incidence is currently in excess of 7%;[reference] thus they were invited to participate as well. Our final selection of sites combines rural (Tanzania, Zimbabwe, Thailand, and KwaZulu-Natal) and an urban (Soweto) location. The cultural circumstances between the sub-Saharan African sites vary widely.\n\nDefinition of community\n\nEach of the three southern African sites (Harare, Zimbabwe; and Soweto and Vulindlela, South Africa) selected eight communities, the East African (Tanzanian) site selected 10 communities, and Thailand selected 14 communities . . . They are of a population size of approximately 10,000 . . . which fosters social familiarity and connectedness, and they are geographically distinct. Communities are defined primarily geographically for operational purposes for the study, taking into account these dimensions of social communality. The communities chosen within each country and site are selected to be sufficiently distant from each other so that there would be little cross-contamination or little possibility that individuals from a control community would benefit from the activities in the intervention community.\nBack to top\n\n\n\n\n\n\n10. Eligibility criteria\n\n\n\n\n\n\n\nInclusion and exclusion criteria for participants. If applicable, eligibility criteria for study centres and individuals who will perform the interventions (eg, surgeons, psychotherapists)\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nEligibility criteria for potential trial participants define the study population. They can relate to demographic information; type or severity of the health condition; comorbidities; previous or current treatment; diagnostic procedures; pregnancy; or other relevant considerations.125 In trials of operator-dependent interventions such as surgery and psychotherapy, it is usually important to promote consistency of intervention delivery by also defining the eligibility criteria for care providers and centres where the intervention will be administered.126\nClear delineation of eligibility criteria serves several purposes. It enables study personnel to apply these criteria consistently throughout the trial.127 The choice of eligibility criteria can affect recruitment and attrition,67 114 115 117 118 128 129 130 as well as outcome event rates.39 131 In addition, the criteria convey key information related to external validity (generalisability or applicability).132 The importance of transparent documentation is highlighted by evidence that the eligibility criteria listed in publications are often different from those specified in the protocol.125 133 134\nCertain eligibility criteria warrant explicit justification in the protocol, particularly when they limit the trial sample to a narrow subset of the population.132 135 136 The appropriateness of restrictive participant selection depends on the trial objectives.137 When trial participants differ substantially from the overall population to whom the intervention will be applied, the trial results may not reflect the impact in real world practice settings\n\n\nExamples\n\nPatients (or a representative) must provide written, informed consent before any study procedures occur (see Appendix 1 for sample Informed Consent Form) . . .\n\n5.1. Inclusion Criteria\nPatients eligible for the trial must comply with all of the following at randomization:\n\nAge ≥16 years\nCurrent admission under the care of the heart-failure service at the site…\n\n5.2. Exclusion Criteria\n\nAcute decompensation thought by the attending heart-failure physician to require or be likely to require PAC [pulmonary-artery catheter] during the next 24 hours. Such patients should be entered into the PAC Registry (see below).\nInability to undergo PAC placement within the next 12 hours…\n\nPatients enrolled in other investigational drug studies are potential candidates for ESCAPE. As the ESCAPE protocol does not involve any investigational agents or techniques, patients would be eligible for dual randomization if they are on stable doses of the investigational drugs. . . .\nStudy Network, Training, and Responsibilities\n. . . To qualify, physicians responsible for PAC [pulmonary-artery catheter] placements will be required to show proof of insertion of ≥50 PACs in the previous year with a complication rate of <5%. Further, clinicians will need to show competence in the following areas to participate in the study: 1) insertion techniques and cardiovascular anatomy; 2) oxygen dynamics; . . . and 7) common PAC complications.[reference] . . . we will assume basic competence in these areas after satisfactory completion of the PACEP [PAC educational programme] module.\n\nTrial centre requirements\n\nA number of guidelines have stated thrombolysis should only be considered if the patient is admitted to a specialist centre with appropriate experience and expertise.[reference] Hospitals participating in IST-3 [third International Stroke Trial] should have an organized acute stroke service. The components of effective stroke unit care have been identified . . . In brief, the facilities (details of these requirements are specified in the separate operations manual) should include:\n\nWritten protocol for the acute assessment of patients with suspected acute stroke to include interventions to reduce time from onset to treatment.\nImmediate access to CT [computed tomographic] or MR [magnetic resonance] brain scanning (preferably 24 hours a day).\n\nA treatment area where thrombolysis may be administered and the patient monitored according to trial protocol, preferably an acute stroke unit.\nBack to top\n\n\n\n\n\n\n11a. Interventions: description\n\n\n\n\n\n\n\nInterventions for each group with sufficient detail to allow replication, including how and when they will be administered\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nStudies of trials and systematic reviews have shown that important elements of the interventions are not described in half of the publications.146 147 If such elements are also missing from the protocol, or if the protocol simply refers to other documents that are not freely accessible, then it can be impossible for healthcare providers, systematic reviewers, policymakers, and others to fully understand, implement, or evaluate the trial intervention.148 This principle applies to all types of interventions, but is particularly true for complex interventions (eg, health service delivery; psychotherapy), which consist of interconnected components that can vary between healthcare providers and settings.\nFor drugs, biological agents, or placebos, the protocol description should include the generic name, manufacturer, constituent components, route of administration, and dosing schedule (including titration and run-in periods, if applicable).149 150 The description of non-drug interventions—such as devices, procedures, policies, models of care, or counselling—is generally more complex and warrants additional details about the setting (Item 9) and individuals administering the interventions. For example, the level of pre-trial expertise (Item 10) and specific training of individuals administering these complex interventions are often relevant to describe (eg, for surgeons, psychologists, physiotherapists). When intervention delivery is subject to variation, it is important to state whether the same individuals will deliver the trial interventions in all study groups, or whether different individuals will manage each study group—in which case it can be difficult to separate the effect of the intervention from that of the individual delivering it. Interventions that consist of usual care or standard of care require further elaboration in the protocol, as this care can vary substantially across centres and patients, as well as over the duration of the trial.\n\n\nExamples\n\nFor a given trial participant, the assigned study intervention may need to be modified or discontinued by trial investigators for various reasons, including harms, improved health status, lack of efficacy, and withdrawal of participant consent. Comparability across study groups can be improved, and subjectivity in care decisions reduced, by defining standard criteria for intervention modifications and discontinuations in the protocol. Regardless of any decision to modify or discontinue their assigned intervention, study participants should be retained in the trial whenever possible to enable follow-up data collection and prevent missing data (Item 18b)\n\nBack to top\n\n\n\n\n\n\n11b. Interventions: modifications\n\n\n\n\n\n\n\nCriteria for discontinuing or modifying allocated interventions for a given trial participant (eg, drug dose change in response to harms, participant request, or improving / worsening disease)\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nStudies of trials and systematic reviews have shown that important elements of the interventions are not described in half of the publications.146 147 If such elements are also missing from the protocol, or if the protocol simply refers to other documents that are not freely accessible, then it can be impossible for healthcare providers, systematic reviewers, policymakers, and others to fully understand, implement, or evaluate the trial intervention.148 This principle applies to all types of interventions, but is particularly true for complex interventions (eg, health service delivery; psychotherapy), which consist of interconnected components that can vary between healthcare providers and settings.\nFor drugs, biological agents, or placebos, the protocol description should include the generic name, manufacturer, constituent components, route of administration, and dosing schedule (including titration and run-in periods, if applicable).149 150 The description of non-drug interventions—such as devices, procedures, policies, models of care, or counselling—is generally more complex and warrants additional details about the setting (Item 9) and individuals administering the interventions. For example, the level of pre-trial expertise (Item 10) and specific training of individuals administering these complex interventions are often relevant to describe (eg, for surgeons, psychologists, physiotherapists). When intervention delivery is subject to variation, it is important to state whether the same individuals will deliver the trial interventions in all study groups, or whether different individuals will manage each study group—in which case it can be difficult to separate the effect of the intervention from that of the individual delivering it. Interventions that consist of usual care or standard of care require further elaboration in the protocol, as this care can vary substantially across centres and patients, as well as over the duration of the trial.\n\n\nExamples\n\nEligible patients will be randomised in equal proportions between IL-1ra [interleukin-1 receptor antagonist] and placebo, receiving either a once daily, subcutaneous (s.c.) injection of IL-1ra (dose 100 mg per 24 h) for 14 days, or a daily s.c. injection of placebo for 14 days . . .\n\n\nThe study drug and placebo will be provided by Amgen Inc in its commercially available recombinant form . . . The study drug and placebo will be relabelled by Amgen, in collaboration with CTEU [Clinical Trials and Evaluation Unit] according to MHRA [Medicines and Healthcare Products Regulatory Agency] guidelines.\n\n\nThe first dose of IL-1ra will be given within 24 h +2 h of the positive Troponin. Injections will be given at a standardised time (24 ± 2 h after the previous dose), immediately after blood sampling. IL-1ra or placebo will [be] administered to the patient by the research nurse while the patient is in hospital. During the hospital stay, the patient will be taught to self-administer the injection by the research nurse and on discharge will continue at home. This has proven possible in other ACS [acute coronary syndrome] trials that required self injection of subcutaneous heparin [reference]. Full written guidance on self injection will also be provided to patients. If self injection is found not to be possible in an individual patient for unexpected reasons, an alternative method will be sought (eg district nurse, or attending the hospital) to try and maintain full compliance with scheduled study drug regimen after discharge. Patients will also be asked to complete a daily injection diary. All personnel will be blinded to the identity of the syringe contents.\n\nBack to top\n\n\n\n\n\n\n11c. Interventions: adherance\n\n\n\n\n\n\n\nStrategies to improve adherence to intervention protocols, and any procedures for monitoring adherence (eg, drug tablet return; laboratory tests)\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAdherence to intervention protocols refers to the degree to which the behaviour of trial participants corresponds to the intervention assigned to them.154 Distinct but related concepts include trial retention (Item 18b) and adherence to the follow-up protocol of procedures and assessments (Item 13).\nOn average, adherence to intervention protocols is higher in clinical trials than in non-research settings.155 Although there is no consensus on the acceptable minimum adherence level in clinical trials, low adherence can have a substantial effect on statistical power and interpretation of trial results.156 157 158 Since fewer participants are receiving the full intervention as intended, non-adherence can reduce the contrast between study groups—leading to decreased study power and increased costs associated with recruiting larger sample sizes for evaluating superiority, or leading to potentially inappropriate conclusions of non-inferiority or equivalence. There is also the possibility of underestimating any efficacy and harms of the study intervention.\nFurthermore, if adherence is a marker for general healthy behaviour associated with better prognosis, then different rates of non-adherence between study groups can lead to a biased estimate of an intervention’s effect. In support of this healthy adherer effect, non-adherers to placebo in clinical studies have been found to have poorer clinical outcomes than adherers.159\nTo help avoid these potential detrimental effects of non-adherence, many trials implement procedures and strategies for monitoring and improving adherence,67 156 157 158 and any such plans should be described in the protocol.160 Among applicable drug trials published in 1997-99, 47% reported monitoring the level of adherence.161 Although each of the many types of monitoring methods has its limitations,157 158 adherence data can help to inform the statistical analysis (Item 20c), trial interpretation, and choice of appropriate adherence strategies to implement in the trial as it progresses or in future trials and clinical practice.\nA variety of adherence strategies exist,156 157 158 and their use can be tailored to the specific type of trial design, intervention, and participant population. It may be desirable to select strategies that can be easily implemented in clinical practice, so that the level of adherence in the real world setting is comparable to that observed in the trial.\n\n\nExamples\n\nAdherence reminder sessions\n\nFace-to-face adherence reminder sessions will take place at the initial product dispensing and each study visit thereafter. This session will include:\n\nThe importance of following study guidelines for adherence to once daily study product\nInstructions about taking study pills including dose timing, storage, and importance of taking pills whole, and what to do in the event of a missed dose.\nInstructions about the purpose, use, and care of the MEMS® cap [medication event monitoring system] and bottle\nNotification that there will be a pill count at every study visit\nReinforcement that study pills may be TDF [tenofovir disproxil fumarate] or placebo\nImportance of calling the clinic if experiencing problems possibly related to study product such as symptoms, lost pills or MEMS® cap.\n\nSubsequent sessions will occur at the follow-up visits. Participants will be asked about any problems they are having taking their study pills or using the MEMS® cap. There will be brief discussion of reasons for missed doses and simple strategies for enhancing adherence, eg, linking pill taking to meals or other daily activities. Participants will have an opportunity to ask questions and key messages from the initial session will be reviewed as needed . . .\nAdherence assessments\nTo enhance validity of data, multiple methods will be used to assess medication adherence including pill count; an electronic medication event monitoring system (MEMS® cap) [reference]; and ACASI [audio-computer administered interview] questionnaire items including a one month visual analogue scale,[reference] reasons for non-compliance, and use of the MEMS® cap. Participants will return the unused tablets and bottle at each follow-up visit. Unused tablets will be counted and recorded on the appropriate CRF [case report form]. Electronic data collected in the MEMS® cap will be downloaded into a designated, secure study computer.\nBack to top\n\n\n\n\n\n\n11d. Interventions: concomitant care\n\n\n\n\n\n\n\nRelevant concomitant care and interventions that are permitted or prohibited during the trial\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nIn a controlled trial, a key goal is to have comparable study groups that differ only by the intervention being evaluated, so that any difference in outcomes can be attributed to effects of the study intervention. Cointervention bias can arise when the study groups receive different concomitant care or interventions (in addition to the assigned trial interventions) that may affect trial outcomes.162 To promote comparability of study groups, the protocol should list the relevant concomitant care and interventions that are allowed (including rescue interventions), as well as any that are prohibited.\n\n\nExamples\n\nRescue Medication\n\nFor weeks 0-3, topical mometasone furoate 0.1% cream or ointment (30 g/week) will be permitted with participants preferably using ointment. Participants will be instructed to apply the topical mometasone furoate to blisters/lesions as required (not to areas of unaffected skin). If the participant is allergic to mometasone furoate or the hospital pharmacy does not stock it, then an alternative topical steroid may be prescribed but this must be in the potent class. In addition, participants will be advised that they can apply a light moisturiser to blisters/lesions at any time during the study.\nFor weeks 3-6, use of mometasone furoate (or other topical corticosteroids) is strongly discouraged to prevent potential systemic effects. Accidental use of mometasone furoate or other potent topical steroid during this period will be classified as a protocol deviation.\nAfter week 6, potent topical corticosteroids (up to 30 g/week) may be used to treat symptoms and localised disease if they would have normally been used as part of normal clinical care by the physician in charge of that patient. This must be recorded on the trial treatment log.\nHowever, those patients who are on a dose reducing regime for oral steroids, 30 g/week of a potent topical steroid will be allowed.\nProhibited Concomitant Medications\nThe administration of live virus vaccines is not permitted for all participants during weeks 0-6 as the investigator is blinded to treatment allocation, and must therefore warn all participants to refrain for [sic] having a live virus vaccine. However, after week 6, once the investigator knows which medication the participant is on, only those taking prednisolone will not be allowed live virus vaccines.\nParticipants should continue to take medications for other conditions as normal. However, if it is anticipated that the participant will need a live virus vaccine during the intervention phase, they will be ineligible for entry into the study\nBack to top\n\n\n\n\n\n\n12. Outcomes\n\n\n\n\n\n\n\nPrimary, secondary, and other outcomes, including the specific measurement variable (eg, systolic blood pressure), analysis metric (eg, change from baseline, final value, time to event), method of aggregation (eg, median, proportion), and time point for each outcome. Explanation of the clinical relevance of chosen efficacy and harm outcomes is strongly recommended\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe trial outcomes are fundamental to study design and interpretation of results. For a given intervention, an outcome can generally reflect efficacy (beneficial effect) or harm (adverse effect). The outcomes of main interest are designated as primary outcomes, which usually appear in the objectives (Item 7) and sample size calculation (Item 14). The remaining outcomes constitute secondary or other outcomes.\nFor each outcome, the trial protocol should define four components: the specific measurement variable, which corresponds to the data collected directly from trial participants (eg, Beck Depression Inventory score, all cause mortality); the participant-level analysis metric, which corresponds to the format of the outcome data that will be used from each trial participant for analysis (eg, change from baseline, final value, time to event); the method of aggregation, which refers to the summary measure format for each study group (eg, mean, proportion with score > 2); and the specific measurement time point of interest for analysis.163\nIt is also important to explain the rationale for the choice of trial outcomes. An ideal outcome is valid, reproducible, relevant to the target population (eg, patients), and responsive to changes in the health condition being studied.67 The use of a continuous versus dichotomous method of aggregation can affect study power and estimates of treatment effect,164 165 and subjective outcomes are more prone to bias from inadequate blinding (ascertainment bias) and allocation concealment (selection bias) than objective outcomes.166 167 Although composite outcomes increase event rates and statistical power, their relevance and interpretation can be unclear if the individual component outcomes vary greatly in event rates, importance to patients, or amount of missing data.\nThe number of primary outcomes should be as small as possible. Although up to 38% of trials define multiple primary outcomes,4 35 163 this practice can introduce problems with multiplicity, selective reporting, and interpretation when there are inconsistent results across outcomes. Problems also arise when trial protocols do not designate any primary outcomes, as seen in half (28/59) of protocols for a sample of trials published from 2002-2008,12 and in 25% of randomised trial protocols that received ethics approval in Denmark in 1994-95.4 Furthermore, major discrepancies in the primary outcomes designated in protocols/registries/regulatory submissions versus final trial publications are common; favour the reporting of statistically significant primary outcomes over non-significant ones; and are often not acknowledged in final publications.172 173 174 175 176 Such bias can only be identified and deterred if trial outcomes are clearly defined beforehand in the protocol and if protocol information is made public.177\nWhere possible, the development and adoption of a common set of key trial outcomes within a specialty can help to deter selective reporting of outcomes and to facilitate comparisons and pooling of results across trials in a meta-analysis.178 179 180 The COMET (Core Outcome Measures in Effectiveness Trials) Initiative aims to facilitate the development and application of such standardised sets of core outcomes for clinical trials of specific conditions (www.comet-initiative.org). Trial investigators are encouraged to ascertain whether there is a core outcome set relevant to their trial and, if so, to include those outcomes in their trial. Existence of a common set of outcomes does not preclude inclusion of additional relevant outcomes for a given trial.\n\n\nExamples\n\nPrimary Outcome Measures\n\nDifference between the two treatment arms in the proportion of participants classed as treatment success at 6 weeks. Treatment success is defined as 3 or less significant blisters present on examination at 6 weeks. Significant blisters are defined as intact blisters containing fluid which are at least 5 mm in diameter. However, if the participant has popped a blister, or the blister is at a site that makes it susceptible to bursting such as the sole of the foot, it can be considered part of the blister count, providing there is a flexible (but not dry) roof present over a moist base. Mucosal blisters will be excluded from the count.\nA survey of the UK DCTN [Dermatology Clinical Trials Network] membership showed that a point estimate of 25% inferiority in effectiveness would be acceptable assuming a gain in the safety profile of at least 10%.\n\nThis measure of success was selected as it was considered to be more clinically relevant than a continuous measure of blister count. It would be less clinically relevant to perform an absolute blister count and report a percentage reduction. Instead, to state that treatment is considered a success if remission is achieved (ie the presence of three or less blisters on physical examination at 6 weeks) more closely reflects clinical practice. In addition, it is far less burdensome on investigators than including a full blister count, which would mean counting in the region of 50-60 blisters in many cases. This outcome measure will be performed as a single blind assessment.\n\n\nDifference between the two treatment arms in the proportion of participants reporting grade 3, 4 and 5 (mortality) adverse events which are possibly, probably or definitely related to BP [bullous pemphigoid] medication in the 52 weeks following randomisation. A modified version of The Common Terminology Criteria for Adverse Events (CTCAE v3.0) will be used to grade adverse events. At each study visit, participants will be questioned about adverse events they have experienced since the last study visit (using a standard list of known side effects of the two study drugs).\n\n\n\nSecondary Outcome Measures\n\n\nFor the secondary and tertiary endpoints a participant will be classed as a treatment success if they have 3 or less significant blisters present on examination and have not had their treatment modified (changed or dose increased) on account of a poor response.\n\nDifference in the proportion of participants who are classed as a treatment success at 6 weeks.\n\nDifference in the proportion of participants in each treatment arm who are classed as treatment success at 6 weeks and are alive at 52 weeks. This measure will provide a good overall comparison of the two treatment arms. . . .\nBack to top\n\n\n\n\n\n\n13. Participant timeline\n\n\n\n\n\n\n\nTime schedule of enrolment, interventions (including any run-ins and washouts), assessments, and visits for participants. A schematic diagram is highly recommended (see Figure)\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nA clear and concise timeline of the study visits, enrolment process, interventions, and assessments performed on participants can help to guide trial conduct and enable external review of participant burden and feasibility. These factors can also affect the decision of potential investigators and participants to join the trial (Item 15).91\nA schematic diagram is highly recommended to efficiently present the overall schedule and time commitment for trial participants in each study group. Though various presentation formats exist, key information to convey includes the timing of each visit, starting from initial eligibility screening through to study close-out; time periods during which trial interventions will be administered; and the procedures and assessments performed at each visit (with reference to specific data collection forms, if relevant)\n\n\nExamples\n\nThe main outcomes of interest are the drug and sex-related HIV and HCV [hepatitis C virus] risk behaviors . . . Clients will be assessed using the full battery of instruments from the Common Assessment Battery (CAB), along with the Self-Efficacy and Stages of Change questionnaires and a Urine Drug Screen after consenting . . . questionnaires will take place for all participants 14-30 days after randomization during which they will be given the Stages of Change and Self-Efficacy questionnaires, the Timeline Follow-Back, and a UA [urine analysis]. Follow-up interviews, using the full battery (CAB and questionnaires), will be collected at 2 months (56 days), 4 months (112 days) and 6 months (168 days) after the randomization date. A 14 day window, defined as 7 days before and 7 days after the due date, will be available to complete the 2 and 4 month follow-up interviews and a 28 day window, defined as 7 days before and 21 days after the due date, will be available to complete the 6 month follow up interview…\n\n\n7.1.1 Common Assessment Battery (CAB)\n\nA Demographic Questionnaire . . .\nThe Composite International Diagnostic Interview Version 2.1…\nThe Addiction Severity Index-Lite (ASI-Lite) . . .\nThe Risk Behavior Survey (RBS), . . .\n7.1.2 Additional Interviews/Questionnaires\nTo assess drug use, urinalysis for morphine, cocaine, amphetamine, and methamphetamine will be performed at the 2-Week Interim Visit, and the 2-, 4-, and 6-month Follow-up visits…\n\nStage of change for quitting drug use will be measured using a modification of the Motivation Scales\n\nTable 3 HIV/HCV risk reduction protocol schedule of forms and procedures (adapted from original table) http://www.bmj.com/content/346/bmj.e7586.long\n\nThe trial consists of a 12-week intervention treatment phase with a 40-week follow-up phase. The total trial period will be 12-months. As shown . . . measurements will be undertaken at four time-points in each group: at baseline, directly after completing the 12-week internet program, and at six and 12-month follow-up\n\nFig 2: Flow of participants http://www.bmj.com/content/bmj/346/bmj.e7586/F2.large.jpg\nBack to top\n\n\n\n\n\n\n14. Sample size\n\n\n\n\n\n\n\nEstimated number of participants needed to achieve study objectives and how it was determined, including clinical and statistical assumptions supporting any sample size calculations\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe planned number of trial participants is a key aspect of study design, budgeting, and feasibility that is usually determined using a formal sample size calculation. If the planned sample size is not derived statistically, then this should be explicitly stated along with a rationale for the intended sample size (eg, exploratory nature of pilot studies; pragmatic considerations for trials in rare diseases).\nFor trials that involve a formal sample size calculation, the guiding principle is that the planned sample size should be large enough to have a high probability (power) of detecting a true effect of a given magnitude, should it exist. Sample size calculations are generally based on one primary outcome; however, it may also be worthwhile to plan for adequate study power or report the power that will be available (given the proposed sample size) for other important outcomes or analyses because trials are often underpowered to detect harms or subgroup effects.\nAmong randomised trial protocols that describe a sample size calculation, 4-40% do not state all components of the calculation.6 11 The protocol should generally include the following: the outcome (Item 12); the values assumed for the outcome in each study group (eg, proportion with event, or mean and standard deviation) (table 4⇓); the statistical test (Item 20a); alpha (type 1 error) level; power; and the calculated sample size per group—both assuming no loss of data and, if relevant, after any inflation for anticipated missing data (Item 20c). Trial investigators are also encouraged to provide a rationale or reference for the outcome values assumed for each study group.187 The values of certain prespecified variables tend to be inappropriately inflated (eg, clinically important treatment effect size)188 189 or underestimated (eg, standard deviation for continuous outcomes),190 leading to trials having less power in the end than what was originally calculated. Finally, when uncertainty of a sample size estimate is acknowledged, methods exist for re-estimating sample size.191 The intended use of such an adaptive design approach should be stated in the protocol.\nFor designs and frameworks other than parallel group superiority trials, additional elements are required in the sample size calculation. For example, an estimate of the standard deviation of within-person changes from baseline should be included for crossover trials192; the intracluster correlation coefficient for cluster randomised trials193; and the equivalence or non-inferiority margin for equivalence or non-inferiority trials respectively.108 194 Such elements are often not described in final trial reports,110 195 196 197 198 and it is unclear how often they are specified in the protocol.\nComplete description of sample size calculations in the protocol enables an assessment of whether the trial will be adequately powered to detect a clinically important difference. It also promotes transparency and discourages inappropriate post hoc revision that is intended to support a favourable interpretation of results or portray consistency between planned and achieved sample sizes.\n\n\nExamples\n\nThe sample size was calculated on the basis of the primary hypothesis. In the exploratory study,[reference] those referred to PEPS [psychoeducation with problem solving] had a greater improvement in social functioning at 6 month follow-up equivalent to 1.05 points on the SFQ [Social Functioning Questionnaire]. However, a number of people received PEPS who were not included in the trial (eg, the wait-list control) and, for this larger sample (N=93), the mean pre-post- treatment difference was 1.79 (pre-treatment mean=13.85, SD=4.21; post-treatment mean=12.06, SD=4.21). (Note: a lower SFQ score is more desirable). This difference of almost 2 points accords with other evidence that this is a clinically significant and important difference.[reference] A reduction of 2 points or more on the SFQ at 1 year follow-up in an RCT of cognitive behaviour therapy in health anxiety was associated with a halving of secondary care appointments (1.24.vs 0.65), a clinically significant reduction in the Hospital Anxiety and Depression Scale (HADS[reference]) Anxiety score of 2.5 (9.9 vs 7.45) and a reduction in health anxiety (the main outcome) of 5.6 points (17.8 vs 12.2) (11 is a normal population score and 18 is pathological).[reference] These findings suggest that improvements in social functioning may accrue over 1 year, hence we expect to find a greater magnitude of response at the 72 week follow-up than we did in the exploratory trial. Therefore, we have powered this trial to be able to detect a difference in SFQ score of 2 points. SFQ standard deviations vary between treatment, control, and the wait-list samples, ranging from 3.78 to 4.53. We have based our sample size estimate on the most conservative (ie, largest) SD [standard deviation]. To detect a mean difference in SFQ score of 2 point (SD = 4.53) at 72 weeks with a two-sided significance level of 1% and power of 80% with equal allocation to two arms would require 120 patients in each arm of the trial. To allow for 30% drop out, 170 will be recruited per arm, ie, 340 in total.”183\n\n\nSuperficial and deep incisional surgical site infection rates for patients in the PDS II® [polydioxanone suture] group are estimated to occur at a rate of 0.12.[reference] The trials by [reference] have shown a reduction of SSI [surgical site infections] of more than 50% (from 10.8% to 4.9% and from 9.2% to 4.3% respectively). Therefore, we estimate a rate of 0.06 for PDS Plus® [triclosan-coated continuous polydioxanone suture].\n\n\nFor a fixed sample size design, the sample size required to achieve a power of 1-β=0.80 for the one-sided chi-square test at level α=0.025 under these assumptions amounts to 2×356=712 (nQuery Advisor®, version 7.0). It can be expected that including covariates of prognostic importance in the logistic regression model as defined for the confirmatory analysis will increase the power as compared to the chi-square test. As the individual results for the primary endpoint are available within 30 days after surgery, the drop-out rate is expected to be small. Nevertheless, a potential dilution of the treatment effect due to drop-outs is taken into account (eg no photographs available, loss to follow up); it is assumed that this can be compensated by additional 5% of patients to be randomized, and therefore the total sample size required for a fixed sample size design amounts to n=712+38=750 patients…\n\n\nAn adaptive interim analysis [reference] will be performed after availability of the results for the primary endpoint for a total of 375 randomized patients (ie, 50% of the number of patients required in a fixed sample size design). The following type I error rates and decision boundaries for the interim and the final analysis are specified:\n\n\n• Overall one-sided type I error rate: 0.025\n\n• Boundary for the one-sided p-value of the first stage for accepting the null-hypothesis within the interim analysis: α0=0.5\n• One-sided local type I error rate for testing the null-hypothesis within the interim analysis: α1=0.0102\n• Boundary for the product of the one-sided p-values of both stages for the rejection of the null-hypothesis in the final analysis: cα=0.0038\nIf the trial will be continued with a second stage after the interim analysis (this is possible if for the one-sided p-value p1 of the interim analysis p1∈]0.0102,0.5[ [ie 0.5≥P1≥0.0102] holds true, the results of the interim analysis can be taken into account for a recalculation of the required sample size. If the sample size recalculation leads to the conclusion that more than 1200 patients are required, the study is stopped, because the related treatment group difference is judged to be of minor clinical importance…\n\nThe actually achieved sample size is then not fixed but random, and a variety of scenarios can be considered. If the sample size is calculated under the same assumptions with respect to the SSI rates for the two groups, applying the same the overall significance level of α=0.025 (one-sided) but employing additionally the defined stopping boundaries and recalculating the sample size for the second stage at a conditional power of 80% on the basis of the SSI rates observed in the interim analysis results in an average total sample size of n=766 patients; the overall power of the study is then 90% (ADDPLAN®, version 5.0)\n\nBack to top\n\n\n\n\n\n\n15. Recruitment\n\n\n\n\n\n\n\nStrategies for achieving adequate participant enrolment to reach target sample size\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe main goal of recruitment is to meet the target sample size (Item 14). However, recruitment difficulties are commonly encountered in clinical trials.209 210 211 212 213 For example, reviews of government funded trials in the US and UK found that two thirds did not reach their recruitment targets.214 215 Low enrolment will reduce statistical power and can lead to early trial stoppage or to extensions with delayed results and greater costs.\nStrategies to promote adequate enrolment are thus important to consider during trial planning. Recruitment strategies can vary depending on the trial topic, context, and site. Different recruitment methods can substantially affect the number and type of trial participants recruited128 209 216 217 218 219 220 and can incur different costs.221 222 223 Design issues such as the number and stringency of eligibility criteria will also directly affect the number of eligible trial participants.\nProtocol descriptions of where participants will be recruited (eg, primary care clinic, community), by whom (eg, surgeon), when (eg, time after diagnosis), and how (eg, advertisements, review of health records) can be helpful for assessing the feasibility of achieving the target sample size and the applicability of the trial results in practice. Other relevant information to explicitly provide in the protocol includes expected recruitment rates, duration of the recruitment period, plans to monitor recruitment during the trial, and any financial or non-financial incentives provided to trial investigators or participants for enrolment (Item 4). If strategies differ by site in multicentre trials, these should be detailed to the extent possible\n\n\nExamples\n\nEach center will screen subjects to achieve screening percentages of 50% women and 33% minority; screening will continue until the target population is achieved (12 subjects/site). We recognize that, because of exclusion by genotype and genotypic variation among diverse populations,[reference], the enrolled cohort may not reflect the screened population. The enrollment period will extend over 12 months.\n\n\nRecruitment strategy\n\nEach clinical center involved in the ACRN [Asthma Clinical Research Network] was chosen based on documentation for patient availability, among other things. It is, however, worthy to note the specific plans of each center…\n\n…The Asthma Clinical Research Center at the Brigham & Women’s Hospital utilizes three primary resources for identifying and recruiting potential subjects as described below.\n\n\n\nResearch Patient Database\n\n\nThe Asthma Clinical Research Center at the Brigham and Women’s Hospital has a database of over 1,500 asthmatics…\n\nAsthma Patient Lists…\nAdvertisements…\n\n…the Madison ACRN site has utilized some additional approaches to target minority recruitment. We have utilized a marketing expert to coordinate and oversee our overall efforts in recruiting and retaining minorities. . . . As a result of his efforts, we have advertised widely in newspapers and other publications that target ethnic minorities, established contacts with various ethnic community, university, church, and business groups, and conducted community-based asthma programs…For example, student groups such as AHANA (a pre-health careers organization focusing on minority concerns) will be contacted…In addition, we will utilize published examples of successful retention strategies such as frequent payment of subject honoraria as study landmarks are achieved and study participant group social events. Study visits will be carefully planned and scheduled to avoid exam-time and university calendar breaks…\n\nThe Harlem Hospital Center Emergency Department (ED) sees an average of eight adult patients per day for asthma. Through the REACH (Reducing Emergency Asthma Care in Harlem) project, we have…successfully recruited and interviewed 380 patients from the ED…\n\n\nResponses to inquiries about participation in research studies are answered by a dedicated phone line that is manned during business hours and answered by voicemail at all other times. A research assistant responds to each inquiry immediately, using a screening instrument…\n\n\nPatients are recruited for clinical trials at the Jefferson Center through two primary mechanisms: (1) local advertising; and (2) identification in the asthma patient registry (database). Local advertising takes advantage of the printed as well as the audio-visual media. Printed media include …All advertising in the printed and audio-visual media has prior approval of the Institutional Review Board.\n\n\nThe Jefferson patient registry (database) has been maintained since 1992 and currently contains 3,100 patients… It is estimated that 300-400 new asthmatic patients are seen each year, while a smaller number become inactive due to relocation, change of health care provider, etc. Once identified in the database, patients potentially eligible for a specific study are contacted by the nurse coordinator who explains the study and ascertains the patient’s interest. If interested, the patient is seen in the clinical research laboratories where more detailed evaluations are made…\n\n\nEach subject will receive financial compensation within FDA [Food and Drug Administration] guidelines for participation in an amount determined by the local center. For subjects who drop out, payments will be pro-rated for the length of time they stayed in the study, but payment will not be made until the study would have been completed had the subject not dropped out.\n\nBack to top\n\n\n\n\n\n\nMethods: Assignment of interventions (for controlled trials)\n\n\n16a. Allocation: sequence generation\n\n\n\n\n\n\n\nMethod of generating the allocation sequence (eg, computer-generated random numbers), and list of any factors for stratification. To reduce predictability of a random sequence, details of any planned restriction (eg, blocking) should be provided in a separate document that is unavailable to those who enrol participants or assign interventions\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nParticipants in a randomised trial should be assigned to study groups using a random (chance) process characterised by unpredictability of assignments. Randomisation decreases selection bias in allocation; helps to facilitate blinding/masking after allocation; and enables the use of probability theory to test whether any difference in outcome between intervention groups reflects chance.17 225 226 227\nUse of terms such as randomisation without further elaboration is not sufficient to describe the allocation process, as these terms have been used inappropriately to describe non-random, deterministic allocation methods such as alternation or allocation by date of birth.121 In general, these non-random allocation methods introduce selection bias and biased estimates of an intervention’s effect size,17 167 228 229 mainly due to the lack of allocation concealment (Item 16b). If non-random allocation is planned, then the specific method and rationale should be stated.\nBox 1 outlines the key elements of the random sequence that should be detailed in the protocol. Three quarters of randomised trial protocols approved by a research ethics committee in Denmark (1994-95) or conducted by a US cooperative cancer research group (1968-2006) did not describe the method of sequence generation.\n\n\nExamples\n\nParticipants will be randomly assigned to either control or experimental group with a 1:1 allocation as per a computer generated randomisation schedule stratified by site and the baseline score of the Action Arm Research Test (ARAT; <=21 versus >21) using permuted blocks of random sizes. The block sizes will not be disclosed, to ensure concealment.\n\nBack to top\n\n\n\n\n\n\n16b. Allocation concealment mechanism\n\n\n\n\n\n\n\nMechanism of implementing the allocation sequence (eg, central telephone; sequentially numbered, opaque, sealed envelopes), describing any steps to conceal the sequence until interventions are assigned\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nSuccessful randomisation in practice depends on two interrelated aspects: 1) generation of an unpredictable allocation sequence (Item 16a) and 2) concealment of that sequence until assignment irreversibly occurs.233 241 The allocation concealment mechanism aims to prevent participants and recruiters from knowing the study group to which the next participant will be assigned. Allocation concealment helps to ensure that a participant’s decision to provide informed consent, or a recruiter’s decision to enrol a participant, is not influenced by knowledge of the group to which they will be allocated if they join the trial.242 Allocation concealment should not be confused with blinding (masking) (Item 17)\nWithout adequate allocation concealment, even random, unpredictable assignment sequences can be subverted.233 241 For example, a common practice is to enclose assignments in sequentially numbered, sealed envelopes. However, if the envelopes are not opaque and contents are visible when held up to a light source, or if the envelopes can be unsealed and resealed, then this method of allocation concealment can be corrupted.\nProtocols should describe the planned allocation concealment mechanism in sufficient detail to enable assessment of its adequacy. In one study of randomised trial protocols in Denmark, over half did not adequately describe allocation concealment methods.2 In contrast, central randomisation was stated as the allocation concealment method in all phase III trial protocols initiated in 1968-2003 by a cooperative cancer research group that used extensive protocol review processes.11 Like sequence generation, inadequate reporting of allocation concealment in trial publications is common and has been associated with inflated effect size estimates.\n\n\nExamples\n\nParticipants will be randomised using TENALEA, which is an online, central randomisation service . . . Allocation concealment will be ensured, as the service will not release the randomisation code until the patient has been recruited into the trial, which takes place after all baseline measurements have been completed\n\nBack to top\n\n\n\n\n\n\n16c. Allocation: implementation\n\n\n\n\n\n\n\nWho will generate the allocation sequence, who will enrol participants, and who will assign participants to interventions\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nBased on the risk of bias associated with some methods of sequence generation and inadequate allocation concealment, trial investigators should strive for complete separation of the individuals involved in the steps before enrolment (sequence generation process and allocation concealment mechanism) from those involved in the implementation of study group assignments. When this separation is not possible, it is important for the investigators to ensure that the assignment schedule is unpredictable and locked away from even the person who generated it. The protocol should specify who will implement the various stages of the randomisation process, how and where the allocation list will be stored, and mechanisms employed to minimise the possibility that those enrolling and assigning participants will obtain access to the list.\n\n\nExamples\n\nRandomization\n\nAll patients who give consent for participation and who fulfil the inclusion criteria will be randomized. Randomisation will be requested by the staff member responsible for recruitment and clinical interviews from CenTrial [Coordination Centre of Clinical Trials].\n\nIn return, CenTrial will send an answer form to the study therapist who is not involved in assessing outcome of the study. This form will include a randomisation number. In every centre closed envelopes with printed randomisation numbers on it are available. For every randomisation number the corresponding code for the therapy group of the randomisation list will be found inside the envelopes. The therapist will open the envelope and will find the treatment condition to be conducted in this patient. The therapist then gives the information about treatment allocation to the patient. Staff responsible for recruitment and symptom ratings is not allowed to receive information about the group allocation…\n\n\n…The allocation sequence will be generated by the Institute for Medical Biometry (IMB) applying a permuted block design with random blocks stratified by study centre and medication compliance (favourable vs. unfavourable)… The block size will be concealed until the primary endpoint will be analysed. Throughout the study, the randomisation will be conducted by CenTrial in order to keep the data management and the statistician blind against the study condition as long as the data bank is open. The randomisation list remains with CenTrial for the whole duration of the study. Thus, randomisation will be conducted without any influence of the principal investigators, raters or therapists.\n\nBack to top\n\n\n\n\n\n\n17a. Blinding (masking)\n\n\n\n\n\n\n\nWho will be blinded after assignment to interventions (eg, trial participants, care providers, outcome assessors, data analysts), and how\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nBlinding or masking (the process of keeping the study group assignment hidden after allocation) is commonly used to reduce the risk of bias in clinical trials with two or more study groups.166 248 Awareness of the intervention assigned to participants can introduce ascertainment bias in the measurement of outcomes, particularly subjective ones (eg, quality of life)166 167; performance bias in the decision to discontinue or modify study interventions (eg, dosing changes) (Item 11b), concomitant interventions, or other aspects of care (Item 11d)229; and exclusion/attrition bias in the decision to withdraw from the trial or to exclude a participant from the analysis.249 250 We have elected to use the term blinding but acknowledge that others prefer the term masking because blind also relates to an ophthalmological condition and health outcomes.251 252\nMany groups can be blinded: trial participants, care providers, data collectors, outcome assessors or committees (Item 5d), data analysts,253 and manuscript writers. Blinding of data monitoring committees is generally discouraged.254 255\nWhen blinding of trial participants and care providers is not possible because of obvious differences between the interventions,256 257 blinding of the outcome assessors can often still be implemented.17 It may also be possible to blind participants or trial personnel to the study hypothesis in terms of which intervention is considered active. For example, in a trial evaluating light therapy for depression, participants were informed that the study involved testing two different forms of light therapy, whereas the true hypothesis was that bright blue light was considered potentially effective and that dim red light was considered placebo.258\nDespite its importance, blinding is often poorly described in trial protocols.3 The protocol should explicitly state who will be blinded to intervention groups—at a minimum, the blinding status of trial participants, care providers, and outcome assessors. Such a description is much preferred over the use of ambiguous terminology such as single blind or double blind.259 260 Protocols should also describe the comparability of blinded interventions (Item 11a)150—for example, similarities in appearance, use of specific flavours to mask a distinctive taste—and the timing of final unblinding of all trial participants (eg, after the creation of a locked analysis data set).3\nFurthermore, any strategies to reduce the potential for unblinding should be described in the protocol, such as pre-trial testing of blinding procedures.261 The use of a fixed code (versus a unique code for each participant) to denote each study group assignment (eg, A=Group 1; B=Group 2) can be problematic, as the unblinding of one participant will result in the inadvertent loss of blinding for all trial participants.\nSome have suggested that the success of blinding be formally tested by asking key trial persons to guess the study group assignment and comparing these responses to what would be expected by chance.262 However, it is unclear how best to interpret the results of such tests.263 264 If done, the planned testing methods should be described in the trial protocol.\n\n\nExamples\n\nAssessments regarding clinical recovery will be conducted by an assessor blind to treatment allocation. The assessor will go through a profound assessment training program… Due to the nature of the intervention neither participants nor staff can be blinded to allocation, but are strongly inculcated not to disclose the allocation status of the participant at the follow up assessments. An employee outside the research team will feed data into the computer in separate datasheets so that the researchers can analyse data without having access to information about the allocation.\n\nBack to top\n\n\n\n\n\n\n17b. Blinding (masking): emergency unblinding\n\n\n\n\n\n\n\nIf blinded, circumstances under which unblinding is permissible, and procedure for revealing a participant’s allocated intervention during the trial\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAmong 58 blinded Danish trials approved in 1994-95, three quarters of protocols described emergency unblinding procedures.3 Such procedures to reveal the assigned intervention in certain circumstances are intended to increase the safety of trial participants by informing the clinical management of harms or other relevant conditions that arise. A clear protocol description of the conditions and procedures for emergency unblinding helps to prevent unnecessary unblinding; facilitates implementation by trial personnel when indicated; and enables evaluation of the appropriateness of the planned procedures. In some cases (eg, minor, reversible harms), stopping and then cautiously reintroducing the assigned intervention in the affected participant can avoid both unblinding and further harm.\n\n\nExamples\n\nTo maintain the overall quality and legitimacy of the clinical trial, code breaks should occur only in exceptional circumstances when knowledge of the actual treatment is absolutely essential for further management of the patient. Investigators are encouraged to discuss with the Medical Advisor or PHRI [Population Health Research Institute] physician if he/she believes that unblinding is necessary.\n\n\nIf unblinding is deemed to be necessary, the investigator should use the system for emergency unblinding through the PHRI toll-free help line as the main system or through the local emergency number as the back-up system.\n\n\nThe Investigator is encouraged to maintain the blind as far as possible. The actual allocation must NOT be disclosed to the patient and/or other study personnel including other site personnel, monitors, corporate sponsors or project office staff; nor should there be any written or verbal disclosure of the code in any of the corresponding patient documents.\n\n\nThe Investigator must report all code breaks (with reason) as they occur on the corresponding CRF [case report form] page.\n\n\nUnblinding should not necessarily be a reason for study drug discontinuation.\n\nBack to top\n\n\n\n\n\n\nMethods: Data collection, management, and analysis\n\n\n18a. Data collection plan\n\n\n\n\n\n\n\nPlans for assessment and collection of outcome, baseline, and other trial data, including any related processes to promote data quality (eg, duplicate measurements, training of assessors) and a description of study instruments (eg, questionnaires, laboratory tests) along with their reliability and validity, if known. Reference to where data collection forms can be found, if not in the protocol\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe validity and reliability of trial data depend on the quality of the data collection methods. The processes of acquiring and recording data often benefit from attention to training of study personnel and use of standardised, pilot tested methods. These should be identical for all study groups, unless precluded by the nature of the intervention.\nThe choice of methods for outcome assessment can affect study conduct and results.268 269 270 271 272 273 Substantially different responses can be obtained for certain outcomes (eg, harms) depending on who answers the questions (eg, the participant or investigator) and how the questions are presented (eg, discrete options or open ended).269 274 275 276 Also, when compared to paper based data collection, the use of electronic handheld devices and internet websites has the potential to improve protocol adherence, data accuracy, user acceptability, and timeliness of receiving data.268 270 271 277\nThe quality of data also depends on the reliability, validity, and responsiveness of data collection instruments such as questionnaires278 or laboratory instruments. Instruments with low inter-rater reliability will reduce statistical power,272 while those with low validity will not accurately measure the intended outcome variable. One study found that only 35% (47/133) of randomised trials in acute stroke used a measure with established reliability or validity.279 Modified versions of validated measurement tools may no longer be considered validated, and use of unpublished measurement scales can introduce bias and inflate treatment effect sizes.280\nStandard processes should be implemented by local study personnel to enhance data quality and reduce bias by detecting and reducing the amount of missing or incomplete data, inaccuracies, and excessive variability in measurements.281 282 283 284 285 Examples include standardised training and testing of outcome assessors to promote consistency; tests of the validity or reliability of study instruments; and duplicate data measurements.\nA clear protocol description of the data collection process—including the personnel, methods, instruments, and measures to promote data quality—can facilitate implementation and helps protocol reviewers to assess their appropriateness. Inclusion of data collection forms in the protocol (ie, as appendices) is highly recommended, as the way in which data are obtained can substantially affect the results. If not included in the protocol, then a reference to where the forms can be found should be provided. If performed, pilot testing and assessment of reliability and validity of the forms should also be described.\n\n\nExamples\n\nPrimary outcome\n\nDelirium recognition: In accordance with national guidelines [reference], the study will identify delirium by using the RASS [Richmond Agitation-Sedation Scale] and the CAM-ICU [Confusion Assessment Method for the intensive care unit] on all patients who are admitted directly from the emergency room or transferred from other services to the ICU. Such assessment will be performed after 24 hours of ICU admission and twice daily until discharge from the hospital . . . RASS has excellent inter-rater reliability among adult medical and surgical ICU patients and has excellent validity when compared to a visual analogue scale and other selected sedation scales[reference] . . . The CAM-ICU was chosen because of its practical use in the ICU wards, its acceptable psychometric properties, and based on the recommendation of national guidelines[reference] . . . The CAM-ICU diagnosis of delirium was validated against the DSM-III-R [Diagnostic and Statistical Manual of Mental Disorders, Third Edition—Revised] delirium criteria determined by a psychiatrist and found to have a sensitivity of 97% and a specificity of 92%.[reference] The CAM-ICU has been developed, validated and applied into ICU settings and multiple investigators have used the same method to identify patients with delirium.[reference]\n\nDelirium severity: Since the CAM-ICU does not evaluate delirium severity, we selected the Delirium Rating Scale revised-1998 (DRS-R-98)[reference] . . . The DRS-R-98 was designed to evaluate the breadth of delirium symptoms for phenomenological studies in addition to measuring symptom severity with high sensitivity and specificity . . . The DRS-R-98 is a 16-item clinician-rated scale with anchored items descriptions . . . The DRS-R-98 has excellent inter-rater reliability (intra-class correlation 0.97) and internal consistency (Cronbach’s alpha 0.94).[reference]\n\n\nSecondary outcomes\n\nThe study will collect demographic and baseline functional information from the patient’s legally authorized representative and/or caregivers. Cognitive function status will be obtained by interviewing the patient’s legally authorized representative using the Informant Questionnaire on Cognitive Decline in the Elderly (IQCODE). IQCODE is a questionnaire that can be completed by a relative or other caregiver to determine whether that person has declined in cognitive functioning. The IQCODE lists 26 everyday situations . . . Each situation is rated by the informant for amount of change over the previous 10 years, using a Likert scale ranging from 1-much improved to 5-much worse. The IQCODE has a sensitivity between 69% to 100% and specificity of 80% to 96% for dementia.[reference]\n\nUtilizing the electronic medical record system (RMRS), we will collect several data points of interest at baseline and throughout the study period . . . We have previously defined hospital-related consequences to include: the number of patients with documented falls, use of physical restraints . . . These will be assessed using the RMRS, direct daily observation, and retrospective review of the electronic medical record. This definition of delirium related hospital complications has been previously used and published.[reference]\n\n\nTraining and certification plans\n\n. . . Each center’s personnel will be trained centrally in the study requirements, standardized measurement of height, weight, and blood pressure, requirements for laboratory specimen collection including morning urine samples, counseling for adherence and the eliciting of information from study participants in a uniform reproducible manner.\n\n…The data to be collected and the procedures to be conducted at each visit will be reviewed in detail. Each of the data collection forms and the nature of the required information will be discussed in detail on an item by item basis. Coordinators will learn how to code medications using the WHODrug software and how to code symptoms using the MedDRA software. Entering data forms, responding to data discrepancy queries and general information about obtaining research quality data will also be covered during the training session.\n\n…\n13.7. Quality Control of the Core Lab\nData from the Core Lab will be securely transmitted in batches and quality controlled in the same manner as Core Coordinating Center data; ie data will be entered and verified in the database on the Cleveland Clinic Foundation SUN with a subset later selected for additional quality control. Appropriate edit checks will be in place at the key entry (database) level.\n\nThe Core Lab is to have an internal quality control system established prior to analyzing any FSGS [focal segmental glomerulosclerosis] samples. This system will be outlined in the Manual of Operations for the Core Lab(s) which is prepared and submitted by the Core Lab to the DCC [data coordinating centre] prior to initiating of the study.\n\n\nAt a minimum this system must include:\n\n\nThe inclusion of at least two known quality control samples; the reported measurements of the quality control samples must fall within specified ranges in order to be certified as acceptable.\nCalibration at FDA approved manufacturers’ recommended schedules.\n\n\n13.8. Quality Control of the Biopsy Committee\n\nThe chair of the pathology committee will circulate to all of the study pathologists . . . samples [sic] biopsy specimens for evaluation after criteria to establish diagnosis of FSGS has been agreed. This internal review process will serve to ensure common criteria and assessment of biopsy specimens for confirmation of diagnosis of FSGS.\nBack to top\n\n\n\n\n\n\n18b. Data collection plan: retention\n\n\n\n\n\n\n\nPlans to promote participant retention and complete follow-up, including list of any outcome data to be collected for participants who discontinue or deviate from intervention protocols\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nTrial investigators must often seek a balance between achieving a sufficiently long follow-up for clinically relevant outcome measurement,122 288 and a sufficiently short follow-up to decrease attrition and maximise completeness of data collection. Non-retention refers to instances where participants are prematurely off-study (ie, consent withdrawn or lost to follow-up) and thus outcome data cannot be obtained from them. The majority of trials will have some degree of non-retention, and the number of these off-study participants usually increases with the length of follow-up.116\nIt is desirable to plan ahead for how retention will be promoted in order to prevent missing data and avoid the associated complexities in both the study analysis (Item 20c) and interpretation. Certain methods can improve participant retention,67 152 289 290 291 292 such as financial reimbursement; systematic methods and reminders for contacting patients, scheduling appointments, and monitoring retention; and limiting participant burden related to follow-up visits and procedures (Item 13). A participant who withdraws consent for follow-up assessment of one outcome may be willing to continue with assessments for other outcomes, if given the option.\nNon-retention should be distinguished from non-adherence.293 Non-adherence refers to deviation from intervention protocols (Item 11c) or from the follow-up schedule of assessments (Item 13), but does not mean that the participant is off-study and no longer in the trial. Because missing data can be a major threat to trial validity and statistical power, non-adherence should not be an automatic reason for ceasing to collect data from the trial participant prior to study completion. In particular for randomised trials, it is widely recommended that all participants be included in an intention to treat analysis, regardless of adherence (Item 20c).\nProtocols should describe any retention strategies and define which outcome data will be recorded from protocol non-adherers.152 Protocols should also detail any plans to record the reasons for non-adherence (eg, discontinuation of intervention due to harms versus lack of efficacy) and non-retention (ie, consent withdrawn; lost to follow-up), as this information can influence the handling of missing data and interpretation of results.\n\n\nExamples\n\n5.2.2 Retention\n\n…As with recruitment, retention addresses all levels of participant.\n\nAt the parent and student level, study investigators and staff:\n\nProvide written feedback to all parents of participating students about the results of the health screenings . . .\nMaintain interest in the study through materials and mailings . . .\nSend letters to parents and students prior to the final data collection, reminding them of the upcoming data collection and the incentives the students will receive.\n\nAt the school level, study investigators and staff:\n\nProvide periodic communications via newsletters and presentations to inform the school officials/staff, students, and parents about type 2 diabetes, the current status of the study, and plans for the next phase, as well as to acknowledge their support.\n\n…Become a presence in the intervention schools to monitor and maintain consistency in implementation, . . . be as flexible as possible with study schedule and proactive in resolving conflicts with schools.\n\nProvide school administration and faculty with the schedule or grid showing how the intervention fits into the school calendar . . .\nSolicit support from parents, school officials/staff, and teachers . . .\nProvide periodic incentives for school staff and teachers.\nProvide monetary incentives for the schools that increase with each year of the study…\nTable 6 Excerpts from table showing compensation provided in study\nhttp://www.bmj.com/content/346/bmj.e7586.long\n\n5.4 Infant Evaluations in the Case of Treatment Discontinuation or Study Withdrawal\n\nAll randomized infants completing the 18-month evaluation schedule will have fulfilled the infant clinical and laboratory evaluation requirements for the study…\n\nAll randomized infants who are prematurely discontinued from study drug will be considered off study drug/on study and will follow the same schedule of events as those infants who continue study treatment except adherence assessment. All of these infants will be followed through 18 months as scheduled.\n\n\nRandomized infants prematurely discontinued from the study before the 6-month evaluation will have the following clinical and laboratory evaluations performed, if possible:…\n\n\nRoche Amplicor HIV-1 DNA PCR [polymerase chain reaction] and cell pellet storage\n\nPlasma for storage (for NVP [nevirapine] resistance, HIV-1 RNA PCR and NVP concentration)\n…\nRandomized infants prematurely discontinued from the study at any time after the 6-month evaluation will have the following clinical and laboratory evaluations performed, if possible:…\n\n5.5 Participant Retention\n\nOnce an infant is enrolled or randomized, the study site will make every reasonable effort to follow the infant for the entire study period . . . It is projected that the rate of loss-to-follow-up on an annual basis will be at most 5% . . . Study site staff are responsible for developing and implementing local standard operating procedures to achieve this level of follow-up.\n\n5.6 Participant Withdrawal\n\nParticipants may withdraw from the study for any reason at any time. The investigator also may withdraw participants from the study in order to protect their safety and/or if they are unwilling or unable to comply with required study procedures after consultation with the Protocol Chair, National Institutes of Health (NIH) Medical Officers, Statistical and Data Management Center (SDMC) Protocol Statistician, and Coordinating and Operations Center (CORE) Protocol Specialist.\n\nParticipants also may be withdrawn if the study sponsor or government or regulatory authorities terminate the study prior to its planned end date.\n\n\nNote: Early discontinuation of study product for any reason is not a reason for withdrawal from the study\n\nBack to top\n\n\n\n\n\n\n19. Data management\n\n\n\n\n\n\n\nPlans for data entry, coding, security, and storage, including any related processes to promote data quality (eg, double data entry; range checks for data values). Reference to where details of data management procedures can be found, if not in the protocol\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nCareful planning of data management with appropriate personnel can help to prevent flaws that compromise data validity. The protocol should provide a full description of the data entry and coding processes, along with measures to promote their quality, or provide key elements and a reference to where full information can be found. These details are particularly important for the primary outcome data. The protocol should also document data security measures to prevent unauthorised access to or loss of participant data, as well as plans for data storage (including timeframe) during and after the trial. This information facilitates an assessment of adherence to applicable standards and regulations.\nDifferences in data entry methods can affect the trial in terms of data accuracy,268 cost, and efficiency.271 For example, when compared with paper case report forms, electronic data capture can reduce the time required for data entry, query resolution, and database release by combining data entry with data collection (Item 18a).271 277 When data are collected on paper forms, data entry can be performed locally or at a central site. Local data entry can enable fast correction of missing or inaccurate data, while central data entry facilitates blinding (masking), standardisation, and training of a core group of data entry personnel.\nRaw, non-numeric data are usually coded for ease of data storage, review, tabulation, and analysis. It is important to define standard coding practices to reduce errors and observer variation. When data entry and coding are performed by different individuals, it is particularly important that the personnel use unambiguous, standardised terminology and abbreviations to avoid misinterpretation.\nAs with data collection (Item 18a), standard processes are often implemented to improve the accuracy of data entry and coding.281 284 Common examples include double data entry296; verification that the data are in the proper format (eg, integer) or within an expected range of values; and independent source document verification of a random subset of data to identify missing or apparently erroneous values. Though widely performed to detect data entry errors, the time and costs of independent double data entry from paper forms need to be weighed against the magnitude of reduction in error rates compared to single-data entry.297 298 299\n\n\nExamples\n\n13.9.2. Data Forms and Data Entry\n\nIn the FSGS-CT [focal segmental glomerulosclerosis—clinical trial], all data will be entered electronically. This may be done at a Core Coordinating Center or at the participating site where the data originated. Original study forms will be entered and kept on file at the participating site. A subset will be requested later for quality control; when a form is selected, the participating site staff will pull that form, copy it, and sent [sic] the copy to the DCC [data coordinating center] for re-entry.\n…Participant files are to be stored in numerical order and stored in a secure and accessible place and manner. Participant files will be maintained in storage for a period of 3 years after completion of the study.\n13.9.3. Data Transmission and Editing\nThe data entry screens will resemble the paper forms approved by the steering committee. Data integrity will be enforced through a variety of mechanisms. Referential data rules, valid values, range checks, and consistency checks against data already stored in the database (ie, longitudinal checks) will be supported. The option to chose [sic] a value from a list of valid codes and a description of what each code means will be available where applicable. Checks will be applied at the time of data entry into a specific field and/or before the data is written (committed) to the database. Modifications to data written to the database will be documented through either the data change system or an inquiry system. Data entered into the database will be retrievable for viewing through the data entry applications. The type of activity that an individual user may undertake is regulated by the privileges associated with his/her user identification code and password.\n13.9.4. Data Discrepancy Inquiries and Reports to Core Coordinating Centers\nAdditional errors will be detected by programs designed to detect missing data or specific errors in the data. These errors will be summarized along with detailed descriptions for each specific problem in Data Query Reports, which will be sent to the Data Managers at the Core Coordinating Centers…\n…The Data Manager who receives the inquiry will respond by checking the original forms for inconsistency, checking other sources to determine the correction, modifying the original (paper) form entering a response to the query. Note that it will be necessary for Data Managers to respond to each inquiry received in order to obtain closure on the queried item.\n\nThe Core Coordinating Center and participating site personnel will be responsible for making appropriate corrections to the original paper forms whenever any data item is changed . . . Written documentation of changes will be available via electronic logs and audit trails.\n\n…\nBiopsy and biochemistry reports will be sent via e-mail when data are received from the Core Lab.\n…\n13.9.5. Security and Back-Up of Data\n… All forms, diskettes and tapes related to study data will be kept in locked cabinets. Access to the study data will be restricted. In addition, Core Coordinating Centers will only have access to their own center’s data. A password system will be utilized to control access . . . These passwords will be changed on a regular basis. All reports prepared by the DCC will be prepared such that no individual subject can be identified.\n\nA complete back up of the primary DCC database will be performed twice a month. These tapes will be stored off-site in a climate-controlled facility and will be retained indefinitely. Incremental data back-ups will be performed on a daily basis. These tapes will be retained for at least one week on-site. Back-ups of periodic data analysis files will also be kept. These tapes will be retained at the off-site location until the Study is completed and the database is on file with NIH [National Institutes of Health]. In addition to the system back-ups, additional measures will be taken to back-up and export the database on a regular basis at the database management level…\n\n\n13.9.6. Study status reports\n\nThe DCC will send weekly email reports with information on missing data, missing forms, and missing visits. Personnel at the Core Coordinating Center and the Participating Sites should review these reports for accuracy and report any discrepancies to the DCC…\n…\n13.9.8. Description of Hardware at DCC\nA SUN Workstation environment is maintained in the department with a SUN SPARCstation 10 model 41 as the server . . . Primary access to the departments [sic] computing facilities will be through the Internet . . . For maximum programming efficiency, the Oracle database management system and the SAS and BMDP statistical analysis systems will be employed for this study. . . .\n\nOracle facilitates sophisticated integrity checks through a variety of mechanisms including stored procedures, stored triggers, and declarative database integrity—for between table verifications. Oracle allows data checks to be programmed once in the database rather than repeating the same checks among many applications . . . Security is enforced through passwords and may be assigned at different levels to groups and individuals.\n\nBack to top\n\n\n\n\n\n\n20a. Statistics: outcomes\n\n\n\n\n\n\n\nStatistical methods for analysing primary and secondary outcomes. Reference to where other details of the statistical analysis plan can be found, if not in the protocol\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe protocol should indicate explicitly each intended analysis comparing study groups. An unambiguous, complete, and transparent description of statistical methods facilitates execution, replication, critical appraisal, and the ability to track any changes from the original pre-specified methods.\nResults for the primary outcome can be substantially affected by the choice of analysis methods. When investigators apply more than one analysis strategy for a specific primary outcome, there is potential for inappropriate selective reporting of the most interesting result.6 The protocol should prespecify the main (primary) analysis of the primary outcome (Item 12), including the analysis methods to be used for statistical comparisons (Items 20a and 20b); precisely which trial participants will be included (Item 20c); and how missing data will be handled (Item 20c). Additionally, it is helpful to indicate the effect measure (eg, relative risk) and significance level that will be used, as well as the intended use of confidence intervals when presenting results.\nThe same considerations will often apply equally to prespecified secondary and exploratory outcomes. In some instances, descriptive approaches to evaluating rare outcomes such as adverse events—might be preferred over formal analysis given the lack of power.300 Adequately powered analyses may require preplanned meta-analyses with results from other studies.\nMost trials are affected to some extent by multiplicity issues.301 302 When multiple statistical comparisons are performed (eg, multiple study groups, outcomes, interim analyses), the risk of false positive (type 1) error is inflated and there is increased potential for selective reporting of favourable comparisons in the final trial report. For trials with more than two study groups, it is important to specify in the protocol which comparisons (of two or more study groups) will be performed and, if relevant, which will be the main comparison of interest. The same principle of specifying the main comparison also applies when there is more than one outcome, including when the same variable is measured at several time points (Item 12). Any statistical approaches to account for multiple comparisons and time points should also be described.\nFinally, different trial designs dictate the most appropriate analysis plan and any additional relevant information that should be included in the protocol. For example, cluster, factorial, crossover, and within-person randomised trials require specific statistical considerations, such as how clustering will be handled in a cluster randomised trial.\n\n\nExamples\n\nThe intervention arm (SMS [short message system (text message)]) will be compared against the control (SOC [standard of care]) for all primary analysis. We will use chi-squared test for binary outcomes, and T-test for continuous outcomes. For subgroup analyses, we will use regression methods with appropriate interaction terms (respective subgroup×treatment group). Multivariable analyses will be based on logistic regression . . . for binary outcomes and linear regression for continuous outcomes. We will examine the residual to assess model assumptions and goodness-of-fit. For timed endpoints such as mortality we will use the Kaplan-Meier survival analysis followed by multivariable Cox proportional hazards model for adjusting for baseline variables. We will calculate Relative Risk (RR) and RR Reductions (RRR) with corresponding 95% confidence intervals to compare dichotomous variables, and difference in means will be used for additional analysis of continuous variables. P-values will be reported to four decimal places with p-values less than 0.001 reported as p < 0.001. Up-to-date versions of SAS (Cary, NC) and SPSS (Chicago, IL) will be used to conduct analyses. For all tests, we will use 2-sided p-values with alpha≤0.05 level of significance. We will use the Bonferroni method to appropriately adjust the overall level of significance for multiple primary outcomes, and secondary outcomes.\n\n\nTo assess the impact of potential clustering for patients cared by the same clinic, we will use generalized estimating equations [GEE] assuming an exchangeable correlation structure. Table [7] provides a summary of methods of analysis for each variable. Professional academic statisticians (LT, RN) blinded to study groups will conduct all analyses.\n\nTable 7 Variables, measures, and methods of analysis (reproduced from original table http://www.bmj.com/content/346/bmj.e7586.long\nBack to top\n\n\n\n\n\n\n20b. Statistics: additional analyses\n\n\n\n\n\n\n\nMethods for any additional analyses (eg, subgroup and adjusted analyses)\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nSubgroup analysis\nSubgroup analyses explore whether estimated treatment effects vary significantly between subcategories of trial participants. As these data can help tailor healthcare decisions to individual patients, a modest number of prespecified subgroup analyses can be sensible.\nHowever, subgroup analyses are problematic if they are inappropriately conducted or selectively reported. Subgroup analyses described in protocols or grant applications do not match those reported in subsequent publications for more than two thirds of randomised trials, suggesting that subgroup analyses are often selectively reported or not prespecified.6 7 305 Post hoc (data driven) analyses have a high risk of spurious findings and are discouraged.306 Conducting a large number of subgroup comparisons leads to issues of multiplicity, even when all of the comparisons have been pre-specified. Furthermore, when subgroups are based on variables measured after randomisation, the analyses are particularly susceptible to bias.307\nPreplanned subgroup analyses should be clearly specified in the protocol with respect to the precise baseline variables to be examined, the definition of the subgroup categories (including cut-off boundaries for continuous or ordinal variables), the statistical method to be used, and the hypothesised direction of the subgroup effect based on plausibility.308 309\nAdjusted analysis\nSome trials prespecify adjusted analyses to account for imbalances between study groups (eg, chance imbalance across study groups in small trials), improve power, or account for a known prognostic variable. Adjustment is often recommended for any variables used in the allocation process (eg, in stratified randomisation), on the principle that the analysis strategy should match the design.310 Most trial protocols and publications do not adequately address issues of adjustment, particularly the description of variables.6 310\nIt is important that trial investigators indicate in the protocol if there is an intention to perform or consider adjusted analyses, explicitly specifying any variables for adjustment and how continuous variables will be handled. When both unadjusted and adjusted analyses are intended, the main analysis should be identified (Item 20a). It may not always be clear, in advance, which variables will be important for adjustment. In such situations, the objective criteria to be used to select variables should be prespecified. As with subgroup analyses, adjustment variables based on post-randomisation data rather than baseline data can introduce bias.311 312\n\n\nExamples\n\nWe plan to conduct two subgroup analyses, both with strong biological rationale and possible interaction effects. The first will compare hazard ratios of re-operation based upon the degree of soft tissue injury (Gustilo-Anderson Type I/II open fractures vs. Gustilo-Anderson Type IIIA/B open fractures). The second will compare hazard ratios of re-operation between fractures of the upper and lower extremity. We will test if the treatment effects differ with fracture types and extremities by putting their main effect and interaction terms in the Cox regression. For the comparison of pressure, we anticipate that the low/gravity flow will be more effective in the Type IIIA-B open fracture than in the Type I/II open fracture, and be more effective in the upper extremity than the lower extremity. For the comparison of solution, we anticipate that soap will do better in the Type IIIA-B open fracture than in the Type I/II open fracture, and better in the upper extremity than the lower extremity.\n\n\nA secondary analysis of the primary endpoint will adjust for those pre-randomization variables which might reasonably be expected to be predictive of favorable outcomes. Generalized linear models will be used to model the proportion of subjects with neurologically intact (MRS ≤ 3 [Modified Rankin Score]) survival to hospital discharge by ITD [impedance threshold device]/sham device group adjusted for site (dummy variables modeling the 11 ROC [Resuscitation Outcomes Consortium] sites), patient sex, patient age (continuous variable), witness status (dummy variables modeling the three categories of unwitnessed arrest, non-EMS [emergency medical services] witnessed arrest, and EMS witnessed arrest), location of arrest (public versus non-public), time or response (continuous variable modeling minutes between call to 911 and arrival of EMS providers on scene), presenting rhythm (dummy variables modeling asystole, PEA [pulseless electrical activity], VT/VF [ventricular tachycardia/fibrillation], or unknown), and treatment assignment in the Analyze Late vs. Analyze Early intervention. The test statistic used to assess any benefit of the ITD relative to the sham device will be computed as the generalized linear model regression coefficient divided by the estimated robust standard error based on the Huber- White sandwich estimator[reference] in order to account for within group variability which might depart from the classical assumptions. Statistical inference will be based on one-sided P values and 95% confidence intervals which adjust for the stopping rule used for the primary analysis.\n\nBack to top\n\n\n\n\n\n\n20c. Statistics: analysis population and missing data\n\n\n\n\n\n\n\nDefinition of analysis population relating to protocol non-adherence (eg, as randomised analysis), and any statistical methods to handle missing data (eg, multiple imputation)\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nIn order to preserve the unique benefit of randomisation as a mechanism to avoid selection bias, an as randomised analysis retains participants in the group to which they were originally allocated. To prevent attrition bias, outcome data obtained from all participants are included in the data analysis, regardless of protocol adherence (Items 11c and 18b).249 250 These two conditions (ie, all participants, as randomised) define an intention to treat analysis, which is widely recommended as the preferred analysis strategy.17\nSome trialists use other types of data analyses (commonly labelled as modified intention to treat or per protocol) that exclude data from certain participants—such as those who are found to be ineligible after randomisation or who deviate from the intervention or follow-up protocols. This exclusion of data from protocol non-adherers can introduce bias, particularly if the frequency of and the reasons for non-adherence vary between the study groups.314 315 In some trials, the participants to be included in the analysis will vary by outcome—for example, analysis of harms (adverse events) is sometimes restricted to participants who received the intervention, so that absence or occurrence of harm is not attributed to a treatment that was never received.\nProtocols should explicitly describe which participants will be included in the main analyses (eg, all randomised participants, regardless of protocol adherence) and define the study group in which they will be analysed (eg, as randomised). In one cohort of randomised trials approved in 1994-5, this information was missing in half of the protocols.6 The ambiguous use of labels such as intention to treat or per protocol should be avoided unless they are fully defined in the protocol.6 314 Most analyses labelled as intention to treat do not actually adhere to its definition because of missing data or exclusion of participants who do not meet certain post-randomisation criteria (eg, specific level of adherence to intervention).6 316 Other ambiguous labels such as modified intention to treat are also variably defined from one trial to another.314\nIn addition to defining the analysis population, it is necessary to address the problem of missing data in the protocol. Most trials have some degree of missing data,317 318 which can introduce bias depending on the pattern of missingness (eg, not missing at random). Strategies to maximise follow-up and prevent missing data, as well as the recording of reasons for missing data, are thus important to develop and document (Item 18b).152\nThe protocol should also state how missing data will be handled in the analysis and detail any planned methods to impute (estimate) missing outcome data, including which variables will be used in the imputation process (if applicable).152 Different statistical approaches can lead to different results and conclusions,317 319 but one study found that only 23% of trial protocols specified the planned statistical methods to account for missing data.6\nImputation of missing data allows the analysis to conform to intention to treat analysis but requires strong assumptions that are untestable and may be hard to justify.152 318 320 321 Methods of multiple imputation are more complex but are widely preferred to single imputation methods (eg, last observation carried forward; baseline observation carried forward), as the latter introduce greater bias and produce confidence intervals that are too narrow.152 320 321 322 Specific issues arise when outcome data are missing for crossover or cluster randomised trials.323 Finally, sensitivity analyses are highly recommended to assess the robustness of trial results under different methods of handling missing data.152 324\n\n\nExamples\n\nNevertheless, we propose to test non-inferiority using two analysis sets; the intention-to-treat set, considering all patients as randomized regardless of whether they received the randomized treatment, and the per protocol analysis set. Criteria for determining the per protocol group assignment would be established by the Steering Committee and approved by the PSMB [performance and safety monitoring board] before the trial begins. Given our expectation that very few patients will crossover or be lost to follow-up, these analyses should agree very closely. We propose declaring medical management non-inferior to interventional therapy, only if shown to be non-inferior using both the intention to treat and per protocol analysis sets.\n\n…\n10.4.7 Imputation Procedure for Missing Data\nWhile the analysis of the primary endpoint (death or stroke) will be based on a log-rank test and, therefore, not affected by patient withdrawals (as they will be censored) provided that dropping out is unrelated to prognosis; other outcomes, such as the Rankin Score at five years post-randomization, could be missing for patients who withdraw from the trial. We will report reasons for withdrawal for each randomization group and compare the reasons qualitatively . . . The effect that any missing data might have on results will be assessed via sensitivity analysis of augmented data sets. Dropouts (essentially, participants who withdraw consent for continued follow-up) will be included in the analysis by modern imputation methods for missing data.\n\nThe main feature of the approach is the creation of a set of clinically reasonable imputations for the respective outcome for each dropout. This will be accomplished using a set of repeated imputations created by predictive models based on the majority of participants with complete data. The imputation models will reflect uncertainty in the modeling process and inherent variability in patient outcomes, as reflected in the complete data.\n\n\nAfter the imputations are completed, all of the data (complete and imputed) will be combined and the analysis performed for each imputed-and-completed dataset. Rubin’s method of multiple (ie, repeated) imputation will be used to estimate treatment effect. We propose to use 15 datasets (an odd number to allow use of one of the datasets to represent the median analytic result).\n\n\nThese methods are preferable to simple mean imputation, or simple best-worst or worst-worst imputation, because the categorization of patients into clinically meaningful subgroups, and the imputation of their missing data by appropriately different models, accords well with best clinical judgment concerning the likely outcomes of the dropouts, and therefore will enhance the trial’s results.\n\nBack to top\n\n\n\n\n\n\nMethods: Monitoring\n\n\n21a. Data monitoring: formal committee\n\n\n\n\n\n\n\nComposition of data monitoring committee (DMC); summary of its role and reporting structure; statement of whether it is independent from the sponsor and competing interests; and reference to where further details about its charter can be found, if not in the protocol. Alternatively, an explanation of why a DMC is not needed\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nFor some trials, there are important reasons for periodic inspection of the accumulating outcome data by study group. In principle, a trial should be modified or discontinued when the accumulated data have sufficiently disturbed the clinical equipoise that justified the initiation of the trial. Data monitoring can also inform aspects of trial conduct, such as recruitment, and identify the need to make adjustments.\nThe decision to have a data monitoring committee (DMC) will be influenced by local standards. While certain trials warrant some form of data monitoring, many do not need a formal committee,326 such as trials with a short duration or known minimal risks. A DMC was described in 65% (98/150) of cancer trial protocols with time-to-event outcomes in Italy in 2000-5,327 and in 17% (12/70) of protocols for Danish randomised trials approved in 1994-5.6 About 40% of clinical trials registered on ClinicalTrials.gov from 2007-2010 reported having a DMC.328 The protocol should either state that there will be a DMC and provide further details, as discussed below, or indicate that there will not be a DMC, preferably with reasons.\nWhen formal data monitoring is performed, it is often done by a DMC consisting of members from a variety of disciplines.254 329 The primary role of a DMC is to periodically review the accumulating data and determine if a trial should be modified or discontinued. The DMC does not usually have executive power; rather, it communicates the outcome of its deliberations to the trial steering committee or sponsor.\nIndependence, in particular from the sponsor and trial investigators, is a key characteristic of the DMC and can be broadly defined as the committee comprising members who are completely uninvolved in the running of the trial and who cannot be unfairly influenced (either directly or indirectly) by people, or institutions, involved in the trial.254 DMC members are usually required to declare any competing interests (Item 28). Among the 12 trial protocols that described a DMC and were approved in Denmark in 1994-5,6 four explicitly stated that the DMC was independent from the sponsor and investigators; three had non-independent DMCs; and independence was unclear for the remaining five protocols.\nThe protocol should name the chair and members of the DMC. If the members are not yet known, the protocol can indicate the intended size and characteristics of the membership until further details are available. The protocol should also indicate the DMC’s roles and responsibilities, planned method of functioning, and degree of independence from those conducting, sponsoring, or funding the trial.254 330 331 A charter is recommended for detailing this information331; if this charter is not appended to the protocol, the protocol should indicate whether a charter exists or will be developed, and if so, where it can be accessed.\n\n\nExamples\n\nAppendix 3. Charter and responsibilities of the Data Monitoring Committee\n\nA Data Monitoring Committee (DMC) has been established. The DMC is independent of the study organisers. During the period of recruitment to the study, interim analyses will be supplied, in strict confidence, to the DMC, together with any other analyses that the committee may request. This may include analyses of data from other comparable trials. In the light of these interim analyses, the DMC will advise the TSC [trial steering committee] if, in its view:\n\n\nthe active intervention has been proved, beyond reasonable doubt*, to be different from the control (standard management) for all or some types of participants, and\n\n\n\nthe evidence on the economic outcomes is sufficient to guide a decision from health care providers regarding recommendation of early lens extraction for PACG [primary angle closure glaucoma].\n\nThe TSC can then decide whether or not to modify intake to the trial. Unless this happens, however, the TSC, PMG [project management group], clinical collaborators and study office staff (except those who supply the confidential analyses) will remain ignorant of the interim results.\nThe frequency of interim analyses will depend on the judgement of the Chair of the DMC, in consultation with the TSC. However, we anticipate that there might be three interim analyses and one final analysis.\n\nThe Chair is Mr D.G.-H., with Dr D.C., and Professor B.D. Terms of reference for the DMC are available on request from the EAGLE [Effectiveness in Angle Closure Glaucoma of Lens Extraction] study office.\n\n\n*Appropriate criteria for proof beyond reasonable doubt cannot be specified precisely. A difference of at least three standard deviation [sic] in the interim analysis of a major endpoint may be needed to justify halting, or modifying, such a study prematurely.[reference]\n\nBack to top\n\n\n\n\n\n\n21b. Data monitoring: interim analysis\n\n\n\n\n\n\n\nDescription of any interim analyses and stopping guidelines, including who will have access to these interim results and make the final decision to terminate the trial\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nInterim analyses can be conducted as part of an adaptive trial design to formally monitor the accumulating data in clinical trials. They are generally performed in trials that have a DMC, longer duration of recruitment, and potentially serious outcomes. Interim analyses were described in 71% (106/150) of cancer trial protocols with time-to-event outcomes in Italy in 2000-5,327 and in 19% (13/70) of protocols for Danish randomised trials approved in 1994-5.6 The results of these analyses, along with non-statistical criteria, can be part of a stopping guideline that helps inform whether the trial should be continued, modified, or halted earlier than intended for benefit, harm, or futility. Criteria for stopping for harm are often different from those for benefit and might not employ a formal statistical criterion.333 Stopping for futility occurs in instances where, if the study were to continue, it is unlikely that an important effect would be seen (ie, low chance of rejecting null hypothesis). Multiple analyses of the accumulating data increase the risk of a false positive (type I) error, and various statistical strategies have been developed to compensate for this inflated risk.254 333 334 335 Aside from informing stopping guidelines, prespecified interim analyses can be used for other trial adaptations such as sample size re-estimation, alteration to the proportion of participants allocated to each study group, and changes to eligibility criteria.111\nA complete description of any interim analysis plan, even if it is only to be performed at the request of an oversight body (eg, DMC), should be provided in the protocol—including the statistical methods, who will perform the analyses, and when they will be conducted (timing and indications). If applicable, details should also be provided about the decision criteria—statistical or other—that will be adopted to judge the interim results as part of a guideline for early stopping or other adaptations. Among 86 protocols for randomised trials with a time-to-event cancer outcome that proposed efficacy interim analyses, all stated the planned timing of the analyses, 91% specified the overall reason to be used for stopping (eg, superiority, futility), and 94% detailed the statistical approach.327\nIn addition, it is important to state who will see the outcome data while the trial is ongoing, whether these individuals will remain blinded (masked) to study groups, and how the integrity of the trial implementation will be protected (eg, maintaining blinding) when any adaptations to the trial are made. A third of protocols for industry initiated randomised trials receiving Danish ethics approval in 1994-95 stated that the sponsor had access to accumulating trial data, which can introduce potential bias due to competing interests.10 Finally, the protocol should specify who has the ultimate authority to stop or modify the trial—eg, the principal investigator, trial steering committee, or sponsor.\n\n\nExamples\n\nPremature termination of the study\n\nAn interim-analysis is performed on the primary endpoint when 50% of patients have been randomised and have completed the 6 months follow-up. The interim-analysis is performed by an independent statistician, blinded for the treatment allocation. The statistician will report to the independent DSMC [data and safety monitoring committee]. The DSMC will have unblinded access to all data and will discuss the results of the interim-analysis with the steering committee in a joint meeting. The steering committee decides on the continuation of the trial and will report to the central ethics committee. The Peto approach is used: the trial will be ended using symmetric stopping boundaries at P < 0.001 [reference]. The trial will not be stopped in case of futility, unless the DSMC during the course of safety monitoring advices [sic] otherwise. In this case DSMC will discuss potential stopping for futility with the trial steering committee.\nBack to top\n\n\n\n\n\n\n22. Harms\n\n\n\n\n\n\n\nPlans for collecting, assessing, reporting, and managing solicited and spontaneously reported adverse events and other unintended effects of trial interventions or trial conduct\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nEvaluation of harms has a key role in monitoring the condition of participants during a trial and in enabling appropriate management of adverse events. Documentation of trial related adverse events also informs clinical practice and the conduct of ongoing and future studies. We use the term harms instead of safety to better reflect the negative effects of interventions.300 An adverse event refers to an untoward occurrence during the trial, which may or may not be causally related to the intervention or other aspects of trial participation.300 336 This definition includes unfavourable changes in symptoms, signs, laboratory values, or health conditions. In the context of clinical trials, it can be difficult to attribute causation for a given adverse event. An adverse effect is a type of adverse event that can be attributed to the intervention.\nHarms can be specified as primary or secondary outcomes (Item 12) or can be assessed as part of routine monitoring. To the extent possible, distinctions should be made between adverse events that are anticipated versus unanticipated, and solicited versus unsolicited, because expectation can influence the number and perceived severity of recorded events. For example, providing statements in the informed consent process about the possibility of a particular adverse effect or using structured, as opposed to open ended, questionnaires for data collection, can increase the reporting of specific events (priming).269 337 338 339 The timeframe for recording adverse events can also affect the type of data obtained.340 341\nThe protocol should describe the procedures for and frequency of harms data collection, the overall surveillance timeframe, any instruments to be used, and their validity and reliability, if known. Substantial discrepancies have been observed between protocol specified plans for adverse event collection and reporting, and what is described in final publications.5 Although trials are often not powered to detect important differences in rates of uncommon adverse events, it is also important to describe plans for data analysis, including formal hypothesis testing or descriptive statistics.300 342\nFinally, the protocol should address the reporting of harms to relevant groups (eg, sponsor, research ethics committee/institutional review board, data monitoring committee, regulatory agency), which is an important process that is subject to local regulation.343 Key considerations include the severity of the adverse event, determination of potential causality, and whether it represents an unexpected or anticipated event. For multicentre studies, procedures and timing should be outlined for central collection, evaluation, and reporting of pooled harms data.\n\n\nExamples\n\nSecondary outcomes\n\n…In our study an adverse event will be defined as any untoward medical occurrence in a subject without regard to the possibility of a causal relationship. Adverse events will be collected after the subject has provided consent and enrolled in the study. If a subject experiences an adverse event after the informed consent document is signed (entry) but the subject has not started to receive study intervention, the event will be reported as not related to study drug. All adverse events occurring after entry into the study and until hospital discharge will be recorded. An adverse event that meets the criteria for a serious adverse event (SAE) between study enrollment and hospital discharge will be reported to the local IRB [institutional review board] as an SAE. If haloperidol is discontinued as a result of an adverse event, study personnel will document the circumstances and data leading to discontinuation of treatment. A serious adverse event for this study is any untoward medical occurrence that is believed by the investigators to be causally related to study-drug and results in any of the following: Life-threatening condition (that is, immediate risk of death); severe or permanent disability, prolonged hospitalization, or a significant hazard as determined by the data safety monitoring board. Serious adverse events occurring after a subject is discontinued from the study will NOT be reported unless the investigators feels that the event may have been caused by the study drug or a protocol procedure. Investigators will determine relatedness of an event to study drug based on a temporal relationship to the study drug, as well as whether the event is unexpected or unexplained given the subject’s clinical course, previous medical conditions, and concomitant medications.\n…The study will monitor for the following movement-related adverse effects daily through patient examination and chart review: dystonia, akathisia, pseudoparkinsonism, akinesia, and neuroleptic malignant syndrome. Study personnel will use the Simpson-Angus [reference] and Barnes Akathisia [reference] scales to monitor movement-related effects.\n…\nFor secondary outcomes, binary measures, eg mortality and complications, logistic regression will be used to test the intervention effect, controlling for covariates when appropriate…\nBack to top\n\n\n\n\n\n\n23. Auditing\n\n\n\n\n\n\n\nFrequency and procedures for auditing trial conduct, if any, and whether the process will be independent from investigators and the sponsor\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAuditing involves periodic independent review of core trial processes and documents. It is distinct from routine day-to-day measures to promote data quality (Items 18a and 19). Auditing is intended to preserve the integrity of the trial by independently verifying a variety of processes and prompting corrective action if necessary. The processes reviewed can relate to participant enrolment, consent, eligibility, and allocation to study groups; adherence to trial interventions and policies to protect participants, including reporting of harms (Item 22); and completeness, accuracy, and timeliness of data collection. In addition, an audit can verify adherence to applicable policies such as the International Conference on Harmonisation Good Clinical Practice and regulatory agency guidelines.160\nIn multicentre trials, auditing is usually considered both overall and for each recruiting centre. Audits can be done by exploring the trial dataset or performing site visits. Audits might be initially conducted across all sites, and subsequently conducted using a risk based approach that focuses, for example, on sites that have the highest enrolment rates, large numbers of withdrawals, or atypical (low or high) numbers of reported adverse events.\nIf auditing is planned, the procedures and anticipated frequency should be outlined in the protocol, including a description of the personnel involved and their degree of independence from the trial investigators and sponsor. If procedures are further detailed elsewhere (eg, audit manual), then the protocol should reference where the full details can be obtained.\n\n\nExamples\n\n11.4 Data Monitoring and Quality Assurance\n\nThrough the combination of our web-based, instantaneous electronic validation, the DCC’s [data coordinating centre] daily visual cross-validation of the data for complex errors, and regular on-site monitoring, the quality and completeness of the data will be reflective of the state of the art in clinical trials.\n\nBoth the European and US DCCs will conduct monitoring of source documents via fax at all enrolling ARUBA [A Randomised trial of Unruptured Brain Arteriovenous malformations] sites and will conduct at least one on-site monitoring visit per year over the course of the study at 100% of clinical sites (with repeat visits to sites where performance is a concern). Monitoring of European study sites will be assured by the European Coordinating Center (Paris). The primary objectives of the DCC during the on-site visits are to educate, support and solve problems. The monitors will discuss the protocol in detail and identify and clarify any areas of weakness. At the start of the trial, the monitors will conduct a tutorial on the web-based data entry system. The coordinators will practice entering data so that the monitors can confirm that the coordinators are proficient in all aspects of data entry, query response, and communication with the DCC. They will audit the overall quality and completeness of the data, examine source documents, interview investigators and coordinators, and confirm that the clinical center has complied with the requirements of the protocol. The monitors will verify that all adverse events were documented in the correct format, and are consistent with protocol definition.\n\n\nThe monitors will review the source documents as needed, to determine whether the data reported in the Web-based system are complete and accurate. Source documents are defined as medical charts, associated reports and records including initial hospital admission report…\n\n\nThe monitors will confirm that the regulatory binder is complete and that all associated documents are up to date. The regulatory binder should include the protocol and informed consent (all revisions), IRB [institutional review board] approvals for all of the above documents, IRB correspondence, case report forms, investigator’s agreements…\n\n\nScheduling monitoring visits will be a function of patient enrollment, site status and other commitments. The DCC will notify the site in writing at least three weeks prior to a scheduled visit. The investigators must be available to meet with the monitors. Although notification of the visits will include the list of patients scheduled to be reviewed, the monitors reserve the right to review additional ARUBA patients.\n\n\nIf a problem is identified during the visit (ie, poor communication with the DCC, inadequate or insufficient staff to conduct the study, missing study documents) the monitor will assist the site in resolving the issues. Some issues may require input from the Operations Committee, Steering Committee or one of the principal investigators.\n\n\nThe focus of the visit/electronic monitoring will be on source document review and confirmation of adverse events. The monitor will verify the following variables for all patients: initials, date of birth, sex, signed informed consent, eligibility criteria, date of randomization, treatment assignment, adverse events, and endpoints…\n\nBack to top\n\n\n\n\n\n\nEthics and dissemination\n\n\n24. Research ethics approval\n\n\n\n\n\n\n\nPlans for seeking research ethics committee / institutional review board (REC / IRB) approval\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nA universal requirement for the ethical conduct of clinical research is the review and approval of the research protocol by qualified individuals who are not associated with the research team and have no disqualifying competing interests as reviewers.1 The review is typically conducted by a formal REC/IRB in accordance with jurisdictional policy. Despite the importance of ethics review, approval by a REC/IRB is not always obtained. Among 767 trials published in leading general medical journals from 1993-95, 37 authors (5%) disclosed that such approval had not been sought for their trials.344 The protocol should document where approval has been obtained, or outline plans to seek such approval.\n\n\nExamples\n\nThis protocol and the template informed consent forms contained in Appendix II will be reviewed and approved by the sponsor and the applicable IRBs/ECs [institutional review boards/ethical committees] with respect to scientific content and compliance with applicable research and human subjects regulations. . . .\n\n\nThe protocol, site-specific informed consent forms (local language and English versions), participant education and recruitment materials, and other requested documents—and any subsequent modifications — also will be reviewed and approved by the ethical review bodies. . .\n\n\nSubsequent to initial review and approval, the responsible local Institutional Review Boards/Ethical Committees (IRBs/ECs) will review the protocol at least annually. The Investigator will make safety and progress reports to the IRBs/ECs at least annually and within three months of study termination or completion at his/her site. These reports will include the total number of participants enrolled . . . and summaries of each DSMB [data safety and monitoring board] review of safety and/or efficacy.\n\nBack to top\n\n\n\n\n\n\n25. Protocol amendments\n\n\n\n\n\n\n\nPlans for communicating important protocol modifications (eg, changes to eligibility criteria, outcomes, analyses) to relevant parties (eg, investigators, REC / IRBs, trial participants, trial registries, journals, regulators)\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAfter initial ethics approval, about half of trials have subsequent protocol amendments submitted to the REC/IRB.125 346 347 While some amendments may be unavoidable, a study of pharmaceutical industry trials found that according to the sponsors, a third of amendments could have been prevented with greater attention to key issues during protocol development.346 Substantive amendments can generate challenges to data analysis and interpretation if they occur part way through the trial (eg, changes in eligibility criteria),348 and can introduce bias if the changes are made based on the trial data.173 174 175 176 The implementation and communication of amendments are also burdensome and potentially costly.346\nNumerous studies have revealed substantive changes between prespecified methods (eg, as stated in approved protocols, registries, or regulatory agency submissions) and those described in trial publications, including changes to primary outcomes,12 172 173 174 175 176 sample size calculations,6 eligibility criteria,125 133 134 as well as methods of allocation concealment,2 blinding,3 and statistical analysis.6 7 8 174 These substantive modifications are rarely acknowledged in the final trial reports, providing an inaccurate impression of trial integrity.\nIt is important that substantive protocol amendments be reviewed by an independent party, such as the REC/IRB, and transparently described in trial reports. The notion of substantive is variably defined by authorities, but in general refers to a protocol amendment that can affect the safety of trial participants or the scientific validity, scope, or ethical rigour of the trial.349 350 To reflect the degree of oversight for the trial and adherence to applicable regulation, the protocol should describe the process for making amendments, including who will be responsible for the decision to amend the protocol and how substantive changes will be communicated to relevant stakeholders (eg, REC/IRBs, trial registries, regulatory agencies). Version control using protocol identifiers and dates (Item 3), as well as a list of amendments, can help to track the history of amendments and identify the most recent protocol version.\n\n\nExamples\n\n13.10 Modification of the Protocol\n\nAny modifications to the protocol which may impact on the conduct of the study, potential benefit of the patient or may affect patient safety, including changes of study objectives, study design, patient population, sample sizes, study procedures, or significant administrative aspects will require a formal amendment to the protocol. Such amendment will be agreed upon by BCIRG [Breast Cancer International Research Group] and Aventis, and approved by the Ethics Committee/IRB [institutional review board] prior to implementation and notified to the health authorities in accordance with local regulations.\n\nAdministrative changes of the protocol are minor corrections and/or clarifications that have no effect on the way the study is to be conducted. These administrative changes will be agreed upon by BCIRG and Aventis, and will be documented in a memorandum. The Ethics Committee/IRB may be notified of administrative changes at the discretion of BCIRG\n\nBack to top\n\n\n\n\n\n\n26a. Consent or assent\n\n\n\n\n\n\n\nWho will obtain informed consent or assent from potential trial participants or authorised surrogates, and how (see Item 32)\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe notion of acquiring informed consent involves the presentation of comprehensible information about the research to potential participants, confirmation that they understand the research, and assurance that their agreement to participate is voluntary. The process typically involves discussion between the potential participant and an individual knowledgeable about the research; the presentation of written material (eg, information leaflet or consent document); and the opportunity for potential participants to ask questions. Surveys of trial investigators reveal that appropriate informed consent is not always obtained.344 352\nThe content, quantity, and mode of delivery of consent information can affect trial recruitment, participant comprehension, anxiety, retention rates, and recruitment costs.68 114 218 292 353 354 355 We recommend that a model consent or assent form be provided as a protocol appendix (Item 32). Assent represents a minor’s affirmative agreement to participate in the trial, which typically involves signing a document that provides age appropriate information about the study.\nThe protocol should include details of the consent process as well as the status, experience, and training (if applicable) of the research team members who will conduct it. In paediatric research, regulations may stipulate obtaining affirmative assent for participation from children above a certain age.356 The protocol should then describe how pertinent information will be provided to potential participants and how their understanding and assent will be ascertained. When potential participants lack decisional capacity for reasons other than young age (eg, mental status), and proxy consent can be obtained from a legally-authorised representative, the protocol should describe who will determine an individual’s decisional capacity, whether a formal capacity instrument will be utilised, and how the individual’s informed agreement to continue participation will be secured should they regain decisional capacity. For certain trials, such as cluster randomised trials, it may not be possible to acquire individual informed consent from participants before randomisation, and the consent process may be modified or waived. An explanation should be provided in the protocol in these instances.\n\n\nExamples\n\n…Trained Research Nurses will introduce the trial to patients who will be shown a video regarding the main aspects of the trial. Patients will also receive information sheets. Research Nurses will discuss the trial with patients in light of the information provided in the video and information sheets. Patients will then be able to have an informed discussion with the participating consultant. Research Nurses will obtain written consent from patients willing to participate in the trial. Information sheets and consent forms are provided for all parents involved in the trial however these have been amended accordingly in order to provide separate information sheets and consent form [sic] which are suitable for children and teenagers. All information sheets, consent forms and the video transcript have been translated into Bengali, Punjabi, Gujarati, and Urdu. There are also separate information sheets and consent forms for the cohort group.\n\nBack to top\n\n\n\n\n\n\n26b. Consent or assent: ancillary studies\n\n\n\n\n\n\n\nAdditional consent provisions for collection and use of participant data and biological specimens in ancillary studies, if applicable\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAncillary studies involve the collection or derivation of data for purposes that are separate from the main trial. The acquisition and storage of data and biological specimens for ancillary studies is increasingly common in the context of clinical trials (Item 33). Specimens may be used for a specified subset of studies or for submission to biorepositories for future specified or unspecified research.\nAncillary studies have additional processes and considerations relating to consent, which should be detailed in the protocol. Guidance for the creation of a simplified informed consent document for biobanking is available.358 Participants can be given several options to consider with respect to their participation in ancillary research: consent for the use of their data and specimens in specified protocols; consent for use in future research unrelated to the clinical condition under study; consent for submission to an unrelated biorepository; and consent to be contacted by trial investigators for further informational and consent-related purposes. This is commonly referred to as tiered consent. Participants should also be informed about whether their withdrawal from the ancillary research is possible (eg, the data and specimens are coded and identifiable); what withdrawal means in this context (eg, used specimens and data derived from them cannot be withdrawn); and what information derived from the specimen related research will be provided to them, if any.\n\n\nExamples\n\n6.4.1. Samples for Biorepositories\n\nAdditional biological samples will be obtained to be stored for use in future studies of the pathobiology of FSGS [focal segmental glomerulosclerosis]. A materials consent will be obtained to specifically address the collection of these …urine, serum and plasma specimens…\n14.3.4. Instructions for Preparation of Requests for an Ancillary Study\n… A signed consent must be obtained from every participant in the ancillary study, if the data collection/request is not covered in the original informed consent process for the main FSGS Clinical Trial.\n…\nA copy of the IRB [institutional review board] letter for the ancillary study should be sent to the DCC [data coordinating centre]. If a separate consent form is required for the ancillary study, a copy of the signed ancillary study consent form for each study participant must be included in the FSGS-CT [clinical trial] record. A data file tracking all signed ancillary consent forms must be maintained by the ancillary study and an electronic copy of that file must be delivered to the FSGS-CT DCC.\nBack to top\n\n\n\n\n\n\n27. Confidentiality\n\n\n\n\n\n\n\nHow personal information about potential and enrolled participants will be collected, shared, and maintained in order to protect confidentiality before, during, and after the trial\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nPersonal information about participants is acquired during the process of trial recruitment, eligibility screening, and data collection. Much of this information consists of private details over which people customarily wish to maintain control, such as their health status, personal genotype, and social and family history.\nThe protocol should describe the means whereby personal information is collected, kept secure, and maintained. In general, this involves: 1) the creation of coded, depersonalised data where the participant’s identifying information is replaced by an unrelated sequence of characters; 2) secure maintenance of the data and the linking code in separate locations using encrypted digital files within password protected folders and storage media; and 3) limiting access to the minimum number of individuals necessary for quality control, audit, and analysis. The protocol should also describe how the confidentiality of data will be preserved when the data are transmitted to sponsors and coinvestigators (eg, virtual private network internet transmission).\n\n\nExamples\n\n8.5 Confidentiality\n\nAll study-related information will be stored securely at the study site. All participant information will be stored in locked file cabinets in areas with limited access. All laboratory specimens, reports, data collection, process, and administrative forms will be identified by a coded ID [identification] number only to maintain participant confidentiality. All records that contain names or other personal identifiers, such as locator forms and informed consent forms, will be stored separately from study records identified by code number. All local databases will be secured with password-protected access systems. Forms, lists, logbooks, appointment books, and any other listings that link participant ID numbers to other identifying information will be stored in a separate, locked file in an area with limited access.\n\nAll HIV test results will be kept strictly confidential, all counseling and blood draws will be conducted in private rooms, and study staff will be required to sign agreements to preserve the confidentiality of all participants. Study staff will never inform network members of the serostatus of other members of their group, but counselors will provide general messages about the prevalence of HIV in the study population in the interests of emphasizing harm reduction.\n\n\nParticipants’ study information will not be released outside of the study without the written permission of the participant, except as necessary for monitoring by NIAID [National Institute of Allergy and Infectious Diseases] and/or its contractors . . . representatives of the HPTN CORE [HIV Prevention Trials Network Coordinating and Operations Center] . . . and US or in-country government and regulatory authorities.\n\nBack to top\n\n\n\n\n\n\n28. Declaration of interests\n\n\n\n\n\n\n\nFinancial and other competing interests for principal investigators for the overall trial and each study site\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nCompeting interests, or conflicts of interest, exist when there is potential for divergence between an individual’s or institution’s private interests and their responsibilities to scientific and publishing activities.360 More positive outcomes, larger treatment effect sizes, and more favourable interpretation of results have been found in clinical trials with pharmaceutical industry sponsorship (Item 4)27 36 37 38 42 and investigators who have declared competing interests,57 60 compared to those without such interests. Although competing interests are most often associated with drug and device industries, they may exist with support from or affiliation with government agencies, charities, not for profit organisations, and professional and civic organisations.\nCompeting interests do not in themselves imply wrongdoing. Their disclosure and regular updating enables appropriate management plans to be developed and implemented, and facilitates transparent assessment of the potential for bias.\nMany trials and non-industry sponsors have a conflict of interest policy for their investigators, and checklists are available to guide potential interests that should be disclosed and regularly updated by trial investigators.361 362 Types of financial ties include salary support or grants; ownership of stock or options; honorariums (eg, for advice, authorship, or public speaking); paid consultancy or service on advisory boards and medical education companies; and receipt of patents or patents pending. Non-financial competing interests include academic commitments; personal or professional relationships; and political, religious, or other affiliations with special interests or advocacy positions.\n\n\nExamples\n\nPS:\n\n\nWas the Principal Investigator of the second International Stroke Trial (IST-2) to evaluate a neuroprotective compound (619c89). . .\nHas received lecture fees and travel expenses from Bayer and from Boehringer Ingelheim for lectures given at international conferences.\nHe serves on the Independent Data Monitoring and Safety Board of the RELY trial, funded by Boehringer Ingelheim and receives attendance fees and travel expenses for attending board meetings.\nHe does not have any paid consultancies with pharmaceutical companies, and is not a member of the Speaker’s Panel of any company.\n\nKBS:\nReceived an honorarium for a lecture from Boehringer Ingelheim and had costs for participating in scientific meetings reimbursed…\nBack to top\n\n\n\n\n\n\n29. Data access\n\n\n\n\n\n\n\nStatement of who will have access to the final trial dataset, and disclosure of contractual agreements that limit such access for investigators\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe validity of results from interventional trials can be verified only by individuals who have full access to the complete final dataset. For some multicentre trials, only the steering group has access to the full trial dataset in order to ensure that the overall results are not disclosed by an individual study site prior to the main publication. Many of these trials will allow site investigators to access the full dataset if a formal request describing their plans is approved by the steering group. The World Medical Association supports the principle that trial investigators retain the right to access data.363 However, among protocols of industry initiated randomised trials published in 2008-9 in the Lancet or approved in 2004 by a Danish ethics committee, 30-39% stated that the sponsor owned the data while 0-3% stated that principal investigators had access to all trial data.10 364 Similar constraints were found in Danish trial protocols from 1994-5.10\nThe protocol should identify the individuals involved in the trial who will have access to the full dataset. Any restrictions in access for trial investigators should also be explicitly described.\n\n\nExamples\n\n12.10.1 Intra-Study Data Sharing\n\nThe Data Management Coordinating Center will oversee the intra-study data sharing process, with input from the Data Management Subcommittee.\n\nAll Principal Investigators (both US and host country) will be given access to the cleaned data sets. Project data sets will be housed on the Project Accept Web site and/or the file transfer protocol site created for the study, and all data sets will be password protected. Project Principal Investigators will have direct access to their own site’s data sets, and will have access to other sites data by request. To ensure confidentiality, data dispersed to project team members will be blinded of any identifying participant information.\n\nBack to top\n\n\n\n\n\n\n30. Ancillary and post trial care\n\n\n\n\n\n\n\nProvisions, if any, for ancillary and post-trial care, and for compensation to those who suffer harm from trial participation\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe provision of ancillary care refers to the provision of care beyond that immediately required for the proper and safe conduct of the trial, and the treatment of immediate adverse events related to trial procedures. It is generally agreed that trial sponsors and investigators should plan to provide care for participants’ healthcare needs that arise as a direct consequence of trial participation (eg, intervention related harms). It is also important to consider whether care should be provided for certain ancillary needs that may otherwise arise during trial participation. Provision of care for ancillary needs reflects the fact that participants implicitly, but unavoidably, entrust certain aspects of their health to the research team. The scope of entrustment will vary depending on the nature of the trial (eg, setting, health condition under study, investigations performed).366 Additional factors that influence the strength of the claim to ancillary care include participants’ vulnerabilities; uncompensated burdens and harms; the intensity and duration of the participant-researcher relationship; and the degree to which participants are uniquely dependent on the research team for health care.367\nThe Declaration of Helsinki states that the protocol should describe arrangements for post-study access by study participants to interventions identified as beneficial in the study or access to other appropriate care or benefits.1 This principle is particularly applicable—and controversial—when research enabling the development and regulatory approval of interventions is performed in countries where subsequent access to the interventions is limited by cost or lack of availability.368\nThe protocol should describe any plans to provide or pay for ancillary care during the trial and identify any interventions, benefits, or other care that the sponsor will continue to provide to participants and host communities after the trial is completed.369 Any plans to compensate participants for trial related harms should also be outlined.\n\n\nExamples\n\nPatients that are enrolled into the study are covered by indemnity for negligent harm through the standard NHS [National Health Service] Indemnity arrangements. The University of Sheffield has insurance to cover for non-negligent harm associated with the protocol . . . This will include cover for additional health care, compensation or damages whether awarded voluntarily by the Sponsor, or by claims pursued through the courts. Incidences judged to arise from negligence (including those due to major protocol violations) will not be covered by study insurance policies. The liability of the manufacturer of IL1RA (Amgen Corporation) is strictly limited to those claims arising from faulty manufacturing of the commercial product and not to any aspects of the conduct of the study.\n\n\n13.6 Access to Effective Products\n\nShould this study provide evidence of the effectiveness of TDF [tenofovir disoproxil fumarate], FTC [emtricitabine]/TDF and/or tenofovir 1% gel in preventing HIV infection, it will be critical to provide access to the effective product(s) to study participants, their communities, and the worldwide population at risk for HIV infection in a timely manner. In preparation for this study, discussions have begun with Gilead Sciences, Inc. and CONRAD [Contraceptive Research and Development Organization] to ensure such access. Considerations under discussion include licensing agreements and preferred pricing arrangements for the study communities and other resource-poor settings.\n\nWhile this study is ongoing, the MTN [Microbicide Trials Network] will continue these discussions. In addition, discussions will be initiated with other public and private funding sources such as the WHO, UNAIDS, Gates Foundation, and appropriate site government agencies that may be able to purchase product supplies in bulk and offer them at low or no cost to the study communities and other resource-poor communities most in need of the product(s). Operations and marketing research also may be conducted to determine how best to package and distribute the products, and maximize their acceptability and use, in at-risk populations.\n\nBack to top\n\n\n\n\n\n\n31a. Dissemination policy: trial results\n\n\n\n\n\n\n\nPlans for investigators and sponsor to communicate trial results to participants, healthcare professionals, the public, and other relevant groups (eg, via publication, reporting in results databases, or other data sharing arrangements), including any publication restrictions\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nA fundamental ethical principle in clinical trials is that the potential risks incurred by study participants should be balanced by the benefit of contributing to publicly available knowledge.371 Unfortunately, about half of clinical trials remain unpublished.80 83 Trials with statistically non-significant results or industry funding are more prone to non-publication,36 38 80 81 82 83 although government funded trials are also susceptible.81 When published, trials with non-significant results often have a longer delay to publication.80 83 Overall, the medical literature represents a biased subset of existing data, potentially leading to overestimation of benefits, underestimation of harms, and a detrimental impact on patient care and research.80 372 373 374 375 376 377\nAlthough peer reviewers can be biased in favour of positive findings,378 lack of publication appears to be primarily due to trial investigators or sponsors failing to submit negative or null results, rather than journals rejecting them.80 379 A plan to disseminate trial results to key stakeholders should be outlined in the protocol, including a process and timeframe for approving and submitting reports for dissemination (eg, via journal publication, trial registry, trial website), and an explicit statement that the results will be disseminated regardless of the magnitude or direction of effect.\nFurthermore, any conditions relating to the investigators’ right to publish or present trial results should be explicitly described. Publication restrictions have been imposed by various groups, including industry sponsors or the trial steering group (eg, to maintain the integrity of the overall dataset).10 380 These restrictions are sometimes not described in the protocol but rather in separate publication agreements.10 However, as they can interfere with the ethical responsibility of investigators and sponsors to disseminate trial results in an unbiased and timely manner,38 381 382 383 384 any restrictions should be disclosed in the protocol for review by REC/IRBs, funders, and other stakeholders. A review of industry initiated randomised trial protocols approved in Denmark in 1994-95 revealed that 91% had publication restrictions imposed by sponsors; similar constraints were noted for protocols approved in 2004.10\n\n\nExamples\n\n\nPublication Policy\n\n\nThe Publications subcommittee will review all publications following the guidelines given below and report its recommendations to the Steering Committee.\nA. Data analysis and release of results\nThe scientific integrity of the project requires that the data from all BEST [Beta-Blocker Evaluation of Survival Trial] sites be analyzed study-wide and reported as such. Thus, an individual center is not expected to report the data collected from its center alone . . . all presentations and publications are expected to protect the integrity of the major objective(s) of the study; data that break the blind will not be presented prior to the release of mainline results. Recommendations as to the timing of presentation of such endpoint data and the meetings at which they might be presented will be given by the Steering Committee.\nB. Review process\nEach paper or abstract, as described below, must be submitted to the appropriate Subcommittee for review of its appropriateness and scientific merit prior to submission. The Subcommittee may recommend changes to the authors and will finally submit its recommendations to the Steering Committee for approval.\nC. Primary outcome papers\nThe primary outcome papers of BEST are papers that present outcome data . . . The determination of whether or not a particular analysis represents a primary outcome will be made by the Steering Committee on the recommendation of the Publications Subcommittee . . .\nD. Other study papers, abstracts and presentations\nAll studies other than those designated as Primary Outcome fall within this category . . . All papers and abstracts must be approved by the Publications Committee before they are submitted.\n\nIt is possible that in certain instances BEST may be asked to contribute papers to workshops, symposia, volumes, etc. The individuals to work on such requests should be appointed by the Executive Committee, but where time permits, a proposal will be circulated soliciting other participants as in the case of other study papers as described in the Application Review Process.\n\n\n\nClose-out Procedures\n\n\nBEST may terminate at the planned target of 1.5 years after the last participant has been randomized, or at an earlier or later date if the circumstances warrant… Regardless of the timing and circumstances of the end of the study, close-out will proceed in two stages:\nInterim period for analysis and documentation of study results.\nDebriefing of participants and dissemination of study results.\nA. Interim\nEvery attempt will be made to reduce to an absolute minimum the interval between the completion of data collection and the release of the study results. We expect to take about 3 to 4 months to compile the final results paper for an appropriate journal.\nB. Reporting of study results\nThe study results will be released to the participating physicians, referring physicians, patients and the general medical community.\nBack to top\n\n\n\n\n\n\n31b. Dissemination policy: authorship\n\n\n\n\n\n\n\nAuthorship eligibility guidelines and any intended use of professional writers\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nSubstantive contributions to the design, conduct, interpretation, and reporting of a clinical trial are recognised through the granting of authorship on the final trial report. Authorship guidelines in the protocol are intended to help enhance transparency and avoid disputes or misunderstanding after trial completion. These guidelines should define criteria for individually named authors or group authorship.385\nIndividuals who fulfil authorship criteria should not remain hidden (ghost authorship) and should have final authority over manuscript content.9 386 387 Similarly, those who do not fulfil such criteria should not be granted authorship (guest authorship).386 388 The International Committee of Medical Journal Editors has defined authorship criteria for manuscripts submitted for publication,389 although these criteria have reportedly been open to abuse.390 If some protocol authors are not named authors of subsequent publications, their role in protocol design should at least be acknowledged in the published report. Among 44 protocols of industry initiated trials, 75% had evidence of ghost authorship when compared with corresponding journal publications.9\nProfessional medical writers are sometimes hired to improve clarity and structure in a trial report, and guidelines for ethical collaborative writing have been developed.391 392 Because the drafting of text can influence how the study results and conclusions are portrayed, plans for the employment of writers and their funding source should be acknowledged in both protocols and trial reports.\n\n\nExamples\n\n17.4. Assignment of Writing Committees\n\nTopics suggested for presentation or publication will be circulated to the PIs [principal investigators] of the CCCs [core coordinating centers], the DCC [data coordinating centre], Core Lab and the NIH [National Institutes of Health]. These groups are requested to suggest and justify names for authors to be reviewed by the PC [publications committee]. . . If a topic is suggested by a participant of the FSGS-CT [focal segmental glomerulosclerosis—clinical trial], the writing committee will be formed as just described except that the person making the suggestion may be considered as the lead author. The PI of an ancillary study should be considered for lead author of material derived from this study. Disputes regarding authorship will be settled by the Study Chair after consultation with the Chair of the PC…\n17.5. Reports of the FSGS-CT: Classes of Reports\nThere are three classes of reports of the FSGS-CT:\nA. Reports of the major outcomes of the Study.\nB. Reports addressing in detail one aspect of the FSGS-CT, but in which the data are derived from the entire study.\nC. Reports of data derived from a subset of centers by members of the FSGS-CT, (eg, sub-studies or ancillary studies), or reports of investigations initiated outside of the FSGS-CT, but using data or samples collected by the FSGS-CT. . .\n17.6. Authorship Policy\nThe authors of FSGS publications will be listed as detailed below.\nType A publications:\nabstracts: from the FSGS Clinical Trial Groupx, presented by XXXX.\npapers: from the FSGS Clinical Trial Groupx, prepared by XXXX.\nxThe FSGS participant box, detailed below, must be included in these papers. If a journal’s publication policy does not allow authorship by a group, the authors will be listed first as in Type B publications.\nType B publications:\n…\n17.7. Authorship: Professional Participants Listing in the FSGS Participant Box\nThe FSGS participant box will list all professionals that have participated in the FSGS-CT for a minimum of one year.\nBack to top\n\n\n\n\n\n\n31c. Dissemination policy: reproducible research\n\n\n\n\n\n\n\nPlans, if any, for granting public access to the full protocol, participant-level dataset, and statistical code\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nGiven the central role of protocols in enhancing transparency, reproducibility, and interpretation of trial results, there is a strong ethical and scientific imperative to ensure that full protocols are made publicly available.24 394 395 High quality protocols contain relevant details on study design and conduct that are generally not available in journal publications or trial registries.84 396 It is also important to make available the full study report, such as the clinical study report submitted to regulatory agencies by industry sponsors.377 396 397 398 399 400 This detailed report provides the most comprehensive description of trial methods (including the full protocol) and all published and unpublished analyses. In addition, there have increasingly been calls to improve the availability of participant-level datasets and statistical code after journal publication to enable verification and replication of analyses, facilitate pooling with other studies, and accelerate research through open knowledge sharing.372 401 402 403 404 405 406\nAvenues for providing access to full protocols include journals,407 408 trial websites, and trial registries.163 Several journals and funders support the sharing of participant level data,405 409 410 411 while others routinely publish a statement regarding sharing of protocols, statistical codes, and datasets for all of their published research articles.412 413\nThe protocol should indicate whether the trial protocol, full study report, anonymised participant level dataset, and statistical code for generating the results will be made publicly available; and if so, describe the timeframe and any other conditions for access.\n\n\nExamples\n\nData sharing statement No later than 3 years after the collection of the 1-year postrandomisation interviews, we will deliver a completely deidentified data set to an appropriate data archive for sharing purposes.\n\nBack to top\n\n\n\n\n\n\nAppendices\n\n\n32. Informed consent materials\n\n\n\n\n\n\n\nModel consent form and other related documentation given to participants and authorised surrogates\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe Declaration of Helsinki states that each potential trial participant must normally, at a minimum, be adequately informed about the purpose of the trial; potential benefits and risks; their right to refuse participation or to withdraw consent at any time; institutional affiliation and potential competing interests of the researcher; and sources of trial funding.1 There are rare exceptions where deferred consent can be acceptable, such as trials involving unconscious patients in emergency situations.\nSpecial attention is required to ensure that relevant information is provided and appropriate modes of delivery are used during the consent process (Item 26).414 Consent and participant information forms are often written at a much higher reading level than is acceptable for the general population.415 Depending on the nature of the trial, several different consent documents may be needed. For example, a paediatric trial may involve both parental permission and participant assent documents. For multicentre trials, a model or sample document is typically drafted for distribution to local investigators, who may then revise the document to comply with local requirements.\n\n\nExamples\n\nAPPENDIX 7 SAMPLE PATIENT INFORMED CONSENT\n\n\nNote: . . . Each Ethics Committee or Institutional Review Board will revise and adapt according to their own institution’s guidelines.\n\n\nMULTICENTER PHASE III RANDOMIZED TRIAL COMPARING DOXORUBICIN AND CYCLOPHOSPHAMIDE . . .\n\n\nStudy number: BCIRG 006 (TAX GMA 302)\n\n\nInvestigator name:\n\n\nAddress:\n\n\nConsent Form:\n\n\nThis consent form is part of the informed consent process. It is designed to give you an idea of what this research study is about and what will happen to you if you choose to be in the study…\n\nBack to top\n\n\n\n\n\n\n33. Biological specimens\n\n\n\n\n\n\n\nPlans for collection, laboratory evaluation, and storage of biological specimens for genetic or molecular analysis in the current trial and for future use in ancillary studies, if applicable\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nBiological specimens (eg, biopsy tissue; blood for DNA extraction) obtained during the conduct of clinical trials can be stored in repositories—often designated as biobanks—for the current trial and future research. This process is usually governed by local regulation and has particular ethical considerations (Item 26b).\nIf the trial involves genetic or molecular analysis of biological specimens derived from humans, or if any specimens will be stored for future use (specified or unspecified), the protocol should describe details about specimen collection, storage, and evaluation, including the location of repositories. In addition, the protocol should state whether collected samples and associated participant related data will be de-identified or coded to protect participant confidentiality. If a repository is overseen by a named research ethics committee/institutional review board, then this information should also be provided.\n\n\nExamples\n\nWhite Blood Cell and Plasma Collection Procedures\n\n1.0 Objectives\n1.1 To provide a resource for studies of early markers, etiology, and genetic risk factors for prostate cancer and other diseases.\n2.0 Background\nThe Prostate Cancer Prevention Trial (PCPT) is a randomized double blind chemoprevention trial…\nInitial blood collection was specifically for the analysis of PSA [prostate specific antigen] and storage of serum . . . an additional blood collection will be carried out using anticoagulant so that plasma and white blood cells can be isolated. Plasma will allow the analysis of additional biomarkers . . . This DNA will be used (among other possible uses) for studies to investigate polymorphisms in genes which may influence prostate cancer risk . . .\n\nThe PCPT WBC [white blood cell] sample will be available to PCPT investigators as well as outside researchers who have important, timely hypotheses to test.\n\nBecause the sample bank is a limited resource, proposals to use it will be evaluated in terms of scientific relevance, significance, and validity as well as the potential impact of the proposed study. The amount and type of material needed will also be considered and the efficient use of material will be required. Strict confidentiality will be exercised and the information provided to investigators will not contain personal identifiers.\n\nWhen specific uses of the WBC samples are approved, the SWOG-9217 protocol will be amended.\n\n\nParticipation in this research is not required for continued participation in the PCPT.\n\n\n3.0 Methods\n\n3.1 Because the original model consent form did not specifically address genetic studies, participants will be asked to sign an additional consent form to document their consent to the collection and submission of additional blood samples for storage and future testing (including genetic analysis).\n3.2 Institutions will be asked to submit additional materials from participants who consent to the additional blood collection. The blood is to be collected, processed and shipped as described in the PCPT Study Manual.\n3.3 NCI-Frederick Cancer Research Development Center (FCRDC) in Frederick, Maryland will serve as the processing, aliquotting and storage facility.\n3.4 Upon arrival at FCRDC the blood will be pooled and centrifuged. Plasma will be separated into 5 x 1.8 ml aliquots and frozen . . .\n3.5 All samples will be logged in and aliquots will be bar coded with a unique storage ID. These data will be electronically transmitted to the Statistical Center for verification.\n3.6 The scientists who will carry out analyses on these materials will not have access to personal identifiers and will not be able to link the results of these tests to personal identifier information. No individual results will be presented in publications or other reports…\n3.7 Participants will not be informed on an individual basis of any results from these studies…\n4.0 Sample analysis\n4.1 Investigators planning to submit NIH [National Institutes of Health] grant applications must obtain approval for their study and specimen access from the PCPT Serum and Tissue Utilization Committee before submission of a grant proposal. Potential investigators will be required to submit a brief abstract and 1-4 page outline… This proposal will be circulated for review to members of the PCPT Serum and Tissue Utilization Committee and two ad hoc members having relevant expertise…\n4.2 It is anticipated that proposals will be reviewed once a year . . . Approval by this group as well as appropriate Institutional Review Board approval from the investigator’s institution will be required before release of samples.\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-1_title-1",
    "href": "guidelines/spirit/index.html#sec-1_title-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nThe title provides an important means of trial identification. A succinct description that conveys the topic (study population, interventions), acronym (if any), and basic study design—including the method of intervention allocation (eg, parallel group randomised trial; single-group trial)—will facilitate retrieval from literature or internet searches and rapid judgment of relevance.20 It can also be helpful to include the trial framework (eg, superiority, non-inferiority), study objective or primary outcome, and if relevant, the study phase (eg, phase II)."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-2a_trial_registration-1",
    "href": "guidelines/spirit/index.html#sec-2a_trial_registration-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nThere are compelling ethical and scientific reasons for trial registration.22 23 24 Documentation of a trial’s existence on a publicly accessible registry can help to increase transparency,24 25 decrease unnecessary duplication of research effort, facilitate identification of ongoing trials for prospective participants, and identify selective reporting of study results.26 27 28 As mandated by the International Committee of Medical Journal Editors (ICMJE) and jurisdictional legislation,29 30 31 registration of clinical trials should occur before recruitment of the first trial participant.\nWe recommend that registry names and trial identifiers assigned by the registries be prominently placed in the protocol, such as on the cover page. If the trial is not yet registered, the intended registry should be indicated and the protocol updated upon registration. When registration in multiple registries is required (eg, to meet local regulation), each identifier should be clearly listed in the protocol and each registry."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-2a_trial_registration-2",
    "href": "guidelines/spirit/index.html#sec-2a_trial_registration-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nEudraCT: 2010-019180-10\n\nClinicalTrials.gov: NCT01066572\nISRCTN: 54540667\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-2b_trial_registration_data_set-1",
    "href": "guidelines/spirit/index.html#sec-2b_trial_registration_data_set-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nIn addition to a trial registration number, the World Health Organization (WHO) recommends a minimum standard list of items to be included in a trial registry in order for a trial to be considered fully registered (www.who.int/ictrp/network/trds/en/index.html). These standards are supported by ICMJE, other journal editors, and jurisdictional legislation.29 30 31 We recommend that the WHO Trial Registration Data Set be included in the protocol to serve as a brief structured summary of the trial. Its inclusion in the protocol can also signal updates for the registry when associated protocol sections are amended—thereby promoting consistency between information in the protocol and registry."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-2b_trial_registration_data_set-2",
    "href": "guidelines/spirit/index.html#sec-2b_trial_registration_data_set-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nExample of trial registration data\n\nhttp://www.bmj.com/content/346/bmj.e7586## Table 2\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-3_protocol_version-1",
    "href": "guidelines/spirit/index.html#sec-3_protocol_version-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nSequentially labelling and dating each protocol version helps to mitigate potential confusion over which document is the most recent. Explicitly listing the changes made relative to the previous protocol version is also important (see Item 25). Transparent tracking of versions and amendments facilitates trial conduct, review, and oversight."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-3_protocol_version-2",
    "href": "guidelines/spirit/index.html#sec-3_protocol_version-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nIssue date: 25 Jul 2005\n\nProtocol amendment number: 05\nAuthors: MD, JH\nRevision chronology:\nUM . . . 00, 2004-Jan-30 Original\nUM . . . 01, 2004-Feb-7 Amendment 01.:\nPrimary reason for amendment: changes in Section 7.1 regarding composition of comparator placebo\nAdditional changes (these changes in and of themselves would not justify a protocol amendment): correction of typographical error in Section 3.3 . . .\nUM . . . 05, 2005-Jul-25 Amendment No.5:\nAt the request of US FDA statements were added to the protocol to better clarify and define the algorithm for determining clinical or microbiological failures prior to the follow-up visit.\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-4_funding-1",
    "href": "guidelines/spirit/index.html#sec-4_funding-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nA description of the sources of financial and non-financial support provides relevant information to assess study feasibility and potential competing interests (Item 28). Although both industry funded and non-industry funded trials are susceptible to bias,4 35 the former are more likely to report trial results and conclusions that favour their own interventions.27 36 37 38 39 This tendency could be due to industry trials being more likely to select effective interventions for evaluation (Item 6a), to use less effective control interventions (Item 6b), or to selectively report outcomes (Item 12), analyses (Item 20) or full studies (Item 31).38 40 41 42 43 Non-financial support (eg, provision of drugs) from industry has not been shown to be associated with biased results, although few studies have examined this issue.44 45\nAt a minimum, the protocol should identify the sources of financial and non-financial support; the specific type (eg, funds, equipment, drugs, services) and time period of support; and any vested interest that the funder may have in the trial. If a trial is not yet funded when the protocol is first written, the proposed sources of support should be listed and updated as funders are confirmed.\nNo clear consensus exists regarding the level of additional funding details that should be provided in the trial protocol as opposed to trial contracts, although full disclosure of funding information in the protocol can help to better identify financial competing interests. Some jurisdictional guidelines require more detailed disclosure, including monetary amounts granted from each funder, the mechanism of providing financial support (eg, paid in fixed sum or per recruited participant), and the specific fund recipient (eg, trial investigator, department/institute).46 Detailed disclosure allows research ethics committees/institutional review boards (REC/IRBs) to assess whether the reimbursement amount is reasonable in relation to the time and expenses incurred for trial conduct."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-4_funding-2",
    "href": "guidelines/spirit/index.html#sec-4_funding-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nTranexamic acid will be manufactured by Pharmacia (Pfizer, Sandwich, UK) and placebo by South Devon Healthcare NHS Trust, UK. The treatment packs will be prepared by an independent clinical trial supply company (Brecon Pharmaceuticals Limited, Hereford, UK) . . .\n\n\nLSHTM [London School of Hygiene and Tropical Medicine] is funding the run-in costs for the WOMAN trial and up to 2,000 patients’ recruitment. The main phase is funded by the UK Department of Health and the Wellcome Trust. Funding for this trial covers meetings and central organisational costs only. Pfizer, the manufacturer of tranexamic acid, have provided the funding for the trial drug and placebo used for this trial. An educational grant, equipment and consumables for ROTEM [thromboelastometry procedure] analysis has been provided by Tem Innovations GmbH, M.-Kollar-Str. 13-15, 81829 Munich, Germany for use in the WOMAN-ETAC study. An application for funding to support local organisational costs has been made to University of Ibadan Senate Research Grant. The design, management, analysis and reporting of the study are entirely independent of the manufacturers of tranexamic acid and Tem Innovations GmbH\n\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-5a_roles_and_responsibilities_contributorship-1",
    "href": "guidelines/spirit/index.html#sec-5a_roles_and_responsibilities_contributorship-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nIndividuals who contribute substantively to protocol development and drafting should have their contributions reported. As with authorship of journal articles,48 listing the protocol contributors, their affiliations, and their roles in the protocol development process provides due recognition, accountability, and transparency. Naming of contributors can also help to identify competing interests and reduce ghost authorship (Items 28 and 31b).9 10 If professional medical writers are employed to draft the protocol, then this should be acknowledged as well.\nNaming of authors and statements of contributorship are standard for protocols published in journals such as Trials49 but are uncommon for unpublished protocols. Only five of 44 industry-initiated protocols approved in 1994-95 by a Danish research ethics committee explicitly identified the protocol authors."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-5a_roles_and_responsibilities_contributorship-2",
    "href": "guidelines/spirit/index.html#sec-5a_roles_and_responsibilities_contributorship-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nRTL [address], EJM [address], AK [address] . . .\n\nAuthors’ contributions\nRTL conceived of the study. AK, EN, SB, PR, WJ, JH, and MC initiated the study design and JK and LG helped with implementation. RTL, JK, LG, and FP are grant holders. LT and EM provided statistical expertise in clinical trial design and RN is conducting the primary statistical analysis. All authors contributed to refinement of the study protocol and approved the final manuscript\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-5b_roles_and_responsibilities_sponsor_contact_information-1",
    "href": "guidelines/spirit/index.html#sec-5b_roles_and_responsibilities_sponsor_contact_information-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nThe sponsor can be defined as the individual, company, institution, or organisation assuming overall responsibility for the initiation and management of the trial, and is not necessarily the main funder.51 52 In general, the company is the sponsor in industry initiated trials, while the funding agency or institution of the principal investigator is often the sponsor for investigator initiated trials. For some investigator initiated trials, the principal investigator can be considered to be a sponsor-investigator who assumes both sponsor and investigator roles.51 53\nIdentification of the trial sponsor provides transparency and accountability. The protocol should identify the name, contact information, and if applicable, the regulatory agency identifying number of the sponsor"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-5b_roles_and_responsibilities_sponsor_contact_information-2",
    "href": "guidelines/spirit/index.html#sec-5b_roles_and_responsibilities_sponsor_contact_information-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nTrial Sponsor: University of Nottingham\n\nSponsor’s Reference: RIS 8024 . . .\nContact name: Mr PC\nAddress: King’s Meadow Campus . . .\nTelephone: . . .\nEmail: . . .\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-5c_roles_and_responsibilities_sponsor_and_funder-1",
    "href": "guidelines/spirit/index.html#sec-5c_roles_and_responsibilities_sponsor_and_funder-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nThere is potential for bias when the trial sponsor or funder (sometimes the same entity) has competing interests (Item 28) and substantial influence on the planning, conduct, or reporting of a trial. Empirical research indicates that specific forms of bias tend to be more prevalent in trials funded by industry compared to those funded by non-commercial sources.36 37 38 45 55 56 57 58 59 60 The design, analysis, interpretation, and reporting of most industry-initiated trials are controlled by the sponsor; this authority is often enforced by contractual agreements signed between the sponsor and trial investigators (Item 29).10 61\nThe protocol should explicitly outline the roles and responsibilities of the sponsor and any funders in study design, conduct, data analysis and interpretation, manuscript writing, and dissemination of results. It is also important to state whether the sponsor or funder controls the final decision regarding any of these aspects of the trial.\nDespite the importance of declaring the roles of the trial sponsor and funders, few protocols explicitly do so. Among 44 protocols for industry-initiated trials receiving ethics approval in Denmark from 1994-95, none stated explicitly who had contributed to the design of the trial."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-5c_roles_and_responsibilities_sponsor_and_funder-2",
    "href": "guidelines/spirit/index.html#sec-5c_roles_and_responsibilities_sponsor_and_funder-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nThis funding source had no role in the design of this study and will not have any role during its execution, analyses, interpretation of the data, or decision to submit results.\n\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-5d_roles_and_responsibilities_committees-1",
    "href": "guidelines/spirit/index.html#sec-5d_roles_and_responsibilities_committees-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nThe protocol should outline the general membership of the various committees or groups involved in trial coordination and conduct; describe the roles and responsibilities of each; and (when known) identify the chairs and members. This information helps to ensure that roles and responsibilities are clearly understood at the trial onset, and facilitates communication from external parties regarding the trial. It also enables readers to understand the mandate and expertise of those responsible for overseeing participant safety, study design, database integrity, and study conduct. For example, empirical evidence supports the pivotal role of an epidemiologist or biostatistician in designing and conducting higher quality trials.63 64"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-5d_roles_and_responsibilities_committees-2",
    "href": "guidelines/spirit/index.html#sec-5d_roles_and_responsibilities_committees-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nPrincipal investigator and research physician:\n\nDesign and conduct of RITUXVAS\nPreparation of protocol and revisions\nPreparation of investigators brochure (IB) and CRFs [case report forms]\nOrganising steering committee meetings\nManaging CTO [clinical trials office]\nPublication of study reports\nMembers of TMC [Trial Management Committee]\nSteering committee (SC)\n(see title page for members)\nAgreement of final protocol\nAll lead investigators will be steering committee members. One lead investigator per country will be nominated as national coordinator.\nRecruitment of patients and liaising with principle [sic] investigator\nReviewing progress of study and if necessary agreeing changes to the protocol and/or investigators brochure to facilitate the smooth running of the study.\n\nTrial management committee (TMC):\n\n(Principle [sic] investigator, research physician, administrator)\nStudy planning\nOrganisation of steering committee meetings\nProvide annual risk report MHRA [Medicines and Healthcare Products Regulatory Agency] and ethics committee\nSUSAR [Serious unexpected suspected adverse events] reporting to MHRA and Roche\nResponsible for trial master file\nBudget administration and contractual issues with individual centres\nAdvice for lead investigators\nAudit of 6 monthly feedback forms and decide when site visit to occur.\nAssistance with international review, board/independent ethics committee applications\nData verification\nRandomisation\nOrganisation of central serum sample collection\n\nData manager:\n\nMaintenance of trial IT system and data entry\nData verification\n\nLead investigators:\n\nIn each participating centre a lead investigator (senior nephrologist/rheumatologist/ immunologist) will be identified, to be responsible for identification, recruitment, data collection and completion of CRFs, along with follow up of study patients and adherence to study protocol and investigators brochure. . . . Lead investigators will be steering committee members, with one investigator per country being nominated as national coordinator.\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-6a_background_and_rationale-1",
    "href": "guidelines/spirit/index.html#sec-6a_background_and_rationale-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nThe value of a research question, as well as the ethical and scientific justification for a trial, depend to a large degree on the uncertainty of the comparative benefits or harms of the interventions, which depends in turn on the existing body of knowledge on the topic. The background section of a protocol should summarise the importance of the research question, justify the need for the trial in the context of available evidence, and present any available data regarding the potential effects of the interventions (efficacy and harms).66 67 This information is particularly important to the trial participants and personnel, as it provides motivation for contributing to the trial.68 69 It is also relevant to funders, REC/IRBs, and other stakeholders who evaluate the scientific and ethical basis for trial conduct.\nTo place the trial in the context of available evidence, it is strongly recommended that an up-to-date systematic review of relevant studies be summarised and cited in the protocol.70 Several funders request this information in grant applications.71 72 Failure to review the cumulated evidence can lead to unnecessary duplication of research or to trial participants being deprived of effective, or exposed to harmful, interventions.73 74 75 76 A minority of published trial reports cite a systematic review of pre-existing evidence,77 78 and in one survey only half of trial investigators were aware of a relevant existing review when they had designed their trial.79 Given that about half of trials remain unpublished,80 81 82 and that published trials often represent a biased subset of all trials,80 83 it is important that systematic reviews include a search of online resources such as trial registries, results databases, and regulatory agency websites."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-6a_background_and_rationale-2",
    "href": "guidelines/spirit/index.html#sec-6a_background_and_rationale-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nBackground\n\nIntroduction: For people at ages 5 to 45 years, trauma is second only to HIV/AIDS as a cause of death. . . .\nMechanisms: The haemostatic system helps to maintain the integrity of the circulatory system after severe vascular injury, whether traumatic or surgical in origin.[reference] Major surgery and trauma trigger similar haemostatic responses . . . Antifibrinolytic agents have been shown to reduce blood loss in patients with both normal and exaggerated fibrinolytic responses to surgery, and do so without apparently increasing the risk of post-operative complications, . . .\nExisting knowledge: Systemic antifibrinolytic agents are widely used in major surgery to prevent fibrinolysis and thus reduce surgical blood loss. A recent systematic review [reference] of randomised controlled trials of antifibrinolytic agents (mainly aprotinin or tranexamic acid) in elective surgical patients identified 89 trials including 8,580 randomised patients (74 trials in cardiac, eight in orthopaedic, four in liver, and three in vascular surgery). The results showed that these treatments reduced the numbers needing transfusion by one third, reduced the volume needed per transfusion by one unit, and halved the need for further surgery to control bleeding. These differences were all highly statistically significant. There was also a statistically non-significant reduction in the risk of death (RR=0.85: 95% CI 0.63 to 1.14) in the antifibrinolytic treated group.\nNeed for a trial: A simple and widely practicable treatment that reduces blood loss following trauma might prevent thousands of premature trauma deaths each year and secondly could reduce exposure to the risks of blood transfusion. Blood is a scarce and expensive resource and major concerns remain about the risk of transfusion-transmitted infection. . . . A large randomised trial is therefore needed of the use of a simple, inexpensive, widely practicable antifibrinolytic treatment such as tranexamic acid . . . in a wide range of trauma patients who, when they reach hospital are thought to be at risk of major haemorrhage that could significantly affect their chances of survival.\nDose selection\nThe systematic review of randomised controlled trials of antifibrinolytic agents in surgery showed that dose regimens of tranexamic acid vary widely.[reference] . . .\nIn this emergency situation, administration of a fixed dose would be more practicable as determining the weight of a patient would be impossible. Therefore a fixed dose within the dose range which has been shown to inhibit fibrinolysis and provide haemostatic benefit is being used for this trial. . . . The planned duration of administration allows for the full effect of tranexamic acid on the immediate risk of haemorrhage without extending too far into the acute phase response seen after surgery and trauma.\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-6b_background_and_rationale_choice_of_comparators-1",
    "href": "guidelines/spirit/index.html#sec-6b_background_and_rationale_choice_of_comparators-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nThe choice of control interventions has important implications for trial ethics, recruitment, results, and interpretation. In trials comparing an intervention to an active control or usual care, a clear description of the rationale for the comparator intervention will facilitate understanding of its appropriateness.86 87 For example, a trial in which the control group receives an inappropriately low dose of an active drug will overestimate the relative efficacy of the study intervention in clinical practice; conversely, an inappropriately high dose in the control group will lead to an underestimate of the relative harms of the study intervention.87 88\nThe appropriateness of using placebo-only control groups has been the subject of extensive debate and merits careful consideration of the existence of other effective treatments, the potential risks to trial participants, and the need for assay sensitivity—that is, ability to distinguish an effective intervention from less effective or ineffective interventions.89 90 In addition, surveys have demonstrated that a potential barrier to trial participation is the possibility of being allocated a placebo-only or active control intervention that is perceived to be less desirable than the study intervention.68 69 91 92 Evidence also suggests that enrolled participants perceive the effect of a given intervention differently depending on whether the control group consists of an active comparator or only placebo.93 94 95 96\nFinally, studies suggest that some active comparators in head-to-head randomised trials are presumed by trial investigators to be effective despite having never previously been shown to be superior to placebo.74 97 In a systematic review of over 100 head-to-head antibiotic trials for mild to moderate chronic obstructive pulmonary disease,74 cumulative meta-analysis of preceding placebo controlled trials did not show a significant effect of antibiotics over placebo. Such studies again highlight the importance of providing a thorough background and rationale for a trial and the choice of comparators—including data from an up-to-date systematic review—to enable potential participants, physicians, REC/IRBs, and funders to discern the merit of the trial."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-6b_background_and_rationale_choice_of_comparators-2",
    "href": "guidelines/spirit/index.html#sec-6b_background_and_rationale_choice_of_comparators-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nChoice of comparator\n\nIn spite of the increasing numbers of resistant strains, chloroquine monotherapy is still recommended as standard blood-stage therapy for patients with P [Plasmodium] vivax malaria in the countries in which this trial will be conducted. Its selection as comparator is therefore justified. The adult dose of chloroquine will be 620 mg for 2 days followed by 310 mg on the third day and for children 10 mg/kg for the first two days and 5 mg/kg for the third day. Total dose is in accordance with the current practice in the countries where the study is conducted. The safety profile of chloroquine is well established and known. Although generally well tolerated, the following side-effects of chloroquine treatment have been described:\n\nGastro-intestinal disturbances, headache, hypotension, convulsions, visual disturbances, depigmentation or loss of hair, skin reactions (rashes, pruritus) and, rarely, bone-marrow suppression and hypersensitivity reactions such as urticaria and angioedema. Their occurrence during the present trial may however be unlikely given the short (3-day) duration of treatment.\n\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-7_objectives-1",
    "href": "guidelines/spirit/index.html#sec-7_objectives-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nThe study objectives reflect the scientific questions to be answered by the trial, and define its purpose and scope. They are closely tied to the trial design (Item 8) and analysis methods (Item 20). For example, the sample size calculation and statistical analyses for superiority trials will differ from those investigating non-inferiority.\nThe objectives are generally phrased using neutral wording (eg, to compare the effect of treatment A versus treatment B on outcome X) rather than in terms of a particular direction of effect.99 A hypothesis states the predicted effect of the interventions on the trial outcomes. For multiarm trials, the objectives should clarify the way in which all the treatment groups will be compared (eg, A versus B; A versus C)."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-7_objectives-2",
    "href": "guidelines/spirit/index.html#sec-7_objectives-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\n1.1 Research hypothesis\n\nApixaban is noninferior to warfarin for prevention of stroke (hemorrhagic, ischemic or of unspecified type) or systemic embolism in subjects with atrial fibrillation (AF) and additional risk factor(s) for stroke.\n2 Study objectives\n2.1 Primary objective\nTo determine if apixaban is noninferior to warfarin (INR [international normalized ratio] target range 2.0-3.0) in the combined endpoint of stroke (hemorrhagic, ischemic or of unspecified type) and systemic embolism, in subjects with AF and at least one additional risk factor for stroke.\n2.2 Secondary objectives\n2.2.1 Key secondary objectives\nThe key secondary objectives are to determine, in subjects with AF and at least one additional risk factor for stroke, if apixaban is superior to warfarin (INR target range 2.0 - 3.0) for,\n\nthe combined endpoint of stroke (hemorrhagic, ischemic or of unspecified type) and systemic embolism\nmajor bleeding [International Society of Thrombosis and Hemostasis]\nall-cause death\n\n2.2.2 Other secondary objectives\nTo compare, in subjects with AF and at least one additional risk factor for stroke, apixaban and warfarin with respect to:\n\nThe composite endpoint of stroke (ischemic, hemorrhagic, or of unspecified type), systemic embolism and major bleeding, in warfarin naive subjects\nTo assess the safety of apixaban in subjects with AF and at least one additional risk factor for stroke.\n\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-8_trial_design-1",
    "href": "guidelines/spirit/index.html#sec-8_trial_design-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nThe most common design for published randomised trials is the parallel group, two arm, superiority trial with 1:1 allocation ratio.101 Other trial types include crossover, cluster, factorial, split body, and n of 1 randomised trials, as well as single group trials and non-randomised comparative trials.\nFor trials with more than one study group, the allocation ratio reflects the intended relative number of participants in each group (eg, 1:1 or 2:1). Unequal allocation ratios are used for a variety of reasons, including potential cost savings, allowance for learning curves, and ethical considerations when the balance of existing evidence appears to be in favour of one intervention over the other.102 Evidence also suggests a preference of some participants for enrolling in trials with an allocation ratio that favours allocation to an active treatment.92\nThe framework of a trial refers to its overall objective to test the superiority, non-inferiority, or equivalence of one intervention with another, or in the case of exploratory pilot trials, to gather preliminary information on the intervention (eg, harms, pharmacokinetics) and the feasibility of conducting a full-scale trial.\nIt is important to specify and explain the choice of study design because of its close relation to the trial objectives (Item 7) and its influence on the study methods, conduct, costs,103 results,104 105 106 and interpretation. For example, factorial and non-inferiority trials can involve more complex methods, analyses, and interpretations than parallel group superiority trials.107 108 In addition, the interpretation of trial results in published reports is not always consistent with the pre-specified trial framework,6 109 110 especially among reports claiming post hoc equivalence based on a failure to demonstrate superiority rather than a specific test of equivalence.109\nThere is increasing interest in adaptive designs for clinical trials, defined as the use of accumulating data to decide how to modify aspects of a study as it continues, without undermining the validity and integrity of the trial.111 112 Examples of potential adaptations include stopping the trial early, modifying the allocation ratio, re-estimating the sample size, and changing the eligibility criteria. The most valid adaptive designs are those in which the opportunity to make adaptations is based on prespecified decision rules that are fully documented in the protocol (Item 21b)."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-8_trial_design-2",
    "href": "guidelines/spirit/index.html#sec-8_trial_design-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nThe PROUD trial is designed as a randomised, controlled, observer, surgeon and patient blinded multicenter superiority trial with two parallel groups and a primary endpoint of wound infection during 30 days after surgery . . . randomization will be performed as block randomization with a 1:1 allocation.\n\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-9_study_setting-1",
    "href": "guidelines/spirit/index.html#sec-9_study_setting-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nA description of the environment in which a trial will be conducted provides important context in terms of the applicability of the study results; the existence and type of applicable local regulation and ethics oversight; and the type of healthcare and research infrastructure available. These considerations can vary substantially within and between countries.\nAt a minimum, the countries , type of setting (eg, urban versus rural), and the likely number of study sites should be reported in the protocol. These factors have been associated with recruitment success and degree of attrition for some trials,68 91 92 114 115 116 117 but not for others.118 119 Trial location has also been associated with trial outcome,120 aspects of trial quality (eg, authenticity of randomisation121), and generalisability.122"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-9_study_setting-2",
    "href": "guidelines/spirit/index.html#sec-9_study_setting-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nSelection of countries\n\n. . . To detect an intervention-related difference in HIV incidences with the desired power, the baseline incidences at the sites must be sufficiently high. We chose the participating sites so that the average baseline annual incidence across all communities in the study is likely to reach at least 3%. The various sites in sub-Saharan Africa met this criterion, but we also wanted sites in Asia to extend the generalizability of the intervention. The only location in Asia with sufficient incidence at the community level is in ethnic minority communities in Northern Thailand, where HIV incidence is currently in excess of 7%;[reference] thus they were invited to participate as well. Our final selection of sites combines rural (Tanzania, Zimbabwe, Thailand, and KwaZulu-Natal) and an urban (Soweto) location. The cultural circumstances between the sub-Saharan African sites vary widely.\n\nDefinition of community\n\nEach of the three southern African sites (Harare, Zimbabwe; and Soweto and Vulindlela, South Africa) selected eight communities, the East African (Tanzanian) site selected 10 communities, and Thailand selected 14 communities . . . They are of a population size of approximately 10,000 . . . which fosters social familiarity and connectedness, and they are geographically distinct. Communities are defined primarily geographically for operational purposes for the study, taking into account these dimensions of social communality. The communities chosen within each country and site are selected to be sufficiently distant from each other so that there would be little cross-contamination or little possibility that individuals from a control community would benefit from the activities in the intervention community.\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-10_eligibility_criteria-1",
    "href": "guidelines/spirit/index.html#sec-10_eligibility_criteria-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nEligibility criteria for potential trial participants define the study population. They can relate to demographic information; type or severity of the health condition; comorbidities; previous or current treatment; diagnostic procedures; pregnancy; or other relevant considerations.125 In trials of operator-dependent interventions such as surgery and psychotherapy, it is usually important to promote consistency of intervention delivery by also defining the eligibility criteria for care providers and centres where the intervention will be administered.126\nClear delineation of eligibility criteria serves several purposes. It enables study personnel to apply these criteria consistently throughout the trial.127 The choice of eligibility criteria can affect recruitment and attrition,67 114 115 117 118 128 129 130 as well as outcome event rates.39 131 In addition, the criteria convey key information related to external validity (generalisability or applicability).132 The importance of transparent documentation is highlighted by evidence that the eligibility criteria listed in publications are often different from those specified in the protocol.125 133 134\nCertain eligibility criteria warrant explicit justification in the protocol, particularly when they limit the trial sample to a narrow subset of the population.132 135 136 The appropriateness of restrictive participant selection depends on the trial objectives.137 When trial participants differ substantially from the overall population to whom the intervention will be applied, the trial results may not reflect the impact in real world practice settings"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-10_eligibility_criteria-2",
    "href": "guidelines/spirit/index.html#sec-10_eligibility_criteria-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nPatients (or a representative) must provide written, informed consent before any study procedures occur (see Appendix 1 for sample Informed Consent Form) . . .\n\n5.1. Inclusion Criteria\nPatients eligible for the trial must comply with all of the following at randomization:\n\nAge ≥16 years\nCurrent admission under the care of the heart-failure service at the site…\n\n5.2. Exclusion Criteria\n\nAcute decompensation thought by the attending heart-failure physician to require or be likely to require PAC [pulmonary-artery catheter] during the next 24 hours. Such patients should be entered into the PAC Registry (see below).\nInability to undergo PAC placement within the next 12 hours…\n\nPatients enrolled in other investigational drug studies are potential candidates for ESCAPE. As the ESCAPE protocol does not involve any investigational agents or techniques, patients would be eligible for dual randomization if they are on stable doses of the investigational drugs. . . .\nStudy Network, Training, and Responsibilities\n. . . To qualify, physicians responsible for PAC [pulmonary-artery catheter] placements will be required to show proof of insertion of ≥50 PACs in the previous year with a complication rate of <5%. Further, clinicians will need to show competence in the following areas to participate in the study: 1) insertion techniques and cardiovascular anatomy; 2) oxygen dynamics; . . . and 7) common PAC complications.[reference] . . . we will assume basic competence in these areas after satisfactory completion of the PACEP [PAC educational programme] module.\n\nTrial centre requirements\n\nA number of guidelines have stated thrombolysis should only be considered if the patient is admitted to a specialist centre with appropriate experience and expertise.[reference] Hospitals participating in IST-3 [third International Stroke Trial] should have an organized acute stroke service. The components of effective stroke unit care have been identified . . . In brief, the facilities (details of these requirements are specified in the separate operations manual) should include:\n\nWritten protocol for the acute assessment of patients with suspected acute stroke to include interventions to reduce time from onset to treatment.\nImmediate access to CT [computed tomographic] or MR [magnetic resonance] brain scanning (preferably 24 hours a day).\n\nA treatment area where thrombolysis may be administered and the patient monitored according to trial protocol, preferably an acute stroke unit.\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-11a_interventions_description-1",
    "href": "guidelines/spirit/index.html#sec-11a_interventions_description-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nStudies of trials and systematic reviews have shown that important elements of the interventions are not described in half of the publications.146 147 If such elements are also missing from the protocol, or if the protocol simply refers to other documents that are not freely accessible, then it can be impossible for healthcare providers, systematic reviewers, policymakers, and others to fully understand, implement, or evaluate the trial intervention.148 This principle applies to all types of interventions, but is particularly true for complex interventions (eg, health service delivery; psychotherapy), which consist of interconnected components that can vary between healthcare providers and settings.\nFor drugs, biological agents, or placebos, the protocol description should include the generic name, manufacturer, constituent components, route of administration, and dosing schedule (including titration and run-in periods, if applicable).149 150 The description of non-drug interventions—such as devices, procedures, policies, models of care, or counselling—is generally more complex and warrants additional details about the setting (Item 9) and individuals administering the interventions. For example, the level of pre-trial expertise (Item 10) and specific training of individuals administering these complex interventions are often relevant to describe (eg, for surgeons, psychologists, physiotherapists). When intervention delivery is subject to variation, it is important to state whether the same individuals will deliver the trial interventions in all study groups, or whether different individuals will manage each study group—in which case it can be difficult to separate the effect of the intervention from that of the individual delivering it. Interventions that consist of usual care or standard of care require further elaboration in the protocol, as this care can vary substantially across centres and patients, as well as over the duration of the trial."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-11a_interventions_description-2",
    "href": "guidelines/spirit/index.html#sec-11a_interventions_description-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nFor a given trial participant, the assigned study intervention may need to be modified or discontinued by trial investigators for various reasons, including harms, improved health status, lack of efficacy, and withdrawal of participant consent. Comparability across study groups can be improved, and subjectivity in care decisions reduced, by defining standard criteria for intervention modifications and discontinuations in the protocol. Regardless of any decision to modify or discontinue their assigned intervention, study participants should be retained in the trial whenever possible to enable follow-up data collection and prevent missing data (Item 18b)\n\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-11b_interventions_modifications-1",
    "href": "guidelines/spirit/index.html#sec-11b_interventions_modifications-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nStudies of trials and systematic reviews have shown that important elements of the interventions are not described in half of the publications.146 147 If such elements are also missing from the protocol, or if the protocol simply refers to other documents that are not freely accessible, then it can be impossible for healthcare providers, systematic reviewers, policymakers, and others to fully understand, implement, or evaluate the trial intervention.148 This principle applies to all types of interventions, but is particularly true for complex interventions (eg, health service delivery; psychotherapy), which consist of interconnected components that can vary between healthcare providers and settings.\nFor drugs, biological agents, or placebos, the protocol description should include the generic name, manufacturer, constituent components, route of administration, and dosing schedule (including titration and run-in periods, if applicable).149 150 The description of non-drug interventions—such as devices, procedures, policies, models of care, or counselling—is generally more complex and warrants additional details about the setting (Item 9) and individuals administering the interventions. For example, the level of pre-trial expertise (Item 10) and specific training of individuals administering these complex interventions are often relevant to describe (eg, for surgeons, psychologists, physiotherapists). When intervention delivery is subject to variation, it is important to state whether the same individuals will deliver the trial interventions in all study groups, or whether different individuals will manage each study group—in which case it can be difficult to separate the effect of the intervention from that of the individual delivering it. Interventions that consist of usual care or standard of care require further elaboration in the protocol, as this care can vary substantially across centres and patients, as well as over the duration of the trial."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-11b_interventions_modifications-2",
    "href": "guidelines/spirit/index.html#sec-11b_interventions_modifications-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nEligible patients will be randomised in equal proportions between IL-1ra [interleukin-1 receptor antagonist] and placebo, receiving either a once daily, subcutaneous (s.c.) injection of IL-1ra (dose 100 mg per 24 h) for 14 days, or a daily s.c. injection of placebo for 14 days . . .\n\n\nThe study drug and placebo will be provided by Amgen Inc in its commercially available recombinant form . . . The study drug and placebo will be relabelled by Amgen, in collaboration with CTEU [Clinical Trials and Evaluation Unit] according to MHRA [Medicines and Healthcare Products Regulatory Agency] guidelines.\n\n\nThe first dose of IL-1ra will be given within 24 h +2 h of the positive Troponin. Injections will be given at a standardised time (24 ± 2 h after the previous dose), immediately after blood sampling. IL-1ra or placebo will [be] administered to the patient by the research nurse while the patient is in hospital. During the hospital stay, the patient will be taught to self-administer the injection by the research nurse and on discharge will continue at home. This has proven possible in other ACS [acute coronary syndrome] trials that required self injection of subcutaneous heparin [reference]. Full written guidance on self injection will also be provided to patients. If self injection is found not to be possible in an individual patient for unexpected reasons, an alternative method will be sought (eg district nurse, or attending the hospital) to try and maintain full compliance with scheduled study drug regimen after discharge. Patients will also be asked to complete a daily injection diary. All personnel will be blinded to the identity of the syringe contents.\n\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-11c_interventions_adherance-1",
    "href": "guidelines/spirit/index.html#sec-11c_interventions_adherance-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nAdherence to intervention protocols refers to the degree to which the behaviour of trial participants corresponds to the intervention assigned to them.154 Distinct but related concepts include trial retention (Item 18b) and adherence to the follow-up protocol of procedures and assessments (Item 13).\nOn average, adherence to intervention protocols is higher in clinical trials than in non-research settings.155 Although there is no consensus on the acceptable minimum adherence level in clinical trials, low adherence can have a substantial effect on statistical power and interpretation of trial results.156 157 158 Since fewer participants are receiving the full intervention as intended, non-adherence can reduce the contrast between study groups—leading to decreased study power and increased costs associated with recruiting larger sample sizes for evaluating superiority, or leading to potentially inappropriate conclusions of non-inferiority or equivalence. There is also the possibility of underestimating any efficacy and harms of the study intervention.\nFurthermore, if adherence is a marker for general healthy behaviour associated with better prognosis, then different rates of non-adherence between study groups can lead to a biased estimate of an intervention’s effect. In support of this healthy adherer effect, non-adherers to placebo in clinical studies have been found to have poorer clinical outcomes than adherers.159\nTo help avoid these potential detrimental effects of non-adherence, many trials implement procedures and strategies for monitoring and improving adherence,67 156 157 158 and any such plans should be described in the protocol.160 Among applicable drug trials published in 1997-99, 47% reported monitoring the level of adherence.161 Although each of the many types of monitoring methods has its limitations,157 158 adherence data can help to inform the statistical analysis (Item 20c), trial interpretation, and choice of appropriate adherence strategies to implement in the trial as it progresses or in future trials and clinical practice.\nA variety of adherence strategies exist,156 157 158 and their use can be tailored to the specific type of trial design, intervention, and participant population. It may be desirable to select strategies that can be easily implemented in clinical practice, so that the level of adherence in the real world setting is comparable to that observed in the trial."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-11c_interventions_adherance-2",
    "href": "guidelines/spirit/index.html#sec-11c_interventions_adherance-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nAdherence reminder sessions\n\nFace-to-face adherence reminder sessions will take place at the initial product dispensing and each study visit thereafter. This session will include:\n\nThe importance of following study guidelines for adherence to once daily study product\nInstructions about taking study pills including dose timing, storage, and importance of taking pills whole, and what to do in the event of a missed dose.\nInstructions about the purpose, use, and care of the MEMS® cap [medication event monitoring system] and bottle\nNotification that there will be a pill count at every study visit\nReinforcement that study pills may be TDF [tenofovir disproxil fumarate] or placebo\nImportance of calling the clinic if experiencing problems possibly related to study product such as symptoms, lost pills or MEMS® cap.\n\nSubsequent sessions will occur at the follow-up visits. Participants will be asked about any problems they are having taking their study pills or using the MEMS® cap. There will be brief discussion of reasons for missed doses and simple strategies for enhancing adherence, eg, linking pill taking to meals or other daily activities. Participants will have an opportunity to ask questions and key messages from the initial session will be reviewed as needed . . .\nAdherence assessments\nTo enhance validity of data, multiple methods will be used to assess medication adherence including pill count; an electronic medication event monitoring system (MEMS® cap) [reference]; and ACASI [audio-computer administered interview] questionnaire items including a one month visual analogue scale,[reference] reasons for non-compliance, and use of the MEMS® cap. Participants will return the unused tablets and bottle at each follow-up visit. Unused tablets will be counted and recorded on the appropriate CRF [case report form]. Electronic data collected in the MEMS® cap will be downloaded into a designated, secure study computer.\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-11d_interventions_concomitant_care-1",
    "href": "guidelines/spirit/index.html#sec-11d_interventions_concomitant_care-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nIn a controlled trial, a key goal is to have comparable study groups that differ only by the intervention being evaluated, so that any difference in outcomes can be attributed to effects of the study intervention. Cointervention bias can arise when the study groups receive different concomitant care or interventions (in addition to the assigned trial interventions) that may affect trial outcomes.162 To promote comparability of study groups, the protocol should list the relevant concomitant care and interventions that are allowed (including rescue interventions), as well as any that are prohibited."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-11d_interventions_concomitant_care-2",
    "href": "guidelines/spirit/index.html#sec-11d_interventions_concomitant_care-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nRescue Medication\n\nFor weeks 0-3, topical mometasone furoate 0.1% cream or ointment (30 g/week) will be permitted with participants preferably using ointment. Participants will be instructed to apply the topical mometasone furoate to blisters/lesions as required (not to areas of unaffected skin). If the participant is allergic to mometasone furoate or the hospital pharmacy does not stock it, then an alternative topical steroid may be prescribed but this must be in the potent class. In addition, participants will be advised that they can apply a light moisturiser to blisters/lesions at any time during the study.\nFor weeks 3-6, use of mometasone furoate (or other topical corticosteroids) is strongly discouraged to prevent potential systemic effects. Accidental use of mometasone furoate or other potent topical steroid during this period will be classified as a protocol deviation.\nAfter week 6, potent topical corticosteroids (up to 30 g/week) may be used to treat symptoms and localised disease if they would have normally been used as part of normal clinical care by the physician in charge of that patient. This must be recorded on the trial treatment log.\nHowever, those patients who are on a dose reducing regime for oral steroids, 30 g/week of a potent topical steroid will be allowed.\nProhibited Concomitant Medications\nThe administration of live virus vaccines is not permitted for all participants during weeks 0-6 as the investigator is blinded to treatment allocation, and must therefore warn all participants to refrain for [sic] having a live virus vaccine. However, after week 6, once the investigator knows which medication the participant is on, only those taking prednisolone will not be allowed live virus vaccines.\nParticipants should continue to take medications for other conditions as normal. However, if it is anticipated that the participant will need a live virus vaccine during the intervention phase, they will be ineligible for entry into the study\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-12_outcomes-1",
    "href": "guidelines/spirit/index.html#sec-12_outcomes-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nThe trial outcomes are fundamental to study design and interpretation of results. For a given intervention, an outcome can generally reflect efficacy (beneficial effect) or harm (adverse effect). The outcomes of main interest are designated as primary outcomes, which usually appear in the objectives (Item 7) and sample size calculation (Item 14). The remaining outcomes constitute secondary or other outcomes.\nFor each outcome, the trial protocol should define four components: the specific measurement variable, which corresponds to the data collected directly from trial participants (eg, Beck Depression Inventory score, all cause mortality); the participant-level analysis metric, which corresponds to the format of the outcome data that will be used from each trial participant for analysis (eg, change from baseline, final value, time to event); the method of aggregation, which refers to the summary measure format for each study group (eg, mean, proportion with score > 2); and the specific measurement time point of interest for analysis.163\nIt is also important to explain the rationale for the choice of trial outcomes. An ideal outcome is valid, reproducible, relevant to the target population (eg, patients), and responsive to changes in the health condition being studied.67 The use of a continuous versus dichotomous method of aggregation can affect study power and estimates of treatment effect,164 165 and subjective outcomes are more prone to bias from inadequate blinding (ascertainment bias) and allocation concealment (selection bias) than objective outcomes.166 167 Although composite outcomes increase event rates and statistical power, their relevance and interpretation can be unclear if the individual component outcomes vary greatly in event rates, importance to patients, or amount of missing data.\nThe number of primary outcomes should be as small as possible. Although up to 38% of trials define multiple primary outcomes,4 35 163 this practice can introduce problems with multiplicity, selective reporting, and interpretation when there are inconsistent results across outcomes. Problems also arise when trial protocols do not designate any primary outcomes, as seen in half (28/59) of protocols for a sample of trials published from 2002-2008,12 and in 25% of randomised trial protocols that received ethics approval in Denmark in 1994-95.4 Furthermore, major discrepancies in the primary outcomes designated in protocols/registries/regulatory submissions versus final trial publications are common; favour the reporting of statistically significant primary outcomes over non-significant ones; and are often not acknowledged in final publications.172 173 174 175 176 Such bias can only be identified and deterred if trial outcomes are clearly defined beforehand in the protocol and if protocol information is made public.177\nWhere possible, the development and adoption of a common set of key trial outcomes within a specialty can help to deter selective reporting of outcomes and to facilitate comparisons and pooling of results across trials in a meta-analysis.178 179 180 The COMET (Core Outcome Measures in Effectiveness Trials) Initiative aims to facilitate the development and application of such standardised sets of core outcomes for clinical trials of specific conditions (www.comet-initiative.org). Trial investigators are encouraged to ascertain whether there is a core outcome set relevant to their trial and, if so, to include those outcomes in their trial. Existence of a common set of outcomes does not preclude inclusion of additional relevant outcomes for a given trial."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-12_outcomes-2",
    "href": "guidelines/spirit/index.html#sec-12_outcomes-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nPrimary Outcome Measures\n\nDifference between the two treatment arms in the proportion of participants classed as treatment success at 6 weeks. Treatment success is defined as 3 or less significant blisters present on examination at 6 weeks. Significant blisters are defined as intact blisters containing fluid which are at least 5 mm in diameter. However, if the participant has popped a blister, or the blister is at a site that makes it susceptible to bursting such as the sole of the foot, it can be considered part of the blister count, providing there is a flexible (but not dry) roof present over a moist base. Mucosal blisters will be excluded from the count.\nA survey of the UK DCTN [Dermatology Clinical Trials Network] membership showed that a point estimate of 25% inferiority in effectiveness would be acceptable assuming a gain in the safety profile of at least 10%.\n\nThis measure of success was selected as it was considered to be more clinically relevant than a continuous measure of blister count. It would be less clinically relevant to perform an absolute blister count and report a percentage reduction. Instead, to state that treatment is considered a success if remission is achieved (ie the presence of three or less blisters on physical examination at 6 weeks) more closely reflects clinical practice. In addition, it is far less burdensome on investigators than including a full blister count, which would mean counting in the region of 50-60 blisters in many cases. This outcome measure will be performed as a single blind assessment.\n\n\nDifference between the two treatment arms in the proportion of participants reporting grade 3, 4 and 5 (mortality) adverse events which are possibly, probably or definitely related to BP [bullous pemphigoid] medication in the 52 weeks following randomisation. A modified version of The Common Terminology Criteria for Adverse Events (CTCAE v3.0) will be used to grade adverse events. At each study visit, participants will be questioned about adverse events they have experienced since the last study visit (using a standard list of known side effects of the two study drugs).\n\n\n\nSecondary Outcome Measures\n\n\nFor the secondary and tertiary endpoints a participant will be classed as a treatment success if they have 3 or less significant blisters present on examination and have not had their treatment modified (changed or dose increased) on account of a poor response.\n\nDifference in the proportion of participants who are classed as a treatment success at 6 weeks.\n\nDifference in the proportion of participants in each treatment arm who are classed as treatment success at 6 weeks and are alive at 52 weeks. This measure will provide a good overall comparison of the two treatment arms. . . .\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-13_participant_timeline-1",
    "href": "guidelines/spirit/index.html#sec-13_participant_timeline-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nA clear and concise timeline of the study visits, enrolment process, interventions, and assessments performed on participants can help to guide trial conduct and enable external review of participant burden and feasibility. These factors can also affect the decision of potential investigators and participants to join the trial (Item 15).91\nA schematic diagram is highly recommended to efficiently present the overall schedule and time commitment for trial participants in each study group. Though various presentation formats exist, key information to convey includes the timing of each visit, starting from initial eligibility screening through to study close-out; time periods during which trial interventions will be administered; and the procedures and assessments performed at each visit (with reference to specific data collection forms, if relevant)"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-13_participant_timeline-2",
    "href": "guidelines/spirit/index.html#sec-13_participant_timeline-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nThe main outcomes of interest are the drug and sex-related HIV and HCV [hepatitis C virus] risk behaviors . . . Clients will be assessed using the full battery of instruments from the Common Assessment Battery (CAB), along with the Self-Efficacy and Stages of Change questionnaires and a Urine Drug Screen after consenting . . . questionnaires will take place for all participants 14-30 days after randomization during which they will be given the Stages of Change and Self-Efficacy questionnaires, the Timeline Follow-Back, and a UA [urine analysis]. Follow-up interviews, using the full battery (CAB and questionnaires), will be collected at 2 months (56 days), 4 months (112 days) and 6 months (168 days) after the randomization date. A 14 day window, defined as 7 days before and 7 days after the due date, will be available to complete the 2 and 4 month follow-up interviews and a 28 day window, defined as 7 days before and 21 days after the due date, will be available to complete the 6 month follow up interview…\n\n\n7.1.1 Common Assessment Battery (CAB)\n\nA Demographic Questionnaire . . .\nThe Composite International Diagnostic Interview Version 2.1…\nThe Addiction Severity Index-Lite (ASI-Lite) . . .\nThe Risk Behavior Survey (RBS), . . .\n7.1.2 Additional Interviews/Questionnaires\nTo assess drug use, urinalysis for morphine, cocaine, amphetamine, and methamphetamine will be performed at the 2-Week Interim Visit, and the 2-, 4-, and 6-month Follow-up visits…\n\nStage of change for quitting drug use will be measured using a modification of the Motivation Scales\n\nTable 3 HIV/HCV risk reduction protocol schedule of forms and procedures (adapted from original table) http://www.bmj.com/content/346/bmj.e7586.long\n\nThe trial consists of a 12-week intervention treatment phase with a 40-week follow-up phase. The total trial period will be 12-months. As shown . . . measurements will be undertaken at four time-points in each group: at baseline, directly after completing the 12-week internet program, and at six and 12-month follow-up\n\nFig 2: Flow of participants http://www.bmj.com/content/bmj/346/bmj.e7586/F2.large.jpg\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-14_sample_size-1",
    "href": "guidelines/spirit/index.html#sec-14_sample_size-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nThe planned number of trial participants is a key aspect of study design, budgeting, and feasibility that is usually determined using a formal sample size calculation. If the planned sample size is not derived statistically, then this should be explicitly stated along with a rationale for the intended sample size (eg, exploratory nature of pilot studies; pragmatic considerations for trials in rare diseases).\nFor trials that involve a formal sample size calculation, the guiding principle is that the planned sample size should be large enough to have a high probability (power) of detecting a true effect of a given magnitude, should it exist. Sample size calculations are generally based on one primary outcome; however, it may also be worthwhile to plan for adequate study power or report the power that will be available (given the proposed sample size) for other important outcomes or analyses because trials are often underpowered to detect harms or subgroup effects.\nAmong randomised trial protocols that describe a sample size calculation, 4-40% do not state all components of the calculation.6 11 The protocol should generally include the following: the outcome (Item 12); the values assumed for the outcome in each study group (eg, proportion with event, or mean and standard deviation) (table 4⇓); the statistical test (Item 20a); alpha (type 1 error) level; power; and the calculated sample size per group—both assuming no loss of data and, if relevant, after any inflation for anticipated missing data (Item 20c). Trial investigators are also encouraged to provide a rationale or reference for the outcome values assumed for each study group.187 The values of certain prespecified variables tend to be inappropriately inflated (eg, clinically important treatment effect size)188 189 or underestimated (eg, standard deviation for continuous outcomes),190 leading to trials having less power in the end than what was originally calculated. Finally, when uncertainty of a sample size estimate is acknowledged, methods exist for re-estimating sample size.191 The intended use of such an adaptive design approach should be stated in the protocol.\nFor designs and frameworks other than parallel group superiority trials, additional elements are required in the sample size calculation. For example, an estimate of the standard deviation of within-person changes from baseline should be included for crossover trials192; the intracluster correlation coefficient for cluster randomised trials193; and the equivalence or non-inferiority margin for equivalence or non-inferiority trials respectively.108 194 Such elements are often not described in final trial reports,110 195 196 197 198 and it is unclear how often they are specified in the protocol.\nComplete description of sample size calculations in the protocol enables an assessment of whether the trial will be adequately powered to detect a clinically important difference. It also promotes transparency and discourages inappropriate post hoc revision that is intended to support a favourable interpretation of results or portray consistency between planned and achieved sample sizes."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-14_sample_size-2",
    "href": "guidelines/spirit/index.html#sec-14_sample_size-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nThe sample size was calculated on the basis of the primary hypothesis. In the exploratory study,[reference] those referred to PEPS [psychoeducation with problem solving] had a greater improvement in social functioning at 6 month follow-up equivalent to 1.05 points on the SFQ [Social Functioning Questionnaire]. However, a number of people received PEPS who were not included in the trial (eg, the wait-list control) and, for this larger sample (N=93), the mean pre-post- treatment difference was 1.79 (pre-treatment mean=13.85, SD=4.21; post-treatment mean=12.06, SD=4.21). (Note: a lower SFQ score is more desirable). This difference of almost 2 points accords with other evidence that this is a clinically significant and important difference.[reference] A reduction of 2 points or more on the SFQ at 1 year follow-up in an RCT of cognitive behaviour therapy in health anxiety was associated with a halving of secondary care appointments (1.24.vs 0.65), a clinically significant reduction in the Hospital Anxiety and Depression Scale (HADS[reference]) Anxiety score of 2.5 (9.9 vs 7.45) and a reduction in health anxiety (the main outcome) of 5.6 points (17.8 vs 12.2) (11 is a normal population score and 18 is pathological).[reference] These findings suggest that improvements in social functioning may accrue over 1 year, hence we expect to find a greater magnitude of response at the 72 week follow-up than we did in the exploratory trial. Therefore, we have powered this trial to be able to detect a difference in SFQ score of 2 points. SFQ standard deviations vary between treatment, control, and the wait-list samples, ranging from 3.78 to 4.53. We have based our sample size estimate on the most conservative (ie, largest) SD [standard deviation]. To detect a mean difference in SFQ score of 2 point (SD = 4.53) at 72 weeks with a two-sided significance level of 1% and power of 80% with equal allocation to two arms would require 120 patients in each arm of the trial. To allow for 30% drop out, 170 will be recruited per arm, ie, 340 in total.”183\n\n\nSuperficial and deep incisional surgical site infection rates for patients in the PDS II® [polydioxanone suture] group are estimated to occur at a rate of 0.12.[reference] The trials by [reference] have shown a reduction of SSI [surgical site infections] of more than 50% (from 10.8% to 4.9% and from 9.2% to 4.3% respectively). Therefore, we estimate a rate of 0.06 for PDS Plus® [triclosan-coated continuous polydioxanone suture].\n\n\nFor a fixed sample size design, the sample size required to achieve a power of 1-β=0.80 for the one-sided chi-square test at level α=0.025 under these assumptions amounts to 2×356=712 (nQuery Advisor®, version 7.0). It can be expected that including covariates of prognostic importance in the logistic regression model as defined for the confirmatory analysis will increase the power as compared to the chi-square test. As the individual results for the primary endpoint are available within 30 days after surgery, the drop-out rate is expected to be small. Nevertheless, a potential dilution of the treatment effect due to drop-outs is taken into account (eg no photographs available, loss to follow up); it is assumed that this can be compensated by additional 5% of patients to be randomized, and therefore the total sample size required for a fixed sample size design amounts to n=712+38=750 patients…\n\n\nAn adaptive interim analysis [reference] will be performed after availability of the results for the primary endpoint for a total of 375 randomized patients (ie, 50% of the number of patients required in a fixed sample size design). The following type I error rates and decision boundaries for the interim and the final analysis are specified:\n\n\n• Overall one-sided type I error rate: 0.025\n\n• Boundary for the one-sided p-value of the first stage for accepting the null-hypothesis within the interim analysis: α0=0.5\n• One-sided local type I error rate for testing the null-hypothesis within the interim analysis: α1=0.0102\n• Boundary for the product of the one-sided p-values of both stages for the rejection of the null-hypothesis in the final analysis: cα=0.0038\nIf the trial will be continued with a second stage after the interim analysis (this is possible if for the one-sided p-value p1 of the interim analysis p1∈]0.0102,0.5[ [ie 0.5≥P1≥0.0102] holds true, the results of the interim analysis can be taken into account for a recalculation of the required sample size. If the sample size recalculation leads to the conclusion that more than 1200 patients are required, the study is stopped, because the related treatment group difference is judged to be of minor clinical importance…\n\nThe actually achieved sample size is then not fixed but random, and a variety of scenarios can be considered. If the sample size is calculated under the same assumptions with respect to the SSI rates for the two groups, applying the same the overall significance level of α=0.025 (one-sided) but employing additionally the defined stopping boundaries and recalculating the sample size for the second stage at a conditional power of 80% on the basis of the SSI rates observed in the interim analysis results in an average total sample size of n=766 patients; the overall power of the study is then 90% (ADDPLAN®, version 5.0)\n\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-15_recruitment-1",
    "href": "guidelines/spirit/index.html#sec-15_recruitment-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nThe main goal of recruitment is to meet the target sample size (Item 14). However, recruitment difficulties are commonly encountered in clinical trials.209 210 211 212 213 For example, reviews of government funded trials in the US and UK found that two thirds did not reach their recruitment targets.214 215 Low enrolment will reduce statistical power and can lead to early trial stoppage or to extensions with delayed results and greater costs.\nStrategies to promote adequate enrolment are thus important to consider during trial planning. Recruitment strategies can vary depending on the trial topic, context, and site. Different recruitment methods can substantially affect the number and type of trial participants recruited128 209 216 217 218 219 220 and can incur different costs.221 222 223 Design issues such as the number and stringency of eligibility criteria will also directly affect the number of eligible trial participants.\nProtocol descriptions of where participants will be recruited (eg, primary care clinic, community), by whom (eg, surgeon), when (eg, time after diagnosis), and how (eg, advertisements, review of health records) can be helpful for assessing the feasibility of achieving the target sample size and the applicability of the trial results in practice. Other relevant information to explicitly provide in the protocol includes expected recruitment rates, duration of the recruitment period, plans to monitor recruitment during the trial, and any financial or non-financial incentives provided to trial investigators or participants for enrolment (Item 4). If strategies differ by site in multicentre trials, these should be detailed to the extent possible"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-15_recruitment-2",
    "href": "guidelines/spirit/index.html#sec-15_recruitment-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nEach center will screen subjects to achieve screening percentages of 50% women and 33% minority; screening will continue until the target population is achieved (12 subjects/site). We recognize that, because of exclusion by genotype and genotypic variation among diverse populations,[reference], the enrolled cohort may not reflect the screened population. The enrollment period will extend over 12 months.\n\n\nRecruitment strategy\n\nEach clinical center involved in the ACRN [Asthma Clinical Research Network] was chosen based on documentation for patient availability, among other things. It is, however, worthy to note the specific plans of each center…\n\n…The Asthma Clinical Research Center at the Brigham & Women’s Hospital utilizes three primary resources for identifying and recruiting potential subjects as described below.\n\n\n\nResearch Patient Database\n\n\nThe Asthma Clinical Research Center at the Brigham and Women’s Hospital has a database of over 1,500 asthmatics…\n\nAsthma Patient Lists…\nAdvertisements…\n\n…the Madison ACRN site has utilized some additional approaches to target minority recruitment. We have utilized a marketing expert to coordinate and oversee our overall efforts in recruiting and retaining minorities. . . . As a result of his efforts, we have advertised widely in newspapers and other publications that target ethnic minorities, established contacts with various ethnic community, university, church, and business groups, and conducted community-based asthma programs…For example, student groups such as AHANA (a pre-health careers organization focusing on minority concerns) will be contacted…In addition, we will utilize published examples of successful retention strategies such as frequent payment of subject honoraria as study landmarks are achieved and study participant group social events. Study visits will be carefully planned and scheduled to avoid exam-time and university calendar breaks…\n\nThe Harlem Hospital Center Emergency Department (ED) sees an average of eight adult patients per day for asthma. Through the REACH (Reducing Emergency Asthma Care in Harlem) project, we have…successfully recruited and interviewed 380 patients from the ED…\n\n\nResponses to inquiries about participation in research studies are answered by a dedicated phone line that is manned during business hours and answered by voicemail at all other times. A research assistant responds to each inquiry immediately, using a screening instrument…\n\n\nPatients are recruited for clinical trials at the Jefferson Center through two primary mechanisms: (1) local advertising; and (2) identification in the asthma patient registry (database). Local advertising takes advantage of the printed as well as the audio-visual media. Printed media include …All advertising in the printed and audio-visual media has prior approval of the Institutional Review Board.\n\n\nThe Jefferson patient registry (database) has been maintained since 1992 and currently contains 3,100 patients… It is estimated that 300-400 new asthmatic patients are seen each year, while a smaller number become inactive due to relocation, change of health care provider, etc. Once identified in the database, patients potentially eligible for a specific study are contacted by the nurse coordinator who explains the study and ascertains the patient’s interest. If interested, the patient is seen in the clinical research laboratories where more detailed evaluations are made…\n\n\nEach subject will receive financial compensation within FDA [Food and Drug Administration] guidelines for participation in an amount determined by the local center. For subjects who drop out, payments will be pro-rated for the length of time they stayed in the study, but payment will not be made until the study would have been completed had the subject not dropped out.\n\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-16a_allocation_sequence_generation-1",
    "href": "guidelines/spirit/index.html#sec-16a_allocation_sequence_generation-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nParticipants in a randomised trial should be assigned to study groups using a random (chance) process characterised by unpredictability of assignments. Randomisation decreases selection bias in allocation; helps to facilitate blinding/masking after allocation; and enables the use of probability theory to test whether any difference in outcome between intervention groups reflects chance.17 225 226 227\nUse of terms such as randomisation without further elaboration is not sufficient to describe the allocation process, as these terms have been used inappropriately to describe non-random, deterministic allocation methods such as alternation or allocation by date of birth.121 In general, these non-random allocation methods introduce selection bias and biased estimates of an intervention’s effect size,17 167 228 229 mainly due to the lack of allocation concealment (Item 16b). If non-random allocation is planned, then the specific method and rationale should be stated.\nBox 1 outlines the key elements of the random sequence that should be detailed in the protocol. Three quarters of randomised trial protocols approved by a research ethics committee in Denmark (1994-95) or conducted by a US cooperative cancer research group (1968-2006) did not describe the method of sequence generation."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-16a_allocation_sequence_generation-2",
    "href": "guidelines/spirit/index.html#sec-16a_allocation_sequence_generation-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nParticipants will be randomly assigned to either control or experimental group with a 1:1 allocation as per a computer generated randomisation schedule stratified by site and the baseline score of the Action Arm Research Test (ARAT; <=21 versus >21) using permuted blocks of random sizes. The block sizes will not be disclosed, to ensure concealment.\n\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-16b_allocation_concealment_mechanism-1",
    "href": "guidelines/spirit/index.html#sec-16b_allocation_concealment_mechanism-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nSuccessful randomisation in practice depends on two interrelated aspects: 1) generation of an unpredictable allocation sequence (Item 16a) and 2) concealment of that sequence until assignment irreversibly occurs.233 241 The allocation concealment mechanism aims to prevent participants and recruiters from knowing the study group to which the next participant will be assigned. Allocation concealment helps to ensure that a participant’s decision to provide informed consent, or a recruiter’s decision to enrol a participant, is not influenced by knowledge of the group to which they will be allocated if they join the trial.242 Allocation concealment should not be confused with blinding (masking) (Item 17)\nWithout adequate allocation concealment, even random, unpredictable assignment sequences can be subverted.233 241 For example, a common practice is to enclose assignments in sequentially numbered, sealed envelopes. However, if the envelopes are not opaque and contents are visible when held up to a light source, or if the envelopes can be unsealed and resealed, then this method of allocation concealment can be corrupted.\nProtocols should describe the planned allocation concealment mechanism in sufficient detail to enable assessment of its adequacy. In one study of randomised trial protocols in Denmark, over half did not adequately describe allocation concealment methods.2 In contrast, central randomisation was stated as the allocation concealment method in all phase III trial protocols initiated in 1968-2003 by a cooperative cancer research group that used extensive protocol review processes.11 Like sequence generation, inadequate reporting of allocation concealment in trial publications is common and has been associated with inflated effect size estimates."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-16b_allocation_concealment_mechanism-2",
    "href": "guidelines/spirit/index.html#sec-16b_allocation_concealment_mechanism-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nParticipants will be randomised using TENALEA, which is an online, central randomisation service . . . Allocation concealment will be ensured, as the service will not release the randomisation code until the patient has been recruited into the trial, which takes place after all baseline measurements have been completed\n\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-16c_allocation_implementation-1",
    "href": "guidelines/spirit/index.html#sec-16c_allocation_implementation-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nBased on the risk of bias associated with some methods of sequence generation and inadequate allocation concealment, trial investigators should strive for complete separation of the individuals involved in the steps before enrolment (sequence generation process and allocation concealment mechanism) from those involved in the implementation of study group assignments. When this separation is not possible, it is important for the investigators to ensure that the assignment schedule is unpredictable and locked away from even the person who generated it. The protocol should specify who will implement the various stages of the randomisation process, how and where the allocation list will be stored, and mechanisms employed to minimise the possibility that those enrolling and assigning participants will obtain access to the list."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-16c_allocation_implementation-2",
    "href": "guidelines/spirit/index.html#sec-16c_allocation_implementation-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nRandomization\n\nAll patients who give consent for participation and who fulfil the inclusion criteria will be randomized. Randomisation will be requested by the staff member responsible for recruitment and clinical interviews from CenTrial [Coordination Centre of Clinical Trials].\n\nIn return, CenTrial will send an answer form to the study therapist who is not involved in assessing outcome of the study. This form will include a randomisation number. In every centre closed envelopes with printed randomisation numbers on it are available. For every randomisation number the corresponding code for the therapy group of the randomisation list will be found inside the envelopes. The therapist will open the envelope and will find the treatment condition to be conducted in this patient. The therapist then gives the information about treatment allocation to the patient. Staff responsible for recruitment and symptom ratings is not allowed to receive information about the group allocation…\n\n\n…The allocation sequence will be generated by the Institute for Medical Biometry (IMB) applying a permuted block design with random blocks stratified by study centre and medication compliance (favourable vs. unfavourable)… The block size will be concealed until the primary endpoint will be analysed. Throughout the study, the randomisation will be conducted by CenTrial in order to keep the data management and the statistician blind against the study condition as long as the data bank is open. The randomisation list remains with CenTrial for the whole duration of the study. Thus, randomisation will be conducted without any influence of the principal investigators, raters or therapists.\n\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-17a_blinding_masking-1",
    "href": "guidelines/spirit/index.html#sec-17a_blinding_masking-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nBlinding or masking (the process of keeping the study group assignment hidden after allocation) is commonly used to reduce the risk of bias in clinical trials with two or more study groups.166 248 Awareness of the intervention assigned to participants can introduce ascertainment bias in the measurement of outcomes, particularly subjective ones (eg, quality of life)166 167; performance bias in the decision to discontinue or modify study interventions (eg, dosing changes) (Item 11b), concomitant interventions, or other aspects of care (Item 11d)229; and exclusion/attrition bias in the decision to withdraw from the trial or to exclude a participant from the analysis.249 250 We have elected to use the term blinding but acknowledge that others prefer the term masking because blind also relates to an ophthalmological condition and health outcomes.251 252\nMany groups can be blinded: trial participants, care providers, data collectors, outcome assessors or committees (Item 5d), data analysts,253 and manuscript writers. Blinding of data monitoring committees is generally discouraged.254 255\nWhen blinding of trial participants and care providers is not possible because of obvious differences between the interventions,256 257 blinding of the outcome assessors can often still be implemented.17 It may also be possible to blind participants or trial personnel to the study hypothesis in terms of which intervention is considered active. For example, in a trial evaluating light therapy for depression, participants were informed that the study involved testing two different forms of light therapy, whereas the true hypothesis was that bright blue light was considered potentially effective and that dim red light was considered placebo.258\nDespite its importance, blinding is often poorly described in trial protocols.3 The protocol should explicitly state who will be blinded to intervention groups—at a minimum, the blinding status of trial participants, care providers, and outcome assessors. Such a description is much preferred over the use of ambiguous terminology such as single blind or double blind.259 260 Protocols should also describe the comparability of blinded interventions (Item 11a)150—for example, similarities in appearance, use of specific flavours to mask a distinctive taste—and the timing of final unblinding of all trial participants (eg, after the creation of a locked analysis data set).3\nFurthermore, any strategies to reduce the potential for unblinding should be described in the protocol, such as pre-trial testing of blinding procedures.261 The use of a fixed code (versus a unique code for each participant) to denote each study group assignment (eg, A=Group 1; B=Group 2) can be problematic, as the unblinding of one participant will result in the inadvertent loss of blinding for all trial participants.\nSome have suggested that the success of blinding be formally tested by asking key trial persons to guess the study group assignment and comparing these responses to what would be expected by chance.262 However, it is unclear how best to interpret the results of such tests.263 264 If done, the planned testing methods should be described in the trial protocol."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-17a_blinding_masking-2",
    "href": "guidelines/spirit/index.html#sec-17a_blinding_masking-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nAssessments regarding clinical recovery will be conducted by an assessor blind to treatment allocation. The assessor will go through a profound assessment training program… Due to the nature of the intervention neither participants nor staff can be blinded to allocation, but are strongly inculcated not to disclose the allocation status of the participant at the follow up assessments. An employee outside the research team will feed data into the computer in separate datasheets so that the researchers can analyse data without having access to information about the allocation.\n\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-17b_blinding_masking_emergency_unblinding-1",
    "href": "guidelines/spirit/index.html#sec-17b_blinding_masking_emergency_unblinding-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nAmong 58 blinded Danish trials approved in 1994-95, three quarters of protocols described emergency unblinding procedures.3 Such procedures to reveal the assigned intervention in certain circumstances are intended to increase the safety of trial participants by informing the clinical management of harms or other relevant conditions that arise. A clear protocol description of the conditions and procedures for emergency unblinding helps to prevent unnecessary unblinding; facilitates implementation by trial personnel when indicated; and enables evaluation of the appropriateness of the planned procedures. In some cases (eg, minor, reversible harms), stopping and then cautiously reintroducing the assigned intervention in the affected participant can avoid both unblinding and further harm."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-17b_blinding_masking_emergency_unblinding-2",
    "href": "guidelines/spirit/index.html#sec-17b_blinding_masking_emergency_unblinding-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nTo maintain the overall quality and legitimacy of the clinical trial, code breaks should occur only in exceptional circumstances when knowledge of the actual treatment is absolutely essential for further management of the patient. Investigators are encouraged to discuss with the Medical Advisor or PHRI [Population Health Research Institute] physician if he/she believes that unblinding is necessary.\n\n\nIf unblinding is deemed to be necessary, the investigator should use the system for emergency unblinding through the PHRI toll-free help line as the main system or through the local emergency number as the back-up system.\n\n\nThe Investigator is encouraged to maintain the blind as far as possible. The actual allocation must NOT be disclosed to the patient and/or other study personnel including other site personnel, monitors, corporate sponsors or project office staff; nor should there be any written or verbal disclosure of the code in any of the corresponding patient documents.\n\n\nThe Investigator must report all code breaks (with reason) as they occur on the corresponding CRF [case report form] page.\n\n\nUnblinding should not necessarily be a reason for study drug discontinuation.\n\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-18a_data_collection_plan-1",
    "href": "guidelines/spirit/index.html#sec-18a_data_collection_plan-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nThe validity and reliability of trial data depend on the quality of the data collection methods. The processes of acquiring and recording data often benefit from attention to training of study personnel and use of standardised, pilot tested methods. These should be identical for all study groups, unless precluded by the nature of the intervention.\nThe choice of methods for outcome assessment can affect study conduct and results.268 269 270 271 272 273 Substantially different responses can be obtained for certain outcomes (eg, harms) depending on who answers the questions (eg, the participant or investigator) and how the questions are presented (eg, discrete options or open ended).269 274 275 276 Also, when compared to paper based data collection, the use of electronic handheld devices and internet websites has the potential to improve protocol adherence, data accuracy, user acceptability, and timeliness of receiving data.268 270 271 277\nThe quality of data also depends on the reliability, validity, and responsiveness of data collection instruments such as questionnaires278 or laboratory instruments. Instruments with low inter-rater reliability will reduce statistical power,272 while those with low validity will not accurately measure the intended outcome variable. One study found that only 35% (47/133) of randomised trials in acute stroke used a measure with established reliability or validity.279 Modified versions of validated measurement tools may no longer be considered validated, and use of unpublished measurement scales can introduce bias and inflate treatment effect sizes.280\nStandard processes should be implemented by local study personnel to enhance data quality and reduce bias by detecting and reducing the amount of missing or incomplete data, inaccuracies, and excessive variability in measurements.281 282 283 284 285 Examples include standardised training and testing of outcome assessors to promote consistency; tests of the validity or reliability of study instruments; and duplicate data measurements.\nA clear protocol description of the data collection process—including the personnel, methods, instruments, and measures to promote data quality—can facilitate implementation and helps protocol reviewers to assess their appropriateness. Inclusion of data collection forms in the protocol (ie, as appendices) is highly recommended, as the way in which data are obtained can substantially affect the results. If not included in the protocol, then a reference to where the forms can be found should be provided. If performed, pilot testing and assessment of reliability and validity of the forms should also be described."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-18a_data_collection_plan-2",
    "href": "guidelines/spirit/index.html#sec-18a_data_collection_plan-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nPrimary outcome\n\nDelirium recognition: In accordance with national guidelines [reference], the study will identify delirium by using the RASS [Richmond Agitation-Sedation Scale] and the CAM-ICU [Confusion Assessment Method for the intensive care unit] on all patients who are admitted directly from the emergency room or transferred from other services to the ICU. Such assessment will be performed after 24 hours of ICU admission and twice daily until discharge from the hospital . . . RASS has excellent inter-rater reliability among adult medical and surgical ICU patients and has excellent validity when compared to a visual analogue scale and other selected sedation scales[reference] . . . The CAM-ICU was chosen because of its practical use in the ICU wards, its acceptable psychometric properties, and based on the recommendation of national guidelines[reference] . . . The CAM-ICU diagnosis of delirium was validated against the DSM-III-R [Diagnostic and Statistical Manual of Mental Disorders, Third Edition—Revised] delirium criteria determined by a psychiatrist and found to have a sensitivity of 97% and a specificity of 92%.[reference] The CAM-ICU has been developed, validated and applied into ICU settings and multiple investigators have used the same method to identify patients with delirium.[reference]\n\nDelirium severity: Since the CAM-ICU does not evaluate delirium severity, we selected the Delirium Rating Scale revised-1998 (DRS-R-98)[reference] . . . The DRS-R-98 was designed to evaluate the breadth of delirium symptoms for phenomenological studies in addition to measuring symptom severity with high sensitivity and specificity . . . The DRS-R-98 is a 16-item clinician-rated scale with anchored items descriptions . . . The DRS-R-98 has excellent inter-rater reliability (intra-class correlation 0.97) and internal consistency (Cronbach’s alpha 0.94).[reference]\n\n\nSecondary outcomes\n\nThe study will collect demographic and baseline functional information from the patient’s legally authorized representative and/or caregivers. Cognitive function status will be obtained by interviewing the patient’s legally authorized representative using the Informant Questionnaire on Cognitive Decline in the Elderly (IQCODE). IQCODE is a questionnaire that can be completed by a relative or other caregiver to determine whether that person has declined in cognitive functioning. The IQCODE lists 26 everyday situations . . . Each situation is rated by the informant for amount of change over the previous 10 years, using a Likert scale ranging from 1-much improved to 5-much worse. The IQCODE has a sensitivity between 69% to 100% and specificity of 80% to 96% for dementia.[reference]\n\nUtilizing the electronic medical record system (RMRS), we will collect several data points of interest at baseline and throughout the study period . . . We have previously defined hospital-related consequences to include: the number of patients with documented falls, use of physical restraints . . . These will be assessed using the RMRS, direct daily observation, and retrospective review of the electronic medical record. This definition of delirium related hospital complications has been previously used and published.[reference]\n\n\nTraining and certification plans\n\n. . . Each center’s personnel will be trained centrally in the study requirements, standardized measurement of height, weight, and blood pressure, requirements for laboratory specimen collection including morning urine samples, counseling for adherence and the eliciting of information from study participants in a uniform reproducible manner.\n\n…The data to be collected and the procedures to be conducted at each visit will be reviewed in detail. Each of the data collection forms and the nature of the required information will be discussed in detail on an item by item basis. Coordinators will learn how to code medications using the WHODrug software and how to code symptoms using the MedDRA software. Entering data forms, responding to data discrepancy queries and general information about obtaining research quality data will also be covered during the training session.\n\n…\n13.7. Quality Control of the Core Lab\nData from the Core Lab will be securely transmitted in batches and quality controlled in the same manner as Core Coordinating Center data; ie data will be entered and verified in the database on the Cleveland Clinic Foundation SUN with a subset later selected for additional quality control. Appropriate edit checks will be in place at the key entry (database) level.\n\nThe Core Lab is to have an internal quality control system established prior to analyzing any FSGS [focal segmental glomerulosclerosis] samples. This system will be outlined in the Manual of Operations for the Core Lab(s) which is prepared and submitted by the Core Lab to the DCC [data coordinating centre] prior to initiating of the study.\n\n\nAt a minimum this system must include:\n\n\nThe inclusion of at least two known quality control samples; the reported measurements of the quality control samples must fall within specified ranges in order to be certified as acceptable.\nCalibration at FDA approved manufacturers’ recommended schedules.\n\n\n13.8. Quality Control of the Biopsy Committee\n\nThe chair of the pathology committee will circulate to all of the study pathologists . . . samples [sic] biopsy specimens for evaluation after criteria to establish diagnosis of FSGS has been agreed. This internal review process will serve to ensure common criteria and assessment of biopsy specimens for confirmation of diagnosis of FSGS.\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-18b_data_collection_plan_retention-1",
    "href": "guidelines/spirit/index.html#sec-18b_data_collection_plan_retention-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nTrial investigators must often seek a balance between achieving a sufficiently long follow-up for clinically relevant outcome measurement,122 288 and a sufficiently short follow-up to decrease attrition and maximise completeness of data collection. Non-retention refers to instances where participants are prematurely off-study (ie, consent withdrawn or lost to follow-up) and thus outcome data cannot be obtained from them. The majority of trials will have some degree of non-retention, and the number of these off-study participants usually increases with the length of follow-up.116\nIt is desirable to plan ahead for how retention will be promoted in order to prevent missing data and avoid the associated complexities in both the study analysis (Item 20c) and interpretation. Certain methods can improve participant retention,67 152 289 290 291 292 such as financial reimbursement; systematic methods and reminders for contacting patients, scheduling appointments, and monitoring retention; and limiting participant burden related to follow-up visits and procedures (Item 13). A participant who withdraws consent for follow-up assessment of one outcome may be willing to continue with assessments for other outcomes, if given the option.\nNon-retention should be distinguished from non-adherence.293 Non-adherence refers to deviation from intervention protocols (Item 11c) or from the follow-up schedule of assessments (Item 13), but does not mean that the participant is off-study and no longer in the trial. Because missing data can be a major threat to trial validity and statistical power, non-adherence should not be an automatic reason for ceasing to collect data from the trial participant prior to study completion. In particular for randomised trials, it is widely recommended that all participants be included in an intention to treat analysis, regardless of adherence (Item 20c).\nProtocols should describe any retention strategies and define which outcome data will be recorded from protocol non-adherers.152 Protocols should also detail any plans to record the reasons for non-adherence (eg, discontinuation of intervention due to harms versus lack of efficacy) and non-retention (ie, consent withdrawn; lost to follow-up), as this information can influence the handling of missing data and interpretation of results."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-18b_data_collection_plan_retention-2",
    "href": "guidelines/spirit/index.html#sec-18b_data_collection_plan_retention-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\n5.2.2 Retention\n\n…As with recruitment, retention addresses all levels of participant.\n\nAt the parent and student level, study investigators and staff:\n\nProvide written feedback to all parents of participating students about the results of the health screenings . . .\nMaintain interest in the study through materials and mailings . . .\nSend letters to parents and students prior to the final data collection, reminding them of the upcoming data collection and the incentives the students will receive.\n\nAt the school level, study investigators and staff:\n\nProvide periodic communications via newsletters and presentations to inform the school officials/staff, students, and parents about type 2 diabetes, the current status of the study, and plans for the next phase, as well as to acknowledge their support.\n\n…Become a presence in the intervention schools to monitor and maintain consistency in implementation, . . . be as flexible as possible with study schedule and proactive in resolving conflicts with schools.\n\nProvide school administration and faculty with the schedule or grid showing how the intervention fits into the school calendar . . .\nSolicit support from parents, school officials/staff, and teachers . . .\nProvide periodic incentives for school staff and teachers.\nProvide monetary incentives for the schools that increase with each year of the study…\nTable 6 Excerpts from table showing compensation provided in study\nhttp://www.bmj.com/content/346/bmj.e7586.long\n\n5.4 Infant Evaluations in the Case of Treatment Discontinuation or Study Withdrawal\n\nAll randomized infants completing the 18-month evaluation schedule will have fulfilled the infant clinical and laboratory evaluation requirements for the study…\n\nAll randomized infants who are prematurely discontinued from study drug will be considered off study drug/on study and will follow the same schedule of events as those infants who continue study treatment except adherence assessment. All of these infants will be followed through 18 months as scheduled.\n\n\nRandomized infants prematurely discontinued from the study before the 6-month evaluation will have the following clinical and laboratory evaluations performed, if possible:…\n\n\nRoche Amplicor HIV-1 DNA PCR [polymerase chain reaction] and cell pellet storage\n\nPlasma for storage (for NVP [nevirapine] resistance, HIV-1 RNA PCR and NVP concentration)\n…\nRandomized infants prematurely discontinued from the study at any time after the 6-month evaluation will have the following clinical and laboratory evaluations performed, if possible:…\n\n5.5 Participant Retention\n\nOnce an infant is enrolled or randomized, the study site will make every reasonable effort to follow the infant for the entire study period . . . It is projected that the rate of loss-to-follow-up on an annual basis will be at most 5% . . . Study site staff are responsible for developing and implementing local standard operating procedures to achieve this level of follow-up.\n\n5.6 Participant Withdrawal\n\nParticipants may withdraw from the study for any reason at any time. The investigator also may withdraw participants from the study in order to protect their safety and/or if they are unwilling or unable to comply with required study procedures after consultation with the Protocol Chair, National Institutes of Health (NIH) Medical Officers, Statistical and Data Management Center (SDMC) Protocol Statistician, and Coordinating and Operations Center (CORE) Protocol Specialist.\n\nParticipants also may be withdrawn if the study sponsor or government or regulatory authorities terminate the study prior to its planned end date.\n\n\nNote: Early discontinuation of study product for any reason is not a reason for withdrawal from the study\n\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-19_data_management-1",
    "href": "guidelines/spirit/index.html#sec-19_data_management-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nCareful planning of data management with appropriate personnel can help to prevent flaws that compromise data validity. The protocol should provide a full description of the data entry and coding processes, along with measures to promote their quality, or provide key elements and a reference to where full information can be found. These details are particularly important for the primary outcome data. The protocol should also document data security measures to prevent unauthorised access to or loss of participant data, as well as plans for data storage (including timeframe) during and after the trial. This information facilitates an assessment of adherence to applicable standards and regulations.\nDifferences in data entry methods can affect the trial in terms of data accuracy,268 cost, and efficiency.271 For example, when compared with paper case report forms, electronic data capture can reduce the time required for data entry, query resolution, and database release by combining data entry with data collection (Item 18a).271 277 When data are collected on paper forms, data entry can be performed locally or at a central site. Local data entry can enable fast correction of missing or inaccurate data, while central data entry facilitates blinding (masking), standardisation, and training of a core group of data entry personnel.\nRaw, non-numeric data are usually coded for ease of data storage, review, tabulation, and analysis. It is important to define standard coding practices to reduce errors and observer variation. When data entry and coding are performed by different individuals, it is particularly important that the personnel use unambiguous, standardised terminology and abbreviations to avoid misinterpretation.\nAs with data collection (Item 18a), standard processes are often implemented to improve the accuracy of data entry and coding.281 284 Common examples include double data entry296; verification that the data are in the proper format (eg, integer) or within an expected range of values; and independent source document verification of a random subset of data to identify missing or apparently erroneous values. Though widely performed to detect data entry errors, the time and costs of independent double data entry from paper forms need to be weighed against the magnitude of reduction in error rates compared to single-data entry.297 298 299"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-19_data_management-2",
    "href": "guidelines/spirit/index.html#sec-19_data_management-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\n13.9.2. Data Forms and Data Entry\n\nIn the FSGS-CT [focal segmental glomerulosclerosis—clinical trial], all data will be entered electronically. This may be done at a Core Coordinating Center or at the participating site where the data originated. Original study forms will be entered and kept on file at the participating site. A subset will be requested later for quality control; when a form is selected, the participating site staff will pull that form, copy it, and sent [sic] the copy to the DCC [data coordinating center] for re-entry.\n…Participant files are to be stored in numerical order and stored in a secure and accessible place and manner. Participant files will be maintained in storage for a period of 3 years after completion of the study.\n13.9.3. Data Transmission and Editing\nThe data entry screens will resemble the paper forms approved by the steering committee. Data integrity will be enforced through a variety of mechanisms. Referential data rules, valid values, range checks, and consistency checks against data already stored in the database (ie, longitudinal checks) will be supported. The option to chose [sic] a value from a list of valid codes and a description of what each code means will be available where applicable. Checks will be applied at the time of data entry into a specific field and/or before the data is written (committed) to the database. Modifications to data written to the database will be documented through either the data change system or an inquiry system. Data entered into the database will be retrievable for viewing through the data entry applications. The type of activity that an individual user may undertake is regulated by the privileges associated with his/her user identification code and password.\n13.9.4. Data Discrepancy Inquiries and Reports to Core Coordinating Centers\nAdditional errors will be detected by programs designed to detect missing data or specific errors in the data. These errors will be summarized along with detailed descriptions for each specific problem in Data Query Reports, which will be sent to the Data Managers at the Core Coordinating Centers…\n…The Data Manager who receives the inquiry will respond by checking the original forms for inconsistency, checking other sources to determine the correction, modifying the original (paper) form entering a response to the query. Note that it will be necessary for Data Managers to respond to each inquiry received in order to obtain closure on the queried item.\n\nThe Core Coordinating Center and participating site personnel will be responsible for making appropriate corrections to the original paper forms whenever any data item is changed . . . Written documentation of changes will be available via electronic logs and audit trails.\n\n…\nBiopsy and biochemistry reports will be sent via e-mail when data are received from the Core Lab.\n…\n13.9.5. Security and Back-Up of Data\n… All forms, diskettes and tapes related to study data will be kept in locked cabinets. Access to the study data will be restricted. In addition, Core Coordinating Centers will only have access to their own center’s data. A password system will be utilized to control access . . . These passwords will be changed on a regular basis. All reports prepared by the DCC will be prepared such that no individual subject can be identified.\n\nA complete back up of the primary DCC database will be performed twice a month. These tapes will be stored off-site in a climate-controlled facility and will be retained indefinitely. Incremental data back-ups will be performed on a daily basis. These tapes will be retained for at least one week on-site. Back-ups of periodic data analysis files will also be kept. These tapes will be retained at the off-site location until the Study is completed and the database is on file with NIH [National Institutes of Health]. In addition to the system back-ups, additional measures will be taken to back-up and export the database on a regular basis at the database management level…\n\n\n13.9.6. Study status reports\n\nThe DCC will send weekly email reports with information on missing data, missing forms, and missing visits. Personnel at the Core Coordinating Center and the Participating Sites should review these reports for accuracy and report any discrepancies to the DCC…\n…\n13.9.8. Description of Hardware at DCC\nA SUN Workstation environment is maintained in the department with a SUN SPARCstation 10 model 41 as the server . . . Primary access to the departments [sic] computing facilities will be through the Internet . . . For maximum programming efficiency, the Oracle database management system and the SAS and BMDP statistical analysis systems will be employed for this study. . . .\n\nOracle facilitates sophisticated integrity checks through a variety of mechanisms including stored procedures, stored triggers, and declarative database integrity—for between table verifications. Oracle allows data checks to be programmed once in the database rather than repeating the same checks among many applications . . . Security is enforced through passwords and may be assigned at different levels to groups and individuals.\n\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-20a_statistics_outcomes-1",
    "href": "guidelines/spirit/index.html#sec-20a_statistics_outcomes-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nThe protocol should indicate explicitly each intended analysis comparing study groups. An unambiguous, complete, and transparent description of statistical methods facilitates execution, replication, critical appraisal, and the ability to track any changes from the original pre-specified methods.\nResults for the primary outcome can be substantially affected by the choice of analysis methods. When investigators apply more than one analysis strategy for a specific primary outcome, there is potential for inappropriate selective reporting of the most interesting result.6 The protocol should prespecify the main (primary) analysis of the primary outcome (Item 12), including the analysis methods to be used for statistical comparisons (Items 20a and 20b); precisely which trial participants will be included (Item 20c); and how missing data will be handled (Item 20c). Additionally, it is helpful to indicate the effect measure (eg, relative risk) and significance level that will be used, as well as the intended use of confidence intervals when presenting results.\nThe same considerations will often apply equally to prespecified secondary and exploratory outcomes. In some instances, descriptive approaches to evaluating rare outcomes such as adverse events—might be preferred over formal analysis given the lack of power.300 Adequately powered analyses may require preplanned meta-analyses with results from other studies.\nMost trials are affected to some extent by multiplicity issues.301 302 When multiple statistical comparisons are performed (eg, multiple study groups, outcomes, interim analyses), the risk of false positive (type 1) error is inflated and there is increased potential for selective reporting of favourable comparisons in the final trial report. For trials with more than two study groups, it is important to specify in the protocol which comparisons (of two or more study groups) will be performed and, if relevant, which will be the main comparison of interest. The same principle of specifying the main comparison also applies when there is more than one outcome, including when the same variable is measured at several time points (Item 12). Any statistical approaches to account for multiple comparisons and time points should also be described.\nFinally, different trial designs dictate the most appropriate analysis plan and any additional relevant information that should be included in the protocol. For example, cluster, factorial, crossover, and within-person randomised trials require specific statistical considerations, such as how clustering will be handled in a cluster randomised trial."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-20a_statistics_outcomes-2",
    "href": "guidelines/spirit/index.html#sec-20a_statistics_outcomes-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nThe intervention arm (SMS [short message system (text message)]) will be compared against the control (SOC [standard of care]) for all primary analysis. We will use chi-squared test for binary outcomes, and T-test for continuous outcomes. For subgroup analyses, we will use regression methods with appropriate interaction terms (respective subgroup×treatment group). Multivariable analyses will be based on logistic regression . . . for binary outcomes and linear regression for continuous outcomes. We will examine the residual to assess model assumptions and goodness-of-fit. For timed endpoints such as mortality we will use the Kaplan-Meier survival analysis followed by multivariable Cox proportional hazards model for adjusting for baseline variables. We will calculate Relative Risk (RR) and RR Reductions (RRR) with corresponding 95% confidence intervals to compare dichotomous variables, and difference in means will be used for additional analysis of continuous variables. P-values will be reported to four decimal places with p-values less than 0.001 reported as p < 0.001. Up-to-date versions of SAS (Cary, NC) and SPSS (Chicago, IL) will be used to conduct analyses. For all tests, we will use 2-sided p-values with alpha≤0.05 level of significance. We will use the Bonferroni method to appropriately adjust the overall level of significance for multiple primary outcomes, and secondary outcomes.\n\n\nTo assess the impact of potential clustering for patients cared by the same clinic, we will use generalized estimating equations [GEE] assuming an exchangeable correlation structure. Table [7] provides a summary of methods of analysis for each variable. Professional academic statisticians (LT, RN) blinded to study groups will conduct all analyses.\n\nTable 7 Variables, measures, and methods of analysis (reproduced from original table http://www.bmj.com/content/346/bmj.e7586.long\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-20b_statistics_additional_analyses-1",
    "href": "guidelines/spirit/index.html#sec-20b_statistics_additional_analyses-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nSubgroup analysis\nSubgroup analyses explore whether estimated treatment effects vary significantly between subcategories of trial participants. As these data can help tailor healthcare decisions to individual patients, a modest number of prespecified subgroup analyses can be sensible.\nHowever, subgroup analyses are problematic if they are inappropriately conducted or selectively reported. Subgroup analyses described in protocols or grant applications do not match those reported in subsequent publications for more than two thirds of randomised trials, suggesting that subgroup analyses are often selectively reported or not prespecified.6 7 305 Post hoc (data driven) analyses have a high risk of spurious findings and are discouraged.306 Conducting a large number of subgroup comparisons leads to issues of multiplicity, even when all of the comparisons have been pre-specified. Furthermore, when subgroups are based on variables measured after randomisation, the analyses are particularly susceptible to bias.307\nPreplanned subgroup analyses should be clearly specified in the protocol with respect to the precise baseline variables to be examined, the definition of the subgroup categories (including cut-off boundaries for continuous or ordinal variables), the statistical method to be used, and the hypothesised direction of the subgroup effect based on plausibility.308 309\nAdjusted analysis\nSome trials prespecify adjusted analyses to account for imbalances between study groups (eg, chance imbalance across study groups in small trials), improve power, or account for a known prognostic variable. Adjustment is often recommended for any variables used in the allocation process (eg, in stratified randomisation), on the principle that the analysis strategy should match the design.310 Most trial protocols and publications do not adequately address issues of adjustment, particularly the description of variables.6 310\nIt is important that trial investigators indicate in the protocol if there is an intention to perform or consider adjusted analyses, explicitly specifying any variables for adjustment and how continuous variables will be handled. When both unadjusted and adjusted analyses are intended, the main analysis should be identified (Item 20a). It may not always be clear, in advance, which variables will be important for adjustment. In such situations, the objective criteria to be used to select variables should be prespecified. As with subgroup analyses, adjustment variables based on post-randomisation data rather than baseline data can introduce bias.311 312"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-20b_statistics_additional_analyses-2",
    "href": "guidelines/spirit/index.html#sec-20b_statistics_additional_analyses-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nWe plan to conduct two subgroup analyses, both with strong biological rationale and possible interaction effects. The first will compare hazard ratios of re-operation based upon the degree of soft tissue injury (Gustilo-Anderson Type I/II open fractures vs. Gustilo-Anderson Type IIIA/B open fractures). The second will compare hazard ratios of re-operation between fractures of the upper and lower extremity. We will test if the treatment effects differ with fracture types and extremities by putting their main effect and interaction terms in the Cox regression. For the comparison of pressure, we anticipate that the low/gravity flow will be more effective in the Type IIIA-B open fracture than in the Type I/II open fracture, and be more effective in the upper extremity than the lower extremity. For the comparison of solution, we anticipate that soap will do better in the Type IIIA-B open fracture than in the Type I/II open fracture, and better in the upper extremity than the lower extremity.\n\n\nA secondary analysis of the primary endpoint will adjust for those pre-randomization variables which might reasonably be expected to be predictive of favorable outcomes. Generalized linear models will be used to model the proportion of subjects with neurologically intact (MRS ≤ 3 [Modified Rankin Score]) survival to hospital discharge by ITD [impedance threshold device]/sham device group adjusted for site (dummy variables modeling the 11 ROC [Resuscitation Outcomes Consortium] sites), patient sex, patient age (continuous variable), witness status (dummy variables modeling the three categories of unwitnessed arrest, non-EMS [emergency medical services] witnessed arrest, and EMS witnessed arrest), location of arrest (public versus non-public), time or response (continuous variable modeling minutes between call to 911 and arrival of EMS providers on scene), presenting rhythm (dummy variables modeling asystole, PEA [pulseless electrical activity], VT/VF [ventricular tachycardia/fibrillation], or unknown), and treatment assignment in the Analyze Late vs. Analyze Early intervention. The test statistic used to assess any benefit of the ITD relative to the sham device will be computed as the generalized linear model regression coefficient divided by the estimated robust standard error based on the Huber- White sandwich estimator[reference] in order to account for within group variability which might depart from the classical assumptions. Statistical inference will be based on one-sided P values and 95% confidence intervals which adjust for the stopping rule used for the primary analysis.\n\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-20c_statistics_analysis_population_and_missing_data-1",
    "href": "guidelines/spirit/index.html#sec-20c_statistics_analysis_population_and_missing_data-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nIn order to preserve the unique benefit of randomisation as a mechanism to avoid selection bias, an as randomised analysis retains participants in the group to which they were originally allocated. To prevent attrition bias, outcome data obtained from all participants are included in the data analysis, regardless of protocol adherence (Items 11c and 18b).249 250 These two conditions (ie, all participants, as randomised) define an intention to treat analysis, which is widely recommended as the preferred analysis strategy.17\nSome trialists use other types of data analyses (commonly labelled as modified intention to treat or per protocol) that exclude data from certain participants—such as those who are found to be ineligible after randomisation or who deviate from the intervention or follow-up protocols. This exclusion of data from protocol non-adherers can introduce bias, particularly if the frequency of and the reasons for non-adherence vary between the study groups.314 315 In some trials, the participants to be included in the analysis will vary by outcome—for example, analysis of harms (adverse events) is sometimes restricted to participants who received the intervention, so that absence or occurrence of harm is not attributed to a treatment that was never received.\nProtocols should explicitly describe which participants will be included in the main analyses (eg, all randomised participants, regardless of protocol adherence) and define the study group in which they will be analysed (eg, as randomised). In one cohort of randomised trials approved in 1994-5, this information was missing in half of the protocols.6 The ambiguous use of labels such as intention to treat or per protocol should be avoided unless they are fully defined in the protocol.6 314 Most analyses labelled as intention to treat do not actually adhere to its definition because of missing data or exclusion of participants who do not meet certain post-randomisation criteria (eg, specific level of adherence to intervention).6 316 Other ambiguous labels such as modified intention to treat are also variably defined from one trial to another.314\nIn addition to defining the analysis population, it is necessary to address the problem of missing data in the protocol. Most trials have some degree of missing data,317 318 which can introduce bias depending on the pattern of missingness (eg, not missing at random). Strategies to maximise follow-up and prevent missing data, as well as the recording of reasons for missing data, are thus important to develop and document (Item 18b).152\nThe protocol should also state how missing data will be handled in the analysis and detail any planned methods to impute (estimate) missing outcome data, including which variables will be used in the imputation process (if applicable).152 Different statistical approaches can lead to different results and conclusions,317 319 but one study found that only 23% of trial protocols specified the planned statistical methods to account for missing data.6\nImputation of missing data allows the analysis to conform to intention to treat analysis but requires strong assumptions that are untestable and may be hard to justify.152 318 320 321 Methods of multiple imputation are more complex but are widely preferred to single imputation methods (eg, last observation carried forward; baseline observation carried forward), as the latter introduce greater bias and produce confidence intervals that are too narrow.152 320 321 322 Specific issues arise when outcome data are missing for crossover or cluster randomised trials.323 Finally, sensitivity analyses are highly recommended to assess the robustness of trial results under different methods of handling missing data.152 324"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-20c_statistics_analysis_population_and_missing_data-2",
    "href": "guidelines/spirit/index.html#sec-20c_statistics_analysis_population_and_missing_data-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nNevertheless, we propose to test non-inferiority using two analysis sets; the intention-to-treat set, considering all patients as randomized regardless of whether they received the randomized treatment, and the per protocol analysis set. Criteria for determining the per protocol group assignment would be established by the Steering Committee and approved by the PSMB [performance and safety monitoring board] before the trial begins. Given our expectation that very few patients will crossover or be lost to follow-up, these analyses should agree very closely. We propose declaring medical management non-inferior to interventional therapy, only if shown to be non-inferior using both the intention to treat and per protocol analysis sets.\n\n…\n10.4.7 Imputation Procedure for Missing Data\nWhile the analysis of the primary endpoint (death or stroke) will be based on a log-rank test and, therefore, not affected by patient withdrawals (as they will be censored) provided that dropping out is unrelated to prognosis; other outcomes, such as the Rankin Score at five years post-randomization, could be missing for patients who withdraw from the trial. We will report reasons for withdrawal for each randomization group and compare the reasons qualitatively . . . The effect that any missing data might have on results will be assessed via sensitivity analysis of augmented data sets. Dropouts (essentially, participants who withdraw consent for continued follow-up) will be included in the analysis by modern imputation methods for missing data.\n\nThe main feature of the approach is the creation of a set of clinically reasonable imputations for the respective outcome for each dropout. This will be accomplished using a set of repeated imputations created by predictive models based on the majority of participants with complete data. The imputation models will reflect uncertainty in the modeling process and inherent variability in patient outcomes, as reflected in the complete data.\n\n\nAfter the imputations are completed, all of the data (complete and imputed) will be combined and the analysis performed for each imputed-and-completed dataset. Rubin’s method of multiple (ie, repeated) imputation will be used to estimate treatment effect. We propose to use 15 datasets (an odd number to allow use of one of the datasets to represent the median analytic result).\n\n\nThese methods are preferable to simple mean imputation, or simple best-worst or worst-worst imputation, because the categorization of patients into clinically meaningful subgroups, and the imputation of their missing data by appropriately different models, accords well with best clinical judgment concerning the likely outcomes of the dropouts, and therefore will enhance the trial’s results.\n\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-21a_data_monitoring_formal_committee-1",
    "href": "guidelines/spirit/index.html#sec-21a_data_monitoring_formal_committee-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nFor some trials, there are important reasons for periodic inspection of the accumulating outcome data by study group. In principle, a trial should be modified or discontinued when the accumulated data have sufficiently disturbed the clinical equipoise that justified the initiation of the trial. Data monitoring can also inform aspects of trial conduct, such as recruitment, and identify the need to make adjustments.\nThe decision to have a data monitoring committee (DMC) will be influenced by local standards. While certain trials warrant some form of data monitoring, many do not need a formal committee,326 such as trials with a short duration or known minimal risks. A DMC was described in 65% (98/150) of cancer trial protocols with time-to-event outcomes in Italy in 2000-5,327 and in 17% (12/70) of protocols for Danish randomised trials approved in 1994-5.6 About 40% of clinical trials registered on ClinicalTrials.gov from 2007-2010 reported having a DMC.328 The protocol should either state that there will be a DMC and provide further details, as discussed below, or indicate that there will not be a DMC, preferably with reasons.\nWhen formal data monitoring is performed, it is often done by a DMC consisting of members from a variety of disciplines.254 329 The primary role of a DMC is to periodically review the accumulating data and determine if a trial should be modified or discontinued. The DMC does not usually have executive power; rather, it communicates the outcome of its deliberations to the trial steering committee or sponsor.\nIndependence, in particular from the sponsor and trial investigators, is a key characteristic of the DMC and can be broadly defined as the committee comprising members who are completely uninvolved in the running of the trial and who cannot be unfairly influenced (either directly or indirectly) by people, or institutions, involved in the trial.254 DMC members are usually required to declare any competing interests (Item 28). Among the 12 trial protocols that described a DMC and were approved in Denmark in 1994-5,6 four explicitly stated that the DMC was independent from the sponsor and investigators; three had non-independent DMCs; and independence was unclear for the remaining five protocols.\nThe protocol should name the chair and members of the DMC. If the members are not yet known, the protocol can indicate the intended size and characteristics of the membership until further details are available. The protocol should also indicate the DMC’s roles and responsibilities, planned method of functioning, and degree of independence from those conducting, sponsoring, or funding the trial.254 330 331 A charter is recommended for detailing this information331; if this charter is not appended to the protocol, the protocol should indicate whether a charter exists or will be developed, and if so, where it can be accessed."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-21a_data_monitoring_formal_committee-2",
    "href": "guidelines/spirit/index.html#sec-21a_data_monitoring_formal_committee-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nAppendix 3. Charter and responsibilities of the Data Monitoring Committee\n\nA Data Monitoring Committee (DMC) has been established. The DMC is independent of the study organisers. During the period of recruitment to the study, interim analyses will be supplied, in strict confidence, to the DMC, together with any other analyses that the committee may request. This may include analyses of data from other comparable trials. In the light of these interim analyses, the DMC will advise the TSC [trial steering committee] if, in its view:\n\n\nthe active intervention has been proved, beyond reasonable doubt*, to be different from the control (standard management) for all or some types of participants, and\n\n\n\nthe evidence on the economic outcomes is sufficient to guide a decision from health care providers regarding recommendation of early lens extraction for PACG [primary angle closure glaucoma].\n\nThe TSC can then decide whether or not to modify intake to the trial. Unless this happens, however, the TSC, PMG [project management group], clinical collaborators and study office staff (except those who supply the confidential analyses) will remain ignorant of the interim results.\nThe frequency of interim analyses will depend on the judgement of the Chair of the DMC, in consultation with the TSC. However, we anticipate that there might be three interim analyses and one final analysis.\n\nThe Chair is Mr D.G.-H., with Dr D.C., and Professor B.D. Terms of reference for the DMC are available on request from the EAGLE [Effectiveness in Angle Closure Glaucoma of Lens Extraction] study office.\n\n\n*Appropriate criteria for proof beyond reasonable doubt cannot be specified precisely. A difference of at least three standard deviation [sic] in the interim analysis of a major endpoint may be needed to justify halting, or modifying, such a study prematurely.[reference]\n\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-21b_data_monitoring_interim_analysis-1",
    "href": "guidelines/spirit/index.html#sec-21b_data_monitoring_interim_analysis-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nInterim analyses can be conducted as part of an adaptive trial design to formally monitor the accumulating data in clinical trials. They are generally performed in trials that have a DMC, longer duration of recruitment, and potentially serious outcomes. Interim analyses were described in 71% (106/150) of cancer trial protocols with time-to-event outcomes in Italy in 2000-5,327 and in 19% (13/70) of protocols for Danish randomised trials approved in 1994-5.6 The results of these analyses, along with non-statistical criteria, can be part of a stopping guideline that helps inform whether the trial should be continued, modified, or halted earlier than intended for benefit, harm, or futility. Criteria for stopping for harm are often different from those for benefit and might not employ a formal statistical criterion.333 Stopping for futility occurs in instances where, if the study were to continue, it is unlikely that an important effect would be seen (ie, low chance of rejecting null hypothesis). Multiple analyses of the accumulating data increase the risk of a false positive (type I) error, and various statistical strategies have been developed to compensate for this inflated risk.254 333 334 335 Aside from informing stopping guidelines, prespecified interim analyses can be used for other trial adaptations such as sample size re-estimation, alteration to the proportion of participants allocated to each study group, and changes to eligibility criteria.111\nA complete description of any interim analysis plan, even if it is only to be performed at the request of an oversight body (eg, DMC), should be provided in the protocol—including the statistical methods, who will perform the analyses, and when they will be conducted (timing and indications). If applicable, details should also be provided about the decision criteria—statistical or other—that will be adopted to judge the interim results as part of a guideline for early stopping or other adaptations. Among 86 protocols for randomised trials with a time-to-event cancer outcome that proposed efficacy interim analyses, all stated the planned timing of the analyses, 91% specified the overall reason to be used for stopping (eg, superiority, futility), and 94% detailed the statistical approach.327\nIn addition, it is important to state who will see the outcome data while the trial is ongoing, whether these individuals will remain blinded (masked) to study groups, and how the integrity of the trial implementation will be protected (eg, maintaining blinding) when any adaptations to the trial are made. A third of protocols for industry initiated randomised trials receiving Danish ethics approval in 1994-95 stated that the sponsor had access to accumulating trial data, which can introduce potential bias due to competing interests.10 Finally, the protocol should specify who has the ultimate authority to stop or modify the trial—eg, the principal investigator, trial steering committee, or sponsor."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-21b_data_monitoring_interim_analysis-2",
    "href": "guidelines/spirit/index.html#sec-21b_data_monitoring_interim_analysis-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nPremature termination of the study\n\nAn interim-analysis is performed on the primary endpoint when 50% of patients have been randomised and have completed the 6 months follow-up. The interim-analysis is performed by an independent statistician, blinded for the treatment allocation. The statistician will report to the independent DSMC [data and safety monitoring committee]. The DSMC will have unblinded access to all data and will discuss the results of the interim-analysis with the steering committee in a joint meeting. The steering committee decides on the continuation of the trial and will report to the central ethics committee. The Peto approach is used: the trial will be ended using symmetric stopping boundaries at P < 0.001 [reference]. The trial will not be stopped in case of futility, unless the DSMC during the course of safety monitoring advices [sic] otherwise. In this case DSMC will discuss potential stopping for futility with the trial steering committee.\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-22_harms-1",
    "href": "guidelines/spirit/index.html#sec-22_harms-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nEvaluation of harms has a key role in monitoring the condition of participants during a trial and in enabling appropriate management of adverse events. Documentation of trial related adverse events also informs clinical practice and the conduct of ongoing and future studies. We use the term harms instead of safety to better reflect the negative effects of interventions.300 An adverse event refers to an untoward occurrence during the trial, which may or may not be causally related to the intervention or other aspects of trial participation.300 336 This definition includes unfavourable changes in symptoms, signs, laboratory values, or health conditions. In the context of clinical trials, it can be difficult to attribute causation for a given adverse event. An adverse effect is a type of adverse event that can be attributed to the intervention.\nHarms can be specified as primary or secondary outcomes (Item 12) or can be assessed as part of routine monitoring. To the extent possible, distinctions should be made between adverse events that are anticipated versus unanticipated, and solicited versus unsolicited, because expectation can influence the number and perceived severity of recorded events. For example, providing statements in the informed consent process about the possibility of a particular adverse effect or using structured, as opposed to open ended, questionnaires for data collection, can increase the reporting of specific events (priming).269 337 338 339 The timeframe for recording adverse events can also affect the type of data obtained.340 341\nThe protocol should describe the procedures for and frequency of harms data collection, the overall surveillance timeframe, any instruments to be used, and their validity and reliability, if known. Substantial discrepancies have been observed between protocol specified plans for adverse event collection and reporting, and what is described in final publications.5 Although trials are often not powered to detect important differences in rates of uncommon adverse events, it is also important to describe plans for data analysis, including formal hypothesis testing or descriptive statistics.300 342\nFinally, the protocol should address the reporting of harms to relevant groups (eg, sponsor, research ethics committee/institutional review board, data monitoring committee, regulatory agency), which is an important process that is subject to local regulation.343 Key considerations include the severity of the adverse event, determination of potential causality, and whether it represents an unexpected or anticipated event. For multicentre studies, procedures and timing should be outlined for central collection, evaluation, and reporting of pooled harms data."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-22_harms-2",
    "href": "guidelines/spirit/index.html#sec-22_harms-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nSecondary outcomes\n\n…In our study an adverse event will be defined as any untoward medical occurrence in a subject without regard to the possibility of a causal relationship. Adverse events will be collected after the subject has provided consent and enrolled in the study. If a subject experiences an adverse event after the informed consent document is signed (entry) but the subject has not started to receive study intervention, the event will be reported as not related to study drug. All adverse events occurring after entry into the study and until hospital discharge will be recorded. An adverse event that meets the criteria for a serious adverse event (SAE) between study enrollment and hospital discharge will be reported to the local IRB [institutional review board] as an SAE. If haloperidol is discontinued as a result of an adverse event, study personnel will document the circumstances and data leading to discontinuation of treatment. A serious adverse event for this study is any untoward medical occurrence that is believed by the investigators to be causally related to study-drug and results in any of the following: Life-threatening condition (that is, immediate risk of death); severe or permanent disability, prolonged hospitalization, or a significant hazard as determined by the data safety monitoring board. Serious adverse events occurring after a subject is discontinued from the study will NOT be reported unless the investigators feels that the event may have been caused by the study drug or a protocol procedure. Investigators will determine relatedness of an event to study drug based on a temporal relationship to the study drug, as well as whether the event is unexpected or unexplained given the subject’s clinical course, previous medical conditions, and concomitant medications.\n…The study will monitor for the following movement-related adverse effects daily through patient examination and chart review: dystonia, akathisia, pseudoparkinsonism, akinesia, and neuroleptic malignant syndrome. Study personnel will use the Simpson-Angus [reference] and Barnes Akathisia [reference] scales to monitor movement-related effects.\n…\nFor secondary outcomes, binary measures, eg mortality and complications, logistic regression will be used to test the intervention effect, controlling for covariates when appropriate…\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-23_auditing-1",
    "href": "guidelines/spirit/index.html#sec-23_auditing-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nAuditing involves periodic independent review of core trial processes and documents. It is distinct from routine day-to-day measures to promote data quality (Items 18a and 19). Auditing is intended to preserve the integrity of the trial by independently verifying a variety of processes and prompting corrective action if necessary. The processes reviewed can relate to participant enrolment, consent, eligibility, and allocation to study groups; adherence to trial interventions and policies to protect participants, including reporting of harms (Item 22); and completeness, accuracy, and timeliness of data collection. In addition, an audit can verify adherence to applicable policies such as the International Conference on Harmonisation Good Clinical Practice and regulatory agency guidelines.160\nIn multicentre trials, auditing is usually considered both overall and for each recruiting centre. Audits can be done by exploring the trial dataset or performing site visits. Audits might be initially conducted across all sites, and subsequently conducted using a risk based approach that focuses, for example, on sites that have the highest enrolment rates, large numbers of withdrawals, or atypical (low or high) numbers of reported adverse events.\nIf auditing is planned, the procedures and anticipated frequency should be outlined in the protocol, including a description of the personnel involved and their degree of independence from the trial investigators and sponsor. If procedures are further detailed elsewhere (eg, audit manual), then the protocol should reference where the full details can be obtained."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-23_auditing-2",
    "href": "guidelines/spirit/index.html#sec-23_auditing-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\n11.4 Data Monitoring and Quality Assurance\n\nThrough the combination of our web-based, instantaneous electronic validation, the DCC’s [data coordinating centre] daily visual cross-validation of the data for complex errors, and regular on-site monitoring, the quality and completeness of the data will be reflective of the state of the art in clinical trials.\n\nBoth the European and US DCCs will conduct monitoring of source documents via fax at all enrolling ARUBA [A Randomised trial of Unruptured Brain Arteriovenous malformations] sites and will conduct at least one on-site monitoring visit per year over the course of the study at 100% of clinical sites (with repeat visits to sites where performance is a concern). Monitoring of European study sites will be assured by the European Coordinating Center (Paris). The primary objectives of the DCC during the on-site visits are to educate, support and solve problems. The monitors will discuss the protocol in detail and identify and clarify any areas of weakness. At the start of the trial, the monitors will conduct a tutorial on the web-based data entry system. The coordinators will practice entering data so that the monitors can confirm that the coordinators are proficient in all aspects of data entry, query response, and communication with the DCC. They will audit the overall quality and completeness of the data, examine source documents, interview investigators and coordinators, and confirm that the clinical center has complied with the requirements of the protocol. The monitors will verify that all adverse events were documented in the correct format, and are consistent with protocol definition.\n\n\nThe monitors will review the source documents as needed, to determine whether the data reported in the Web-based system are complete and accurate. Source documents are defined as medical charts, associated reports and records including initial hospital admission report…\n\n\nThe monitors will confirm that the regulatory binder is complete and that all associated documents are up to date. The regulatory binder should include the protocol and informed consent (all revisions), IRB [institutional review board] approvals for all of the above documents, IRB correspondence, case report forms, investigator’s agreements…\n\n\nScheduling monitoring visits will be a function of patient enrollment, site status and other commitments. The DCC will notify the site in writing at least three weeks prior to a scheduled visit. The investigators must be available to meet with the monitors. Although notification of the visits will include the list of patients scheduled to be reviewed, the monitors reserve the right to review additional ARUBA patients.\n\n\nIf a problem is identified during the visit (ie, poor communication with the DCC, inadequate or insufficient staff to conduct the study, missing study documents) the monitor will assist the site in resolving the issues. Some issues may require input from the Operations Committee, Steering Committee or one of the principal investigators.\n\n\nThe focus of the visit/electronic monitoring will be on source document review and confirmation of adverse events. The monitor will verify the following variables for all patients: initials, date of birth, sex, signed informed consent, eligibility criteria, date of randomization, treatment assignment, adverse events, and endpoints…\n\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-24_research_ethics_approval-1",
    "href": "guidelines/spirit/index.html#sec-24_research_ethics_approval-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nA universal requirement for the ethical conduct of clinical research is the review and approval of the research protocol by qualified individuals who are not associated with the research team and have no disqualifying competing interests as reviewers.1 The review is typically conducted by a formal REC/IRB in accordance with jurisdictional policy. Despite the importance of ethics review, approval by a REC/IRB is not always obtained. Among 767 trials published in leading general medical journals from 1993-95, 37 authors (5%) disclosed that such approval had not been sought for their trials.344 The protocol should document where approval has been obtained, or outline plans to seek such approval."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-24_research_ethics_approval-2",
    "href": "guidelines/spirit/index.html#sec-24_research_ethics_approval-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nThis protocol and the template informed consent forms contained in Appendix II will be reviewed and approved by the sponsor and the applicable IRBs/ECs [institutional review boards/ethical committees] with respect to scientific content and compliance with applicable research and human subjects regulations. . . .\n\n\nThe protocol, site-specific informed consent forms (local language and English versions), participant education and recruitment materials, and other requested documents—and any subsequent modifications — also will be reviewed and approved by the ethical review bodies. . .\n\n\nSubsequent to initial review and approval, the responsible local Institutional Review Boards/Ethical Committees (IRBs/ECs) will review the protocol at least annually. The Investigator will make safety and progress reports to the IRBs/ECs at least annually and within three months of study termination or completion at his/her site. These reports will include the total number of participants enrolled . . . and summaries of each DSMB [data safety and monitoring board] review of safety and/or efficacy.\n\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-25_protocol_amendments-1",
    "href": "guidelines/spirit/index.html#sec-25_protocol_amendments-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nAfter initial ethics approval, about half of trials have subsequent protocol amendments submitted to the REC/IRB.125 346 347 While some amendments may be unavoidable, a study of pharmaceutical industry trials found that according to the sponsors, a third of amendments could have been prevented with greater attention to key issues during protocol development.346 Substantive amendments can generate challenges to data analysis and interpretation if they occur part way through the trial (eg, changes in eligibility criteria),348 and can introduce bias if the changes are made based on the trial data.173 174 175 176 The implementation and communication of amendments are also burdensome and potentially costly.346\nNumerous studies have revealed substantive changes between prespecified methods (eg, as stated in approved protocols, registries, or regulatory agency submissions) and those described in trial publications, including changes to primary outcomes,12 172 173 174 175 176 sample size calculations,6 eligibility criteria,125 133 134 as well as methods of allocation concealment,2 blinding,3 and statistical analysis.6 7 8 174 These substantive modifications are rarely acknowledged in the final trial reports, providing an inaccurate impression of trial integrity.\nIt is important that substantive protocol amendments be reviewed by an independent party, such as the REC/IRB, and transparently described in trial reports. The notion of substantive is variably defined by authorities, but in general refers to a protocol amendment that can affect the safety of trial participants or the scientific validity, scope, or ethical rigour of the trial.349 350 To reflect the degree of oversight for the trial and adherence to applicable regulation, the protocol should describe the process for making amendments, including who will be responsible for the decision to amend the protocol and how substantive changes will be communicated to relevant stakeholders (eg, REC/IRBs, trial registries, regulatory agencies). Version control using protocol identifiers and dates (Item 3), as well as a list of amendments, can help to track the history of amendments and identify the most recent protocol version."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-25_protocol_amendments-2",
    "href": "guidelines/spirit/index.html#sec-25_protocol_amendments-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\n13.10 Modification of the Protocol\n\nAny modifications to the protocol which may impact on the conduct of the study, potential benefit of the patient or may affect patient safety, including changes of study objectives, study design, patient population, sample sizes, study procedures, or significant administrative aspects will require a formal amendment to the protocol. Such amendment will be agreed upon by BCIRG [Breast Cancer International Research Group] and Aventis, and approved by the Ethics Committee/IRB [institutional review board] prior to implementation and notified to the health authorities in accordance with local regulations.\n\nAdministrative changes of the protocol are minor corrections and/or clarifications that have no effect on the way the study is to be conducted. These administrative changes will be agreed upon by BCIRG and Aventis, and will be documented in a memorandum. The Ethics Committee/IRB may be notified of administrative changes at the discretion of BCIRG\n\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-26a_consent_or_assent-1",
    "href": "guidelines/spirit/index.html#sec-26a_consent_or_assent-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nThe notion of acquiring informed consent involves the presentation of comprehensible information about the research to potential participants, confirmation that they understand the research, and assurance that their agreement to participate is voluntary. The process typically involves discussion between the potential participant and an individual knowledgeable about the research; the presentation of written material (eg, information leaflet or consent document); and the opportunity for potential participants to ask questions. Surveys of trial investigators reveal that appropriate informed consent is not always obtained.344 352\nThe content, quantity, and mode of delivery of consent information can affect trial recruitment, participant comprehension, anxiety, retention rates, and recruitment costs.68 114 218 292 353 354 355 We recommend that a model consent or assent form be provided as a protocol appendix (Item 32). Assent represents a minor’s affirmative agreement to participate in the trial, which typically involves signing a document that provides age appropriate information about the study.\nThe protocol should include details of the consent process as well as the status, experience, and training (if applicable) of the research team members who will conduct it. In paediatric research, regulations may stipulate obtaining affirmative assent for participation from children above a certain age.356 The protocol should then describe how pertinent information will be provided to potential participants and how their understanding and assent will be ascertained. When potential participants lack decisional capacity for reasons other than young age (eg, mental status), and proxy consent can be obtained from a legally-authorised representative, the protocol should describe who will determine an individual’s decisional capacity, whether a formal capacity instrument will be utilised, and how the individual’s informed agreement to continue participation will be secured should they regain decisional capacity. For certain trials, such as cluster randomised trials, it may not be possible to acquire individual informed consent from participants before randomisation, and the consent process may be modified or waived. An explanation should be provided in the protocol in these instances."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-26a_consent_or_assent-2",
    "href": "guidelines/spirit/index.html#sec-26a_consent_or_assent-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\n…Trained Research Nurses will introduce the trial to patients who will be shown a video regarding the main aspects of the trial. Patients will also receive information sheets. Research Nurses will discuss the trial with patients in light of the information provided in the video and information sheets. Patients will then be able to have an informed discussion with the participating consultant. Research Nurses will obtain written consent from patients willing to participate in the trial. Information sheets and consent forms are provided for all parents involved in the trial however these have been amended accordingly in order to provide separate information sheets and consent form [sic] which are suitable for children and teenagers. All information sheets, consent forms and the video transcript have been translated into Bengali, Punjabi, Gujarati, and Urdu. There are also separate information sheets and consent forms for the cohort group.\n\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-26b_consent_or_assent_ancillary_studies-1",
    "href": "guidelines/spirit/index.html#sec-26b_consent_or_assent_ancillary_studies-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nAncillary studies involve the collection or derivation of data for purposes that are separate from the main trial. The acquisition and storage of data and biological specimens for ancillary studies is increasingly common in the context of clinical trials (Item 33). Specimens may be used for a specified subset of studies or for submission to biorepositories for future specified or unspecified research.\nAncillary studies have additional processes and considerations relating to consent, which should be detailed in the protocol. Guidance for the creation of a simplified informed consent document for biobanking is available.358 Participants can be given several options to consider with respect to their participation in ancillary research: consent for the use of their data and specimens in specified protocols; consent for use in future research unrelated to the clinical condition under study; consent for submission to an unrelated biorepository; and consent to be contacted by trial investigators for further informational and consent-related purposes. This is commonly referred to as tiered consent. Participants should also be informed about whether their withdrawal from the ancillary research is possible (eg, the data and specimens are coded and identifiable); what withdrawal means in this context (eg, used specimens and data derived from them cannot be withdrawn); and what information derived from the specimen related research will be provided to them, if any."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-26b_consent_or_assent_ancillary_studies-2",
    "href": "guidelines/spirit/index.html#sec-26b_consent_or_assent_ancillary_studies-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\n6.4.1. Samples for Biorepositories\n\nAdditional biological samples will be obtained to be stored for use in future studies of the pathobiology of FSGS [focal segmental glomerulosclerosis]. A materials consent will be obtained to specifically address the collection of these …urine, serum and plasma specimens…\n14.3.4. Instructions for Preparation of Requests for an Ancillary Study\n… A signed consent must be obtained from every participant in the ancillary study, if the data collection/request is not covered in the original informed consent process for the main FSGS Clinical Trial.\n…\nA copy of the IRB [institutional review board] letter for the ancillary study should be sent to the DCC [data coordinating centre]. If a separate consent form is required for the ancillary study, a copy of the signed ancillary study consent form for each study participant must be included in the FSGS-CT [clinical trial] record. A data file tracking all signed ancillary consent forms must be maintained by the ancillary study and an electronic copy of that file must be delivered to the FSGS-CT DCC.\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-27_confidentiality-1",
    "href": "guidelines/spirit/index.html#sec-27_confidentiality-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nPersonal information about participants is acquired during the process of trial recruitment, eligibility screening, and data collection. Much of this information consists of private details over which people customarily wish to maintain control, such as their health status, personal genotype, and social and family history.\nThe protocol should describe the means whereby personal information is collected, kept secure, and maintained. In general, this involves: 1) the creation of coded, depersonalised data where the participant’s identifying information is replaced by an unrelated sequence of characters; 2) secure maintenance of the data and the linking code in separate locations using encrypted digital files within password protected folders and storage media; and 3) limiting access to the minimum number of individuals necessary for quality control, audit, and analysis. The protocol should also describe how the confidentiality of data will be preserved when the data are transmitted to sponsors and coinvestigators (eg, virtual private network internet transmission)."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-27_confidentiality-2",
    "href": "guidelines/spirit/index.html#sec-27_confidentiality-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\n8.5 Confidentiality\n\nAll study-related information will be stored securely at the study site. All participant information will be stored in locked file cabinets in areas with limited access. All laboratory specimens, reports, data collection, process, and administrative forms will be identified by a coded ID [identification] number only to maintain participant confidentiality. All records that contain names or other personal identifiers, such as locator forms and informed consent forms, will be stored separately from study records identified by code number. All local databases will be secured with password-protected access systems. Forms, lists, logbooks, appointment books, and any other listings that link participant ID numbers to other identifying information will be stored in a separate, locked file in an area with limited access.\n\nAll HIV test results will be kept strictly confidential, all counseling and blood draws will be conducted in private rooms, and study staff will be required to sign agreements to preserve the confidentiality of all participants. Study staff will never inform network members of the serostatus of other members of their group, but counselors will provide general messages about the prevalence of HIV in the study population in the interests of emphasizing harm reduction.\n\n\nParticipants’ study information will not be released outside of the study without the written permission of the participant, except as necessary for monitoring by NIAID [National Institute of Allergy and Infectious Diseases] and/or its contractors . . . representatives of the HPTN CORE [HIV Prevention Trials Network Coordinating and Operations Center] . . . and US or in-country government and regulatory authorities.\n\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-28_declaration_of_interests-1",
    "href": "guidelines/spirit/index.html#sec-28_declaration_of_interests-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nCompeting interests, or conflicts of interest, exist when there is potential for divergence between an individual’s or institution’s private interests and their responsibilities to scientific and publishing activities.360 More positive outcomes, larger treatment effect sizes, and more favourable interpretation of results have been found in clinical trials with pharmaceutical industry sponsorship (Item 4)27 36 37 38 42 and investigators who have declared competing interests,57 60 compared to those without such interests. Although competing interests are most often associated with drug and device industries, they may exist with support from or affiliation with government agencies, charities, not for profit organisations, and professional and civic organisations.\nCompeting interests do not in themselves imply wrongdoing. Their disclosure and regular updating enables appropriate management plans to be developed and implemented, and facilitates transparent assessment of the potential for bias.\nMany trials and non-industry sponsors have a conflict of interest policy for their investigators, and checklists are available to guide potential interests that should be disclosed and regularly updated by trial investigators.361 362 Types of financial ties include salary support or grants; ownership of stock or options; honorariums (eg, for advice, authorship, or public speaking); paid consultancy or service on advisory boards and medical education companies; and receipt of patents or patents pending. Non-financial competing interests include academic commitments; personal or professional relationships; and political, religious, or other affiliations with special interests or advocacy positions."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-28_declaration_of_interests-2",
    "href": "guidelines/spirit/index.html#sec-28_declaration_of_interests-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nPS:\n\n\nWas the Principal Investigator of the second International Stroke Trial (IST-2) to evaluate a neuroprotective compound (619c89). . .\nHas received lecture fees and travel expenses from Bayer and from Boehringer Ingelheim for lectures given at international conferences.\nHe serves on the Independent Data Monitoring and Safety Board of the RELY trial, funded by Boehringer Ingelheim and receives attendance fees and travel expenses for attending board meetings.\nHe does not have any paid consultancies with pharmaceutical companies, and is not a member of the Speaker’s Panel of any company.\n\nKBS:\nReceived an honorarium for a lecture from Boehringer Ingelheim and had costs for participating in scientific meetings reimbursed…\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-29_data_access-1",
    "href": "guidelines/spirit/index.html#sec-29_data_access-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nThe validity of results from interventional trials can be verified only by individuals who have full access to the complete final dataset. For some multicentre trials, only the steering group has access to the full trial dataset in order to ensure that the overall results are not disclosed by an individual study site prior to the main publication. Many of these trials will allow site investigators to access the full dataset if a formal request describing their plans is approved by the steering group. The World Medical Association supports the principle that trial investigators retain the right to access data.363 However, among protocols of industry initiated randomised trials published in 2008-9 in the Lancet or approved in 2004 by a Danish ethics committee, 30-39% stated that the sponsor owned the data while 0-3% stated that principal investigators had access to all trial data.10 364 Similar constraints were found in Danish trial protocols from 1994-5.10\nThe protocol should identify the individuals involved in the trial who will have access to the full dataset. Any restrictions in access for trial investigators should also be explicitly described."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-29_data_access-2",
    "href": "guidelines/spirit/index.html#sec-29_data_access-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\n12.10.1 Intra-Study Data Sharing\n\nThe Data Management Coordinating Center will oversee the intra-study data sharing process, with input from the Data Management Subcommittee.\n\nAll Principal Investigators (both US and host country) will be given access to the cleaned data sets. Project data sets will be housed on the Project Accept Web site and/or the file transfer protocol site created for the study, and all data sets will be password protected. Project Principal Investigators will have direct access to their own site’s data sets, and will have access to other sites data by request. To ensure confidentiality, data dispersed to project team members will be blinded of any identifying participant information.\n\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-30_ancillary_and_post_trial_care-1",
    "href": "guidelines/spirit/index.html#sec-30_ancillary_and_post_trial_care-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nThe provision of ancillary care refers to the provision of care beyond that immediately required for the proper and safe conduct of the trial, and the treatment of immediate adverse events related to trial procedures. It is generally agreed that trial sponsors and investigators should plan to provide care for participants’ healthcare needs that arise as a direct consequence of trial participation (eg, intervention related harms). It is also important to consider whether care should be provided for certain ancillary needs that may otherwise arise during trial participation. Provision of care for ancillary needs reflects the fact that participants implicitly, but unavoidably, entrust certain aspects of their health to the research team. The scope of entrustment will vary depending on the nature of the trial (eg, setting, health condition under study, investigations performed).366 Additional factors that influence the strength of the claim to ancillary care include participants’ vulnerabilities; uncompensated burdens and harms; the intensity and duration of the participant-researcher relationship; and the degree to which participants are uniquely dependent on the research team for health care.367\nThe Declaration of Helsinki states that the protocol should describe arrangements for post-study access by study participants to interventions identified as beneficial in the study or access to other appropriate care or benefits.1 This principle is particularly applicable—and controversial—when research enabling the development and regulatory approval of interventions is performed in countries where subsequent access to the interventions is limited by cost or lack of availability.368\nThe protocol should describe any plans to provide or pay for ancillary care during the trial and identify any interventions, benefits, or other care that the sponsor will continue to provide to participants and host communities after the trial is completed.369 Any plans to compensate participants for trial related harms should also be outlined."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-30_ancillary_and_post_trial_care-2",
    "href": "guidelines/spirit/index.html#sec-30_ancillary_and_post_trial_care-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nPatients that are enrolled into the study are covered by indemnity for negligent harm through the standard NHS [National Health Service] Indemnity arrangements. The University of Sheffield has insurance to cover for non-negligent harm associated with the protocol . . . This will include cover for additional health care, compensation or damages whether awarded voluntarily by the Sponsor, or by claims pursued through the courts. Incidences judged to arise from negligence (including those due to major protocol violations) will not be covered by study insurance policies. The liability of the manufacturer of IL1RA (Amgen Corporation) is strictly limited to those claims arising from faulty manufacturing of the commercial product and not to any aspects of the conduct of the study.\n\n\n13.6 Access to Effective Products\n\nShould this study provide evidence of the effectiveness of TDF [tenofovir disoproxil fumarate], FTC [emtricitabine]/TDF and/or tenofovir 1% gel in preventing HIV infection, it will be critical to provide access to the effective product(s) to study participants, their communities, and the worldwide population at risk for HIV infection in a timely manner. In preparation for this study, discussions have begun with Gilead Sciences, Inc. and CONRAD [Contraceptive Research and Development Organization] to ensure such access. Considerations under discussion include licensing agreements and preferred pricing arrangements for the study communities and other resource-poor settings.\n\nWhile this study is ongoing, the MTN [Microbicide Trials Network] will continue these discussions. In addition, discussions will be initiated with other public and private funding sources such as the WHO, UNAIDS, Gates Foundation, and appropriate site government agencies that may be able to purchase product supplies in bulk and offer them at low or no cost to the study communities and other resource-poor communities most in need of the product(s). Operations and marketing research also may be conducted to determine how best to package and distribute the products, and maximize their acceptability and use, in at-risk populations.\n\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-31a_dissemination_policy_trial_results-1",
    "href": "guidelines/spirit/index.html#sec-31a_dissemination_policy_trial_results-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nA fundamental ethical principle in clinical trials is that the potential risks incurred by study participants should be balanced by the benefit of contributing to publicly available knowledge.371 Unfortunately, about half of clinical trials remain unpublished.80 83 Trials with statistically non-significant results or industry funding are more prone to non-publication,36 38 80 81 82 83 although government funded trials are also susceptible.81 When published, trials with non-significant results often have a longer delay to publication.80 83 Overall, the medical literature represents a biased subset of existing data, potentially leading to overestimation of benefits, underestimation of harms, and a detrimental impact on patient care and research.80 372 373 374 375 376 377\nAlthough peer reviewers can be biased in favour of positive findings,378 lack of publication appears to be primarily due to trial investigators or sponsors failing to submit negative or null results, rather than journals rejecting them.80 379 A plan to disseminate trial results to key stakeholders should be outlined in the protocol, including a process and timeframe for approving and submitting reports for dissemination (eg, via journal publication, trial registry, trial website), and an explicit statement that the results will be disseminated regardless of the magnitude or direction of effect.\nFurthermore, any conditions relating to the investigators’ right to publish or present trial results should be explicitly described. Publication restrictions have been imposed by various groups, including industry sponsors or the trial steering group (eg, to maintain the integrity of the overall dataset).10 380 These restrictions are sometimes not described in the protocol but rather in separate publication agreements.10 However, as they can interfere with the ethical responsibility of investigators and sponsors to disseminate trial results in an unbiased and timely manner,38 381 382 383 384 any restrictions should be disclosed in the protocol for review by REC/IRBs, funders, and other stakeholders. A review of industry initiated randomised trial protocols approved in Denmark in 1994-95 revealed that 91% had publication restrictions imposed by sponsors; similar constraints were noted for protocols approved in 2004.10"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-31a_dissemination_policy_trial_results-2",
    "href": "guidelines/spirit/index.html#sec-31a_dissemination_policy_trial_results-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\n\nPublication Policy\n\n\nThe Publications subcommittee will review all publications following the guidelines given below and report its recommendations to the Steering Committee.\nA. Data analysis and release of results\nThe scientific integrity of the project requires that the data from all BEST [Beta-Blocker Evaluation of Survival Trial] sites be analyzed study-wide and reported as such. Thus, an individual center is not expected to report the data collected from its center alone . . . all presentations and publications are expected to protect the integrity of the major objective(s) of the study; data that break the blind will not be presented prior to the release of mainline results. Recommendations as to the timing of presentation of such endpoint data and the meetings at which they might be presented will be given by the Steering Committee.\nB. Review process\nEach paper or abstract, as described below, must be submitted to the appropriate Subcommittee for review of its appropriateness and scientific merit prior to submission. The Subcommittee may recommend changes to the authors and will finally submit its recommendations to the Steering Committee for approval.\nC. Primary outcome papers\nThe primary outcome papers of BEST are papers that present outcome data . . . The determination of whether or not a particular analysis represents a primary outcome will be made by the Steering Committee on the recommendation of the Publications Subcommittee . . .\nD. Other study papers, abstracts and presentations\nAll studies other than those designated as Primary Outcome fall within this category . . . All papers and abstracts must be approved by the Publications Committee before they are submitted.\n\nIt is possible that in certain instances BEST may be asked to contribute papers to workshops, symposia, volumes, etc. The individuals to work on such requests should be appointed by the Executive Committee, but where time permits, a proposal will be circulated soliciting other participants as in the case of other study papers as described in the Application Review Process.\n\n\n\nClose-out Procedures\n\n\nBEST may terminate at the planned target of 1.5 years after the last participant has been randomized, or at an earlier or later date if the circumstances warrant… Regardless of the timing and circumstances of the end of the study, close-out will proceed in two stages:\nInterim period for analysis and documentation of study results.\nDebriefing of participants and dissemination of study results.\nA. Interim\nEvery attempt will be made to reduce to an absolute minimum the interval between the completion of data collection and the release of the study results. We expect to take about 3 to 4 months to compile the final results paper for an appropriate journal.\nB. Reporting of study results\nThe study results will be released to the participating physicians, referring physicians, patients and the general medical community.\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-31b_dissemination_policy_authorship-1",
    "href": "guidelines/spirit/index.html#sec-31b_dissemination_policy_authorship-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nSubstantive contributions to the design, conduct, interpretation, and reporting of a clinical trial are recognised through the granting of authorship on the final trial report. Authorship guidelines in the protocol are intended to help enhance transparency and avoid disputes or misunderstanding after trial completion. These guidelines should define criteria for individually named authors or group authorship.385\nIndividuals who fulfil authorship criteria should not remain hidden (ghost authorship) and should have final authority over manuscript content.9 386 387 Similarly, those who do not fulfil such criteria should not be granted authorship (guest authorship).386 388 The International Committee of Medical Journal Editors has defined authorship criteria for manuscripts submitted for publication,389 although these criteria have reportedly been open to abuse.390 If some protocol authors are not named authors of subsequent publications, their role in protocol design should at least be acknowledged in the published report. Among 44 protocols of industry initiated trials, 75% had evidence of ghost authorship when compared with corresponding journal publications.9\nProfessional medical writers are sometimes hired to improve clarity and structure in a trial report, and guidelines for ethical collaborative writing have been developed.391 392 Because the drafting of text can influence how the study results and conclusions are portrayed, plans for the employment of writers and their funding source should be acknowledged in both protocols and trial reports."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-31b_dissemination_policy_authorship-2",
    "href": "guidelines/spirit/index.html#sec-31b_dissemination_policy_authorship-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\n17.4. Assignment of Writing Committees\n\nTopics suggested for presentation or publication will be circulated to the PIs [principal investigators] of the CCCs [core coordinating centers], the DCC [data coordinating centre], Core Lab and the NIH [National Institutes of Health]. These groups are requested to suggest and justify names for authors to be reviewed by the PC [publications committee]. . . If a topic is suggested by a participant of the FSGS-CT [focal segmental glomerulosclerosis—clinical trial], the writing committee will be formed as just described except that the person making the suggestion may be considered as the lead author. The PI of an ancillary study should be considered for lead author of material derived from this study. Disputes regarding authorship will be settled by the Study Chair after consultation with the Chair of the PC…\n17.5. Reports of the FSGS-CT: Classes of Reports\nThere are three classes of reports of the FSGS-CT:\nA. Reports of the major outcomes of the Study.\nB. Reports addressing in detail one aspect of the FSGS-CT, but in which the data are derived from the entire study.\nC. Reports of data derived from a subset of centers by members of the FSGS-CT, (eg, sub-studies or ancillary studies), or reports of investigations initiated outside of the FSGS-CT, but using data or samples collected by the FSGS-CT. . .\n17.6. Authorship Policy\nThe authors of FSGS publications will be listed as detailed below.\nType A publications:\nabstracts: from the FSGS Clinical Trial Groupx, presented by XXXX.\npapers: from the FSGS Clinical Trial Groupx, prepared by XXXX.\nxThe FSGS participant box, detailed below, must be included in these papers. If a journal’s publication policy does not allow authorship by a group, the authors will be listed first as in Type B publications.\nType B publications:\n…\n17.7. Authorship: Professional Participants Listing in the FSGS Participant Box\nThe FSGS participant box will list all professionals that have participated in the FSGS-CT for a minimum of one year.\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-31c_dissemination_policy_reproducible_research-1",
    "href": "guidelines/spirit/index.html#sec-31c_dissemination_policy_reproducible_research-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nGiven the central role of protocols in enhancing transparency, reproducibility, and interpretation of trial results, there is a strong ethical and scientific imperative to ensure that full protocols are made publicly available.24 394 395 High quality protocols contain relevant details on study design and conduct that are generally not available in journal publications or trial registries.84 396 It is also important to make available the full study report, such as the clinical study report submitted to regulatory agencies by industry sponsors.377 396 397 398 399 400 This detailed report provides the most comprehensive description of trial methods (including the full protocol) and all published and unpublished analyses. In addition, there have increasingly been calls to improve the availability of participant-level datasets and statistical code after journal publication to enable verification and replication of analyses, facilitate pooling with other studies, and accelerate research through open knowledge sharing.372 401 402 403 404 405 406\nAvenues for providing access to full protocols include journals,407 408 trial websites, and trial registries.163 Several journals and funders support the sharing of participant level data,405 409 410 411 while others routinely publish a statement regarding sharing of protocols, statistical codes, and datasets for all of their published research articles.412 413\nThe protocol should indicate whether the trial protocol, full study report, anonymised participant level dataset, and statistical code for generating the results will be made publicly available; and if so, describe the timeframe and any other conditions for access."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-31c_dissemination_policy_reproducible_research-2",
    "href": "guidelines/spirit/index.html#sec-31c_dissemination_policy_reproducible_research-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nData sharing statement No later than 3 years after the collection of the 1-year postrandomisation interviews, we will deliver a completely deidentified data set to an appropriate data archive for sharing purposes.\n\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-32_informed_consent_materials-1",
    "href": "guidelines/spirit/index.html#sec-32_informed_consent_materials-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nThe Declaration of Helsinki states that each potential trial participant must normally, at a minimum, be adequately informed about the purpose of the trial; potential benefits and risks; their right to refuse participation or to withdraw consent at any time; institutional affiliation and potential competing interests of the researcher; and sources of trial funding.1 There are rare exceptions where deferred consent can be acceptable, such as trials involving unconscious patients in emergency situations.\nSpecial attention is required to ensure that relevant information is provided and appropriate modes of delivery are used during the consent process (Item 26).414 Consent and participant information forms are often written at a much higher reading level than is acceptable for the general population.415 Depending on the nature of the trial, several different consent documents may be needed. For example, a paediatric trial may involve both parental permission and participant assent documents. For multicentre trials, a model or sample document is typically drafted for distribution to local investigators, who may then revise the document to comply with local requirements."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-32_informed_consent_materials-2",
    "href": "guidelines/spirit/index.html#sec-32_informed_consent_materials-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nAPPENDIX 7 SAMPLE PATIENT INFORMED CONSENT\n\n\nNote: . . . Each Ethics Committee or Institutional Review Board will revise and adapt according to their own institution’s guidelines.\n\n\nMULTICENTER PHASE III RANDOMIZED TRIAL COMPARING DOXORUBICIN AND CYCLOPHOSPHAMIDE . . .\n\n\nStudy number: BCIRG 006 (TAX GMA 302)\n\n\nInvestigator name:\n\n\nAddress:\n\n\nConsent Form:\n\n\nThis consent form is part of the informed consent process. It is designed to give you an idea of what this research study is about and what will happen to you if you choose to be in the study…\n\nBack to top"
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-33_biological_specimens-1",
    "href": "guidelines/spirit/index.html#sec-33_biological_specimens-1",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Read More",
    "text": "Read More\nBiological specimens (eg, biopsy tissue; blood for DNA extraction) obtained during the conduct of clinical trials can be stored in repositories—often designated as biobanks—for the current trial and future research. This process is usually governed by local regulation and has particular ethical considerations (Item 26b).\nIf the trial involves genetic or molecular analysis of biological specimens derived from humans, or if any specimens will be stored for future use (specified or unspecified), the protocol should describe details about specimen collection, storage, and evaluation, including the location of repositories. In addition, the protocol should state whether collected samples and associated participant related data will be de-identified or coded to protect participant confidentiality. If a repository is overseen by a named research ethics committee/institutional review board, then this information should also be provided."
  },
  {
    "objectID": "guidelines/spirit/index.html#sec-33_biological_specimens-2",
    "href": "guidelines/spirit/index.html#sec-33_biological_specimens-2",
    "title": "The SPIRIT guideline for writing a Protocol of a clinical trial.",
    "section": "Examples",
    "text": "Examples\n\nWhite Blood Cell and Plasma Collection Procedures\n\n1.0 Objectives\n1.1 To provide a resource for studies of early markers, etiology, and genetic risk factors for prostate cancer and other diseases.\n2.0 Background\nThe Prostate Cancer Prevention Trial (PCPT) is a randomized double blind chemoprevention trial…\nInitial blood collection was specifically for the analysis of PSA [prostate specific antigen] and storage of serum . . . an additional blood collection will be carried out using anticoagulant so that plasma and white blood cells can be isolated. Plasma will allow the analysis of additional biomarkers . . . This DNA will be used (among other possible uses) for studies to investigate polymorphisms in genes which may influence prostate cancer risk . . .\n\nThe PCPT WBC [white blood cell] sample will be available to PCPT investigators as well as outside researchers who have important, timely hypotheses to test.\n\nBecause the sample bank is a limited resource, proposals to use it will be evaluated in terms of scientific relevance, significance, and validity as well as the potential impact of the proposed study. The amount and type of material needed will also be considered and the efficient use of material will be required. Strict confidentiality will be exercised and the information provided to investigators will not contain personal identifiers.\n\nWhen specific uses of the WBC samples are approved, the SWOG-9217 protocol will be amended.\n\n\nParticipation in this research is not required for continued participation in the PCPT.\n\n\n3.0 Methods\n\n3.1 Because the original model consent form did not specifically address genetic studies, participants will be asked to sign an additional consent form to document their consent to the collection and submission of additional blood samples for storage and future testing (including genetic analysis).\n3.2 Institutions will be asked to submit additional materials from participants who consent to the additional blood collection. The blood is to be collected, processed and shipped as described in the PCPT Study Manual.\n3.3 NCI-Frederick Cancer Research Development Center (FCRDC) in Frederick, Maryland will serve as the processing, aliquotting and storage facility.\n3.4 Upon arrival at FCRDC the blood will be pooled and centrifuged. Plasma will be separated into 5 x 1.8 ml aliquots and frozen . . .\n3.5 All samples will be logged in and aliquots will be bar coded with a unique storage ID. These data will be electronically transmitted to the Statistical Center for verification.\n3.6 The scientists who will carry out analyses on these materials will not have access to personal identifiers and will not be able to link the results of these tests to personal identifier information. No individual results will be presented in publications or other reports…\n3.7 Participants will not be informed on an individual basis of any results from these studies…\n4.0 Sample analysis\n4.1 Investigators planning to submit NIH [National Institutes of Health] grant applications must obtain approval for their study and specimen access from the PCPT Serum and Tissue Utilization Committee before submission of a grant proposal. Potential investigators will be required to submit a brief abstract and 1-4 page outline… This proposal will be circulated for review to members of the PCPT Serum and Tissue Utilization Committee and two ad hoc members having relevant expertise…\n4.2 It is anticipated that proposals will be reviewed once a year . . . Approval by this group as well as appropriate Institutional Review Board approval from the investigator’s institution will be required before release of samples.\nBack to top"
  },
  {
    "objectID": "guidelines/squire/index.html#about-this-guideline",
    "href": "guidelines/squire/index.html#about-this-guideline",
    "title": "The SQUIRE guideline for writing a quality improvement study.",
    "section": "About this guideline",
    "text": "About this guideline\n\nThis guideline is relevant to reports that describe system level work to improve the quality, safety, and value of healthcare,"
  },
  {
    "objectID": "guidelines/squire/index.html#download-resources",
    "href": "guidelines/squire/index.html#download-resources",
    "title": "The SQUIRE guideline for writing a quality improvement study.",
    "section": "Download Resources",
    "text": "Download Resources\nReporting guidelines are best used early:\n\nUse a template to write quickly and avoid the blank page\nUse a checklist to double-check and reassure others that your work is described transparently\n\n\n\n\nFor drafting\n\nTemplate\n\n\n\nFor checking\n\nChecklist\n\n\n\nFor planning\n\nTo-do list"
  },
  {
    "objectID": "guidelines/squire/index.html#guidance",
    "href": "guidelines/squire/index.html#guidance",
    "title": "The SQUIRE guideline for writing a quality improvement study.",
    "section": "Guidance",
    "text": "Guidance\nApprox. 2 min read\n\nTitle\n\n\n1. Title\n\n\n\n\n\n\n\nIndicate that the manuscript concerns an initiative to improve healthcare\n(broadly defined to include the quality, safety, effectiveness, patientcenteredness,\ntimeliness, cost, efficiency, and equity of healthcare)\n\n\nAbstract\n\n\n02a. Abstract\n\n\n\n\n\n\n\nProvide adequate information to aid in searching and indexing\n\n\n02b. Abstract\n\n\n\n\n\n\n\nSummarize all key information from various sections of the text using the abstract format of the intended publication or a structured summary such as: background, local problem, methods, interventions, results, conclusions\n\n\nIntroduction\n\n\n3. Problem description\n\n\n\n\n\n\n\nNature and significance of the local problem\n\n\n4. Available knowledge\n\n\n\n\n\n\n\nSummary of what is currently known about the problem, including\nrelevant previous studies\n\n\n5. Rationale\n\n\n\n\n\n\n\nInformal or formal frameworks, models, concepts, and / or theories used to\nexplain the problem, any reasons or assumptions that were used to\ndevelop the intervention(s), and reasons why the intervention(s) was\nexpected to work\n\n\n6. Specific aims\n\n\n\n\n\n\n\nPurpose of the project and of this report\n\n\nMethods\n\n\n7. Context\n\n\n\n\n\n\n\nContextual elements considered important at the outset of introducing the intervention(s)\n\n\n08a. Intervention(s)\n\n\n\n\n\n\n\nDescription of the intervention(s) in sufficient detail that others could reproduce it\n\n\n08b. Intervention(s)\n\n\n\n\n\n\n\nSpecifics of the team involved in the work\n\n\n09a. Study of the Intervention(s)\n\n\n\n\n\n\n\nApproach chosen for assessing the impact of the intervention(s)\n\n\n09b. Study of the Intervention(s)\n\n\n\n\n\n\n\nApproach used to establish whether the observed outcomes were due\nto the intervention(s)\n\n\n10a. Measures\n\n\n\n\n\n\n\nMeasures chosen for studying processes and outcomes of the\nintervention(s), including rationale for choosing them, their\noperational definitions, and their validity and reliability\n\n\n10b. Measures\n\n\n\n\n\n\n\nDescription of the approach to the ongoing assessment of contextual elements that contributed to the success, failure, efficiency, and cost\n\n\n10c. Measures\n\n\n\n\n\n\n\nMethods employed for assessing completeness and accuracy of data\n\n\n11a. Analysis\n\n\n\n\n\n\n\nQualitative and quantitative methods used to draw inferences from the data\n\n\n11b. Analysis\n\n\n\n\n\n\n\nMethods for understanding variation within the data, including the\neffects of time as a variable\n\n\n12. Ethical considerations\n\n\n\n\n\n\n\nEthical aspects of implementing and studying the intervention(s) and how they were addressed, including, but not limited to, formal ethics review and potential conflict(s) of interest\n\n\nResults\n\n\n13a. Results\n\n\n\n\n\n\n\nInitial steps of the intervention(s) and their evolution over time (e.g.,\ntime-line diagram, flow chart, or table), including modifications made\nto the intervention during the project\n\n\n13b. Results\n\n\n\n\n\n\n\nDetails of the process measures and outcome\n\n\n13c. Results\n\n\n\n\n\n\n\nContextual elements that interacted with the intervention(s)\n\n\n13d. Results\n\n\n\n\n\n\n\nObserved associations between outcomes, interventions, and relevant\ncontextual elements\n\n\n13e. Results\n\n\n\n\n\n\n\nUnintended consequences such as unexpected benefits, problems,\nfailures, or costs associated with the intervention(s).\n\n\n13f. Results\n\n\n\n\n\n\n\nDetails about missing data\n\n\nDiscussion\n\n\n14a. Summary\n\n\n\n\n\n\n\nKey findings, including relevance to the rationale and specific aims\n\n\n14b. Summary\n\n\n\n\n\n\n\nParticular strengths of the project\n\n\n15a. Interpretation\n\n\n\n\n\n\n\nNature of the association between the intervention(s) and the\noutcomes\n\n\n15b. Interpretation\n\n\n\n\n\n\n\nComparison of results with findings from other publications\n\n\n15c. Interpretation\n\n\n\n\n\n\n\nImpact of the project on people and systems\n\n\n15d. Interpretation\n\n\n\n\n\n\n\nReasons for any differences between observed and anticipated\noutcomes, including the influence of context\n\n\n15e. Interpretation\n\n\n\n\n\n\n\nCosts and strategic trade-offs, including opportunity costs\n\n\n16a. Limitations\n\n\n\n\n\n\n\nLimits to the generalizability of the work\n\n\n16b. Limitations\n\n\n\n\n\n\n\nFactors that might have limited internal validity such as confounding, bias, or imprecision in the design, methods, measurement, or analysis\n\n\n16c. Limitations\n\n\n\n\n\n\n\nEfforts made to minimize and adjust for limitations\n\n\n17a. Conclusion\n\n\n\n\n\n\n\nUsefulness of the work\n\n\n17b. Conclusion\n\n\n\n\n\n\n\nSustainability\n\n\n17c. Conclusion\n\n\n\n\n\n\n\nPotential for spread to other contexts\n\n\n17d. Conclusion\n\n\n\n\n\n\n\nImplications for practice and for further study in the field\n\n\n17e. Conclusion\n\n\n\n\n\n\n\nSuggested next steps\n\n\nOther information\n\n\n18. Funding\n\n\n\n\n\n\n\nSources of funding that supported this work. Role, if any, of the funding organization in the design, implementation, interpretation, and reporting"
  },
  {
    "objectID": "guidelines/srqr/index.html",
    "href": "guidelines/srqr/index.html",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "",
    "text": "Full Title: Standards for Reporting Qualitative Research\nAuthors: Bridget O’Brien, Ilene Harris, Thomas Beckman, Darcy Reed, David Cook\nVersion: 1.1\nDOI: 10.1234/equator/1010101"
  },
  {
    "objectID": "guidelines/srqr/index.html#translations",
    "href": "guidelines/srqr/index.html#translations",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Translations",
    "text": "Translations\nThis guideline is also available in French."
  },
  {
    "objectID": "guidelines/srqr/index.html#about-this-guideline",
    "href": "guidelines/srqr/index.html#about-this-guideline",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "About this guideline",
    "text": "About this guideline\n\nWhen to use this guideline\n\nUse this guideline for writing qualitative research articles. You can use it when describing all kinds of qualitative approaches, methods, and designs.\n\n\nYou can also use this guideline for:\n\nwriting proposals or protocols (use the items within the Introduction and Method sections).\nreviewing the reporting of an article, but not for appraising its quality.\n\n\n\n\n\n\n\n\n\nDo not use this guidance for:\n\nwriting a qualitative evidence synthesis, use ENTREQ instead.\nappraising the quality of qualitative research, use an appraisal tool like CASP-Qual instead. \n\n\n\n\n\n\n\n\n\n\nRelated reporting guidelines:\n\nJARS Qual for writing qualitative, psychology manuscripts\nENTREQ for writing qualitative evidence syntheses\nFor writing studies involving interviews or focus groups, you can use this guideline or COREQ. \n\nFor appraising research consider:\n\nCASP-Qual\n\n\n\n\n\n\nHow to use this guideline\nReporting guidelines help me structure my drafts and develop as a researcher. I use them when teaching and hope that my students continue to use them.Manuel Silva - Researcher\nReporting guidelines are best used early. Read the full guideline below or use a tool when working:\n\nUse a template to draft manuscripts and avoid the blank page \nUse a log book to plan and record information whilst conducting research\nUse a checklist to double-check and reassure others that your article describes all the important details of your work\n\n\n\n\nFor drafting\n\nTemplate\n\n\n\nFor planning\n\nLog book\n\n\n\nFor checking\n\nChecklist\n\n\nThis reporting guideline does not prescribe a rigid format or standardized content. Consider each item and prioritize elements that are most relevant to your study, findings, context, and readers.\nYou may prefer to report an item in a different order, section, or in a table or figure. If you feel confident that an item is less important to your study, you could report it in an appendix or supplement. If you think an item is not applicable, state why. You don’t need to write your article in the order the items are presented.\nIn your methods section, state that you used the SRQR guideline when writing your article and cite it (see How to cite).\n\n\nWhy use this guideline\nFollowing this guideline will make it easier for other researchers to find, use, and synthesise your work. It won’t tell you how to do qualitative research, but it will help you explain your choices so others can understand what you did, why you did it, and what you found. At our journal, we ensure all qualitative articles follow SRQR as this makes them easier for readers to understand and faster to publish.Fiona Ludlow - Editor in Chief\nIt was written by experts from the qualitative research community, using a thorough development process. SRQR is endorsed by hundreds of journals, so using this guideline (and its checklist) will help you meet editors’ expectations."
  },
  {
    "objectID": "guidelines/srqr/index.html#guidance",
    "href": "guidelines/srqr/index.html#guidance",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Guidance",
    "text": "Guidance\nApprox. 17 min read\n\n\n1. Title\n\n\n\n\n\n\n\nDescribe the nature and topic of the study.\nIdentify the study as qualitative or indicate the approach or data collection methods.\n\n\n\n\n\n\nJustification, Examples, and Resources\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\nTraining and Resources\n\n\nWhy readers need this information\nThis allows readers to quickly identify the type of study. Your title will be indexed by search tools so descriptive words will make it easier for others to discover.\n\n\nExamples\n\nResidents learning from a narrative experience with dying patients: a qualitative study.\n\n\nMedical students’ perceptions of the factors influencing their academic performance: an exploratory interview study with high-achieving and re-sitting medical students.\n\n\nUndergraduate rural medical education program development: focus group consultation with the NRHA Rural Medical Educators Group.\n\n\n\n\nTraining and Resources\nSee this article for advice on writing titles that are interesting and descriptive.\nThe EQUATOR Network provides training for writing effective manuscript titles as part of their publication school.\nBack to top\n\n\n\n\n\n\n2. Abstract\n\n\n\n\n\n\n\nInformative key words in the title and abstract make manuscripts easier to find.Lorna Ellis - Evidence Synthesiser\nSummarise key elements of the study including:\n\nbackground about the problem or phenomenon of interest\ndescription of the study purpose or research question\nmethods, including the approach or perspective (e.g., general inductive, grounded theory), context (setting, time period), sample (number and key characteristics of participants, events, documents), data collection strategies (e g., observation, interview, focus group) and data analysis techniques\ndescription of main findings (e.g., themes or inferences) related to the study purpose and/or research question\nstudy implications\n\n\nInformation presented in the abstract should be consistent with the information presented in the full text.\nUse the format of your intended journal. 1\n\n\n\n\n\n\nJustification, Example, and Resources\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExample\nTraining and Resources\n\n\nWhy readers need this information\nA reader should be able to read the abstract independent of the manuscript and get a sense of the background, purpose, methods, main results and implications that will be described in greater detail in the manuscript.\nYour abstract will be indexed by search tools so descriptive words will make it easier for others to discover.\n\n\n\nExample\n\nPurpose: Although academic centers rely on assessments from medical trainees regarding the effectiveness of their faculty as teachers, little is known about how trainees conceptualize and approach their role as assessors of their clinical supervisorsMethod: In 2010, using a constructivist grounded theory approach, five focus group interviews were conducted with 19 residents from an internal medicine residency program. A constant comparative analysis of emergent themes was conducted.Results: Residents viewed clinical teaching assessment (CTA) as a time-consuming task with little reward. They reported struggling throughout the academic year to meet their CTA obligations and described several shortcut strategies they used to reduce their burden. Rather than conceptualizing their assessments as a conduit for both formative and summative feedback, residents perceived CTA as useful for the surveillance of clinical supervisors at the extremes of the spectrum of teaching effectiveness. They put the most effort, including the crafting of written comments, into the CTAs of these outliers. Trainees desired greater transparency in the CTA process and were sceptical regarding the anonymity and perceived validity of their faculty appraisals.Conclusions: Individual and system-based factors conspire to influence postgraduate medical trainees’ motivation for generating high-quality appraisals of clinical teaching. Academic centers need to address these factors if they want to maximize the usefulness of these assessments.\n\n\n\n\nTraining and Resources\nThe EQUATOR Network provides training for writing effective manuscript abstracts as part of their publication school.\nSee this article from the London School of Economics for general abstract writing tips.\nBack to top\n\n\n\n\n\nIntroduction\n\n\n3. Problem Formulation\n\n\n\n\n\n\n\nWhen the problem is described clearly I find it much easier to understand why a study was done, even if its from a different subject area, country, or decade.Nkinda Akaro - Researcher\nDescribe the theoretical and/or practical issues or concerns that make the study necessary, including:\n\nan overview of what is known about the problem\ngaps in current knowledge (the problem statement)\nthe scope of the research problem or phenomena addressed in the study (what will and will not be included)\ntheoretical and/or empirical work directly relevant to the problem or phenomena studied\nthe need for a qualitative approach.2\n\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExample\n\n\nWhy readers need this information\nThis information provides context for the research question or study purpose and situates the study in relation to other work in the field.\n\n\nExample\n\nRegulatory focus theory may therefore offer insight into the variability in responses to feedback, but how well do these experimental findings translate to real clinical situations in which the reality of responsiveness to feedback seems frustratingly complex? …. In order to better elaborate a theory to account for this variability in learner response to feedback, the present study was undertaken. We aimed, in this study, to determine how readily clinical learning events could be classified as activating a promotion or a prevention focus, and to explore, through a careful analysis of doctors’ descriptions of their feedback experiences, the predictive value of regulatory focus theory in the context of real clinical learning situations.\n\n\n\nBack to top\n\n\n\n\n\n\n4. Purpose or research question\n\n\n\n\n\n\n\nA clear purpose frames the article. Rahilah Zayn - Researcher\nInclude a statement of study intent. This can be framed as one or more research questions, purposes, goals, or objectives.3\n\n\n\n\n\n\nJustification, Example, and Resources\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExample\nTraining and Resources\n\n\nWhy readers need this information\nBy clearly stating the purpose of the study, authors set readers’ expectations for the methods, findings and discussion sections of the manuscript.\n\n\nExample\n\nThe purposes of this study were to investigate how medical students recognize, respond to and utilise feedback, and to determine whether there are maturational differences in understandings of the role of feedback across academic years in medical school.\n\n\n\nTraining and Resources\nSee this article for advice on writing qualitative research questions.\nBack to top\n\n\n\n\n\n\nMethods\n\n\n5. Qualitative approach and research paradigm\n\n\n\n\n\n\n\nDescribe your qualitative approach, your guiding theory (if appropriate), and identify the research paradigm.4 I like it when authors describe terms in their own words as well as using the proper terms.Rahilah Zayn - Researcher\nExplain why the selected approach is appropriate for the research question.\nProvide references to theories or traditions that guide the use of the approach as needed.\nI hadn’t heard the term approach before writing-up my study. I realise now that I did have an approach and that it might be different to other researcher’s approaches, so I need to describe things that I felt were obvious.Tim Westland - Researcher\n\nIf you don’t know what your approach or paradigm was, or you don’t think you had one, it’s OK to reflect on this after collecting data and you should still report it. Read this list to see what best describes your work.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nIdentifying the research paradigm helps readers understand whether the researcher assumes that there is a single, objective reality (positivist or post-positivist) and has thus designed the study to describe this reality or whether the researcher assumes multiple, subjective realities and designed the study to describe these multiple realities, with no attempt to merge or reconcile these realities (constructivist/interpretivist). The paradigm has implications for the study design, approach, methods, and techniques to ensure rigour and trustworthiness.\nSince the research paradigm does not necessarily dictate particular approaches or methods, the approach should also be clearly defined. Stating the approach provides readers the opportunity to evaluate the fidelity of the research approach to the research question(s) and consider the rationale for modifications and deviations from the selected approach. Qualitative research also includes an array of methods that can be used across paradigms and approaches. (See also Item 10).\n\n\nExamples\n\nThe study was performed from a constructivist point of view using an interpretative phenomenological epistemology. Based on the notion that social phenomena are constructed by the communal making of meaning about the underlying phenomena, we aimed to construct insightful accounts of lead consultants’ approaches to educational change, rather than to identify the true nature of these approaches. Because the management of change by lead consultants is an under-researched area, we conducted an exploratory qualitative study…\n\n\nGiven the relative dearth of explanatory theories about factors affecting medical students’ emotional reactions, we chose to develop one by applying methods associated with grounded theory, specifically constant comparative analysis, to qualitative data obtained from learning logs and interviews. Our approach was constructivist, deliberately using researchers’ own experiences and acquired knowledge to enhance theoretical sensitivity and enrich theory development.\n\nBack to top\n\n\n\n\n\n\n6. Researcher characteristics and reflexivity\n\n\n\n\n\n\n\nDescribe how roles and identities of research team members influence choice of research approach, data collection, and data analysis.5\nComing from quantitative research it felt strange to include such personal descriptions of the team. But I came to realise why it was useful to others, and that peer reviewers would expect it. Tim Westland - Researcher\nDescribe the perspectives, assumptions, prior knowledge, preliminary hypotheses, and/or motives (the stance) of the members of the research team.\nDescribe the researchers’ relationships to participants in the study and what decisions were made in light of these relationships.6\nIf your research was observational (e.g., ethnography), describe the role of the researcher along a spectrum from passive observer (no involvement in the activity studied) to participant-observer (ranging from some limited involvement in the activity to full involvement).\nThere is no expectation that the study could be precisely replicated; these characteristics and perspectives of the researcher should not be mentioned in the limitation section. (See also Item 14: Data Analysis)\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nIn positivist and post-positivist paradigms, personal characteristics and perspectives of researchers might be viewed as biases that limit the credibility of study findings, while in constructivist or interpretivist paradigms the characteristics and perspectives of the researchers are important contextual factors that are an accepted part of the study design, data collection, and data analysis. These characteristics and perspectives may explain how the researcher(s) obtained access to the site or participants included in the study or may add valuable insight during data analysis.\n\n\nExamples\n\nReflexivity was maintained by the research team through the analysis and writing by recording, discussing and challenging established assumptions. In addition both EH and SV kept reflexive diaries.\n\n\nThe first author conducted all interviews and discussion groups. Her own medical undergraduate training took place between 1995 and 2000. She was not known to the participants of this research prior to undertaking the study and deliberately did not undertake any clinical or teaching activities locally alongside this research. Whilst it was useful to know (from her own background) what the students were talking about medically (and in terms of detecting items of significance), as a researcher she made conscious efforts not to accept potentially common assumptions at face value.\n\nBack to top\n\n\n\n\n\n\n7. Context\n\n\n\n\n\n\n\nDescribe the setting/site(s) in which the study was conducted, the reason(s) why the setting/site(s) was selected, and the salient cultural, political, historical, economic and/or other external factors that influence the study.\nWhen synthesising evidence, context helps me work out how and why different studies and findings fit together (or don’t)Lorna Ellis - Evidence Synthesiser\nAdditional context may be reported with findings (i.e., the Results section) to add evidence for interpretations and to enhance discussion of transferability.\n\n\n\n\n\n\nJustification and Example\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExample\n\n\nWhy readers need this information\nThis helps readers interpret the meaning and significance of the study findings by situating them in social, cultural, temporal and other relevant contexts.\n\n\nExample\n\nWe conducted the study among hospital-based clinical teachers of students in years 4 to 6 of a six-year undergraduate medical program at Maastricht University Medical School. Years 4 to 6 are devoted to clerkships in the academic hospital and affiliated regional hospitals. Rotations differ in duration depending on the type of rotation and the discipline, and the sequence of rotations differs among students. During rotations, students spend time in the wards, the outpatient clinics, and the accident and emergency department. Clerkships in years 4 and 5 last between 4 and 10 weeks (regular clerkship), whereas students in year 6 undertake an 18-week senior clerkship in a discipline of their choice.\n\nBack to top\n\n\n\n\n\n\n8. Sampling strategy\n\n\n\n\n\n\n\nDescribe how and why research participants, sites, documents/artifacts, and/or events were selected for inclusion in (and, if appropriate, exclusion from) the study, along with a justification for this strategy.\nDescribe the sampling strategy rather than simply labeling it (e.g., purposive or snowball), since such labels do not have a universally accepted definition and, more importantly, procedures tend to be study- specific.\n\nDescribe how you established the final sample size:\n\nIf you used a flexible sampling strategy, then explain the criteria used to decide when no further sampling was necessary.\nIf data collection ended once saturation or sufficiency had been reached, then describe the specific criteria used to define saturation or sufficiency.\n\n\n\nDescribe procedures used to recruit participants, including:\n\nwho was involved in recruitment\nwhat their relationship was to participants\nhow and when recruitment occurred\nwhy these procedures were selected. (See also Item 6: Researcher characteristics and reflexivity)\n\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThis information helps readers understand the source of data / findings so they can consider the boundaries of the study and the relevance to their own context. It also gives readers insight into the researchers’ decisions, which can be important for critical appraisal.\n\n\nExamples\n\nAs students’ perceptions were previously shown to be related to gender, age, prior experience and place of attachment,[REF] we purposely selected respondents with different backgrounds. This sampling strategy led to the diversity of gender, age, prior patient experience and place of attachment shown in Table 1.\n\n\nPurposive sampling was directed towards achieving maximum variation in age and specialty, using a snowball approach (a non-probabilistic form of sampling in which persons initially chosen for the sample are used as informants to locate other persons having necessary characteristics making them eligible for the sample).[REF ]\n\n\nPotential participants were all medical students in Years 1 and 2 at the University of Toronto in 2004. Following research ethics board approval, recruitment was conducted via e-mail to class listservs. Participant responses were sent directly to the research assistant, who was unknown to participants, so that the principal investigators did not know who did or did not participate. This process was engaged to protect participants’ anonymity and to avoid any impression of coercion because the lead researcher (SG) was involved in the administration of the undergraduate curriculum at the time. Sample size was estimated to be sufficient based on the principle of theoretical saturation [REF] and our previous experience with this methodology (i.e., with a relatively homogeneous population, we expected to reach saturation with approximately 15 interviews per group). There were no exclusion criteria and we accepted the first 15 students from each class who volunteered.\n\nBack to top\n\n\n\n\n\n\n9. Ethical issues pertaining to human subjects\n\n\n\n\n\n\n\nReport approval for the study from an appropriate institutional review board for research associated with human subjects.\nIf you did not receive ethical approval, describe why.\nDescribe procedures used to protect participants, including:\n\ndata collection (e.g., recruitment and informed consent)\nanalysis (e.g., data security and integrity)\nand reporting of findings (e.g., anonymization of excerpts).\n\nIf you provided compensation or offered incentives to facilitate participation, describe this too.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nJournals will require an ethical approval statement.\nQualitative research often involves interaction between researchers and research participants. Correspondingly, researchers should ensure that participants are fully aware of their participation in a research study, the risks and benefits associated with the study, the steps and precautions the researchers will take to minimize risks, such as loss of privacy and confidentiality, and how the researchers plan to use the data.\n\n\n\nExamples\n\nEthical approval was granted by the University of Otago and student participants were invited to attend each focus group discussion by the university representatives of New Zealand Medical Students Association (NZMSA). Usually, a key concern when collecting data from students is that students may feel vulnerable when sharing their experiences with academic staff during a focus group discussion. However, this potential harm was removed as each group discussion was facilitated by a fellow student, the discussions were transcribed by a professional transcribing service, and only the primary researchers [Names] had access to the raw data.\n\nBack to top\n\n\n\n\n\n\n10. Data collection methods\n\n\n\n\n\n\n\nDescribe data collection methods and design in detail, and justify them in relation to the research question(s), paradigm, approach, and other methods.7\n\nIf data collection and analysis was iterative:\n\ndescribe the iterative process\nif changes occurred during the research process, describe how and why study procedures changed in response to evolving study findings.\n\nIdentify the study period.\nDescribe important characteristics of the individuals conducting interviews, observations or focus groups, and methods used to train these individuals.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThe study period helps readers place the study in temporal context and identify factors not mentioned by the authors that might affect findings, interpretation, and implications. (See Item 8 for ending data collection.)\n\nDescribing researcher characteristics clarifies the relationship between the individuals involved in data collection and the participants in the research and also explains what efforts were made to ensure consistency in the data collection process (See Items 6 and 15.)\n\n\n\n\nExamples\n\nFurther, it was decided that group interviews, also known as focus group discussions would be the best means of data collection. This is a method of data collection that enables group members to feed off each other’s ideas and an effective moderator will maintain group focus whilst at the same time permitting flexibility in the direction those aspects of the discussion might take. [REF]\n\n\nAdjustments to the interview protocol were made according to early experience and information participants had provided (i.e. redundant questions were eliminated; questions were reworded to improve flow and clarity; additional probes were included).\n\n\nFaculty staff were then interviewed individually by a trained study investigator in a 15-minute, semi-structured interview. This sequence was repeated with other video encounters. Table 2 presents examples of interview questions. Each faculty member was interviewed by at least three interviewers over their various interviews. Interviewers were chosen based on their experience in interviewing. All were trained during a half-day meeting to interpret and deliver the interview guide in the same manner in order to elicit information of a consistent type.\n\nBack to top\n\n\n\n\n\n\n11. Data collection instruments and technologies\n\n\n\n\n\n\n\nDescribe all data collection instruments, including their development, and if/how they changed over the course of the study. Cite relevant literatures, theories or conceptual frameworks as appropriate. Consider sharing the data collection instrument(s) or a detailed description of them in the article body, supplementary material, or published elsewhere. I publish my materials on the OSF and then describe and cite them in my articles.Tim Westland - Researcher\nDescribe the use of equipment for audio or video recording, reproduction of paper documents or computer files, or other processes in data collection.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nDescribing instruments and equipment helps readers understand the full context in which data collection occurred and how this context might have affected data collection (e.g., the influence of recording devices on participants’ behaviors; the nature of inferences drawn from live vs. recorded events).\n\n\nExamples\n\nTo facilitate the discussion and to maintain consistency over different sets of discussions, key trigger questions were devised prior to the discussion. The opening trigger question was: Thinking back to some of your best clinical learning placements in 4th and 5th year. What was it about those clinical placements that provided good opportunities for learning?\n\n\nInterviews included discussion of the expectations, processes and consequences of AEE [authentic early experiences]. The interview schedule was derived following identification of questions that could not be fully answered in a systematic review of previous empirical or theoretical literature. It comprised a sequence of topic areas including experiences in action, and areas of frustration in Medical Education such as the learning of content knowledge, achieving functional knowledge, and transfer of knowledge.[REF]… Interested readers can request a copy of the schedules from the corresponding author. Interviews lasted between 20 and 90 min. and discussion groups between 60 and 90 min. All interviews and discussion groups were conducted in private rooms at the participant’s workplace—the medical school for students and faculty, and individual places of work for workplace supervisors (except for one who chose to be interviewed at the medical school). All data were audio-recorded, and transcribed verbatim.\n\nBack to top\n\n\n\n\n\n\n12. Units of study\n\n\n\n\n\n\n\nDescribe the number of participants, documents, or events included in the study (the units of study).\nDescribe characteristics of the participants, documents or events that are relevant to the study purpose and research question(s).8\nInclude the dates or timeframe for participation.\nIf the actual sample differs from the target sample, describe:\n\nthe difference,\nwhy these differences may have occurred,\nhow this might affect the findings.\n\nIf the degree of participation varied among individuals, describe:\n\nthe different levels of participation,\nthe reasons for these differences (i.e., the researchers’ choice or the participants’ preferences),\nand how these different levels of participation were taken into account in the analysis.\n\nThis information could appear in the Methods section as part of the description of the sample, or at the beginning of the Results section to provide context for the findings presented.\n\n\n\n\n\n\nMore Information, Justification, and Example\n\n\n\n\n\nJump to:\n\nHow does this item differ to Sampling (Item 8)?\nWhy readers need this information\nExample\n\n\nHow does this item differ to Sampling (Item 8)?\nThe sampling item (Item 8) describes the target or ideal participants, documents, or events selected for the study. By contrast, this item focuses on description of the actual participants, documents or events included in the study.\n\n\nWhy readers need this information\nThis helps readers know whose experiences and perspective are and are not included.\n\n\n\nExample\n\nOf the 70 Mindful Communication program participants, 46 met the eligibility requirements to participate in the in-depth interviews. We randomly chose and then contacted 22 participants, of whom 20 agreed to be interviewed within six months of completing the program: 15 in person and 5 by telephone. Two declined for lack of time. On reaching saturation after 20 interviews, no further attempts to contact the remaining 24 participants were made.\n\n\n\n\n\nThere were 31 nursing handovers covering 137 patients, and 21 resident handovers covering 101 patients included in this study.\n\nBack to top\n\n\n\n\n\n\n13. Data processing\n\n\n\n\n\n\n\nDescribe the ways in which data are prepared for analysis and managed throughout the analysis process. These activities might include transcription, coding, data entry, and organization of data. (See footnote for audio or visual recordings9).\nDescribe the processes used to organize, compile, and categorize data (e.g., field notes, transcripts, documents, photographs, artifacts) for analysis.\nIf you used transcripts, describe procedures used to check accuracy.\nDescribe procedures used to maintain data security and protect the privacy of participants, as specified in the human subjects protocols (see footnote on anonymisation10 as an example).\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript.\n\n\n\nExamples\n\nInterviews were anonymised and each participant was given a code number.\n\n\nThe interviewers and another member of the research team (H.B.) reviewed transcripts for accuracy.\n\n\nWe collected data throughout the admission process through direct observation, audio- recording, and chart extraction. We audio-recorded, transcribed, and anonymized both the overnight and morning case review discussions. We also observed the morning case review discussions in person and collected field notes. For each case review discussion, we copied the admission notes from the patient’s record and de-identified all data.\n\nBack to top\n\n\n\n\n\n\n14. Data analysis\n\n\n\n\n\n\n\nDescribe your analytic process as transparently as possible.11\nIf you used an approach that has a well-defined process for data analysis (e.g., grounded theory, discourse analysis, phenomenography):\n\ncite the guiding literature\ndescribe your processes in sufficient detail so readers can judge the extent to which your processes align with the guiding approach.\nIf you modified or deviated from the guiding approach, explain and justify these modifications.\n\nSpecify the unit of analysis.12\nExplain the rationale underlying different decisions made throughout the data analysis process to provide as much transparency as possible.13\nIf observations that contrast or deviate from identified concepts or themes were important to your analysis, describe how these discrepancies were handled during the analysis.\nIf you drew upon a theoretical perspective or framework during analysis, describe theoretical or other influences on your analysis scheme or categories if they exist. If you identified a theoretical perspective or framework early in the conception of the study or after reviewing some or all of their data, consider referring to these as sensitizing concepts to acknowledge that the approach is inductive, but with influence from relevant theory, models, or organizational schemes. Alternatively, explain that themes were developed from the data with no external influences.\nDescribe which members of the research team are involved in data analysis and what perspective(s) they bring to the analysis.\nIf software was used to assist with data analysis14, describe how it was used. Simply stating that software was used is insufficient.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nTechniques used for data analysis will depend on the paradigm, approach, and/or data collection methods selected by the researchers. Correspondingly, authors should be as transparent as possible about the analytic process so that readers can follow the logic of inquiry from the research question(s) to the analysis and findings.\n\n\nExamples\n\n…we brought sensitizing concepts to the analysis while we conducted an open, inductive analysis.[REF]In this case the sensitizing concepts arose, a priori to analysis, from a framework derived from the literature [REF] (as described above), in which participants’ motivations to act are based on principles of professionalism, internal affect, or potential implications of their actions.[REF]\n\n\nThrough an iterative process of listening, discussing, and relistening, the team identified and consensually validated emerging themes[REF] and appended segments of dialogue supporting the proposed themes. Recruitment stopped when saturation was reached (no new themes were identified). The team systematically reviewed the themes and sorted them into content domains. The team used an analytic matrix to identify patterns and connections amongst the domains. Two of us not involved in the qualitative coding process (R.E., M.K.) audited the analytic matrix, choice of quotes, and thematic analysis.\n\n\nThe analysis started after the first interview. All data were analyzed with the aid of the audio- coding facility of the NVivo 8: QSR International Pty Ltd, Doncaster, Vic, Australia programme. First, [name] and [name] coded independently from one another, making sure to stay semantically close to the participants’ wording. Then we discussed these open codes and defined axial codes.[REF] New insights about the impact of CST were written down in memos.\n\n\nVideotapes were analysed using immersion/crystallisation methods of qualitative data analysis.[REF] With no pre-existing framework developed in advance for analysis, an inductive approach was used to discover patterns of NVB in the data. A team of six researchers met weekly for 18 months to view videos together. Using a consensus-building approach based on a combination of field notes, opportunistic interviews with the participants, and repeated viewing of the same material, sometimes many months apart, we eventually achieved consensus on verbal, non-verbal, and physical themes and patterns observed in the data. Finally, as a test of goodness-of-fit, we carefully reviewed the videotapes for any deviant cases that did not fit the categories we had developed.\n\n\nAll transcripts were coded thematically by four of the five authors, who met regularly to identify areas of convergence until full agreement was reached. One of the interviewers (P.M.) maintained an audit trail to track the team’s developing thinking. A process of dialectical empiricism[REF] was used to categorise the emergent themes into more abstract concepts…\n\nBack to top\n\n\n\n\n\n\n15. Techniques to enhance trustworthiness\n\n\n\n\n\n\n\nDescribe methods used to ensure trustworthiness and credibility throughout the data collection and analysis process. (See footnote on commonly used techniques15).\nExplain your choice of techniques and why these are appropriate for the particular study.\n\n\n\n\n\n\nJustification, Examples, and Resources\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\nTraining and resources\n\n\nWhy readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript.\n\n\n\nExamples\nMember checking\n\nMember checks [REF] with an external TBL expert (R.L.) supported the validity of these analyses.\n\nTriangulation of data types and data sources\n\nThe interview data were triangulated with the data of 11 student and supervisor focus groups of a previous study, and more specifically, with those data that concern in particular the influence of CST [Communication Skills Training] on the development of patient-centredness…. Triangulation with the focus group data allowed us to broaden the in-depth information from the interviews in the analysis and to share and compare this with information from students and doctors with varying levels of CST (no, limited, full programme) and from two universities (Universities of Antwerp and Ghent). Moreover, this enabled us to better explore the evolution over time, given that the focus groups included participants at different stages of their study: before clerkships (year 4, undergraduate), during clerkships (year 6, undergraduate), after clerkships (year 7, undergraduate) and postgraduate (general practice trainees, and supervising specialists and GPs; Table 1).\n\n\n\n\n\nFinally, as a test of goodness-of-fit, we carefully reviewed the videotapes for any deviant cases that did not fit the categories we had developed.\n\nTriangulation of Researchers + Audit trail\n\nTo ensure rigor and increase authenticity in our methodology, we used two kinds of triangulation—investigator triangulation and data triangulation.[REF] We sought analytical rigor using an audit trail and multiple coders; our coding team included an experienced clinician (M.G.) as well as a nonclinician with expertise in medical communication and team dynamics (L.L.).\n\n\n\nTraining and resources\nSee Lincoln and Guba’s Evaluative Criteria for trustworthiness.\nBack to top\n\n\n\n\n\n\nResults\n\n\n16. Synthesis and interpretation\n\n\n\n\n\n\n\nDescribe the main analytic findings.16\nIn most cases, report a synthesis of the data along with specific quotes, examples, or illustrations derived from the data.\n\nI like when authors use words like many, few, or all to describe frequency.Nkinda Akaro - Researcher\nConsider describing frequency, variety, representativeness, counter-examples, concrete details, contextualization, conditions, and qualifications related to the findings.\nFrequency counts play a limited role in qualitative research, and need not be reported unless they play a meaningful role in interpretation of the data.\nIf your findings include integration with prior literature or theory and/or the development of a theory, model or meta-narrative, consider using tables and figures to communicate these findings.\nItems 16 and 18 can be reported in Results or Discussion sections.17\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript.\n\n\n\nExamples\n\nWe identified four patterns of NVB (non-verbal behavior) that relate to handover quality and have dubbed them: (1) joint focus of attention; (2) the poker hand; (3) parallel play; and (4) kerbside consultation. Each pattern constitutes a transfix, or systematic way of participating non-verbally in the care transfer process. And, although there are variations in each pattern, we have been able to code virtually every handover we have observed in nursing, medicine and surgery into one of these four categories.\n\n\nBecause our participants came from similar educational backgrounds, had studied medicine as their tertiary course, were embedded in the culture of medicine, and were associated in meaningful ways with a single medical school, we approached their transcripts with the assumption that they belonged to a loosely formed discourse community. Although their graduation dates ranged over a period of 50 years and their collective sphere of practice included 10 different specialty areas, there were many similarities in their experiences of enculturation during and after medical school.\n\n\nTheir three major (often overlapping) areas of concern were epistemic (acquiring knowledge and skill), interpersonal (relating to patients, families, colleagues and administrators) and personal (achieving work–life balance). In each of these areas, medical enculturation was achieved by two overlapping processes, absorption and assimilation, each of which may have distinct implications for postgraduate medical education.\n\nBack to top\n\n\n\n\n\n\n17. Links to empirical data\n\n\n\n\n\n\n\nProvide evidence (e.g., quotes, field notes, text excerpts, photographs) to substantiate the more general and abstract concepts or inferences they present as findings.18 I love seeing evidence like this because it really brings results to life. Nkinda Akaro - Researcher\nYou could report this evidence in a table or figure, incorporated into a narrative description of findings, as a stand-alone narrative, or in text blocks embedded in the manuscript text. If you are constrained by word limits or media limitations (e.g.. video), consider sharing data via an appendix, supplemental material, or web-based repository.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript.\n\n\n\nExamples\nSee Frankel et al. for an excellent example of how to use photographs (or snapshots from video) to illustrate and provide supporting evidence for patterns of behavior identified in the analysis. http://qualitysafety.bmj.com/content/21/Suppl_1/i121.\n\nWe identified five interruption types: (1) probing for further data, (2) prompting for expected sequence, (3) teaching around the case, (4) thinking out loud, and (5) providing direction (see Table 1). Several interruption types served both goals of the case review discussions—teaching and patient care. For example, when thinking out loud, supervisors reasoned through problems and taught the team: So that’s the big question, did she have a mechanical fall, or did she have a medicine-related fall? (Case 2). Supervisors prompted for expected sequence, preventing presenters from skipping over information while simultaneously allowing the supervisor to instruct the team on presentation style: So now you can tell me what the rest of his test results are because I haven’t heard those (Case 16).\n\nBack to top\n\n\n\n\n\n\nDiscussion\n\n\n18. Integration with prior work, implications, transferability, and contribution(s) to the field\n\n\n\n\n\n\n\nDescribe how the findings and conclusions connect to, support, elaborate on, or challenge previous findings, experiences, theory, or a guiding paradigm or approach.19\nDescribe how the findings contribute to or advance the field.\nDescribe any implications of the work, such transferability or generalizability.\n\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThe short summary reminds readers of the main findings and may help them assess whether the subsequent interpretation and implications formulated by the authors are supported by the findings.\n\n\nExamples\n\nThis study contributes to the understanding and discussion of the complexity of involving patients in healthcare education. It shows that integrating patient-led teaching into initiatives that are partly faculty-led influences the way in which students perceive learning from and with PIs. What is not known, however, is whether perceptions are also affected by type of health profession and the students’ different orientation towards logics of care and science, and issues of authority and power relations.\n\nFor complete examples of Discussions, see:\n\nHenriksen & Ringsted, 2013\nWesterman et al., 2013.\n\nBack to top\n\n\n\n\n\n\n19. Limitations\n\n\n\n\n\n\n\nDescribe problems or gaps in their efforts to ensure trustworthiness and the potential implications.20 Authors should never feel nervous to report limitations. In fact, thoughtful discussion of limitations is a hallmark of good research!Charles Ruggle - Editor\nDescribe how the chosen paradigm, approach, and methods will influence the situations to which the findings may reasonably apply.21 (See also Item 18.)\nDescribe how specific decisions or events in the conduct of the study strengthen or weaken the rigor of the findings.\n\n\n\n\n\n\nJustification and Example\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExample\n\n\nWhy readers need this information\nAll research has limitations. Discussing them will help readers consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be). If you don’t address limitations, editors and peer reviewers may ask you about them which will delay publication.\n\n\nExample\n\nThe study has several limitations. One is that the focus group interview method reveals students’ perceptions rather than their actual behaviors. Observations of the patient-led teaching encounter may have illuminated an understanding of the relationship between patient instructors and medical students. Another limitation is that the PI-led teaching is optional rather than mandatory, which may have influenced students’ attitudes in a positive direction. Moreover, students who are eager to take on extra-curricular activities may not be representative of the whole population. That only 23 out of 39 students signed up for this study might also have influenced results if the missing group of students represented other perceptions than those present in the focus groups. However the received data from the focus groups were rich in information and diverse perceptions were present. Another limitation is the overrepresentation of women over men in our sample. Even though women are also overrepresented in medical school this might potentially have influenced results, but gender differences in perceptions were nevertheless not identified in the data.\n\nBack to top\n\n\n\n\n\n\nOther\n\n\n20. Conflicts of interest\n\n\n\n\n\n\n\nAuthors shouldn’t feel nervous reporting conflicts of interest. They are very common and rarely influence our decision to publish.Charles Ruggle - Editor\nDescribe any real or potential conflicts of interest that might have influenced or could appear to have influenced the research.\nDescribe:\n\nhow these conflicts were managed in the conduct of the study,\nthe potential impact on study findings and/or conclusions.\n\nSome aspects may be mentioned as part of reflexivity (see Item 6).\n\n\n\n\n\n\nJustification\n\n\n\n\n\n\nWhy readers need this information\nMany factors, including professional and personal relationships and activities, can influence the design, conduct, and reporting of the clinical science that informs health care decision. The potential for conflict of interest exists when these relationships and activities may bias judgment (1). Many stakeholders— editors, peer reviewers, clinicians, educators, policymakers, patients, and the public—rely on the disclosure of authors’ relationships and activities to inform their assessments. Trust in the transparency, consistency, and completeness of these disclosures is essential. - ICMJE\n\n\n\n\n\n\n\n21. Funding\n\n\n\n\n\n\n\nDescribe any sources of funding and other support for the study.\nDescribe the role of funders in data collection, data analysis, and reporting if applicable."
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-title-2",
    "href": "guidelines/srqr/index.html#sec-title-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis allows readers to quickly identify the type of study. Your title will be indexed by search tools so descriptive words will make it easier for others to discover."
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-title-3",
    "href": "guidelines/srqr/index.html#sec-title-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nResidents learning from a narrative experience with dying patients: a qualitative study.\n\n\nMedical students’ perceptions of the factors influencing their academic performance: an exploratory interview study with high-achieving and re-sitting medical students.\n\n\nUndergraduate rural medical education program development: focus group consultation with the NRHA Rural Medical Educators Group."
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-title-4",
    "href": "guidelines/srqr/index.html#sec-title-4",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Training and Resources",
    "text": "Training and Resources\nSee this article for advice on writing titles that are interesting and descriptive.\nThe EQUATOR Network provides training for writing effective manuscript titles as part of their publication school.\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-abstract-2",
    "href": "guidelines/srqr/index.html#sec-abstract-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nA reader should be able to read the abstract independent of the manuscript and get a sense of the background, purpose, methods, main results and implications that will be described in greater detail in the manuscript.\nYour abstract will be indexed by search tools so descriptive words will make it easier for others to discover."
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-abstract-3",
    "href": "guidelines/srqr/index.html#sec-abstract-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nPurpose: Although academic centers rely on assessments from medical trainees regarding the effectiveness of their faculty as teachers, little is known about how trainees conceptualize and approach their role as assessors of their clinical supervisorsMethod: In 2010, using a constructivist grounded theory approach, five focus group interviews were conducted with 19 residents from an internal medicine residency program. A constant comparative analysis of emergent themes was conducted.Results: Residents viewed clinical teaching assessment (CTA) as a time-consuming task with little reward. They reported struggling throughout the academic year to meet their CTA obligations and described several shortcut strategies they used to reduce their burden. Rather than conceptualizing their assessments as a conduit for both formative and summative feedback, residents perceived CTA as useful for the surveillance of clinical supervisors at the extremes of the spectrum of teaching effectiveness. They put the most effort, including the crafting of written comments, into the CTAs of these outliers. Trainees desired greater transparency in the CTA process and were sceptical regarding the anonymity and perceived validity of their faculty appraisals.Conclusions: Individual and system-based factors conspire to influence postgraduate medical trainees’ motivation for generating high-quality appraisals of clinical teaching. Academic centers need to address these factors if they want to maximize the usefulness of these assessments."
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-abstract-4",
    "href": "guidelines/srqr/index.html#sec-abstract-4",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Training and Resources",
    "text": "Training and Resources\nThe EQUATOR Network provides training for writing effective manuscript abstracts as part of their publication school.\nSee this article from the London School of Economics for general abstract writing tips.\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-problem-formulation-2",
    "href": "guidelines/srqr/index.html#sec-problem-formulation-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information provides context for the research question or study purpose and situates the study in relation to other work in the field."
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-problem-formulation-3",
    "href": "guidelines/srqr/index.html#sec-problem-formulation-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nRegulatory focus theory may therefore offer insight into the variability in responses to feedback, but how well do these experimental findings translate to real clinical situations in which the reality of responsiveness to feedback seems frustratingly complex? …. In order to better elaborate a theory to account for this variability in learner response to feedback, the present study was undertaken. We aimed, in this study, to determine how readily clinical learning events could be classified as activating a promotion or a prevention focus, and to explore, through a careful analysis of doctors’ descriptions of their feedback experiences, the predictive value of regulatory focus theory in the context of real clinical learning situations.\n\n\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-purpose-2",
    "href": "guidelines/srqr/index.html#sec-purpose-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nBy clearly stating the purpose of the study, authors set readers’ expectations for the methods, findings and discussion sections of the manuscript."
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-purpose-3",
    "href": "guidelines/srqr/index.html#sec-purpose-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nThe purposes of this study were to investigate how medical students recognize, respond to and utilise feedback, and to determine whether there are maturational differences in understandings of the role of feedback across academic years in medical school."
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-purpose-4",
    "href": "guidelines/srqr/index.html#sec-purpose-4",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Training and Resources",
    "text": "Training and Resources\nSee this article for advice on writing qualitative research questions.\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-qualitative-approach-2",
    "href": "guidelines/srqr/index.html#sec-qualitative-approach-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nIdentifying the research paradigm helps readers understand whether the researcher assumes that there is a single, objective reality (positivist or post-positivist) and has thus designed the study to describe this reality or whether the researcher assumes multiple, subjective realities and designed the study to describe these multiple realities, with no attempt to merge or reconcile these realities (constructivist/interpretivist). The paradigm has implications for the study design, approach, methods, and techniques to ensure rigour and trustworthiness.\nSince the research paradigm does not necessarily dictate particular approaches or methods, the approach should also be clearly defined. Stating the approach provides readers the opportunity to evaluate the fidelity of the research approach to the research question(s) and consider the rationale for modifications and deviations from the selected approach. Qualitative research also includes an array of methods that can be used across paradigms and approaches. (See also Item 10)."
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-qualitative-approach-3",
    "href": "guidelines/srqr/index.html#sec-qualitative-approach-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nThe study was performed from a constructivist point of view using an interpretative phenomenological epistemology. Based on the notion that social phenomena are constructed by the communal making of meaning about the underlying phenomena, we aimed to construct insightful accounts of lead consultants’ approaches to educational change, rather than to identify the true nature of these approaches. Because the management of change by lead consultants is an under-researched area, we conducted an exploratory qualitative study…\n\n\nGiven the relative dearth of explanatory theories about factors affecting medical students’ emotional reactions, we chose to develop one by applying methods associated with grounded theory, specifically constant comparative analysis, to qualitative data obtained from learning logs and interviews. Our approach was constructivist, deliberately using researchers’ own experiences and acquired knowledge to enhance theoretical sensitivity and enrich theory development.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-researcher-characteristics-and-reflexivity-2",
    "href": "guidelines/srqr/index.html#sec-researcher-characteristics-and-reflexivity-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nIn positivist and post-positivist paradigms, personal characteristics and perspectives of researchers might be viewed as biases that limit the credibility of study findings, while in constructivist or interpretivist paradigms the characteristics and perspectives of the researchers are important contextual factors that are an accepted part of the study design, data collection, and data analysis. These characteristics and perspectives may explain how the researcher(s) obtained access to the site or participants included in the study or may add valuable insight during data analysis."
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-researcher-characteristics-and-reflexivity-3",
    "href": "guidelines/srqr/index.html#sec-researcher-characteristics-and-reflexivity-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nReflexivity was maintained by the research team through the analysis and writing by recording, discussing and challenging established assumptions. In addition both EH and SV kept reflexive diaries.\n\n\nThe first author conducted all interviews and discussion groups. Her own medical undergraduate training took place between 1995 and 2000. She was not known to the participants of this research prior to undertaking the study and deliberately did not undertake any clinical or teaching activities locally alongside this research. Whilst it was useful to know (from her own background) what the students were talking about medically (and in terms of detecting items of significance), as a researcher she made conscious efforts not to accept potentially common assumptions at face value.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-context-2",
    "href": "guidelines/srqr/index.html#sec-context-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis helps readers interpret the meaning and significance of the study findings by situating them in social, cultural, temporal and other relevant contexts."
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-context-3",
    "href": "guidelines/srqr/index.html#sec-context-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nWe conducted the study among hospital-based clinical teachers of students in years 4 to 6 of a six-year undergraduate medical program at Maastricht University Medical School. Years 4 to 6 are devoted to clerkships in the academic hospital and affiliated regional hospitals. Rotations differ in duration depending on the type of rotation and the discipline, and the sequence of rotations differs among students. During rotations, students spend time in the wards, the outpatient clinics, and the accident and emergency department. Clerkships in years 4 and 5 last between 4 and 10 weeks (regular clerkship), whereas students in year 6 undertake an 18-week senior clerkship in a discipline of their choice.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-sampling-strategy-2",
    "href": "guidelines/srqr/index.html#sec-sampling-strategy-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information helps readers understand the source of data / findings so they can consider the boundaries of the study and the relevance to their own context. It also gives readers insight into the researchers’ decisions, which can be important for critical appraisal."
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-sampling-strategy-3",
    "href": "guidelines/srqr/index.html#sec-sampling-strategy-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nAs students’ perceptions were previously shown to be related to gender, age, prior experience and place of attachment,[REF] we purposely selected respondents with different backgrounds. This sampling strategy led to the diversity of gender, age, prior patient experience and place of attachment shown in Table 1.\n\n\nPurposive sampling was directed towards achieving maximum variation in age and specialty, using a snowball approach (a non-probabilistic form of sampling in which persons initially chosen for the sample are used as informants to locate other persons having necessary characteristics making them eligible for the sample).[REF ]\n\n\nPotential participants were all medical students in Years 1 and 2 at the University of Toronto in 2004. Following research ethics board approval, recruitment was conducted via e-mail to class listservs. Participant responses were sent directly to the research assistant, who was unknown to participants, so that the principal investigators did not know who did or did not participate. This process was engaged to protect participants’ anonymity and to avoid any impression of coercion because the lead researcher (SG) was involved in the administration of the undergraduate curriculum at the time. Sample size was estimated to be sufficient based on the principle of theoretical saturation [REF] and our previous experience with this methodology (i.e., with a relatively homogeneous population, we expected to reach saturation with approximately 15 interviews per group). There were no exclusion criteria and we accepted the first 15 students from each class who volunteered.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-ethics-2",
    "href": "guidelines/srqr/index.html#sec-ethics-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nJournals will require an ethical approval statement.\nQualitative research often involves interaction between researchers and research participants. Correspondingly, researchers should ensure that participants are fully aware of their participation in a research study, the risks and benefits associated with the study, the steps and precautions the researchers will take to minimize risks, such as loss of privacy and confidentiality, and how the researchers plan to use the data."
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-ethics-3",
    "href": "guidelines/srqr/index.html#sec-ethics-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nEthical approval was granted by the University of Otago and student participants were invited to attend each focus group discussion by the university representatives of New Zealand Medical Students Association (NZMSA). Usually, a key concern when collecting data from students is that students may feel vulnerable when sharing their experiences with academic staff during a focus group discussion. However, this potential harm was removed as each group discussion was facilitated by a fellow student, the discussions were transcribed by a professional transcribing service, and only the primary researchers [Names] had access to the raw data.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-data-collection-methods-2",
    "href": "guidelines/srqr/index.html#sec-data-collection-methods-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThe study period helps readers place the study in temporal context and identify factors not mentioned by the authors that might affect findings, interpretation, and implications. (See Item 8 for ending data collection.)\n\nDescribing researcher characteristics clarifies the relationship between the individuals involved in data collection and the participants in the research and also explains what efforts were made to ensure consistency in the data collection process (See Items 6 and 15.)"
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-data-collection-methods-3",
    "href": "guidelines/srqr/index.html#sec-data-collection-methods-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nFurther, it was decided that group interviews, also known as focus group discussions would be the best means of data collection. This is a method of data collection that enables group members to feed off each other’s ideas and an effective moderator will maintain group focus whilst at the same time permitting flexibility in the direction those aspects of the discussion might take. [REF]\n\n\nAdjustments to the interview protocol were made according to early experience and information participants had provided (i.e. redundant questions were eliminated; questions were reworded to improve flow and clarity; additional probes were included).\n\n\nFaculty staff were then interviewed individually by a trained study investigator in a 15-minute, semi-structured interview. This sequence was repeated with other video encounters. Table 2 presents examples of interview questions. Each faculty member was interviewed by at least three interviewers over their various interviews. Interviewers were chosen based on their experience in interviewing. All were trained during a half-day meeting to interpret and deliver the interview guide in the same manner in order to elicit information of a consistent type.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-data-collection-instruments-2",
    "href": "guidelines/srqr/index.html#sec-data-collection-instruments-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nDescribing instruments and equipment helps readers understand the full context in which data collection occurred and how this context might have affected data collection (e.g., the influence of recording devices on participants’ behaviors; the nature of inferences drawn from live vs. recorded events)."
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-data-collection-instruments-3",
    "href": "guidelines/srqr/index.html#sec-data-collection-instruments-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nTo facilitate the discussion and to maintain consistency over different sets of discussions, key trigger questions were devised prior to the discussion. The opening trigger question was: Thinking back to some of your best clinical learning placements in 4th and 5th year. What was it about those clinical placements that provided good opportunities for learning?\n\n\nInterviews included discussion of the expectations, processes and consequences of AEE [authentic early experiences]. The interview schedule was derived following identification of questions that could not be fully answered in a systematic review of previous empirical or theoretical literature. It comprised a sequence of topic areas including experiences in action, and areas of frustration in Medical Education such as the learning of content knowledge, achieving functional knowledge, and transfer of knowledge.[REF]… Interested readers can request a copy of the schedules from the corresponding author. Interviews lasted between 20 and 90 min. and discussion groups between 60 and 90 min. All interviews and discussion groups were conducted in private rooms at the participant’s workplace—the medical school for students and faculty, and individual places of work for workplace supervisors (except for one who chose to be interviewed at the medical school). All data were audio-recorded, and transcribed verbatim.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-units-of-study-2",
    "href": "guidelines/srqr/index.html#sec-units-of-study-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "How does this item differ to Sampling (Item 8)?",
    "text": "How does this item differ to Sampling (Item 8)?\nThe sampling item (Item 8) describes the target or ideal participants, documents, or events selected for the study. By contrast, this item focuses on description of the actual participants, documents or events included in the study."
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-units-of-study-3",
    "href": "guidelines/srqr/index.html#sec-units-of-study-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis helps readers know whose experiences and perspective are and are not included."
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-units-of-study-4",
    "href": "guidelines/srqr/index.html#sec-units-of-study-4",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nOf the 70 Mindful Communication program participants, 46 met the eligibility requirements to participate in the in-depth interviews. We randomly chose and then contacted 22 participants, of whom 20 agreed to be interviewed within six months of completing the program: 15 in person and 5 by telephone. Two declined for lack of time. On reaching saturation after 20 interviews, no further attempts to contact the remaining 24 participants were made.\n\n\n\n\n\nThere were 31 nursing handovers covering 137 patients, and 21 resident handovers covering 101 patients included in this study.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-data-processing-2",
    "href": "guidelines/srqr/index.html#sec-data-processing-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript."
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-data-processing-3",
    "href": "guidelines/srqr/index.html#sec-data-processing-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nInterviews were anonymised and each participant was given a code number.\n\n\nThe interviewers and another member of the research team (H.B.) reviewed transcripts for accuracy.\n\n\nWe collected data throughout the admission process through direct observation, audio- recording, and chart extraction. We audio-recorded, transcribed, and anonymized both the overnight and morning case review discussions. We also observed the morning case review discussions in person and collected field notes. For each case review discussion, we copied the admission notes from the patient’s record and de-identified all data.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-data-analysis-2",
    "href": "guidelines/srqr/index.html#sec-data-analysis-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nTechniques used for data analysis will depend on the paradigm, approach, and/or data collection methods selected by the researchers. Correspondingly, authors should be as transparent as possible about the analytic process so that readers can follow the logic of inquiry from the research question(s) to the analysis and findings."
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-data-analysis-3",
    "href": "guidelines/srqr/index.html#sec-data-analysis-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\n…we brought sensitizing concepts to the analysis while we conducted an open, inductive analysis.[REF]In this case the sensitizing concepts arose, a priori to analysis, from a framework derived from the literature [REF] (as described above), in which participants’ motivations to act are based on principles of professionalism, internal affect, or potential implications of their actions.[REF]\n\n\nThrough an iterative process of listening, discussing, and relistening, the team identified and consensually validated emerging themes[REF] and appended segments of dialogue supporting the proposed themes. Recruitment stopped when saturation was reached (no new themes were identified). The team systematically reviewed the themes and sorted them into content domains. The team used an analytic matrix to identify patterns and connections amongst the domains. Two of us not involved in the qualitative coding process (R.E., M.K.) audited the analytic matrix, choice of quotes, and thematic analysis.\n\n\nThe analysis started after the first interview. All data were analyzed with the aid of the audio- coding facility of the NVivo 8: QSR International Pty Ltd, Doncaster, Vic, Australia programme. First, [name] and [name] coded independently from one another, making sure to stay semantically close to the participants’ wording. Then we discussed these open codes and defined axial codes.[REF] New insights about the impact of CST were written down in memos.\n\n\nVideotapes were analysed using immersion/crystallisation methods of qualitative data analysis.[REF] With no pre-existing framework developed in advance for analysis, an inductive approach was used to discover patterns of NVB in the data. A team of six researchers met weekly for 18 months to view videos together. Using a consensus-building approach based on a combination of field notes, opportunistic interviews with the participants, and repeated viewing of the same material, sometimes many months apart, we eventually achieved consensus on verbal, non-verbal, and physical themes and patterns observed in the data. Finally, as a test of goodness-of-fit, we carefully reviewed the videotapes for any deviant cases that did not fit the categories we had developed.\n\n\nAll transcripts were coded thematically by four of the five authors, who met regularly to identify areas of convergence until full agreement was reached. One of the interviewers (P.M.) maintained an audit trail to track the team’s developing thinking. A process of dialectical empiricism[REF] was used to categorise the emergent themes into more abstract concepts…\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-trustworthiness-2",
    "href": "guidelines/srqr/index.html#sec-trustworthiness-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript."
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-trustworthiness-3",
    "href": "guidelines/srqr/index.html#sec-trustworthiness-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\nMember checking\n\nMember checks [REF] with an external TBL expert (R.L.) supported the validity of these analyses.\n\nTriangulation of data types and data sources\n\nThe interview data were triangulated with the data of 11 student and supervisor focus groups of a previous study, and more specifically, with those data that concern in particular the influence of CST [Communication Skills Training] on the development of patient-centredness…. Triangulation with the focus group data allowed us to broaden the in-depth information from the interviews in the analysis and to share and compare this with information from students and doctors with varying levels of CST (no, limited, full programme) and from two universities (Universities of Antwerp and Ghent). Moreover, this enabled us to better explore the evolution over time, given that the focus groups included participants at different stages of their study: before clerkships (year 4, undergraduate), during clerkships (year 6, undergraduate), after clerkships (year 7, undergraduate) and postgraduate (general practice trainees, and supervising specialists and GPs; Table 1).\n\n\n\n\n\nFinally, as a test of goodness-of-fit, we carefully reviewed the videotapes for any deviant cases that did not fit the categories we had developed.\n\nTriangulation of Researchers + Audit trail\n\nTo ensure rigor and increase authenticity in our methodology, we used two kinds of triangulation—investigator triangulation and data triangulation.[REF] We sought analytical rigor using an audit trail and multiple coders; our coding team included an experienced clinician (M.G.) as well as a nonclinician with expertise in medical communication and team dynamics (L.L.)."
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-trustworthiness-4",
    "href": "guidelines/srqr/index.html#sec-trustworthiness-4",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Training and resources",
    "text": "Training and resources\nSee Lincoln and Guba’s Evaluative Criteria for trustworthiness.\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-synthesis-and-interpretation-2",
    "href": "guidelines/srqr/index.html#sec-synthesis-and-interpretation-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript."
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-synthesis-and-interpretation-3",
    "href": "guidelines/srqr/index.html#sec-synthesis-and-interpretation-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nWe identified four patterns of NVB (non-verbal behavior) that relate to handover quality and have dubbed them: (1) joint focus of attention; (2) the poker hand; (3) parallel play; and (4) kerbside consultation. Each pattern constitutes a transfix, or systematic way of participating non-verbally in the care transfer process. And, although there are variations in each pattern, we have been able to code virtually every handover we have observed in nursing, medicine and surgery into one of these four categories.\n\n\nBecause our participants came from similar educational backgrounds, had studied medicine as their tertiary course, were embedded in the culture of medicine, and were associated in meaningful ways with a single medical school, we approached their transcripts with the assumption that they belonged to a loosely formed discourse community. Although their graduation dates ranged over a period of 50 years and their collective sphere of practice included 10 different specialty areas, there were many similarities in their experiences of enculturation during and after medical school.\n\n\nTheir three major (often overlapping) areas of concern were epistemic (acquiring knowledge and skill), interpersonal (relating to patients, families, colleagues and administrators) and personal (achieving work–life balance). In each of these areas, medical enculturation was achieved by two overlapping processes, absorption and assimilation, each of which may have distinct implications for postgraduate medical education.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-links-to-empirical-data-2",
    "href": "guidelines/srqr/index.html#sec-links-to-empirical-data-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript."
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-links-to-empirical-data-3",
    "href": "guidelines/srqr/index.html#sec-links-to-empirical-data-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\nSee Frankel et al. for an excellent example of how to use photographs (or snapshots from video) to illustrate and provide supporting evidence for patterns of behavior identified in the analysis. http://qualitysafety.bmj.com/content/21/Suppl_1/i121.\n\nWe identified five interruption types: (1) probing for further data, (2) prompting for expected sequence, (3) teaching around the case, (4) thinking out loud, and (5) providing direction (see Table 1). Several interruption types served both goals of the case review discussions—teaching and patient care. For example, when thinking out loud, supervisors reasoned through problems and taught the team: So that’s the big question, did she have a mechanical fall, or did she have a medicine-related fall? (Case 2). Supervisors prompted for expected sequence, preventing presenters from skipping over information while simultaneously allowing the supervisor to instruct the team on presentation style: So now you can tell me what the rest of his test results are because I haven’t heard those (Case 16).\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-integration-with-prior-work-2",
    "href": "guidelines/srqr/index.html#sec-integration-with-prior-work-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThe short summary reminds readers of the main findings and may help them assess whether the subsequent interpretation and implications formulated by the authors are supported by the findings."
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-integration-with-prior-work-3",
    "href": "guidelines/srqr/index.html#sec-integration-with-prior-work-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nThis study contributes to the understanding and discussion of the complexity of involving patients in healthcare education. It shows that integrating patient-led teaching into initiatives that are partly faculty-led influences the way in which students perceive learning from and with PIs. What is not known, however, is whether perceptions are also affected by type of health profession and the students’ different orientation towards logics of care and science, and issues of authority and power relations.\n\nFor complete examples of Discussions, see:\n\nHenriksen & Ringsted, 2013\nWesterman et al., 2013.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-limitations-2",
    "href": "guidelines/srqr/index.html#sec-limitations-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nAll research has limitations. Discussing them will help readers consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be). If you don’t address limitations, editors and peer reviewers may ask you about them which will delay publication."
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-limitations-3",
    "href": "guidelines/srqr/index.html#sec-limitations-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nThe study has several limitations. One is that the focus group interview method reveals students’ perceptions rather than their actual behaviors. Observations of the patient-led teaching encounter may have illuminated an understanding of the relationship between patient instructors and medical students. Another limitation is that the PI-led teaching is optional rather than mandatory, which may have influenced students’ attitudes in a positive direction. Moreover, students who are eager to take on extra-curricular activities may not be representative of the whole population. That only 23 out of 39 students signed up for this study might also have influenced results if the missing group of students represented other perceptions than those present in the focus groups. However the received data from the focus groups were rich in information and diverse perceptions were present. Another limitation is the overrepresentation of women over men in our sample. Even though women are also overrepresented in medical school this might potentially have influenced results, but gender differences in perceptions were nevertheless not identified in the data.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/index.html#sec-conflicts-of-interest-2",
    "href": "guidelines/srqr/index.html#sec-conflicts-of-interest-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nMany factors, including professional and personal relationships and activities, can influence the design, conduct, and reporting of the clinical science that informs health care decision. The potential for conflict of interest exists when these relationships and activities may bias judgment (1). Many stakeholders— editors, peer reviewers, clinicians, educators, policymakers, patients, and the public—rely on the disclosure of authors’ relationships and activities to inform their assessments. Trust in the transparency, consistency, and completeness of these disclosures is essential. - ICMJE"
  },
  {
    "objectID": "guidelines/srqr/index.html#ready-to-get-started",
    "href": "guidelines/srqr/index.html#ready-to-get-started",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Ready to get started?",
    "text": "Ready to get started?\nDownload resources"
  },
  {
    "objectID": "guidelines/srqr/index.html#how-to-cite",
    "href": "guidelines/srqr/index.html#how-to-cite",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "How to cite",
    "text": "How to cite\nFor attribution, please cite this guideline as:\nBridget O’Brien, Ilene Harris, Thomas Beckman, Darcy Reed, and David Cook. 2023. “The SRQR Guidelines for Writing Qualitative Research Articles version 1.1.” The EQUATOR Network Guideline Dissemination Platform. https://doi.org/10.1234/equator/1010101.\nYou can use your reference manager to save citation information for this webpage, or copy the BibTeX below."
  },
  {
    "objectID": "guidelines/srqr/index.html#faqs",
    "href": "guidelines/srqr/index.html#faqs",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "FAQs",
    "text": "FAQs\n\nWho made this guideline?\n\nBridget O’Brien, PhD has been a faculty member in the Department of Medicine, Division of General Internal Medicine, since 2008. She is a professor of medicine and an education scientist in the Office of Medical Education’s Center for Faculty Educators. As co-director of the Teaching Scholars Program and the UCSF-University of Utrecht Health Professions Education doctoral program she teaches and mentors faculty and learners interested in education research and scholarship. At the San Francisco VA, she directs the Advanced Fellowship in Health Professions Education Evaluation and Research. In 2015 she was selected as one of five national Macy Faculty Scholars supported by the Josiah Macy Jr. Foundation and in 2021 she was selected as a KIPRIME Fellow at the Karolinska Instituet. She is a deputy editor for the journal Academic Medicine.\nDr. Ilene Harris, deceased, was professor and head, Department of Medical Education, University of Illinois at Chicago College of Medicine, Chicago, Illinois.\nDr. Thomas Beckman is professor of medicine and medical education, Department of Medicine, Mayo Clinic College of Medicine, Rochester, Minnesota.\nDr. Darcy Reed is associate professor of medicine and medical education, Department of Medicine, Mayo Clinic College of Medicine, Rochester, Minnesota.\nDr. David Cook is associate director, Mayo Clinic Online Learning, research chair, Mayo Multidisciplinary Simulation Center, and professor of medicine and medical education, Mayo Clinic College of Medicine, Rochester, Minnesota.\n\n\nHow was this guideline made?\nThe developers synthesised 40 sets of recommendations previously proposed by experts in qualitative methods. You can read about their development process here.\n\n\nWhat to do if asked to remove guideline related content\nIf a colleague or reviewer asks you to remove content that is related to this guideline, you can direct them to this guideline and the explanation for why that item is important. If they insist, consider moving the item to a supplement, table or figure.\n\n\nWhere can I get general writing training?\nThe EQUATOR Network provides in-person training for writing research articles.\nAuthorAID have resources, an online course, and mentoring to help authors.\n\n\n\nResearch paradigm\n\n\n\n\n\n\nThe set of beliefs and assumptions that guide the research process. These commonly include positivist, post-positivist, constructivist or interpretivist, and critical theory. Qualitative research generally draws from a post-positivist or constructivist/interpretivist paradigm.”\n\n\n\n\n\n\nInstruments\n\n\n\n\n\n\nData collection instruments include (but are not limited to) interview or focus group guides, observational protocols and prompts for field notes, and data extraction or coding protocols for selection and analysis of documents, photographs, videos, or other artifacts”\n\n\n\n\n\n\nBias\n\n\n\n\n\n\nA term drawn from quantitative research, bias technically means a systematic error, where a particular research finding deviates from a true finding. This might come about through errors in the manner of interviewing, or by errors in sampling. In qualitative research this is a problematic concept, since by definition the qualitative researcher is part of the process, and all researchers are different. This human factor has been said to be both the greatest strength and the greatest weakness of qualitative method. What can be done in commercial qualitative research, however, is to minimise obvious and avoidable sources of bias, for example by not confining all the fieldwork to one social group or geographic location, by taking steps to recognise the personal views of the researcher, (using techniques such as bracketing), and by working in teams.\n\n\n\n\n\n\nSampling strategy\n\n\n\n\n\n\nSeveral sampling strategies are commonly used in qualitative research, although most fall under the umbrella of purposeful (or purposive) sampling.\nPurposeful sampling means that participants, documents, or events are selected for their relevance to the research question, based on guiding theory or experiences and assumptions of the researchers. Over the course of the research process, researchers may determine that additional or different participants, documents, or events should be included to address the research question.\nOther sampling techniques, such as theoretical sampling (seeking examples of theoretical constructs), snowball sampling (using study participants to identify additional participants who meet study criteria), and convenience sampling (including any volunteers with no or minimal criteria for inclusion) may be appropriate depending on the question and approach, so long as the authors provide explanation and justification.\n\n\n\n\n\n\nApproach\n\n\n\n\n\n\nA qualitative approach is a general way of thinking about conducting qualitative research. It describes, either explicitly or implicitly, the purpose of the qualitative research, the role of the researcher(s), the stages of research, and the method of data analysis. Commonly used approaches include ethnography, grounded theory, case study, phenomenology, and narrative research.\n\n\n\n\n\n\nData collection methods\n\n\n\n\n\n\nData collection methods include (but are not limited to) interviews, focus groups, observations (direct or indirect via video), and review of written text, photographs, and other documents or materials.\n\n\n\n\n\n\nIterative\n\n\n\n\n\n\nQualitative research often occurs as an iterative process, meaning that researchers begin data analysis before they complete data collection. The data collection and analysis process may occur in phases or stages. As part of an iterative collection-analysis process, researchers will often alter their data collection methods to explore their preliminary impressions in greater depth and/or actively pursue confirming and disconfirming perspectives.\n\n\n\n\n\n\nStudy period\n\n\n\n\n\n\nThe start and end dates for data collection and analysis.\n\n\n\n\n\n\nEthnography\n\n\n\n\n\n\nThe scientific description of peoples and cultures with their customs, habits, and mutual differences.\nRead more\n\n\n\n\n\n\nGrounded theory\n\n\n\n\n\n\nA method consisting of a set of systematic, but flexible, guidelines for conducting inductive qualitative inquiry aimed toward theory construction. This method focuses squarely on the analytic phases of research, although both data collection and analysis inform and shape each other and are conducted in tandem.\nRead more\n\n\n\n\n\n\nDegree of participation\n\n\n\n\n\n\nFor example, if some participants were observed and interviewed and others only interviewed, or if some participants completed multiple interviews and others completed a single interview, these variations should be explained.\n\n\n\n\n\n\nUnit of analysis\n\n\n\n\n\n\nIn qualitative research, the unit of analysis is not necessarily the same as the unit of sampling (e.g., individual participants or events). Instead, some approaches use specific events as the unit of analysis, such as mentions of a particular topic or experience, or observations of a particular behavior or phenomenon, while others use groups rather than individual group participants. This specification has implications for how the data are organized and analyzed as well as the inferences drawn from the data.\n\n\n\n\n\n\nReflexivity\n\n\n\n\n\n\nReflexivity refers to intentional, systematic consideration of the potential or actual effects of the researcher(s) on all aspects of the study process.\n\n\n\n\n\n\nTransferability\n\n\n\n\n\n\nThe transferability of a research finding is the extent to which it can be applied in other contexts and studies. It is thus equivalent to or a replacement for the terms generalizability and external validity.\n\n\n\n\n\n\nGeneralizability\n\n\n\n\n\n\nThe appropriate scope for generalization of the findings beyond the study (e.g., to other settings, populations, time periods, circumstances)\n\n\n\n\n\n\nAnalytic findings\n\n\n\n\n\n\nAnalytic findings may include interpretations, inferences, narratives, themes, and models.\n\n\n\n\n\n\nFrequency counts\n\n\n\n\n\n\nThe frequency of specific themes or codes."
  },
  {
    "objectID": "guidelines/srqr/qsca.html",
    "href": "guidelines/srqr/qsca.html",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "",
    "text": "Full Title: Standards for Reporting Qualitative Research\nAuthors: Bridget O’Brien, Ilene Harris, Thomas Beckman, Darcy Reed, David Cook\nVersion: 1.1\nDOI: 10.1234/equator/1010101"
  },
  {
    "objectID": "guidelines/srqr/qsca.html#translations",
    "href": "guidelines/srqr/qsca.html#translations",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Translations",
    "text": "Translations\nThis guideline is also available in French."
  },
  {
    "objectID": "guidelines/srqr/qsca.html#about-this-guideline",
    "href": "guidelines/srqr/qsca.html#about-this-guideline",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "About this guideline",
    "text": "About this guideline\n\nWhen to use this guideline\n\nUse this guideline for writing qualitative research articles. You can use it when describing all kinds of qualitative approaches, methods, and designs.\n\n\nYou can also use this guideline for:\n\nwriting proposals or protocols (use the items within the Introduction and Method sections).\nreviewing the reporting of an article, but not for appraising its quality.\n\n\n\n\n\n\n\n\n\nDo not use this guidance for:\n\nwriting a qualitative evidence synthesis, use ENTREQ instead.\nappraising the quality of qualitative research, use an appraisal tool like CASP-Qual instead. \n\n\n\n\n\n\n\n\n\n\nRelated reporting guidelines:\n\nJARS Qual for writing qualitative, psychology manuscripts\nENTREQ for writing qualitative evidence syntheses\nFor writing studies involving interviews or focus groups, you can use this guideline or COREQ. \n\nFor appraising research consider:\n\nCASP-Qual\n\n\n\n\n\n\nHow to use this guideline\nReporting guidelines help me structure my drafts and develop as a researcher. I use them when teaching and hope that my students continue to use them. Manuel Silva - Researcher\nReporting guidelines are best used early. Read the full guideline below or use a tool when working:\n\nUse a template to draft manuscripts and avoid the blank page \nUse a log book to plan and record information whilst conducting research\nUse a checklist to double-check and reassure others that your article describes all the important details of your work\n\n\n\n\nFor drafting\n\nTemplate\n\n\n\nFor planning\n\nLog book\n\n\n\nFor checking\n\nChecklist\n\n\nThis reporting guideline does not prescribe a rigid format or standardized content. Consider each item and prioritize elements that are most relevant to your study, findings, context, and readers.\nYou may prefer to report an item in a different order, section, or in a table or figure. If you feel confident that an item is less important to your study, you could report it in an appendix or supplement. If you think an item is not applicable, state why. You don’t need to write your article in the order the items are presented.\nIn your methods section, state that you used the SRQR guideline when writing your article and cite it (see How to cite).\n\n\nWhy use this guideline\nFollowing this guideline will make it easier for other researchers to find, use, and synthesise your work. It won’t tell you how to do qualitative research, but it will help you explain your choices so others can understand what you did, why you did it, and what you found. At our journal, we ensure all qualitative articles follow SRQR as this makes them easier for readers to understand and faster to publish.Fiona Ludlow - Editor in Chief\nIt was written by experts from the qualitative research community, using a thorough development process. SRQR is endorsed by hundreds of journals, so using this guideline (and its checklist) will help you meet editors’ expectations."
  },
  {
    "objectID": "guidelines/srqr/qsca.html#guidance",
    "href": "guidelines/srqr/qsca.html#guidance",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Guidance",
    "text": "Guidance\nApprox. 17 min read\n\n\n1. Title\n\n\n\n\n\n\n\nDescribe the nature and topic of the study.\nIdentify the study as qualitative or indicate the approach or data collection methods.\n\n\n\n\n\n\nJustification, Examples, and Resources\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\nTraining and Resources\n\n\nWhy readers need this information\nThis allows readers to quickly identify the type of study. Your title will be indexed by search tools so descriptive words will make it easier for others to discover.\n\n\nExamples\n\nResidents learning from a narrative experience with dying patients: a qualitative study.\n\n\nMedical students’ perceptions of the factors influencing their academic performance: an exploratory interview study with high-achieving and re-sitting medical students.\n\n\nUndergraduate rural medical education program development: focus group consultation with the NRHA Rural Medical Educators Group.\n\n\n\n\nTraining and Resources\nSee this article for advice on writing titles that are interesting and descriptive.\nThe EQUATOR Network provides training for writing effective manuscript titles as part of their publication school.\nBack to top\n\n\n\n\n\n\n2. Abstract\n\n\n\n\n\n\n\nInformative key words in the title and abstract make manuscripts easier to find.Lorna Ellis - Evidence Synthesiser\nSummarise key elements of the study including:\n\nbackground about the problem or phenomenon of interest\ndescription of the study purpose or research question\nmethods, including the approach or perspective (e.g., general inductive, grounded theory), context (setting, time period), sample (number and key characteristics of participants, events, documents), data collection strategies (e g., observation, interview, focus group) and data analysis techniques\ndescription of main findings (e.g., themes or inferences) related to the study purpose and/or research question\nstudy implications\n\n\nInformation presented in the abstract should be consistent with the information presented in the full text.\nUse the format of your intended journal. 1\n\n\n\n\n\n\nJustification, Example, and Resources\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExample\nTraining and Resources\n\n\nWhy readers need this information\nA reader should be able to read the abstract independent of the manuscript and get a sense of the background, purpose, methods, main results and implications that will be described in greater detail in the manuscript.\nYour abstract will be indexed by search tools so descriptive words will make it easier for others to discover.\n\n\n\nExample\n\nPurpose: Although academic centers rely on assessments from medical trainees regarding the effectiveness of their faculty as teachers, little is known about how trainees conceptualize and approach their role as assessors of their clinical supervisorsMethod: In 2010, using a constructivist grounded theory approach, five focus group interviews were conducted with 19 residents from an internal medicine residency program. A constant comparative analysis of emergent themes was conducted.Results: Residents viewed clinical teaching assessment (CTA) as a time-consuming task with little reward. They reported struggling throughout the academic year to meet their CTA obligations and described several shortcut strategies they used to reduce their burden. Rather than conceptualizing their assessments as a conduit for both formative and summative feedback, residents perceived CTA as useful for the surveillance of clinical supervisors at the extremes of the spectrum of teaching effectiveness. They put the most effort, including the crafting of written comments, into the CTAs of these outliers. Trainees desired greater transparency in the CTA process and were sceptical regarding the anonymity and perceived validity of their faculty appraisals.Conclusions: Individual and system-based factors conspire to influence postgraduate medical trainees’ motivation for generating high-quality appraisals of clinical teaching. Academic centers need to address these factors if they want to maximize the usefulness of these assessments.\n\n\n\n\nTraining and Resources\nThe EQUATOR Network provides training for writing effective manuscript abstracts as part of their publication school.\nSee this article from the London School of Economics for general abstract writing tips.\nBack to top\n\n\n\n\n\nIntroduction\n\n\n3. Problem Formulation\n\n\n\n\n\n\n\nWhen the problem is described clearly I find it much easier to understand why a study was done, even if its from a different subject area, country, or decade.Nkinda Akaro - Researcher\nDescribe the theoretical and/or practical issues or concerns that make the study necessary, including:\n\nan overview of what is known about the problem\ngaps in current knowledge (the problem statement)\nthe scope of the research problem or phenomena addressed in the study (what will and will not be included)\ntheoretical and/or empirical work directly relevant to the problem or phenomena studied\nthe need for a qualitative approach.2\n\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExample\n\n\nWhy readers need this information\nThis information provides context for the research question or study purpose and situates the study in relation to other work in the field.\n\n\nExample\n\nRegulatory focus theory may therefore offer insight into the variability in responses to feedback, but how well do these experimental findings translate to real clinical situations in which the reality of responsiveness to feedback seems frustratingly complex? …. In order to better elaborate a theory to account for this variability in learner response to feedback, the present study was undertaken. We aimed, in this study, to determine how readily clinical learning events could be classified as activating a promotion or a prevention focus, and to explore, through a careful analysis of doctors’ descriptions of their feedback experiences, the predictive value of regulatory focus theory in the context of real clinical learning situations.\n\n\n\nBack to top\n\n\n\n\n\n\n4. Purpose or research question\n\n\n\n\n\n\n\nA clear purpose frames the article. Rahilah Zayn - Researcher\nInclude a statement of study intent. This can be framed as one or more research questions, purposes, goals, or objectives.3\n\n\n\n\n\n\nJustification, Example, and Resources\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExample\nTraining and Resources\n\n\nWhy readers need this information\nBy clearly stating the purpose of the study, authors set readers’ expectations for the methods, findings and discussion sections of the manuscript.\n\n\nExample\n\nThe purposes of this study were to investigate how medical students recognize, respond to and utilise feedback, and to determine whether there are maturational differences in understandings of the role of feedback across academic years in medical school.\n\n\n\nTraining and Resources\nSee this article for advice on writing qualitative research questions.\nBack to top\n\n\n\n\n\n\nMethods\n\n\n5. Qualitative approach and research paradigm\n\n\n\n\n\n\n\nDescribe your qualitative approach, your guiding theory (if appropriate), and identify the research paradigm.4 I like it when authors describe terms in their own words as well as using the proper terms.Rahilah Zayn - Researcher\nExplain why the selected approach is appropriate for the research question.\nProvide references to theories or traditions that guide the use of the approach as needed.\nI hadn’t heard the term approach before writing-up my study. I realise now that I did have an approach and that it might be different to other researcher’s approaches, so I need to describe things that I felt were obvious.Tim Westland - Researcher\n\nIf you don’t know what your approach or paradigm was, or you don’t think you had one, it’s OK to reflect on this after collecting data and you should still report it. Read this list to see what best describes your work.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nIdentifying the research paradigm helps readers understand whether the researcher assumes that there is a single, objective reality (positivist or post-positivist) and has thus designed the study to describe this reality or whether the researcher assumes multiple, subjective realities and designed the study to describe these multiple realities, with no attempt to merge or reconcile these realities (constructivist/interpretivist). The paradigm has implications for the study design, approach, methods, and techniques to ensure rigour and trustworthiness.\nSince the research paradigm does not necessarily dictate particular approaches or methods, the approach should also be clearly defined. Stating the approach provides readers the opportunity to evaluate the fidelity of the research approach to the research question(s) and consider the rationale for modifications and deviations from the selected approach. Qualitative research also includes an array of methods that can be used across paradigms and approaches. (See also Item 10).\n\n\nExamples\n\nThe study was performed from a constructivist point of view using an interpretative phenomenological epistemology. Based on the notion that social phenomena are constructed by the communal making of meaning about the underlying phenomena, we aimed to construct insightful accounts of lead consultants’ approaches to educational change, rather than to identify the true nature of these approaches. Because the management of change by lead consultants is an under-researched area, we conducted an exploratory qualitative study…\n\n\nGiven the relative dearth of explanatory theories about factors affecting medical students’ emotional reactions, we chose to develop one by applying methods associated with grounded theory, specifically constant comparative analysis, to qualitative data obtained from learning logs and interviews. Our approach was constructivist, deliberately using researchers’ own experiences and acquired knowledge to enhance theoretical sensitivity and enrich theory development.\n\nBack to top\n\n\n\n\n\n\n6. Researcher characteristics and reflexivity\n\n\n\n\n\n\n\nDescribe how roles and identities of research team members influence choice of research approach, data collection, and data analysis.5\nComing from quantitative research it felt strange to include such personal descriptions of the team. But I came to realise why it was useful to others, and that peer reviewers would expect it. Tim Westland - Researcher\nDescribe the perspectives, assumptions, prior knowledge, preliminary hypotheses, and/or motives (the stance) of the members of the research team.\nDescribe the researchers’ relationships to participants in the study and what decisions were made in light of these relationships.6\nIf your research was observational (e.g., ethnography), describe the role of the researcher along a spectrum from passive observer (no involvement in the activity studied) to participant-observer (ranging from some limited involvement in the activity to full involvement).\nThere is no expectation that the study could be precisely replicated; these characteristics and perspectives of the researcher should not be mentioned in the limitation section. (See also Item 14: Data Analysis)\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nIn positivist and post-positivist paradigms, personal characteristics and perspectives of researchers might be viewed as biases that limit the credibility of study findings, while in constructivist or interpretivist paradigms the characteristics and perspectives of the researchers are important contextual factors that are an accepted part of the study design, data collection, and data analysis. These characteristics and perspectives may explain how the researcher(s) obtained access to the site or participants included in the study or may add valuable insight during data analysis.\n\n\nExamples\n\nReflexivity was maintained by the research team through the analysis and writing by recording, discussing and challenging established assumptions. In addition both EH and SV kept reflexive diaries.\n\n\nThe first author conducted all interviews and discussion groups. Her own medical undergraduate training took place between 1995 and 2000. She was not known to the participants of this research prior to undertaking the study and deliberately did not undertake any clinical or teaching activities locally alongside this research. Whilst it was useful to know (from her own background) what the students were talking about medically (and in terms of detecting items of significance), as a researcher she made conscious efforts not to accept potentially common assumptions at face value.\n\nBack to top\n\n\n\n\n\n\n7. Context\n\n\n\n\n\n\n\nDescribe the setting/site(s) in which the study was conducted, the reason(s) why the setting/site(s) was selected, and the salient cultural, political, historical, economic and/or other external factors that influence the study.\nWhen synthesising evidence, context helps me work out how and why different studies and findings fit together (or don’t)Lorna Ellis - Evidence Synthesiser\nAdditional context may be reported with findings (i.e., the Results section) to add evidence for interpretations and to enhance discussion of transferability.\n\n\n\n\n\n\nJustification and Example\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExample\n\n\nWhy readers need this information\nThis helps readers interpret the meaning and significance of the study findings by situating them in social, cultural, temporal and other relevant contexts.\n\n\nExample\n\nWe conducted the study among hospital-based clinical teachers of students in years 4 to 6 of a six-year undergraduate medical program at Maastricht University Medical School. Years 4 to 6 are devoted to clerkships in the academic hospital and affiliated regional hospitals. Rotations differ in duration depending on the type of rotation and the discipline, and the sequence of rotations differs among students. During rotations, students spend time in the wards, the outpatient clinics, and the accident and emergency department. Clerkships in years 4 and 5 last between 4 and 10 weeks (regular clerkship), whereas students in year 6 undertake an 18-week senior clerkship in a discipline of their choice.\n\nBack to top\n\n\n\n\n\n\n8. Sampling strategy\n\n\n\n\n\n\n\nDescribe how and why research participants, sites, documents/artifacts, and/or events were selected for inclusion in (and, if appropriate, exclusion from) the study, along with a justification for this strategy.\nDescribe the sampling strategy rather than simply labeling it (e.g., purposive or snowball), since such labels do not have a universally accepted definition and, more importantly, procedures tend to be study- specific.\n\nDescribe how you established the final sample size:\n\nIf you used a flexible sampling strategy, then explain the criteria used to decide when no further sampling was necessary.\nIf data collection ended once saturation or sufficiency had been reached, then describe the specific criteria used to define saturation or sufficiency.\n\n\n\nDescribe procedures used to recruit participants, including:\n\nwho was involved in recruitment\nwhat their relationship was to participants\nhow and when recruitment occurred\nwhy these procedures were selected. (See also Item 6: Researcher characteristics and reflexivity)\n\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThis information helps readers understand the source of data / findings so they can consider the boundaries of the study and the relevance to their own context. It also gives readers insight into the researchers’ decisions, which can be important for critical appraisal.\n\n\nExamples\n\nAs students’ perceptions were previously shown to be related to gender, age, prior experience and place of attachment,[REF] we purposely selected respondents with different backgrounds. This sampling strategy led to the diversity of gender, age, prior patient experience and place of attachment shown in Table 1.\n\n\nPurposive sampling was directed towards achieving maximum variation in age and specialty, using a snowball approach (a non-probabilistic form of sampling in which persons initially chosen for the sample are used as informants to locate other persons having necessary characteristics making them eligible for the sample).[REF ]\n\n\nPotential participants were all medical students in Years 1 and 2 at the University of Toronto in 2004. Following research ethics board approval, recruitment was conducted via e-mail to class listservs. Participant responses were sent directly to the research assistant, who was unknown to participants, so that the principal investigators did not know who did or did not participate. This process was engaged to protect participants’ anonymity and to avoid any impression of coercion because the lead researcher (SG) was involved in the administration of the undergraduate curriculum at the time. Sample size was estimated to be sufficient based on the principle of theoretical saturation [REF] and our previous experience with this methodology (i.e., with a relatively homogeneous population, we expected to reach saturation with approximately 15 interviews per group). There were no exclusion criteria and we accepted the first 15 students from each class who volunteered.\n\nBack to top\n\n\n\n\n\n\n9. Ethical issues pertaining to human subjects\n\n\n\n\n\n\n\nReport approval for the study from an appropriate institutional review board for research associated with human subjects.\nIf you did not receive ethical approval, describe why.\nDescribe procedures used to protect participants, including:\n\ndata collection (e.g., recruitment and informed consent)\nanalysis (e.g., data security and integrity)\nand reporting of findings (e.g., anonymization of excerpts).\n\nIf you provided compensation or offered incentives to facilitate participation, describe this too.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nJournals will require an ethical approval statement.\nQualitative research often involves interaction between researchers and research participants. Correspondingly, researchers should ensure that participants are fully aware of their participation in a research study, the risks and benefits associated with the study, the steps and precautions the researchers will take to minimize risks, such as loss of privacy and confidentiality, and how the researchers plan to use the data.\n\n\n\nExamples\n\nEthical approval was granted by the University of Otago and student participants were invited to attend each focus group discussion by the university representatives of New Zealand Medical Students Association (NZMSA). Usually, a key concern when collecting data from students is that students may feel vulnerable when sharing their experiences with academic staff during a focus group discussion. However, this potential harm was removed as each group discussion was facilitated by a fellow student, the discussions were transcribed by a professional transcribing service, and only the primary researchers [Names] had access to the raw data.\n\nBack to top\n\n\n\n\n\n\n10. Data collection methods\n\n\n\n\n\n\n\nDescribe data collection methods and design in detail, and justify them in relation to the research question(s), paradigm, approach, and other methods.7\n\nIf data collection and analysis was iterative:\n\ndescribe the iterative process\nif changes occurred during the research process, describe how and why study procedures changed in response to evolving study findings.\n\nIdentify the study period.\nDescribe important characteristics of the individuals conducting interviews, observations or focus groups, and methods used to train these individuals.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThe study period helps readers place the study in temporal context and identify factors not mentioned by the authors that might affect findings, interpretation, and implications. (See Item 8 for ending data collection.)\n\nDescribing researcher characteristics clarifies the relationship between the individuals involved in data collection and the participants in the research and also explains what efforts were made to ensure consistency in the data collection process (See Items 6 and 15.)\n\n\n\n\nExamples\n\nFurther, it was decided that group interviews, also known as focus group discussions would be the best means of data collection. This is a method of data collection that enables group members to feed off each other’s ideas and an effective moderator will maintain group focus whilst at the same time permitting flexibility in the direction those aspects of the discussion might take. [REF]\n\n\nAdjustments to the interview protocol were made according to early experience and information participants had provided (i.e. redundant questions were eliminated; questions were reworded to improve flow and clarity; additional probes were included).\n\n\nFaculty staff were then interviewed individually by a trained study investigator in a 15-minute, semi-structured interview. This sequence was repeated with other video encounters. Table 2 presents examples of interview questions. Each faculty member was interviewed by at least three interviewers over their various interviews. Interviewers were chosen based on their experience in interviewing. All were trained during a half-day meeting to interpret and deliver the interview guide in the same manner in order to elicit information of a consistent type.\n\nBack to top\n\n\n\n\n\n\n11. Data collection instruments and technologies\n\n\n\n\n\n\n\nDescribe all data collection instruments, including their development, and if/how they changed over the course of the study. Cite relevant literatures, theories or conceptual frameworks as appropriate. Consider sharing the data collection instrument(s) or a detailed description of them in the article body, supplementary material, or published elsewhere. I publish my materials on the OSF and then describe and cite them in my articles.Tim Westland - Researcher\nDescribe the use of equipment for audio or video recording, reproduction of paper documents or computer files, or other processes in data collection.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nDescribing instruments and equipment helps readers understand the full context in which data collection occurred and how this context might have affected data collection (e.g., the influence of recording devices on participants’ behaviors; the nature of inferences drawn from live vs. recorded events).\n\n\nExamples\n\nTo facilitate the discussion and to maintain consistency over different sets of discussions, key trigger questions were devised prior to the discussion. The opening trigger question was: Thinking back to some of your best clinical learning placements in 4th and 5th year. What was it about those clinical placements that provided good opportunities for learning?\n\n\nInterviews included discussion of the expectations, processes and consequences of AEE [authentic early experiences]. The interview schedule was derived following identification of questions that could not be fully answered in a systematic review of previous empirical or theoretical literature. It comprised a sequence of topic areas including experiences in action, and areas of frustration in Medical Education such as the learning of content knowledge, achieving functional knowledge, and transfer of knowledge.[REF]… Interested readers can request a copy of the schedules from the corresponding author. Interviews lasted between 20 and 90 min. and discussion groups between 60 and 90 min. All interviews and discussion groups were conducted in private rooms at the participant’s workplace—the medical school for students and faculty, and individual places of work for workplace supervisors (except for one who chose to be interviewed at the medical school). All data were audio-recorded, and transcribed verbatim.\n\nBack to top\n\n\n\n\n\n\n12. Units of study\n\n\n\n\n\n\n\nDescribe the number of participants, documents, or events included in the study (the units of study).\nDescribe characteristics of the participants, documents or events that are relevant to the study purpose and research question(s).8\nInclude the dates or timeframe for participation.\nIf the actual sample differs from the target sample, describe:\n\nthe difference,\nwhy these differences may have occurred,\nhow this might affect the findings.\n\nIf the degree of participation varied among individuals, describe:\n\nthe different levels of participation,\nthe reasons for these differences (i.e., the researchers’ choice or the participants’ preferences),\nand how these different levels of participation were taken into account in the analysis.\n\nThis information could appear in the Methods section as part of the description of the sample, or at the beginning of the Results section to provide context for the findings presented.\n\n\n\n\n\n\nMore Information, Justification, and Example\n\n\n\n\n\nJump to:\n\nHow does this item differ to Sampling (Item 8)?\nWhy readers need this information\nExample\n\n\nHow does this item differ to Sampling (Item 8)?\nThe sampling item (Item 8) describes the target or ideal participants, documents, or events selected for the study. By contrast, this item focuses on description of the actual participants, documents or events included in the study.\n\n\nWhy readers need this information\nThis helps readers know whose experiences and perspective are and are not included.\n\n\n\nExample\n\nOf the 70 Mindful Communication program participants, 46 met the eligibility requirements to participate in the in-depth interviews. We randomly chose and then contacted 22 participants, of whom 20 agreed to be interviewed within six months of completing the program: 15 in person and 5 by telephone. Two declined for lack of time. On reaching saturation after 20 interviews, no further attempts to contact the remaining 24 participants were made.\n\n\n\n\n\nThere were 31 nursing handovers covering 137 patients, and 21 resident handovers covering 101 patients included in this study.\n\nBack to top\n\n\n\n\n\n\n13. Data processing\n\n\n\n\n\n\n\nDescribe the ways in which data are prepared for analysis and managed throughout the analysis process. These activities might include transcription, coding, data entry, and organization of data. (See footnote for audio or visual recordings9).\nDescribe the processes used to organize, compile, and categorize data (e.g., field notes, transcripts, documents, photographs, artifacts) for analysis.\nIf you used transcripts, describe procedures used to check accuracy.\nDescribe procedures used to maintain data security and protect the privacy of participants, as specified in the human subjects protocols (see footnote on anonymisation10 as an example).\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript.\n\n\n\nExamples\n\nInterviews were anonymised and each participant was given a code number.\n\n\nThe interviewers and another member of the research team (H.B.) reviewed transcripts for accuracy.\n\n\nWe collected data throughout the admission process through direct observation, audio- recording, and chart extraction. We audio-recorded, transcribed, and anonymized both the overnight and morning case review discussions. We also observed the morning case review discussions in person and collected field notes. For each case review discussion, we copied the admission notes from the patient’s record and de-identified all data.\n\nBack to top\n\n\n\n\n\n\n14. Data analysis\n\n\n\n\n\n\n\nDescribe your analytic process as transparently as possible.11\nIf you used an approach that has a well-defined process for data analysis (e.g., grounded theory, discourse analysis, phenomenography):\n\ncite the guiding literature\ndescribe your processes in sufficient detail so readers can judge the extent to which your processes align with the guiding approach.\nIf you modified or deviated from the guiding approach, explain and justify these modifications.\n\nSpecify the unit of analysis.12\nExplain the rationale underlying different decisions made throughout the data analysis process to provide as much transparency as possible.13\nIf observations that contrast or deviate from identified concepts or themes were important to your analysis, describe how these discrepancies were handled during the analysis.\nIf you drew upon a theoretical perspective or framework during analysis, describe theoretical or other influences on your analysis scheme or categories if they exist. If you identified a theoretical perspective or framework early in the conception of the study or after reviewing some or all of their data, consider referring to these as sensitizing concepts to acknowledge that the approach is inductive, but with influence from relevant theory, models, or organizational schemes. Alternatively, explain that themes were developed from the data with no external influences.\nDescribe which members of the research team are involved in data analysis and what perspective(s) they bring to the analysis.\nIf software was used to assist with data analysis14, describe how it was used. Simply stating that software was used is insufficient.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nTechniques used for data analysis will depend on the paradigm, approach, and/or data collection methods selected by the researchers. Correspondingly, authors should be as transparent as possible about the analytic process so that readers can follow the logic of inquiry from the research question(s) to the analysis and findings.\n\n\nExamples\n\n…we brought sensitizing concepts to the analysis while we conducted an open, inductive analysis.[REF]In this case the sensitizing concepts arose, a priori to analysis, from a framework derived from the literature [REF] (as described above), in which participants’ motivations to act are based on principles of professionalism, internal affect, or potential implications of their actions.[REF]\n\n\nThrough an iterative process of listening, discussing, and relistening, the team identified and consensually validated emerging themes[REF] and appended segments of dialogue supporting the proposed themes. Recruitment stopped when saturation was reached (no new themes were identified). The team systematically reviewed the themes and sorted them into content domains. The team used an analytic matrix to identify patterns and connections amongst the domains. Two of us not involved in the qualitative coding process (R.E., M.K.) audited the analytic matrix, choice of quotes, and thematic analysis.\n\n\nThe analysis started after the first interview. All data were analyzed with the aid of the audio- coding facility of the NVivo 8: QSR International Pty Ltd, Doncaster, Vic, Australia programme. First, [name] and [name] coded independently from one another, making sure to stay semantically close to the participants’ wording. Then we discussed these open codes and defined axial codes.[REF] New insights about the impact of CST were written down in memos.\n\n\nVideotapes were analysed using immersion/crystallisation methods of qualitative data analysis.[REF] With no pre-existing framework developed in advance for analysis, an inductive approach was used to discover patterns of NVB in the data. A team of six researchers met weekly for 18 months to view videos together. Using a consensus-building approach based on a combination of field notes, opportunistic interviews with the participants, and repeated viewing of the same material, sometimes many months apart, we eventually achieved consensus on verbal, non-verbal, and physical themes and patterns observed in the data. Finally, as a test of goodness-of-fit, we carefully reviewed the videotapes for any deviant cases that did not fit the categories we had developed.\n\n\nAll transcripts were coded thematically by four of the five authors, who met regularly to identify areas of convergence until full agreement was reached. One of the interviewers (P.M.) maintained an audit trail to track the team’s developing thinking. A process of dialectical empiricism[REF] was used to categorise the emergent themes into more abstract concepts…\n\nBack to top\n\n\n\n\n\n\n15. Techniques to enhance trustworthiness\n\n\n\n\n\n\n\nDescribe methods used to ensure trustworthiness and credibility throughout the data collection and analysis process. (See footnote on commonly used techniques15).\nExplain your choice of techniques and why these are appropriate for the particular study.\n\n\n\n\n\n\nJustification, Examples, and Resources\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\nTraining and resources\n\n\nWhy readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript.\n\n\n\nExamples\nMember checking\n\nMember checks [REF] with an external TBL expert (R.L.) supported the validity of these analyses.\n\nTriangulation of data types and data sources\n\nThe interview data were triangulated with the data of 11 student and supervisor focus groups of a previous study, and more specifically, with those data that concern in particular the influence of CST [Communication Skills Training] on the development of patient-centredness…. Triangulation with the focus group data allowed us to broaden the in-depth information from the interviews in the analysis and to share and compare this with information from students and doctors with varying levels of CST (no, limited, full programme) and from two universities (Universities of Antwerp and Ghent). Moreover, this enabled us to better explore the evolution over time, given that the focus groups included participants at different stages of their study: before clerkships (year 4, undergraduate), during clerkships (year 6, undergraduate), after clerkships (year 7, undergraduate) and postgraduate (general practice trainees, and supervising specialists and GPs; Table 1).\n\n\n\n\n\nFinally, as a test of goodness-of-fit, we carefully reviewed the videotapes for any deviant cases that did not fit the categories we had developed.\n\nTriangulation of Researchers + Audit trail\n\nTo ensure rigor and increase authenticity in our methodology, we used two kinds of triangulation—investigator triangulation and data triangulation.[REF] We sought analytical rigor using an audit trail and multiple coders; our coding team included an experienced clinician (M.G.) as well as a nonclinician with expertise in medical communication and team dynamics (L.L.).\n\n\n\nTraining and resources\nSee Lincoln and Guba’s Evaluative Criteria for trustworthiness.\nBack to top\n\n\n\n\n\n\nResults\n\n\n16. Synthesis and interpretation\n\n\n\n\n\n\n\nDescribe the main analytic findings.16\nIn most cases, report a synthesis of the data along with specific quotes, examples, or illustrations derived from the data.\n\nI like when authors use words like many, few, or all to describe frequency.Nkinda Akaro - Researcher\nConsider describing frequency, variety, representativeness, counter-examples, concrete details, contextualization, conditions, and qualifications related to the findings.\nFrequency counts play a limited role in qualitative research, and need not be reported unless they play a meaningful role in interpretation of the data.\nIf your findings include integration with prior literature or theory and/or the development of a theory, model or meta-narrative, consider using tables and figures to communicate these findings.\nItems 16 and 18 can be reported in Results or Discussion sections.17\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript.\n\n\n\nExamples\n\nWe identified four patterns of NVB (non-verbal behavior) that relate to handover quality and have dubbed them: (1) joint focus of attention; (2) the poker hand; (3) parallel play; and (4) kerbside consultation. Each pattern constitutes a transfix, or systematic way of participating non-verbally in the care transfer process. And, although there are variations in each pattern, we have been able to code virtually every handover we have observed in nursing, medicine and surgery into one of these four categories.\n\n\nBecause our participants came from similar educational backgrounds, had studied medicine as their tertiary course, were embedded in the culture of medicine, and were associated in meaningful ways with a single medical school, we approached their transcripts with the assumption that they belonged to a loosely formed discourse community. Although their graduation dates ranged over a period of 50 years and their collective sphere of practice included 10 different specialty areas, there were many similarities in their experiences of enculturation during and after medical school.\n\n\nTheir three major (often overlapping) areas of concern were epistemic (acquiring knowledge and skill), interpersonal (relating to patients, families, colleagues and administrators) and personal (achieving work–life balance). In each of these areas, medical enculturation was achieved by two overlapping processes, absorption and assimilation, each of which may have distinct implications for postgraduate medical education.\n\nBack to top\n\n\n\n\n\n\n17. Links to empirical data\n\n\n\n\n\n\n\nProvide evidence (e.g., quotes, field notes, text excerpts, photographs) to substantiate the more general and abstract concepts or inferences they present as findings.18 I love seeing evidence like this because it really brings results to life. Nkinda Akaro - Researcher\nYou could report this evidence in a table or figure, incorporated into a narrative description of findings, as a stand-alone narrative, or in text blocks embedded in the manuscript text. If you are constrained by word limits or media limitations (e.g.. video), consider sharing data via an appendix, supplemental material, or web-based repository.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript.\n\n\n\nExamples\nSee Frankel et al. for an excellent example of how to use photographs (or snapshots from video) to illustrate and provide supporting evidence for patterns of behavior identified in the analysis. http://qualitysafety.bmj.com/content/21/Suppl_1/i121.\n\nWe identified five interruption types: (1) probing for further data, (2) prompting for expected sequence, (3) teaching around the case, (4) thinking out loud, and (5) providing direction (see Table 1). Several interruption types served both goals of the case review discussions—teaching and patient care. For example, when thinking out loud, supervisors reasoned through problems and taught the team: So that’s the big question, did she have a mechanical fall, or did she have a medicine-related fall? (Case 2). Supervisors prompted for expected sequence, preventing presenters from skipping over information while simultaneously allowing the supervisor to instruct the team on presentation style: So now you can tell me what the rest of his test results are because I haven’t heard those (Case 16).\n\nBack to top\n\n\n\n\n\n\nDiscussion\n\n\n18. Integration with prior work, implications, transferability, and contribution(s) to the field\n\n\n\n\n\n\n\nDescribe how the findings and conclusions connect to, support, elaborate on, or challenge previous findings, experiences, theory, or a guiding paradigm or approach.19\nDescribe how the findings contribute to or advance the field.\nDescribe any implications of the work, such transferability or generalizability.\n\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThe short summary reminds readers of the main findings and may help them assess whether the subsequent interpretation and implications formulated by the authors are supported by the findings.\n\n\nExamples\n\nThis study contributes to the understanding and discussion of the complexity of involving patients in healthcare education. It shows that integrating patient-led teaching into initiatives that are partly faculty-led influences the way in which students perceive learning from and with PIs. What is not known, however, is whether perceptions are also affected by type of health profession and the students’ different orientation towards logics of care and science, and issues of authority and power relations.\n\nFor complete examples of Discussions, see:\n\nHenriksen & Ringsted, 2013\nWesterman et al., 2013.\n\nBack to top\n\n\n\n\n\n\n19. Limitations\n\n\n\n\n\n\n\nDescribe problems or gaps in their efforts to ensure trustworthiness and the potential implications.20 Authors should never feel nervous to report limitations. In fact, thoughtful discussion of limitations is a hallmark of good research!Charles Ruggle - Editor\nDescribe how the chosen paradigm, approach, and methods will influence the situations to which the findings may reasonably apply.21 (See also Item 18.)\nDescribe how specific decisions or events in the conduct of the study strengthen or weaken the rigor of the findings.\n\n\n\n\n\n\nJustification and Example\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExample\n\n\nWhy readers need this information\nAll research has limitations. Discussing them will help readers consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be). If you don’t address limitations, editors and peer reviewers may ask you about them which will delay publication.\n\n\nExample\n\nThe study has several limitations. One is that the focus group interview method reveals students’ perceptions rather than their actual behaviors. Observations of the patient-led teaching encounter may have illuminated an understanding of the relationship between patient instructors and medical students. Another limitation is that the PI-led teaching is optional rather than mandatory, which may have influenced students’ attitudes in a positive direction. Moreover, students who are eager to take on extra-curricular activities may not be representative of the whole population. That only 23 out of 39 students signed up for this study might also have influenced results if the missing group of students represented other perceptions than those present in the focus groups. However the received data from the focus groups were rich in information and diverse perceptions were present. Another limitation is the overrepresentation of women over men in our sample. Even though women are also overrepresented in medical school this might potentially have influenced results, but gender differences in perceptions were nevertheless not identified in the data.\n\nBack to top\n\n\n\n\n\n\nOther\n\n\n20. Conflicts of interest\n\n\n\n\n\n\n\nAuthors shouldn’t feel nervous reporting conflicts of interest. They are very common and rarely influence our decision to publish.Charles Ruggle - Editor\nDescribe any real or potential conflicts of interest that might have influenced or could appear to have influenced the research.\nDescribe:\n\nhow these conflicts were managed in the conduct of the study,\nthe potential impact on study findings and/or conclusions.\n\nSome aspects may be mentioned as part of reflexivity (see Item 6).\n\n\n\n\n\n\nJustification\n\n\n\n\n\n\nWhy readers need this information\nMany factors, including professional and personal relationships and activities, can influence the design, conduct, and reporting of the clinical science that informs health care decision. The potential for conflict of interest exists when these relationships and activities may bias judgment (1). Many stakeholders— editors, peer reviewers, clinicians, educators, policymakers, patients, and the public—rely on the disclosure of authors’ relationships and activities to inform their assessments. Trust in the transparency, consistency, and completeness of these disclosures is essential. - ICMJE\n\n\n\n\n\n\n\n21. Funding\n\n\n\n\n\n\n\nDescribe any sources of funding and other support for the study.\nDescribe the role of funders in data collection, data analysis, and reporting if applicable."
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-title-2",
    "href": "guidelines/srqr/qsca.html#sec-title-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis allows readers to quickly identify the type of study. Your title will be indexed by search tools so descriptive words will make it easier for others to discover."
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-title-3",
    "href": "guidelines/srqr/qsca.html#sec-title-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nResidents learning from a narrative experience with dying patients: a qualitative study.\n\n\nMedical students’ perceptions of the factors influencing their academic performance: an exploratory interview study with high-achieving and re-sitting medical students.\n\n\nUndergraduate rural medical education program development: focus group consultation with the NRHA Rural Medical Educators Group."
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-title-4",
    "href": "guidelines/srqr/qsca.html#sec-title-4",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Training and Resources",
    "text": "Training and Resources\nSee this article for advice on writing titles that are interesting and descriptive.\nThe EQUATOR Network provides training for writing effective manuscript titles as part of their publication school.\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-abstract-2",
    "href": "guidelines/srqr/qsca.html#sec-abstract-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nA reader should be able to read the abstract independent of the manuscript and get a sense of the background, purpose, methods, main results and implications that will be described in greater detail in the manuscript.\nYour abstract will be indexed by search tools so descriptive words will make it easier for others to discover."
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-abstract-3",
    "href": "guidelines/srqr/qsca.html#sec-abstract-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nPurpose: Although academic centers rely on assessments from medical trainees regarding the effectiveness of their faculty as teachers, little is known about how trainees conceptualize and approach their role as assessors of their clinical supervisorsMethod: In 2010, using a constructivist grounded theory approach, five focus group interviews were conducted with 19 residents from an internal medicine residency program. A constant comparative analysis of emergent themes was conducted.Results: Residents viewed clinical teaching assessment (CTA) as a time-consuming task with little reward. They reported struggling throughout the academic year to meet their CTA obligations and described several shortcut strategies they used to reduce their burden. Rather than conceptualizing their assessments as a conduit for both formative and summative feedback, residents perceived CTA as useful for the surveillance of clinical supervisors at the extremes of the spectrum of teaching effectiveness. They put the most effort, including the crafting of written comments, into the CTAs of these outliers. Trainees desired greater transparency in the CTA process and were sceptical regarding the anonymity and perceived validity of their faculty appraisals.Conclusions: Individual and system-based factors conspire to influence postgraduate medical trainees’ motivation for generating high-quality appraisals of clinical teaching. Academic centers need to address these factors if they want to maximize the usefulness of these assessments."
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-abstract-4",
    "href": "guidelines/srqr/qsca.html#sec-abstract-4",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Training and Resources",
    "text": "Training and Resources\nThe EQUATOR Network provides training for writing effective manuscript abstracts as part of their publication school.\nSee this article from the London School of Economics for general abstract writing tips.\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-problem-formulation-2",
    "href": "guidelines/srqr/qsca.html#sec-problem-formulation-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information provides context for the research question or study purpose and situates the study in relation to other work in the field."
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-problem-formulation-3",
    "href": "guidelines/srqr/qsca.html#sec-problem-formulation-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nRegulatory focus theory may therefore offer insight into the variability in responses to feedback, but how well do these experimental findings translate to real clinical situations in which the reality of responsiveness to feedback seems frustratingly complex? …. In order to better elaborate a theory to account for this variability in learner response to feedback, the present study was undertaken. We aimed, in this study, to determine how readily clinical learning events could be classified as activating a promotion or a prevention focus, and to explore, through a careful analysis of doctors’ descriptions of their feedback experiences, the predictive value of regulatory focus theory in the context of real clinical learning situations.\n\n\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-purpose-2",
    "href": "guidelines/srqr/qsca.html#sec-purpose-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nBy clearly stating the purpose of the study, authors set readers’ expectations for the methods, findings and discussion sections of the manuscript."
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-purpose-3",
    "href": "guidelines/srqr/qsca.html#sec-purpose-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nThe purposes of this study were to investigate how medical students recognize, respond to and utilise feedback, and to determine whether there are maturational differences in understandings of the role of feedback across academic years in medical school."
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-purpose-4",
    "href": "guidelines/srqr/qsca.html#sec-purpose-4",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Training and Resources",
    "text": "Training and Resources\nSee this article for advice on writing qualitative research questions.\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-qualitative-approach-2",
    "href": "guidelines/srqr/qsca.html#sec-qualitative-approach-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nIdentifying the research paradigm helps readers understand whether the researcher assumes that there is a single, objective reality (positivist or post-positivist) and has thus designed the study to describe this reality or whether the researcher assumes multiple, subjective realities and designed the study to describe these multiple realities, with no attempt to merge or reconcile these realities (constructivist/interpretivist). The paradigm has implications for the study design, approach, methods, and techniques to ensure rigour and trustworthiness.\nSince the research paradigm does not necessarily dictate particular approaches or methods, the approach should also be clearly defined. Stating the approach provides readers the opportunity to evaluate the fidelity of the research approach to the research question(s) and consider the rationale for modifications and deviations from the selected approach. Qualitative research also includes an array of methods that can be used across paradigms and approaches. (See also Item 10)."
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-qualitative-approach-3",
    "href": "guidelines/srqr/qsca.html#sec-qualitative-approach-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nThe study was performed from a constructivist point of view using an interpretative phenomenological epistemology. Based on the notion that social phenomena are constructed by the communal making of meaning about the underlying phenomena, we aimed to construct insightful accounts of lead consultants’ approaches to educational change, rather than to identify the true nature of these approaches. Because the management of change by lead consultants is an under-researched area, we conducted an exploratory qualitative study…\n\n\nGiven the relative dearth of explanatory theories about factors affecting medical students’ emotional reactions, we chose to develop one by applying methods associated with grounded theory, specifically constant comparative analysis, to qualitative data obtained from learning logs and interviews. Our approach was constructivist, deliberately using researchers’ own experiences and acquired knowledge to enhance theoretical sensitivity and enrich theory development.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-researcher-characteristics-and-reflexivity-2",
    "href": "guidelines/srqr/qsca.html#sec-researcher-characteristics-and-reflexivity-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nIn positivist and post-positivist paradigms, personal characteristics and perspectives of researchers might be viewed as biases that limit the credibility of study findings, while in constructivist or interpretivist paradigms the characteristics and perspectives of the researchers are important contextual factors that are an accepted part of the study design, data collection, and data analysis. These characteristics and perspectives may explain how the researcher(s) obtained access to the site or participants included in the study or may add valuable insight during data analysis."
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-researcher-characteristics-and-reflexivity-3",
    "href": "guidelines/srqr/qsca.html#sec-researcher-characteristics-and-reflexivity-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nReflexivity was maintained by the research team through the analysis and writing by recording, discussing and challenging established assumptions. In addition both EH and SV kept reflexive diaries.\n\n\nThe first author conducted all interviews and discussion groups. Her own medical undergraduate training took place between 1995 and 2000. She was not known to the participants of this research prior to undertaking the study and deliberately did not undertake any clinical or teaching activities locally alongside this research. Whilst it was useful to know (from her own background) what the students were talking about medically (and in terms of detecting items of significance), as a researcher she made conscious efforts not to accept potentially common assumptions at face value.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-context-2",
    "href": "guidelines/srqr/qsca.html#sec-context-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis helps readers interpret the meaning and significance of the study findings by situating them in social, cultural, temporal and other relevant contexts."
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-context-3",
    "href": "guidelines/srqr/qsca.html#sec-context-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nWe conducted the study among hospital-based clinical teachers of students in years 4 to 6 of a six-year undergraduate medical program at Maastricht University Medical School. Years 4 to 6 are devoted to clerkships in the academic hospital and affiliated regional hospitals. Rotations differ in duration depending on the type of rotation and the discipline, and the sequence of rotations differs among students. During rotations, students spend time in the wards, the outpatient clinics, and the accident and emergency department. Clerkships in years 4 and 5 last between 4 and 10 weeks (regular clerkship), whereas students in year 6 undertake an 18-week senior clerkship in a discipline of their choice.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-sampling-strategy-2",
    "href": "guidelines/srqr/qsca.html#sec-sampling-strategy-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information helps readers understand the source of data / findings so they can consider the boundaries of the study and the relevance to their own context. It also gives readers insight into the researchers’ decisions, which can be important for critical appraisal."
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-sampling-strategy-3",
    "href": "guidelines/srqr/qsca.html#sec-sampling-strategy-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nAs students’ perceptions were previously shown to be related to gender, age, prior experience and place of attachment,[REF] we purposely selected respondents with different backgrounds. This sampling strategy led to the diversity of gender, age, prior patient experience and place of attachment shown in Table 1.\n\n\nPurposive sampling was directed towards achieving maximum variation in age and specialty, using a snowball approach (a non-probabilistic form of sampling in which persons initially chosen for the sample are used as informants to locate other persons having necessary characteristics making them eligible for the sample).[REF ]\n\n\nPotential participants were all medical students in Years 1 and 2 at the University of Toronto in 2004. Following research ethics board approval, recruitment was conducted via e-mail to class listservs. Participant responses were sent directly to the research assistant, who was unknown to participants, so that the principal investigators did not know who did or did not participate. This process was engaged to protect participants’ anonymity and to avoid any impression of coercion because the lead researcher (SG) was involved in the administration of the undergraduate curriculum at the time. Sample size was estimated to be sufficient based on the principle of theoretical saturation [REF] and our previous experience with this methodology (i.e., with a relatively homogeneous population, we expected to reach saturation with approximately 15 interviews per group). There were no exclusion criteria and we accepted the first 15 students from each class who volunteered.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-ethics-2",
    "href": "guidelines/srqr/qsca.html#sec-ethics-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nJournals will require an ethical approval statement.\nQualitative research often involves interaction between researchers and research participants. Correspondingly, researchers should ensure that participants are fully aware of their participation in a research study, the risks and benefits associated with the study, the steps and precautions the researchers will take to minimize risks, such as loss of privacy and confidentiality, and how the researchers plan to use the data."
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-ethics-3",
    "href": "guidelines/srqr/qsca.html#sec-ethics-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nEthical approval was granted by the University of Otago and student participants were invited to attend each focus group discussion by the university representatives of New Zealand Medical Students Association (NZMSA). Usually, a key concern when collecting data from students is that students may feel vulnerable when sharing their experiences with academic staff during a focus group discussion. However, this potential harm was removed as each group discussion was facilitated by a fellow student, the discussions were transcribed by a professional transcribing service, and only the primary researchers [Names] had access to the raw data.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-data-collection-methods-2",
    "href": "guidelines/srqr/qsca.html#sec-data-collection-methods-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThe study period helps readers place the study in temporal context and identify factors not mentioned by the authors that might affect findings, interpretation, and implications. (See Item 8 for ending data collection.)\n\nDescribing researcher characteristics clarifies the relationship between the individuals involved in data collection and the participants in the research and also explains what efforts were made to ensure consistency in the data collection process (See Items 6 and 15.)"
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-data-collection-methods-3",
    "href": "guidelines/srqr/qsca.html#sec-data-collection-methods-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nFurther, it was decided that group interviews, also known as focus group discussions would be the best means of data collection. This is a method of data collection that enables group members to feed off each other’s ideas and an effective moderator will maintain group focus whilst at the same time permitting flexibility in the direction those aspects of the discussion might take. [REF]\n\n\nAdjustments to the interview protocol were made according to early experience and information participants had provided (i.e. redundant questions were eliminated; questions were reworded to improve flow and clarity; additional probes were included).\n\n\nFaculty staff were then interviewed individually by a trained study investigator in a 15-minute, semi-structured interview. This sequence was repeated with other video encounters. Table 2 presents examples of interview questions. Each faculty member was interviewed by at least three interviewers over their various interviews. Interviewers were chosen based on their experience in interviewing. All were trained during a half-day meeting to interpret and deliver the interview guide in the same manner in order to elicit information of a consistent type.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-data-collection-instruments-2",
    "href": "guidelines/srqr/qsca.html#sec-data-collection-instruments-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nDescribing instruments and equipment helps readers understand the full context in which data collection occurred and how this context might have affected data collection (e.g., the influence of recording devices on participants’ behaviors; the nature of inferences drawn from live vs. recorded events)."
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-data-collection-instruments-3",
    "href": "guidelines/srqr/qsca.html#sec-data-collection-instruments-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nTo facilitate the discussion and to maintain consistency over different sets of discussions, key trigger questions were devised prior to the discussion. The opening trigger question was: Thinking back to some of your best clinical learning placements in 4th and 5th year. What was it about those clinical placements that provided good opportunities for learning?\n\n\nInterviews included discussion of the expectations, processes and consequences of AEE [authentic early experiences]. The interview schedule was derived following identification of questions that could not be fully answered in a systematic review of previous empirical or theoretical literature. It comprised a sequence of topic areas including experiences in action, and areas of frustration in Medical Education such as the learning of content knowledge, achieving functional knowledge, and transfer of knowledge.[REF]… Interested readers can request a copy of the schedules from the corresponding author. Interviews lasted between 20 and 90 min. and discussion groups between 60 and 90 min. All interviews and discussion groups were conducted in private rooms at the participant’s workplace—the medical school for students and faculty, and individual places of work for workplace supervisors (except for one who chose to be interviewed at the medical school). All data were audio-recorded, and transcribed verbatim.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-units-of-study-2",
    "href": "guidelines/srqr/qsca.html#sec-units-of-study-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "How does this item differ to Sampling (Item 8)?",
    "text": "How does this item differ to Sampling (Item 8)?\nThe sampling item (Item 8) describes the target or ideal participants, documents, or events selected for the study. By contrast, this item focuses on description of the actual participants, documents or events included in the study."
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-units-of-study-3",
    "href": "guidelines/srqr/qsca.html#sec-units-of-study-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis helps readers know whose experiences and perspective are and are not included."
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-units-of-study-4",
    "href": "guidelines/srqr/qsca.html#sec-units-of-study-4",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nOf the 70 Mindful Communication program participants, 46 met the eligibility requirements to participate in the in-depth interviews. We randomly chose and then contacted 22 participants, of whom 20 agreed to be interviewed within six months of completing the program: 15 in person and 5 by telephone. Two declined for lack of time. On reaching saturation after 20 interviews, no further attempts to contact the remaining 24 participants were made.\n\n\n\n\n\nThere were 31 nursing handovers covering 137 patients, and 21 resident handovers covering 101 patients included in this study.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-data-processing-2",
    "href": "guidelines/srqr/qsca.html#sec-data-processing-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript."
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-data-processing-3",
    "href": "guidelines/srqr/qsca.html#sec-data-processing-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nInterviews were anonymised and each participant was given a code number.\n\n\nThe interviewers and another member of the research team (H.B.) reviewed transcripts for accuracy.\n\n\nWe collected data throughout the admission process through direct observation, audio- recording, and chart extraction. We audio-recorded, transcribed, and anonymized both the overnight and morning case review discussions. We also observed the morning case review discussions in person and collected field notes. For each case review discussion, we copied the admission notes from the patient’s record and de-identified all data.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-data-analysis-2",
    "href": "guidelines/srqr/qsca.html#sec-data-analysis-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nTechniques used for data analysis will depend on the paradigm, approach, and/or data collection methods selected by the researchers. Correspondingly, authors should be as transparent as possible about the analytic process so that readers can follow the logic of inquiry from the research question(s) to the analysis and findings."
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-data-analysis-3",
    "href": "guidelines/srqr/qsca.html#sec-data-analysis-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\n…we brought sensitizing concepts to the analysis while we conducted an open, inductive analysis.[REF]In this case the sensitizing concepts arose, a priori to analysis, from a framework derived from the literature [REF] (as described above), in which participants’ motivations to act are based on principles of professionalism, internal affect, or potential implications of their actions.[REF]\n\n\nThrough an iterative process of listening, discussing, and relistening, the team identified and consensually validated emerging themes[REF] and appended segments of dialogue supporting the proposed themes. Recruitment stopped when saturation was reached (no new themes were identified). The team systematically reviewed the themes and sorted them into content domains. The team used an analytic matrix to identify patterns and connections amongst the domains. Two of us not involved in the qualitative coding process (R.E., M.K.) audited the analytic matrix, choice of quotes, and thematic analysis.\n\n\nThe analysis started after the first interview. All data were analyzed with the aid of the audio- coding facility of the NVivo 8: QSR International Pty Ltd, Doncaster, Vic, Australia programme. First, [name] and [name] coded independently from one another, making sure to stay semantically close to the participants’ wording. Then we discussed these open codes and defined axial codes.[REF] New insights about the impact of CST were written down in memos.\n\n\nVideotapes were analysed using immersion/crystallisation methods of qualitative data analysis.[REF] With no pre-existing framework developed in advance for analysis, an inductive approach was used to discover patterns of NVB in the data. A team of six researchers met weekly for 18 months to view videos together. Using a consensus-building approach based on a combination of field notes, opportunistic interviews with the participants, and repeated viewing of the same material, sometimes many months apart, we eventually achieved consensus on verbal, non-verbal, and physical themes and patterns observed in the data. Finally, as a test of goodness-of-fit, we carefully reviewed the videotapes for any deviant cases that did not fit the categories we had developed.\n\n\nAll transcripts were coded thematically by four of the five authors, who met regularly to identify areas of convergence until full agreement was reached. One of the interviewers (P.M.) maintained an audit trail to track the team’s developing thinking. A process of dialectical empiricism[REF] was used to categorise the emergent themes into more abstract concepts…\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-trustworthiness-2",
    "href": "guidelines/srqr/qsca.html#sec-trustworthiness-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript."
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-trustworthiness-3",
    "href": "guidelines/srqr/qsca.html#sec-trustworthiness-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\nMember checking\n\nMember checks [REF] with an external TBL expert (R.L.) supported the validity of these analyses.\n\nTriangulation of data types and data sources\n\nThe interview data were triangulated with the data of 11 student and supervisor focus groups of a previous study, and more specifically, with those data that concern in particular the influence of CST [Communication Skills Training] on the development of patient-centredness…. Triangulation with the focus group data allowed us to broaden the in-depth information from the interviews in the analysis and to share and compare this with information from students and doctors with varying levels of CST (no, limited, full programme) and from two universities (Universities of Antwerp and Ghent). Moreover, this enabled us to better explore the evolution over time, given that the focus groups included participants at different stages of their study: before clerkships (year 4, undergraduate), during clerkships (year 6, undergraduate), after clerkships (year 7, undergraduate) and postgraduate (general practice trainees, and supervising specialists and GPs; Table 1).\n\n\n\n\n\nFinally, as a test of goodness-of-fit, we carefully reviewed the videotapes for any deviant cases that did not fit the categories we had developed.\n\nTriangulation of Researchers + Audit trail\n\nTo ensure rigor and increase authenticity in our methodology, we used two kinds of triangulation—investigator triangulation and data triangulation.[REF] We sought analytical rigor using an audit trail and multiple coders; our coding team included an experienced clinician (M.G.) as well as a nonclinician with expertise in medical communication and team dynamics (L.L.)."
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-trustworthiness-4",
    "href": "guidelines/srqr/qsca.html#sec-trustworthiness-4",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Training and resources",
    "text": "Training and resources\nSee Lincoln and Guba’s Evaluative Criteria for trustworthiness.\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-synthesis-and-interpretation-2",
    "href": "guidelines/srqr/qsca.html#sec-synthesis-and-interpretation-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript."
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-synthesis-and-interpretation-3",
    "href": "guidelines/srqr/qsca.html#sec-synthesis-and-interpretation-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nWe identified four patterns of NVB (non-verbal behavior) that relate to handover quality and have dubbed them: (1) joint focus of attention; (2) the poker hand; (3) parallel play; and (4) kerbside consultation. Each pattern constitutes a transfix, or systematic way of participating non-verbally in the care transfer process. And, although there are variations in each pattern, we have been able to code virtually every handover we have observed in nursing, medicine and surgery into one of these four categories.\n\n\nBecause our participants came from similar educational backgrounds, had studied medicine as their tertiary course, were embedded in the culture of medicine, and were associated in meaningful ways with a single medical school, we approached their transcripts with the assumption that they belonged to a loosely formed discourse community. Although their graduation dates ranged over a period of 50 years and their collective sphere of practice included 10 different specialty areas, there were many similarities in their experiences of enculturation during and after medical school.\n\n\nTheir three major (often overlapping) areas of concern were epistemic (acquiring knowledge and skill), interpersonal (relating to patients, families, colleagues and administrators) and personal (achieving work–life balance). In each of these areas, medical enculturation was achieved by two overlapping processes, absorption and assimilation, each of which may have distinct implications for postgraduate medical education.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-links-to-empirical-data-2",
    "href": "guidelines/srqr/qsca.html#sec-links-to-empirical-data-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript."
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-links-to-empirical-data-3",
    "href": "guidelines/srqr/qsca.html#sec-links-to-empirical-data-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\nSee Frankel et al. for an excellent example of how to use photographs (or snapshots from video) to illustrate and provide supporting evidence for patterns of behavior identified in the analysis. http://qualitysafety.bmj.com/content/21/Suppl_1/i121.\n\nWe identified five interruption types: (1) probing for further data, (2) prompting for expected sequence, (3) teaching around the case, (4) thinking out loud, and (5) providing direction (see Table 1). Several interruption types served both goals of the case review discussions—teaching and patient care. For example, when thinking out loud, supervisors reasoned through problems and taught the team: So that’s the big question, did she have a mechanical fall, or did she have a medicine-related fall? (Case 2). Supervisors prompted for expected sequence, preventing presenters from skipping over information while simultaneously allowing the supervisor to instruct the team on presentation style: So now you can tell me what the rest of his test results are because I haven’t heard those (Case 16).\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-integration-with-prior-work-2",
    "href": "guidelines/srqr/qsca.html#sec-integration-with-prior-work-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThe short summary reminds readers of the main findings and may help them assess whether the subsequent interpretation and implications formulated by the authors are supported by the findings."
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-integration-with-prior-work-3",
    "href": "guidelines/srqr/qsca.html#sec-integration-with-prior-work-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nThis study contributes to the understanding and discussion of the complexity of involving patients in healthcare education. It shows that integrating patient-led teaching into initiatives that are partly faculty-led influences the way in which students perceive learning from and with PIs. What is not known, however, is whether perceptions are also affected by type of health profession and the students’ different orientation towards logics of care and science, and issues of authority and power relations.\n\nFor complete examples of Discussions, see:\n\nHenriksen & Ringsted, 2013\nWesterman et al., 2013.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-limitations-2",
    "href": "guidelines/srqr/qsca.html#sec-limitations-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nAll research has limitations. Discussing them will help readers consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be). If you don’t address limitations, editors and peer reviewers may ask you about them which will delay publication."
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-limitations-3",
    "href": "guidelines/srqr/qsca.html#sec-limitations-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nThe study has several limitations. One is that the focus group interview method reveals students’ perceptions rather than their actual behaviors. Observations of the patient-led teaching encounter may have illuminated an understanding of the relationship between patient instructors and medical students. Another limitation is that the PI-led teaching is optional rather than mandatory, which may have influenced students’ attitudes in a positive direction. Moreover, students who are eager to take on extra-curricular activities may not be representative of the whole population. That only 23 out of 39 students signed up for this study might also have influenced results if the missing group of students represented other perceptions than those present in the focus groups. However the received data from the focus groups were rich in information and diverse perceptions were present. Another limitation is the overrepresentation of women over men in our sample. Even though women are also overrepresented in medical school this might potentially have influenced results, but gender differences in perceptions were nevertheless not identified in the data.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsca.html#sec-conflicts-of-interest-2",
    "href": "guidelines/srqr/qsca.html#sec-conflicts-of-interest-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nMany factors, including professional and personal relationships and activities, can influence the design, conduct, and reporting of the clinical science that informs health care decision. The potential for conflict of interest exists when these relationships and activities may bias judgment (1). Many stakeholders— editors, peer reviewers, clinicians, educators, policymakers, patients, and the public—rely on the disclosure of authors’ relationships and activities to inform their assessments. Trust in the transparency, consistency, and completeness of these disclosures is essential. - ICMJE"
  },
  {
    "objectID": "guidelines/srqr/qsca.html#ready-to-get-started",
    "href": "guidelines/srqr/qsca.html#ready-to-get-started",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Ready to get started?",
    "text": "Ready to get started?\nDownload resources"
  },
  {
    "objectID": "guidelines/srqr/qsca.html#how-to-cite",
    "href": "guidelines/srqr/qsca.html#how-to-cite",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "How to cite",
    "text": "How to cite\nFor attribution, please cite this guideline as:\nBridget O’Brien, Ilene Harris, Thomas Beckman, Darcy Reed, and David Cook. 2023. “The SRQR Guidelines for Writing Qualitative Research Articles version 1.1.” The EQUATOR Network Guideline Dissemination Platform. https://doi.org/10.1234/equator/1010101.\nYou can use your reference manager to save citation information for this webpage, or copy the BibTeX below."
  },
  {
    "objectID": "guidelines/srqr/qsca.html#faqs",
    "href": "guidelines/srqr/qsca.html#faqs",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "FAQs",
    "text": "FAQs\n\nWho made this guideline?\n\nBridget O’Brien, PhD has been a faculty member in the Department of Medicine, Division of General Internal Medicine, since 2008. She is a professor of medicine and an education scientist in the Office of Medical Education’s Center for Faculty Educators. As co-director of the Teaching Scholars Program and the UCSF-University of Utrecht Health Professions Education doctoral program she teaches and mentors faculty and learners interested in education research and scholarship. At the San Francisco VA, she directs the Advanced Fellowship in Health Professions Education Evaluation and Research. In 2015 she was selected as one of five national Macy Faculty Scholars supported by the Josiah Macy Jr. Foundation and in 2021 she was selected as a KIPRIME Fellow at the Karolinska Instituet. She is a deputy editor for the journal Academic Medicine.\nDr. Ilene Harris, deceased, was professor and head, Department of Medical Education, University of Illinois at Chicago College of Medicine, Chicago, Illinois.\nDr. Thomas Beckman is professor of medicine and medical education, Department of Medicine, Mayo Clinic College of Medicine, Rochester, Minnesota.\nDr. Darcy Reed is associate professor of medicine and medical education, Department of Medicine, Mayo Clinic College of Medicine, Rochester, Minnesota.\nDr. David Cook is associate director, Mayo Clinic Online Learning, research chair, Mayo Multidisciplinary Simulation Center, and professor of medicine and medical education, Mayo Clinic College of Medicine, Rochester, Minnesota.\n\n\nHow was this guideline made?\nThe developers synthesised 40 sets of recommendations previously proposed by experts in qualitative methods. You can read about their development process here.\n\n\nWhat to do if asked to remove guideline related content\nIf a colleague or reviewer asks you to remove content that is related to this guideline, you can direct them to this guideline and the explanation for why that item is important. If they insist, consider moving the item to a supplement, table or figure.\n\n\nWhere can I get general writing training?\nThe EQUATOR Network provides in-person training for writing research articles.\nAuthorAID have resources, an online course, and mentoring to help authors.\n\n\n\nResearch paradigm\n\n\n\n\n\n\nThe set of beliefs and assumptions that guide the research process. These commonly include positivist, post-positivist, constructivist or interpretivist, and critical theory. Qualitative research generally draws from a post-positivist or constructivist/interpretivist paradigm.”\n\n\n\n\n\n\nInstruments\n\n\n\n\n\n\nData collection instruments include (but are not limited to) interview or focus group guides, observational protocols and prompts for field notes, and data extraction or coding protocols for selection and analysis of documents, photographs, videos, or other artifacts”\n\n\n\n\n\n\nBias\n\n\n\n\n\n\nA term drawn from quantitative research, bias technically means a systematic error, where a particular research finding deviates from a true finding. This might come about through errors in the manner of interviewing, or by errors in sampling. In qualitative research this is a problematic concept, since by definition the qualitative researcher is part of the process, and all researchers are different. This human factor has been said to be both the greatest strength and the greatest weakness of qualitative method. What can be done in commercial qualitative research, however, is to minimise obvious and avoidable sources of bias, for example by not confining all the fieldwork to one social group or geographic location, by taking steps to recognise the personal views of the researcher, (using techniques such as bracketing), and by working in teams.\n\n\n\n\n\n\nSampling strategy\n\n\n\n\n\n\nSeveral sampling strategies are commonly used in qualitative research, although most fall under the umbrella of purposeful (or purposive) sampling.\nPurposeful sampling means that participants, documents, or events are selected for their relevance to the research question, based on guiding theory or experiences and assumptions of the researchers. Over the course of the research process, researchers may determine that additional or different participants, documents, or events should be included to address the research question.\nOther sampling techniques, such as theoretical sampling (seeking examples of theoretical constructs), snowball sampling (using study participants to identify additional participants who meet study criteria), and convenience sampling (including any volunteers with no or minimal criteria for inclusion) may be appropriate depending on the question and approach, so long as the authors provide explanation and justification.\n\n\n\n\n\n\nApproach\n\n\n\n\n\n\nA qualitative approach is a general way of thinking about conducting qualitative research. It describes, either explicitly or implicitly, the purpose of the qualitative research, the role of the researcher(s), the stages of research, and the method of data analysis. Commonly used approaches include ethnography, grounded theory, case study, phenomenology, and narrative research.\n\n\n\n\n\n\nData collection methods\n\n\n\n\n\n\nData collection methods include (but are not limited to) interviews, focus groups, observations (direct or indirect via video), and review of written text, photographs, and other documents or materials.\n\n\n\n\n\n\nIterative\n\n\n\n\n\n\nQualitative research often occurs as an iterative process, meaning that researchers begin data analysis before they complete data collection. The data collection and analysis process may occur in phases or stages. As part of an iterative collection-analysis process, researchers will often alter their data collection methods to explore their preliminary impressions in greater depth and/or actively pursue confirming and disconfirming perspectives.\n\n\n\n\n\n\nStudy period\n\n\n\n\n\n\nThe start and end dates for data collection and analysis.\n\n\n\n\n\n\nEthnography\n\n\n\n\n\n\nThe scientific description of peoples and cultures with their customs, habits, and mutual differences.\nRead more\n\n\n\n\n\n\nGrounded theory\n\n\n\n\n\n\nA method consisting of a set of systematic, but flexible, guidelines for conducting inductive qualitative inquiry aimed toward theory construction. This method focuses squarely on the analytic phases of research, although both data collection and analysis inform and shape each other and are conducted in tandem.\nRead more\n\n\n\n\n\n\nDegree of participation\n\n\n\n\n\n\nFor example, if some participants were observed and interviewed and others only interviewed, or if some participants completed multiple interviews and others completed a single interview, these variations should be explained.\n\n\n\n\n\n\nUnit of analysis\n\n\n\n\n\n\nIn qualitative research, the unit of analysis is not necessarily the same as the unit of sampling (e.g., individual participants or events). Instead, some approaches use specific events as the unit of analysis, such as mentions of a particular topic or experience, or observations of a particular behavior or phenomenon, while others use groups rather than individual group participants. This specification has implications for how the data are organized and analyzed as well as the inferences drawn from the data.\n\n\n\n\n\n\nReflexivity\n\n\n\n\n\n\nReflexivity refers to intentional, systematic consideration of the potential or actual effects of the researcher(s) on all aspects of the study process.\n\n\n\n\n\n\nTransferability\n\n\n\n\n\n\nThe transferability of a research finding is the extent to which it can be applied in other contexts and studies. It is thus equivalent to or a replacement for the terms generalizability and external validity.\n\n\n\n\n\n\nGeneralizability\n\n\n\n\n\n\nThe appropriate scope for generalization of the findings beyond the study (e.g., to other settings, populations, time periods, circumstances)\n\n\n\n\n\n\nAnalytic findings\n\n\n\n\n\n\nAnalytic findings may include interpretations, inferences, narratives, themes, and models.\n\n\n\n\n\n\nFrequency counts\n\n\n\n\n\n\nThe frequency of specific themes or codes."
  },
  {
    "objectID": "guidelines/srqr/qscb.html",
    "href": "guidelines/srqr/qscb.html",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "",
    "text": "Full Title: Standards for Reporting Qualitative Research\nAuthors: Bridget O’Brien, Ilene Harris, Thomas Beckman, Darcy Reed, David Cook\nVersion: 1.1\nDOI: 10.1234/equator/1010101"
  },
  {
    "objectID": "guidelines/srqr/qscb.html#translations",
    "href": "guidelines/srqr/qscb.html#translations",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Translations",
    "text": "Translations\nThis guideline is also available in French."
  },
  {
    "objectID": "guidelines/srqr/qscb.html#about-this-guideline",
    "href": "guidelines/srqr/qscb.html#about-this-guideline",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "About this guideline",
    "text": "About this guideline\n\nWhen to use this guideline\n\nUse this guideline for writing qualitative research articles. You can use it when describing all kinds of qualitative approaches, methods, and designs.\n\n\nYou can also use this guideline for:\n\nwriting proposals or protocols (use the items within the Introduction and Method sections).\nreviewing the reporting of an article, but not for appraising its quality.\n\n\n\n\n\n\n\n\n\nDo not use this guidance for:\n\nwriting a qualitative evidence synthesis, use ENTREQ instead.\nappraising the quality of qualitative research, use an appraisal tool like CASP-Qual instead. \n\n\n\n\n\n\n\n\n\n\nRelated reporting guidelines:\n\nJARS Qual for writing qualitative, psychology manuscripts\nENTREQ for writing qualitative evidence syntheses\nFor writing studies involving interviews or focus groups, you can use this guideline or COREQ. \n\nFor appraising research consider:\n\nCASP-Qual\n\n\n\n\n\n\nHow to use this guideline\nReporting guidelines help me structure my drafts and develop as a researcher. I use them when teaching and hope that my students continue to use them. Manuel Silva - Researcher\nReporting guidelines are best used early. Read the full guideline below or use a tool when working:\n\nUse a template to draft manuscripts and avoid the blank page \nUse a log book to plan and record information whilst conducting research\nUse a checklist to double-check and reassure others that your article describes all the important details of your work\n\n\n\n\nFor drafting\n\nTemplate\n\n\n\nFor planning\n\nLog book\n\n\n\nFor checking\n\nChecklist\n\n\nThis reporting guideline does not prescribe a rigid format or standardized content. Consider each item and prioritize elements that are most relevant to your study, findings, context, and readers.\nYou may prefer to report an item in a different order, section, or in a table or figure. If you feel confident that an item is less important to your study, you could report it in an appendix or supplement. If you think an item is not applicable, state why. You don’t need to write your article in the order the items are presented.\nIn your methods section, state that you used the SRQR guideline when writing your article and cite it (see How to cite).\n\n\nWhy use this guideline\nFollowing this guideline will make it easier for other researchers to find, use, and synthesise your work. It won’t tell you how to do qualitative research, but it will help you explain your choices so others can understand what you did, why you did it, and what you found. At our journal, we ensure all qualitative articles follow SRQR as this makes them easier for readers to understand and faster to publish.Fiona Ludlow - Editor in Chief\nIt was written by experts from the qualitative research community, using a thorough development process. SRQR is endorsed by hundreds of journals, so using this guideline (and its checklist) will help you meet editors’ expectations."
  },
  {
    "objectID": "guidelines/srqr/qscb.html#guidance",
    "href": "guidelines/srqr/qscb.html#guidance",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Guidance",
    "text": "Guidance\nApprox. 17 min read\n\n\n1. Title\n\n\n\n\n\n\n\nDescribe the nature and topic of the study.\nIdentify the study as qualitative or indicate the approach or data collection methods.\n\n\n\n\n\n\nJustification, Examples, and Resources\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\nTraining and Resources\n\n\nWhy readers need this information\nThis allows readers to quickly identify the type of study. Your title will be indexed by search tools so descriptive words will make it easier for others to discover.\n\n\nExamples\n\nResidents learning from a narrative experience with dying patients: a qualitative study.\n\n\nMedical students’ perceptions of the factors influencing their academic performance: an exploratory interview study with high-achieving and re-sitting medical students.\n\n\nUndergraduate rural medical education program development: focus group consultation with the NRHA Rural Medical Educators Group.\n\n\n\n\nTraining and Resources\nSee this article for advice on writing titles that are interesting and descriptive.\nThe EQUATOR Network provides training for writing effective manuscript titles as part of their publication school.\nBack to top\n\n\n\n\n\n\n2. Abstract\n\n\n\n\n\n\n\nInformative key words in the title and abstract make manuscripts easier to find.Lorna Ellis - Evidence Synthesiser\nSummarise key elements of the study including:\n\nbackground about the problem or phenomenon of interest\ndescription of the study purpose or research question\nmethods, including the approach or perspective (e.g., general inductive, grounded theory), context (setting, time period), sample (number and key characteristics of participants, events, documents), data collection strategies (e g., observation, interview, focus group) and data analysis techniques\ndescription of main findings (e.g., themes or inferences) related to the study purpose and/or research question\nstudy implications\n\n\nInformation presented in the abstract should be consistent with the information presented in the full text.\nUse the format of your intended journal. 1\n\n\n\n\n\n\nJustification, Example, and Resources\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExample\nTraining and Resources\n\n\nWhy readers need this information\nA reader should be able to read the abstract independent of the manuscript and get a sense of the background, purpose, methods, main results and implications that will be described in greater detail in the manuscript.\nYour abstract will be indexed by search tools so descriptive words will make it easier for others to discover.\n\n\n\nExample\n\nPurpose: Although academic centers rely on assessments from medical trainees regarding the effectiveness of their faculty as teachers, little is known about how trainees conceptualize and approach their role as assessors of their clinical supervisorsMethod: In 2010, using a constructivist grounded theory approach, five focus group interviews were conducted with 19 residents from an internal medicine residency program. A constant comparative analysis of emergent themes was conducted.Results: Residents viewed clinical teaching assessment (CTA) as a time-consuming task with little reward. They reported struggling throughout the academic year to meet their CTA obligations and described several shortcut strategies they used to reduce their burden. Rather than conceptualizing their assessments as a conduit for both formative and summative feedback, residents perceived CTA as useful for the surveillance of clinical supervisors at the extremes of the spectrum of teaching effectiveness. They put the most effort, including the crafting of written comments, into the CTAs of these outliers. Trainees desired greater transparency in the CTA process and were sceptical regarding the anonymity and perceived validity of their faculty appraisals.Conclusions: Individual and system-based factors conspire to influence postgraduate medical trainees’ motivation for generating high-quality appraisals of clinical teaching. Academic centers need to address these factors if they want to maximize the usefulness of these assessments.\n\n\n\n\nTraining and Resources\nThe EQUATOR Network provides training for writing effective manuscript abstracts as part of their publication school.\nSee this article from the London School of Economics for general abstract writing tips.\nBack to top\n\n\n\n\n\nIntroduction\n\n\n3. Problem Formulation\n\n\n\n\n\n\n\nWhen the problem is described clearly I find it much easier to understand why a study was done, even if its from a different subject area, country, or decade.Nkinda Akaro - Researcher\nDescribe the theoretical and/or practical issues or concerns that make the study necessary, including:\n\nan overview of what is known about the problem\ngaps in current knowledge (the problem statement)\nthe scope of the research problem or phenomena addressed in the study (what will and will not be included)\ntheoretical and/or empirical work directly relevant to the problem or phenomena studied\nthe need for a qualitative approach.2\n\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExample\n\n\nWhy readers need this information\nThis information provides context for the research question or study purpose and situates the study in relation to other work in the field.\n\n\nExample\n\nRegulatory focus theory may therefore offer insight into the variability in responses to feedback, but how well do these experimental findings translate to real clinical situations in which the reality of responsiveness to feedback seems frustratingly complex? …. In order to better elaborate a theory to account for this variability in learner response to feedback, the present study was undertaken. We aimed, in this study, to determine how readily clinical learning events could be classified as activating a promotion or a prevention focus, and to explore, through a careful analysis of doctors’ descriptions of their feedback experiences, the predictive value of regulatory focus theory in the context of real clinical learning situations.\n\n\n\nBack to top\n\n\n\n\n\n\n4. Purpose or research question\n\n\n\n\n\n\n\nA clear purpose frames the article. Rahilah Zayn - Researcher\nInclude a statement of study intent. This can be framed as one or more research questions, purposes, goals, or objectives.3\n\n\n\n\n\n\nJustification, Example, and Resources\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExample\nTraining and Resources\n\n\nWhy readers need this information\nBy clearly stating the purpose of the study, authors set readers’ expectations for the methods, findings and discussion sections of the manuscript.\n\n\nExample\n\nThe purposes of this study were to investigate how medical students recognize, respond to and utilise feedback, and to determine whether there are maturational differences in understandings of the role of feedback across academic years in medical school.\n\n\n\nTraining and Resources\nSee this article for advice on writing qualitative research questions.\nBack to top\n\n\n\n\n\n\nMethods\n\n\n5. Qualitative approach and research paradigm\n\n\n\n\n\n\n\nDescribe your qualitative approach, your guiding theory (if appropriate), and identify the research paradigm.4 I like it when authors describe terms in their own words as well as using the proper terms.Rahilah Zayn - Researcher\nExplain why the selected approach is appropriate for the research question.\nProvide references to theories or traditions that guide the use of the approach as needed.\nI hadn’t heard the term approach before writing-up my study. I realise now that I did have an approach and that it might be different to other researcher’s approaches, so I need to describe things that I felt were obvious.Tim Westland - Researcher\n\nIf you don’t know what your approach or paradigm was, or you don’t think you had one, it’s OK to reflect on this after collecting data and you should still report it. Read this list to see what best describes your work.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nIdentifying the research paradigm helps readers understand whether the researcher assumes that there is a single, objective reality (positivist or post-positivist) and has thus designed the study to describe this reality or whether the researcher assumes multiple, subjective realities and designed the study to describe these multiple realities, with no attempt to merge or reconcile these realities (constructivist/interpretivist). The paradigm has implications for the study design, approach, methods, and techniques to ensure rigour and trustworthiness.\nSince the research paradigm does not necessarily dictate particular approaches or methods, the approach should also be clearly defined. Stating the approach provides readers the opportunity to evaluate the fidelity of the research approach to the research question(s) and consider the rationale for modifications and deviations from the selected approach. Qualitative research also includes an array of methods that can be used across paradigms and approaches. (See also Item 10).\n\n\nExamples\n\nThe study was performed from a constructivist point of view using an interpretative phenomenological epistemology. Based on the notion that social phenomena are constructed by the communal making of meaning about the underlying phenomena, we aimed to construct insightful accounts of lead consultants’ approaches to educational change, rather than to identify the true nature of these approaches. Because the management of change by lead consultants is an under-researched area, we conducted an exploratory qualitative study…\n\n\nGiven the relative dearth of explanatory theories about factors affecting medical students’ emotional reactions, we chose to develop one by applying methods associated with grounded theory, specifically constant comparative analysis, to qualitative data obtained from learning logs and interviews. Our approach was constructivist, deliberately using researchers’ own experiences and acquired knowledge to enhance theoretical sensitivity and enrich theory development.\n\nBack to top\n\n\n\n\n\n\n6. Researcher characteristics and reflexivity\n\n\n\n\n\n\n\nDescribe how roles and identities of research team members influence choice of research approach, data collection, and data analysis.5\nComing from quantitative research it felt strange to include such personal descriptions of the team. But I came to realise why it was useful to others, and that peer reviewers would expect it. Tim Westland - Researcher\nDescribe the perspectives, assumptions, prior knowledge, preliminary hypotheses, and/or motives (the stance) of the members of the research team.\nDescribe the researchers’ relationships to participants in the study and what decisions were made in light of these relationships.6\nIf your research was observational (e.g., ethnography), describe the role of the researcher along a spectrum from passive observer (no involvement in the activity studied) to participant-observer (ranging from some limited involvement in the activity to full involvement).\nThere is no expectation that the study could be precisely replicated; these characteristics and perspectives of the researcher should not be mentioned in the limitation section. (See also Item 14: Data Analysis)\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nIn positivist and post-positivist paradigms, personal characteristics and perspectives of researchers might be viewed as biases that limit the credibility of study findings, while in constructivist or interpretivist paradigms the characteristics and perspectives of the researchers are important contextual factors that are an accepted part of the study design, data collection, and data analysis. These characteristics and perspectives may explain how the researcher(s) obtained access to the site or participants included in the study or may add valuable insight during data analysis.\n\n\nExamples\n\nReflexivity was maintained by the research team through the analysis and writing by recording, discussing and challenging established assumptions. In addition both EH and SV kept reflexive diaries.\n\n\nThe first author conducted all interviews and discussion groups. Her own medical undergraduate training took place between 1995 and 2000. She was not known to the participants of this research prior to undertaking the study and deliberately did not undertake any clinical or teaching activities locally alongside this research. Whilst it was useful to know (from her own background) what the students were talking about medically (and in terms of detecting items of significance), as a researcher she made conscious efforts not to accept potentially common assumptions at face value.\n\nBack to top\n\n\n\n\n\n\n7. Context\n\n\n\n\n\n\n\nDescribe the setting/site(s) in which the study was conducted, the reason(s) why the setting/site(s) was selected, and the salient cultural, political, historical, economic and/or other external factors that influence the study.\nWhen synthesising evidence, context helps me work out how and why different studies and findings fit together (or don’t)Lorna Ellis - Evidence Synthesiser\nAdditional context may be reported with findings (i.e., the Results section) to add evidence for interpretations and to enhance discussion of transferability.\n\n\n\n\n\n\nJustification and Example\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExample\n\n\nWhy readers need this information\nThis helps readers interpret the meaning and significance of the study findings by situating them in social, cultural, temporal and other relevant contexts.\n\n\nExample\n\nWe conducted the study among hospital-based clinical teachers of students in years 4 to 6 of a six-year undergraduate medical program at Maastricht University Medical School. Years 4 to 6 are devoted to clerkships in the academic hospital and affiliated regional hospitals. Rotations differ in duration depending on the type of rotation and the discipline, and the sequence of rotations differs among students. During rotations, students spend time in the wards, the outpatient clinics, and the accident and emergency department. Clerkships in years 4 and 5 last between 4 and 10 weeks (regular clerkship), whereas students in year 6 undertake an 18-week senior clerkship in a discipline of their choice.\n\nBack to top\n\n\n\n\n\n\n8. Sampling strategy\n\n\n\n\n\n\n\nDescribe how and why research participants, sites, documents/artifacts, and/or events were selected for inclusion in (and, if appropriate, exclusion from) the study, along with a justification for this strategy.\nDescribe the sampling strategy rather than simply labeling it (e.g., purposive or snowball), since such labels do not have a universally accepted definition and, more importantly, procedures tend to be study- specific.\n\nDescribe how you established the final sample size:\n\nIf you used a flexible sampling strategy, then explain the criteria used to decide when no further sampling was necessary.\nIf data collection ended once saturation or sufficiency had been reached, then describe the specific criteria used to define saturation or sufficiency.\n\n\n\nDescribe procedures used to recruit participants, including:\n\nwho was involved in recruitment\nwhat their relationship was to participants\nhow and when recruitment occurred\nwhy these procedures were selected. (See also Item 6: Researcher characteristics and reflexivity)\n\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThis information helps readers understand the source of data / findings so they can consider the boundaries of the study and the relevance to their own context. It also gives readers insight into the researchers’ decisions, which can be important for critical appraisal.\n\n\nExamples\n\nAs students’ perceptions were previously shown to be related to gender, age, prior experience and place of attachment,[REF] we purposely selected respondents with different backgrounds. This sampling strategy led to the diversity of gender, age, prior patient experience and place of attachment shown in Table 1.\n\n\nPurposive sampling was directed towards achieving maximum variation in age and specialty, using a snowball approach (a non-probabilistic form of sampling in which persons initially chosen for the sample are used as informants to locate other persons having necessary characteristics making them eligible for the sample).[REF ]\n\n\nPotential participants were all medical students in Years 1 and 2 at the University of Toronto in 2004. Following research ethics board approval, recruitment was conducted via e-mail to class listservs. Participant responses were sent directly to the research assistant, who was unknown to participants, so that the principal investigators did not know who did or did not participate. This process was engaged to protect participants’ anonymity and to avoid any impression of coercion because the lead researcher (SG) was involved in the administration of the undergraduate curriculum at the time. Sample size was estimated to be sufficient based on the principle of theoretical saturation [REF] and our previous experience with this methodology (i.e., with a relatively homogeneous population, we expected to reach saturation with approximately 15 interviews per group). There were no exclusion criteria and we accepted the first 15 students from each class who volunteered.\n\nBack to top\n\n\n\n\n\n\n9. Ethical issues pertaining to human subjects\n\n\n\n\n\n\n\nReport approval for the study from an appropriate institutional review board for research associated with human subjects.\nIf you did not receive ethical approval, describe why.\nDescribe procedures used to protect participants, including:\n\ndata collection (e.g., recruitment and informed consent)\nanalysis (e.g., data security and integrity)\nand reporting of findings (e.g., anonymization of excerpts).\n\nIf you provided compensation or offered incentives to facilitate participation, describe this too.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nJournals will require an ethical approval statement.\nQualitative research often involves interaction between researchers and research participants. Correspondingly, researchers should ensure that participants are fully aware of their participation in a research study, the risks and benefits associated with the study, the steps and precautions the researchers will take to minimize risks, such as loss of privacy and confidentiality, and how the researchers plan to use the data.\n\n\n\nExamples\n\nEthical approval was granted by the University of Otago and student participants were invited to attend each focus group discussion by the university representatives of New Zealand Medical Students Association (NZMSA). Usually, a key concern when collecting data from students is that students may feel vulnerable when sharing their experiences with academic staff during a focus group discussion. However, this potential harm was removed as each group discussion was facilitated by a fellow student, the discussions were transcribed by a professional transcribing service, and only the primary researchers [Names] had access to the raw data.\n\nBack to top\n\n\n\n\n\n\n10. Data collection methods\n\n\n\n\n\n\n\nDescribe data collection methods and design in detail, and justify them in relation to the research question(s), paradigm, approach, and other methods.7\n\nIf data collection and analysis was iterative:\n\ndescribe the iterative process\nif changes occurred during the research process, describe how and why study procedures changed in response to evolving study findings.\n\nIdentify the study period.\nDescribe important characteristics of the individuals conducting interviews, observations or focus groups, and methods used to train these individuals.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThe study period helps readers place the study in temporal context and identify factors not mentioned by the authors that might affect findings, interpretation, and implications. (See Item 8 for ending data collection.)\n\nDescribing researcher characteristics clarifies the relationship between the individuals involved in data collection and the participants in the research and also explains what efforts were made to ensure consistency in the data collection process (See Items 6 and 15.)\n\n\n\n\nExamples\n\nFurther, it was decided that group interviews, also known as focus group discussions would be the best means of data collection. This is a method of data collection that enables group members to feed off each other’s ideas and an effective moderator will maintain group focus whilst at the same time permitting flexibility in the direction those aspects of the discussion might take. [REF]\n\n\nAdjustments to the interview protocol were made according to early experience and information participants had provided (i.e. redundant questions were eliminated; questions were reworded to improve flow and clarity; additional probes were included).\n\n\nFaculty staff were then interviewed individually by a trained study investigator in a 15-minute, semi-structured interview. This sequence was repeated with other video encounters. Table 2 presents examples of interview questions. Each faculty member was interviewed by at least three interviewers over their various interviews. Interviewers were chosen based on their experience in interviewing. All were trained during a half-day meeting to interpret and deliver the interview guide in the same manner in order to elicit information of a consistent type.\n\nBack to top\n\n\n\n\n\n\n11. Data collection instruments and technologies\n\n\n\n\n\n\n\nDescribe all data collection instruments, including their development, and if/how they changed over the course of the study. Cite relevant literatures, theories or conceptual frameworks as appropriate. Consider sharing the data collection instrument(s) or a detailed description of them in the article body, supplementary material, or published elsewhere. I publish my materials on the OSF and then describe and cite them in my articles.Tim Westland - Researcher\nDescribe the use of equipment for audio or video recording, reproduction of paper documents or computer files, or other processes in data collection.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nDescribing instruments and equipment helps readers understand the full context in which data collection occurred and how this context might have affected data collection (e.g., the influence of recording devices on participants’ behaviors; the nature of inferences drawn from live vs. recorded events).\n\n\nExamples\n\nTo facilitate the discussion and to maintain consistency over different sets of discussions, key trigger questions were devised prior to the discussion. The opening trigger question was: Thinking back to some of your best clinical learning placements in 4th and 5th year. What was it about those clinical placements that provided good opportunities for learning?\n\n\nInterviews included discussion of the expectations, processes and consequences of AEE [authentic early experiences]. The interview schedule was derived following identification of questions that could not be fully answered in a systematic review of previous empirical or theoretical literature. It comprised a sequence of topic areas including experiences in action, and areas of frustration in Medical Education such as the learning of content knowledge, achieving functional knowledge, and transfer of knowledge.[REF]… Interested readers can request a copy of the schedules from the corresponding author. Interviews lasted between 20 and 90 min. and discussion groups between 60 and 90 min. All interviews and discussion groups were conducted in private rooms at the participant’s workplace—the medical school for students and faculty, and individual places of work for workplace supervisors (except for one who chose to be interviewed at the medical school). All data were audio-recorded, and transcribed verbatim.\n\nBack to top\n\n\n\n\n\n\n12. Units of study\n\n\n\n\n\n\n\nDescribe the number of participants, documents, or events included in the study (the units of study).\nDescribe characteristics of the participants, documents or events that are relevant to the study purpose and research question(s).8\nInclude the dates or timeframe for participation.\nIf the actual sample differs from the target sample, describe:\n\nthe difference,\nwhy these differences may have occurred,\nhow this might affect the findings.\n\nIf the degree of participation varied among individuals, describe:\n\nthe different levels of participation,\nthe reasons for these differences (i.e., the researchers’ choice or the participants’ preferences),\nand how these different levels of participation were taken into account in the analysis.\n\nThis information could appear in the Methods section as part of the description of the sample, or at the beginning of the Results section to provide context for the findings presented.\n\n\n\n\n\n\nMore Information, Justification, and Example\n\n\n\n\n\nJump to:\n\nHow does this item differ to Sampling (Item 8)?\nWhy readers need this information\nExample\n\n\nHow does this item differ to Sampling (Item 8)?\nThe sampling item (Item 8) describes the target or ideal participants, documents, or events selected for the study. By contrast, this item focuses on description of the actual participants, documents or events included in the study.\n\n\nWhy readers need this information\nThis helps readers know whose experiences and perspective are and are not included.\n\n\n\nExample\n\nOf the 70 Mindful Communication program participants, 46 met the eligibility requirements to participate in the in-depth interviews. We randomly chose and then contacted 22 participants, of whom 20 agreed to be interviewed within six months of completing the program: 15 in person and 5 by telephone. Two declined for lack of time. On reaching saturation after 20 interviews, no further attempts to contact the remaining 24 participants were made.\n\n\n\n\n\nThere were 31 nursing handovers covering 137 patients, and 21 resident handovers covering 101 patients included in this study.\n\nBack to top\n\n\n\n\n\n\n13. Data processing\n\n\n\n\n\n\n\nDescribe the ways in which data are prepared for analysis and managed throughout the analysis process. These activities might include transcription, coding, data entry, and organization of data. (See footnote for audio or visual recordings9).\nDescribe the processes used to organize, compile, and categorize data (e.g., field notes, transcripts, documents, photographs, artifacts) for analysis.\nIf you used transcripts, describe procedures used to check accuracy.\nDescribe procedures used to maintain data security and protect the privacy of participants, as specified in the human subjects protocols (see footnote on anonymisation10 as an example).\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript.\n\n\n\nExamples\n\nInterviews were anonymised and each participant was given a code number.\n\n\nThe interviewers and another member of the research team (H.B.) reviewed transcripts for accuracy.\n\n\nWe collected data throughout the admission process through direct observation, audio- recording, and chart extraction. We audio-recorded, transcribed, and anonymized both the overnight and morning case review discussions. We also observed the morning case review discussions in person and collected field notes. For each case review discussion, we copied the admission notes from the patient’s record and de-identified all data.\n\nBack to top\n\n\n\n\n\n\n14. Data analysis\n\n\n\n\n\n\n\nDescribe your analytic process as transparently as possible.11\nIf you used an approach that has a well-defined process for data analysis (e.g., grounded theory, discourse analysis, phenomenography):\n\ncite the guiding literature\ndescribe your processes in sufficient detail so readers can judge the extent to which your processes align with the guiding approach.\nIf you modified or deviated from the guiding approach, explain and justify these modifications.\n\nSpecify the unit of analysis.12\nExplain the rationale underlying different decisions made throughout the data analysis process to provide as much transparency as possible.13\nIf observations that contrast or deviate from identified concepts or themes were important to your analysis, describe how these discrepancies were handled during the analysis.\nIf you drew upon a theoretical perspective or framework during analysis, describe theoretical or other influences on your analysis scheme or categories if they exist. If you identified a theoretical perspective or framework early in the conception of the study or after reviewing some or all of their data, consider referring to these as sensitizing concepts to acknowledge that the approach is inductive, but with influence from relevant theory, models, or organizational schemes. Alternatively, explain that themes were developed from the data with no external influences.\nDescribe which members of the research team are involved in data analysis and what perspective(s) they bring to the analysis.\nIf software was used to assist with data analysis14, describe how it was used. Simply stating that software was used is insufficient.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nTechniques used for data analysis will depend on the paradigm, approach, and/or data collection methods selected by the researchers. Correspondingly, authors should be as transparent as possible about the analytic process so that readers can follow the logic of inquiry from the research question(s) to the analysis and findings.\n\n\nExamples\n\n…we brought sensitizing concepts to the analysis while we conducted an open, inductive analysis.[REF]In this case the sensitizing concepts arose, a priori to analysis, from a framework derived from the literature [REF] (as described above), in which participants’ motivations to act are based on principles of professionalism, internal affect, or potential implications of their actions.[REF]\n\n\nThrough an iterative process of listening, discussing, and relistening, the team identified and consensually validated emerging themes[REF] and appended segments of dialogue supporting the proposed themes. Recruitment stopped when saturation was reached (no new themes were identified). The team systematically reviewed the themes and sorted them into content domains. The team used an analytic matrix to identify patterns and connections amongst the domains. Two of us not involved in the qualitative coding process (R.E., M.K.) audited the analytic matrix, choice of quotes, and thematic analysis.\n\n\nThe analysis started after the first interview. All data were analyzed with the aid of the audio- coding facility of the NVivo 8: QSR International Pty Ltd, Doncaster, Vic, Australia programme. First, [name] and [name] coded independently from one another, making sure to stay semantically close to the participants’ wording. Then we discussed these open codes and defined axial codes.[REF] New insights about the impact of CST were written down in memos.\n\n\nVideotapes were analysed using immersion/crystallisation methods of qualitative data analysis.[REF] With no pre-existing framework developed in advance for analysis, an inductive approach was used to discover patterns of NVB in the data. A team of six researchers met weekly for 18 months to view videos together. Using a consensus-building approach based on a combination of field notes, opportunistic interviews with the participants, and repeated viewing of the same material, sometimes many months apart, we eventually achieved consensus on verbal, non-verbal, and physical themes and patterns observed in the data. Finally, as a test of goodness-of-fit, we carefully reviewed the videotapes for any deviant cases that did not fit the categories we had developed.\n\n\nAll transcripts were coded thematically by four of the five authors, who met regularly to identify areas of convergence until full agreement was reached. One of the interviewers (P.M.) maintained an audit trail to track the team’s developing thinking. A process of dialectical empiricism[REF] was used to categorise the emergent themes into more abstract concepts…\n\nBack to top\n\n\n\n\n\n\n15. Techniques to enhance trustworthiness\n\n\n\n\n\n\n\nDescribe methods used to ensure trustworthiness and credibility throughout the data collection and analysis process. (See footnote on commonly used techniques15).\nExplain your choice of techniques and why these are appropriate for the particular study.\n\n\n\n\n\n\nJustification, Examples, and Resources\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\nTraining and resources\n\n\nWhy readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript.\n\n\n\nExamples\nMember checking\n\nMember checks [REF] with an external TBL expert (R.L.) supported the validity of these analyses.\n\nTriangulation of data types and data sources\n\nThe interview data were triangulated with the data of 11 student and supervisor focus groups of a previous study, and more specifically, with those data that concern in particular the influence of CST [Communication Skills Training] on the development of patient-centredness…. Triangulation with the focus group data allowed us to broaden the in-depth information from the interviews in the analysis and to share and compare this with information from students and doctors with varying levels of CST (no, limited, full programme) and from two universities (Universities of Antwerp and Ghent). Moreover, this enabled us to better explore the evolution over time, given that the focus groups included participants at different stages of their study: before clerkships (year 4, undergraduate), during clerkships (year 6, undergraduate), after clerkships (year 7, undergraduate) and postgraduate (general practice trainees, and supervising specialists and GPs; Table 1).\n\n\n\n\n\nFinally, as a test of goodness-of-fit, we carefully reviewed the videotapes for any deviant cases that did not fit the categories we had developed.\n\nTriangulation of Researchers + Audit trail\n\nTo ensure rigor and increase authenticity in our methodology, we used two kinds of triangulation—investigator triangulation and data triangulation.[REF] We sought analytical rigor using an audit trail and multiple coders; our coding team included an experienced clinician (M.G.) as well as a nonclinician with expertise in medical communication and team dynamics (L.L.).\n\n\n\nTraining and resources\nSee Lincoln and Guba’s Evaluative Criteria for trustworthiness.\nBack to top\n\n\n\n\n\n\nResults\n\n\n16. Synthesis and interpretation\n\n\n\n\n\n\n\nDescribe the main analytic findings.16\nIn most cases, report a synthesis of the data along with specific quotes, examples, or illustrations derived from the data.\n\nI like when authors use words like many, few, or all to describe frequency.Nkinda Akaro - Researcher\nConsider describing frequency, variety, representativeness, counter-examples, concrete details, contextualization, conditions, and qualifications related to the findings.\nFrequency counts play a limited role in qualitative research, and need not be reported unless they play a meaningful role in interpretation of the data.\nIf your findings include integration with prior literature or theory and/or the development of a theory, model or meta-narrative, consider using tables and figures to communicate these findings.\nItems 16 and 18 can be reported in Results or Discussion sections.17\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript.\n\n\n\nExamples\n\nWe identified four patterns of NVB (non-verbal behavior) that relate to handover quality and have dubbed them: (1) joint focus of attention; (2) the poker hand; (3) parallel play; and (4) kerbside consultation. Each pattern constitutes a transfix, or systematic way of participating non-verbally in the care transfer process. And, although there are variations in each pattern, we have been able to code virtually every handover we have observed in nursing, medicine and surgery into one of these four categories.\n\n\nBecause our participants came from similar educational backgrounds, had studied medicine as their tertiary course, were embedded in the culture of medicine, and were associated in meaningful ways with a single medical school, we approached their transcripts with the assumption that they belonged to a loosely formed discourse community. Although their graduation dates ranged over a period of 50 years and their collective sphere of practice included 10 different specialty areas, there were many similarities in their experiences of enculturation during and after medical school.\n\n\nTheir three major (often overlapping) areas of concern were epistemic (acquiring knowledge and skill), interpersonal (relating to patients, families, colleagues and administrators) and personal (achieving work–life balance). In each of these areas, medical enculturation was achieved by two overlapping processes, absorption and assimilation, each of which may have distinct implications for postgraduate medical education.\n\nBack to top\n\n\n\n\n\n\n17. Links to empirical data\n\n\n\n\n\n\n\nProvide evidence (e.g., quotes, field notes, text excerpts, photographs) to substantiate the more general and abstract concepts or inferences they present as findings.18 I love seeing evidence like this because it really brings results to life. Nkinda Akaro - Researcher\nYou could report this evidence in a table or figure, incorporated into a narrative description of findings, as a stand-alone narrative, or in text blocks embedded in the manuscript text. If you are constrained by word limits or media limitations (e.g.. video), consider sharing data via an appendix, supplemental material, or web-based repository.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript.\n\n\n\nExamples\nSee Frankel et al. for an excellent example of how to use photographs (or snapshots from video) to illustrate and provide supporting evidence for patterns of behavior identified in the analysis. http://qualitysafety.bmj.com/content/21/Suppl_1/i121.\n\nWe identified five interruption types: (1) probing for further data, (2) prompting for expected sequence, (3) teaching around the case, (4) thinking out loud, and (5) providing direction (see Table 1). Several interruption types served both goals of the case review discussions—teaching and patient care. For example, when thinking out loud, supervisors reasoned through problems and taught the team: So that’s the big question, did she have a mechanical fall, or did she have a medicine-related fall? (Case 2). Supervisors prompted for expected sequence, preventing presenters from skipping over information while simultaneously allowing the supervisor to instruct the team on presentation style: So now you can tell me what the rest of his test results are because I haven’t heard those (Case 16).\n\nBack to top\n\n\n\n\n\n\nDiscussion\n\n\n18. Integration with prior work, implications, transferability, and contribution(s) to the field\n\n\n\n\n\n\n\nDescribe how the findings and conclusions connect to, support, elaborate on, or challenge previous findings, experiences, theory, or a guiding paradigm or approach.19\nDescribe how the findings contribute to or advance the field.\nDescribe any implications of the work, such transferability or generalizability.\n\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThe short summary reminds readers of the main findings and may help them assess whether the subsequent interpretation and implications formulated by the authors are supported by the findings.\n\n\nExamples\n\nThis study contributes to the understanding and discussion of the complexity of involving patients in healthcare education. It shows that integrating patient-led teaching into initiatives that are partly faculty-led influences the way in which students perceive learning from and with PIs. What is not known, however, is whether perceptions are also affected by type of health profession and the students’ different orientation towards logics of care and science, and issues of authority and power relations.\n\nFor complete examples of Discussions, see:\n\nHenriksen & Ringsted, 2013\nWesterman et al., 2013.\n\nBack to top\n\n\n\n\n\n\n19. Limitations\n\n\n\n\n\n\n\nDescribe problems or gaps in their efforts to ensure trustworthiness and the potential implications.20 Authors should never feel nervous to report limitations. In fact, thoughtful discussion of limitations is a hallmark of good research!Charles Ruggle - Editor\nDescribe how the chosen paradigm, approach, and methods will influence the situations to which the findings may reasonably apply.21 (See also Item 18.)\nDescribe how specific decisions or events in the conduct of the study strengthen or weaken the rigor of the findings.\n\n\n\n\n\n\nJustification and Example\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExample\n\n\nWhy readers need this information\nAll research has limitations. Discussing them will help readers consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be). If you don’t address limitations, editors and peer reviewers may ask you about them which will delay publication.\n\n\nExample\n\nThe study has several limitations. One is that the focus group interview method reveals students’ perceptions rather than their actual behaviors. Observations of the patient-led teaching encounter may have illuminated an understanding of the relationship between patient instructors and medical students. Another limitation is that the PI-led teaching is optional rather than mandatory, which may have influenced students’ attitudes in a positive direction. Moreover, students who are eager to take on extra-curricular activities may not be representative of the whole population. That only 23 out of 39 students signed up for this study might also have influenced results if the missing group of students represented other perceptions than those present in the focus groups. However the received data from the focus groups were rich in information and diverse perceptions were present. Another limitation is the overrepresentation of women over men in our sample. Even though women are also overrepresented in medical school this might potentially have influenced results, but gender differences in perceptions were nevertheless not identified in the data.\n\nBack to top\n\n\n\n\n\n\nOther\n\n\n20. Conflicts of interest\n\n\n\n\n\n\n\nAuthors shouldn’t feel nervous reporting conflicts of interest. They are very common and rarely influence our decision to publish.Charles Ruggle - Editor\nDescribe any real or potential conflicts of interest that might have influenced or could appear to have influenced the research.\nDescribe:\n\nhow these conflicts were managed in the conduct of the study,\nthe potential impact on study findings and/or conclusions.\n\nSome aspects may be mentioned as part of reflexivity (see Item 6).\n\n\n\n\n\n\nJustification\n\n\n\n\n\n\nWhy readers need this information\nMany factors, including professional and personal relationships and activities, can influence the design, conduct, and reporting of the clinical science that informs health care decision. The potential for conflict of interest exists when these relationships and activities may bias judgment (1). Many stakeholders— editors, peer reviewers, clinicians, educators, policymakers, patients, and the public—rely on the disclosure of authors’ relationships and activities to inform their assessments. Trust in the transparency, consistency, and completeness of these disclosures is essential. - ICMJE\n\n\n\n\n\n\n\n21. Funding\n\n\n\n\n\n\n\nDescribe any sources of funding and other support for the study.\nDescribe the role of funders in data collection, data analysis, and reporting if applicable."
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-title-2",
    "href": "guidelines/srqr/qscb.html#sec-title-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis allows readers to quickly identify the type of study. Your title will be indexed by search tools so descriptive words will make it easier for others to discover."
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-title-3",
    "href": "guidelines/srqr/qscb.html#sec-title-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nResidents learning from a narrative experience with dying patients: a qualitative study.\n\n\nMedical students’ perceptions of the factors influencing their academic performance: an exploratory interview study with high-achieving and re-sitting medical students.\n\n\nUndergraduate rural medical education program development: focus group consultation with the NRHA Rural Medical Educators Group."
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-title-4",
    "href": "guidelines/srqr/qscb.html#sec-title-4",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Training and Resources",
    "text": "Training and Resources\nSee this article for advice on writing titles that are interesting and descriptive.\nThe EQUATOR Network provides training for writing effective manuscript titles as part of their publication school.\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-abstract-2",
    "href": "guidelines/srqr/qscb.html#sec-abstract-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nA reader should be able to read the abstract independent of the manuscript and get a sense of the background, purpose, methods, main results and implications that will be described in greater detail in the manuscript.\nYour abstract will be indexed by search tools so descriptive words will make it easier for others to discover."
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-abstract-3",
    "href": "guidelines/srqr/qscb.html#sec-abstract-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nPurpose: Although academic centers rely on assessments from medical trainees regarding the effectiveness of their faculty as teachers, little is known about how trainees conceptualize and approach their role as assessors of their clinical supervisorsMethod: In 2010, using a constructivist grounded theory approach, five focus group interviews were conducted with 19 residents from an internal medicine residency program. A constant comparative analysis of emergent themes was conducted.Results: Residents viewed clinical teaching assessment (CTA) as a time-consuming task with little reward. They reported struggling throughout the academic year to meet their CTA obligations and described several shortcut strategies they used to reduce their burden. Rather than conceptualizing their assessments as a conduit for both formative and summative feedback, residents perceived CTA as useful for the surveillance of clinical supervisors at the extremes of the spectrum of teaching effectiveness. They put the most effort, including the crafting of written comments, into the CTAs of these outliers. Trainees desired greater transparency in the CTA process and were sceptical regarding the anonymity and perceived validity of their faculty appraisals.Conclusions: Individual and system-based factors conspire to influence postgraduate medical trainees’ motivation for generating high-quality appraisals of clinical teaching. Academic centers need to address these factors if they want to maximize the usefulness of these assessments."
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-abstract-4",
    "href": "guidelines/srqr/qscb.html#sec-abstract-4",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Training and Resources",
    "text": "Training and Resources\nThe EQUATOR Network provides training for writing effective manuscript abstracts as part of their publication school.\nSee this article from the London School of Economics for general abstract writing tips.\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-problem-formulation-2",
    "href": "guidelines/srqr/qscb.html#sec-problem-formulation-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information provides context for the research question or study purpose and situates the study in relation to other work in the field."
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-problem-formulation-3",
    "href": "guidelines/srqr/qscb.html#sec-problem-formulation-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nRegulatory focus theory may therefore offer insight into the variability in responses to feedback, but how well do these experimental findings translate to real clinical situations in which the reality of responsiveness to feedback seems frustratingly complex? …. In order to better elaborate a theory to account for this variability in learner response to feedback, the present study was undertaken. We aimed, in this study, to determine how readily clinical learning events could be classified as activating a promotion or a prevention focus, and to explore, through a careful analysis of doctors’ descriptions of their feedback experiences, the predictive value of regulatory focus theory in the context of real clinical learning situations.\n\n\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-purpose-2",
    "href": "guidelines/srqr/qscb.html#sec-purpose-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nBy clearly stating the purpose of the study, authors set readers’ expectations for the methods, findings and discussion sections of the manuscript."
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-purpose-3",
    "href": "guidelines/srqr/qscb.html#sec-purpose-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nThe purposes of this study were to investigate how medical students recognize, respond to and utilise feedback, and to determine whether there are maturational differences in understandings of the role of feedback across academic years in medical school."
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-purpose-4",
    "href": "guidelines/srqr/qscb.html#sec-purpose-4",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Training and Resources",
    "text": "Training and Resources\nSee this article for advice on writing qualitative research questions.\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-qualitative-approach-2",
    "href": "guidelines/srqr/qscb.html#sec-qualitative-approach-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nIdentifying the research paradigm helps readers understand whether the researcher assumes that there is a single, objective reality (positivist or post-positivist) and has thus designed the study to describe this reality or whether the researcher assumes multiple, subjective realities and designed the study to describe these multiple realities, with no attempt to merge or reconcile these realities (constructivist/interpretivist). The paradigm has implications for the study design, approach, methods, and techniques to ensure rigour and trustworthiness.\nSince the research paradigm does not necessarily dictate particular approaches or methods, the approach should also be clearly defined. Stating the approach provides readers the opportunity to evaluate the fidelity of the research approach to the research question(s) and consider the rationale for modifications and deviations from the selected approach. Qualitative research also includes an array of methods that can be used across paradigms and approaches. (See also Item 10)."
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-qualitative-approach-3",
    "href": "guidelines/srqr/qscb.html#sec-qualitative-approach-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nThe study was performed from a constructivist point of view using an interpretative phenomenological epistemology. Based on the notion that social phenomena are constructed by the communal making of meaning about the underlying phenomena, we aimed to construct insightful accounts of lead consultants’ approaches to educational change, rather than to identify the true nature of these approaches. Because the management of change by lead consultants is an under-researched area, we conducted an exploratory qualitative study…\n\n\nGiven the relative dearth of explanatory theories about factors affecting medical students’ emotional reactions, we chose to develop one by applying methods associated with grounded theory, specifically constant comparative analysis, to qualitative data obtained from learning logs and interviews. Our approach was constructivist, deliberately using researchers’ own experiences and acquired knowledge to enhance theoretical sensitivity and enrich theory development.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-researcher-characteristics-and-reflexivity-2",
    "href": "guidelines/srqr/qscb.html#sec-researcher-characteristics-and-reflexivity-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nIn positivist and post-positivist paradigms, personal characteristics and perspectives of researchers might be viewed as biases that limit the credibility of study findings, while in constructivist or interpretivist paradigms the characteristics and perspectives of the researchers are important contextual factors that are an accepted part of the study design, data collection, and data analysis. These characteristics and perspectives may explain how the researcher(s) obtained access to the site or participants included in the study or may add valuable insight during data analysis."
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-researcher-characteristics-and-reflexivity-3",
    "href": "guidelines/srqr/qscb.html#sec-researcher-characteristics-and-reflexivity-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nReflexivity was maintained by the research team through the analysis and writing by recording, discussing and challenging established assumptions. In addition both EH and SV kept reflexive diaries.\n\n\nThe first author conducted all interviews and discussion groups. Her own medical undergraduate training took place between 1995 and 2000. She was not known to the participants of this research prior to undertaking the study and deliberately did not undertake any clinical or teaching activities locally alongside this research. Whilst it was useful to know (from her own background) what the students were talking about medically (and in terms of detecting items of significance), as a researcher she made conscious efforts not to accept potentially common assumptions at face value.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-context-2",
    "href": "guidelines/srqr/qscb.html#sec-context-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis helps readers interpret the meaning and significance of the study findings by situating them in social, cultural, temporal and other relevant contexts."
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-context-3",
    "href": "guidelines/srqr/qscb.html#sec-context-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nWe conducted the study among hospital-based clinical teachers of students in years 4 to 6 of a six-year undergraduate medical program at Maastricht University Medical School. Years 4 to 6 are devoted to clerkships in the academic hospital and affiliated regional hospitals. Rotations differ in duration depending on the type of rotation and the discipline, and the sequence of rotations differs among students. During rotations, students spend time in the wards, the outpatient clinics, and the accident and emergency department. Clerkships in years 4 and 5 last between 4 and 10 weeks (regular clerkship), whereas students in year 6 undertake an 18-week senior clerkship in a discipline of their choice.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-sampling-strategy-2",
    "href": "guidelines/srqr/qscb.html#sec-sampling-strategy-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information helps readers understand the source of data / findings so they can consider the boundaries of the study and the relevance to their own context. It also gives readers insight into the researchers’ decisions, which can be important for critical appraisal."
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-sampling-strategy-3",
    "href": "guidelines/srqr/qscb.html#sec-sampling-strategy-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nAs students’ perceptions were previously shown to be related to gender, age, prior experience and place of attachment,[REF] we purposely selected respondents with different backgrounds. This sampling strategy led to the diversity of gender, age, prior patient experience and place of attachment shown in Table 1.\n\n\nPurposive sampling was directed towards achieving maximum variation in age and specialty, using a snowball approach (a non-probabilistic form of sampling in which persons initially chosen for the sample are used as informants to locate other persons having necessary characteristics making them eligible for the sample).[REF ]\n\n\nPotential participants were all medical students in Years 1 and 2 at the University of Toronto in 2004. Following research ethics board approval, recruitment was conducted via e-mail to class listservs. Participant responses were sent directly to the research assistant, who was unknown to participants, so that the principal investigators did not know who did or did not participate. This process was engaged to protect participants’ anonymity and to avoid any impression of coercion because the lead researcher (SG) was involved in the administration of the undergraduate curriculum at the time. Sample size was estimated to be sufficient based on the principle of theoretical saturation [REF] and our previous experience with this methodology (i.e., with a relatively homogeneous population, we expected to reach saturation with approximately 15 interviews per group). There were no exclusion criteria and we accepted the first 15 students from each class who volunteered.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-ethics-2",
    "href": "guidelines/srqr/qscb.html#sec-ethics-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nJournals will require an ethical approval statement.\nQualitative research often involves interaction between researchers and research participants. Correspondingly, researchers should ensure that participants are fully aware of their participation in a research study, the risks and benefits associated with the study, the steps and precautions the researchers will take to minimize risks, such as loss of privacy and confidentiality, and how the researchers plan to use the data."
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-ethics-3",
    "href": "guidelines/srqr/qscb.html#sec-ethics-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nEthical approval was granted by the University of Otago and student participants were invited to attend each focus group discussion by the university representatives of New Zealand Medical Students Association (NZMSA). Usually, a key concern when collecting data from students is that students may feel vulnerable when sharing their experiences with academic staff during a focus group discussion. However, this potential harm was removed as each group discussion was facilitated by a fellow student, the discussions were transcribed by a professional transcribing service, and only the primary researchers [Names] had access to the raw data.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-data-collection-methods-2",
    "href": "guidelines/srqr/qscb.html#sec-data-collection-methods-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThe study period helps readers place the study in temporal context and identify factors not mentioned by the authors that might affect findings, interpretation, and implications. (See Item 8 for ending data collection.)\n\nDescribing researcher characteristics clarifies the relationship between the individuals involved in data collection and the participants in the research and also explains what efforts were made to ensure consistency in the data collection process (See Items 6 and 15.)"
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-data-collection-methods-3",
    "href": "guidelines/srqr/qscb.html#sec-data-collection-methods-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nFurther, it was decided that group interviews, also known as focus group discussions would be the best means of data collection. This is a method of data collection that enables group members to feed off each other’s ideas and an effective moderator will maintain group focus whilst at the same time permitting flexibility in the direction those aspects of the discussion might take. [REF]\n\n\nAdjustments to the interview protocol were made according to early experience and information participants had provided (i.e. redundant questions were eliminated; questions were reworded to improve flow and clarity; additional probes were included).\n\n\nFaculty staff were then interviewed individually by a trained study investigator in a 15-minute, semi-structured interview. This sequence was repeated with other video encounters. Table 2 presents examples of interview questions. Each faculty member was interviewed by at least three interviewers over their various interviews. Interviewers were chosen based on their experience in interviewing. All were trained during a half-day meeting to interpret and deliver the interview guide in the same manner in order to elicit information of a consistent type.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-data-collection-instruments-2",
    "href": "guidelines/srqr/qscb.html#sec-data-collection-instruments-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nDescribing instruments and equipment helps readers understand the full context in which data collection occurred and how this context might have affected data collection (e.g., the influence of recording devices on participants’ behaviors; the nature of inferences drawn from live vs. recorded events)."
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-data-collection-instruments-3",
    "href": "guidelines/srqr/qscb.html#sec-data-collection-instruments-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nTo facilitate the discussion and to maintain consistency over different sets of discussions, key trigger questions were devised prior to the discussion. The opening trigger question was: Thinking back to some of your best clinical learning placements in 4th and 5th year. What was it about those clinical placements that provided good opportunities for learning?\n\n\nInterviews included discussion of the expectations, processes and consequences of AEE [authentic early experiences]. The interview schedule was derived following identification of questions that could not be fully answered in a systematic review of previous empirical or theoretical literature. It comprised a sequence of topic areas including experiences in action, and areas of frustration in Medical Education such as the learning of content knowledge, achieving functional knowledge, and transfer of knowledge.[REF]… Interested readers can request a copy of the schedules from the corresponding author. Interviews lasted between 20 and 90 min. and discussion groups between 60 and 90 min. All interviews and discussion groups were conducted in private rooms at the participant’s workplace—the medical school for students and faculty, and individual places of work for workplace supervisors (except for one who chose to be interviewed at the medical school). All data were audio-recorded, and transcribed verbatim.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-units-of-study-2",
    "href": "guidelines/srqr/qscb.html#sec-units-of-study-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "How does this item differ to Sampling (Item 8)?",
    "text": "How does this item differ to Sampling (Item 8)?\nThe sampling item (Item 8) describes the target or ideal participants, documents, or events selected for the study. By contrast, this item focuses on description of the actual participants, documents or events included in the study."
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-units-of-study-3",
    "href": "guidelines/srqr/qscb.html#sec-units-of-study-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis helps readers know whose experiences and perspective are and are not included."
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-units-of-study-4",
    "href": "guidelines/srqr/qscb.html#sec-units-of-study-4",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nOf the 70 Mindful Communication program participants, 46 met the eligibility requirements to participate in the in-depth interviews. We randomly chose and then contacted 22 participants, of whom 20 agreed to be interviewed within six months of completing the program: 15 in person and 5 by telephone. Two declined for lack of time. On reaching saturation after 20 interviews, no further attempts to contact the remaining 24 participants were made.\n\n\n\n\n\nThere were 31 nursing handovers covering 137 patients, and 21 resident handovers covering 101 patients included in this study.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-data-processing-2",
    "href": "guidelines/srqr/qscb.html#sec-data-processing-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript."
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-data-processing-3",
    "href": "guidelines/srqr/qscb.html#sec-data-processing-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nInterviews were anonymised and each participant was given a code number.\n\n\nThe interviewers and another member of the research team (H.B.) reviewed transcripts for accuracy.\n\n\nWe collected data throughout the admission process through direct observation, audio- recording, and chart extraction. We audio-recorded, transcribed, and anonymized both the overnight and morning case review discussions. We also observed the morning case review discussions in person and collected field notes. For each case review discussion, we copied the admission notes from the patient’s record and de-identified all data.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-data-analysis-2",
    "href": "guidelines/srqr/qscb.html#sec-data-analysis-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nTechniques used for data analysis will depend on the paradigm, approach, and/or data collection methods selected by the researchers. Correspondingly, authors should be as transparent as possible about the analytic process so that readers can follow the logic of inquiry from the research question(s) to the analysis and findings."
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-data-analysis-3",
    "href": "guidelines/srqr/qscb.html#sec-data-analysis-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\n…we brought sensitizing concepts to the analysis while we conducted an open, inductive analysis.[REF]In this case the sensitizing concepts arose, a priori to analysis, from a framework derived from the literature [REF] (as described above), in which participants’ motivations to act are based on principles of professionalism, internal affect, or potential implications of their actions.[REF]\n\n\nThrough an iterative process of listening, discussing, and relistening, the team identified and consensually validated emerging themes[REF] and appended segments of dialogue supporting the proposed themes. Recruitment stopped when saturation was reached (no new themes were identified). The team systematically reviewed the themes and sorted them into content domains. The team used an analytic matrix to identify patterns and connections amongst the domains. Two of us not involved in the qualitative coding process (R.E., M.K.) audited the analytic matrix, choice of quotes, and thematic analysis.\n\n\nThe analysis started after the first interview. All data were analyzed with the aid of the audio- coding facility of the NVivo 8: QSR International Pty Ltd, Doncaster, Vic, Australia programme. First, [name] and [name] coded independently from one another, making sure to stay semantically close to the participants’ wording. Then we discussed these open codes and defined axial codes.[REF] New insights about the impact of CST were written down in memos.\n\n\nVideotapes were analysed using immersion/crystallisation methods of qualitative data analysis.[REF] With no pre-existing framework developed in advance for analysis, an inductive approach was used to discover patterns of NVB in the data. A team of six researchers met weekly for 18 months to view videos together. Using a consensus-building approach based on a combination of field notes, opportunistic interviews with the participants, and repeated viewing of the same material, sometimes many months apart, we eventually achieved consensus on verbal, non-verbal, and physical themes and patterns observed in the data. Finally, as a test of goodness-of-fit, we carefully reviewed the videotapes for any deviant cases that did not fit the categories we had developed.\n\n\nAll transcripts were coded thematically by four of the five authors, who met regularly to identify areas of convergence until full agreement was reached. One of the interviewers (P.M.) maintained an audit trail to track the team’s developing thinking. A process of dialectical empiricism[REF] was used to categorise the emergent themes into more abstract concepts…\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-trustworthiness-2",
    "href": "guidelines/srqr/qscb.html#sec-trustworthiness-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript."
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-trustworthiness-3",
    "href": "guidelines/srqr/qscb.html#sec-trustworthiness-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\nMember checking\n\nMember checks [REF] with an external TBL expert (R.L.) supported the validity of these analyses.\n\nTriangulation of data types and data sources\n\nThe interview data were triangulated with the data of 11 student and supervisor focus groups of a previous study, and more specifically, with those data that concern in particular the influence of CST [Communication Skills Training] on the development of patient-centredness…. Triangulation with the focus group data allowed us to broaden the in-depth information from the interviews in the analysis and to share and compare this with information from students and doctors with varying levels of CST (no, limited, full programme) and from two universities (Universities of Antwerp and Ghent). Moreover, this enabled us to better explore the evolution over time, given that the focus groups included participants at different stages of their study: before clerkships (year 4, undergraduate), during clerkships (year 6, undergraduate), after clerkships (year 7, undergraduate) and postgraduate (general practice trainees, and supervising specialists and GPs; Table 1).\n\n\n\n\n\nFinally, as a test of goodness-of-fit, we carefully reviewed the videotapes for any deviant cases that did not fit the categories we had developed.\n\nTriangulation of Researchers + Audit trail\n\nTo ensure rigor and increase authenticity in our methodology, we used two kinds of triangulation—investigator triangulation and data triangulation.[REF] We sought analytical rigor using an audit trail and multiple coders; our coding team included an experienced clinician (M.G.) as well as a nonclinician with expertise in medical communication and team dynamics (L.L.)."
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-trustworthiness-4",
    "href": "guidelines/srqr/qscb.html#sec-trustworthiness-4",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Training and resources",
    "text": "Training and resources\nSee Lincoln and Guba’s Evaluative Criteria for trustworthiness.\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-synthesis-and-interpretation-2",
    "href": "guidelines/srqr/qscb.html#sec-synthesis-and-interpretation-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript."
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-synthesis-and-interpretation-3",
    "href": "guidelines/srqr/qscb.html#sec-synthesis-and-interpretation-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nWe identified four patterns of NVB (non-verbal behavior) that relate to handover quality and have dubbed them: (1) joint focus of attention; (2) the poker hand; (3) parallel play; and (4) kerbside consultation. Each pattern constitutes a transfix, or systematic way of participating non-verbally in the care transfer process. And, although there are variations in each pattern, we have been able to code virtually every handover we have observed in nursing, medicine and surgery into one of these four categories.\n\n\nBecause our participants came from similar educational backgrounds, had studied medicine as their tertiary course, were embedded in the culture of medicine, and were associated in meaningful ways with a single medical school, we approached their transcripts with the assumption that they belonged to a loosely formed discourse community. Although their graduation dates ranged over a period of 50 years and their collective sphere of practice included 10 different specialty areas, there were many similarities in their experiences of enculturation during and after medical school.\n\n\nTheir three major (often overlapping) areas of concern were epistemic (acquiring knowledge and skill), interpersonal (relating to patients, families, colleagues and administrators) and personal (achieving work–life balance). In each of these areas, medical enculturation was achieved by two overlapping processes, absorption and assimilation, each of which may have distinct implications for postgraduate medical education.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-links-to-empirical-data-2",
    "href": "guidelines/srqr/qscb.html#sec-links-to-empirical-data-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript."
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-links-to-empirical-data-3",
    "href": "guidelines/srqr/qscb.html#sec-links-to-empirical-data-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\nSee Frankel et al. for an excellent example of how to use photographs (or snapshots from video) to illustrate and provide supporting evidence for patterns of behavior identified in the analysis. http://qualitysafety.bmj.com/content/21/Suppl_1/i121.\n\nWe identified five interruption types: (1) probing for further data, (2) prompting for expected sequence, (3) teaching around the case, (4) thinking out loud, and (5) providing direction (see Table 1). Several interruption types served both goals of the case review discussions—teaching and patient care. For example, when thinking out loud, supervisors reasoned through problems and taught the team: So that’s the big question, did she have a mechanical fall, or did she have a medicine-related fall? (Case 2). Supervisors prompted for expected sequence, preventing presenters from skipping over information while simultaneously allowing the supervisor to instruct the team on presentation style: So now you can tell me what the rest of his test results are because I haven’t heard those (Case 16).\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-integration-with-prior-work-2",
    "href": "guidelines/srqr/qscb.html#sec-integration-with-prior-work-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThe short summary reminds readers of the main findings and may help them assess whether the subsequent interpretation and implications formulated by the authors are supported by the findings."
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-integration-with-prior-work-3",
    "href": "guidelines/srqr/qscb.html#sec-integration-with-prior-work-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nThis study contributes to the understanding and discussion of the complexity of involving patients in healthcare education. It shows that integrating patient-led teaching into initiatives that are partly faculty-led influences the way in which students perceive learning from and with PIs. What is not known, however, is whether perceptions are also affected by type of health profession and the students’ different orientation towards logics of care and science, and issues of authority and power relations.\n\nFor complete examples of Discussions, see:\n\nHenriksen & Ringsted, 2013\nWesterman et al., 2013.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-limitations-2",
    "href": "guidelines/srqr/qscb.html#sec-limitations-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nAll research has limitations. Discussing them will help readers consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be). If you don’t address limitations, editors and peer reviewers may ask you about them which will delay publication."
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-limitations-3",
    "href": "guidelines/srqr/qscb.html#sec-limitations-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nThe study has several limitations. One is that the focus group interview method reveals students’ perceptions rather than their actual behaviors. Observations of the patient-led teaching encounter may have illuminated an understanding of the relationship between patient instructors and medical students. Another limitation is that the PI-led teaching is optional rather than mandatory, which may have influenced students’ attitudes in a positive direction. Moreover, students who are eager to take on extra-curricular activities may not be representative of the whole population. That only 23 out of 39 students signed up for this study might also have influenced results if the missing group of students represented other perceptions than those present in the focus groups. However the received data from the focus groups were rich in information and diverse perceptions were present. Another limitation is the overrepresentation of women over men in our sample. Even though women are also overrepresented in medical school this might potentially have influenced results, but gender differences in perceptions were nevertheless not identified in the data.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscb.html#sec-conflicts-of-interest-2",
    "href": "guidelines/srqr/qscb.html#sec-conflicts-of-interest-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nMany factors, including professional and personal relationships and activities, can influence the design, conduct, and reporting of the clinical science that informs health care decision. The potential for conflict of interest exists when these relationships and activities may bias judgment (1). Many stakeholders— editors, peer reviewers, clinicians, educators, policymakers, patients, and the public—rely on the disclosure of authors’ relationships and activities to inform their assessments. Trust in the transparency, consistency, and completeness of these disclosures is essential. - ICMJE"
  },
  {
    "objectID": "guidelines/srqr/qscb.html#ready-to-get-started",
    "href": "guidelines/srqr/qscb.html#ready-to-get-started",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Ready to get started?",
    "text": "Ready to get started?\nDownload resources"
  },
  {
    "objectID": "guidelines/srqr/qscb.html#how-to-cite",
    "href": "guidelines/srqr/qscb.html#how-to-cite",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "How to cite",
    "text": "How to cite\nFor attribution, please cite this guideline as:\nBridget O’Brien, Ilene Harris, Thomas Beckman, Darcy Reed, and David Cook. 2023. “The SRQR Guidelines for Writing Qualitative Research Articles version 1.1.” The EQUATOR Network Guideline Dissemination Platform. https://doi.org/10.1234/equator/1010101.\nYou can use your reference manager to save citation information for this webpage, or copy the BibTeX below."
  },
  {
    "objectID": "guidelines/srqr/qscb.html#faqs",
    "href": "guidelines/srqr/qscb.html#faqs",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "FAQs",
    "text": "FAQs\n\nWho made this guideline?\n\nBridget O’Brien, PhD has been a faculty member in the Department of Medicine, Division of General Internal Medicine, since 2008. She is a professor of medicine and an education scientist in the Office of Medical Education’s Center for Faculty Educators. As co-director of the Teaching Scholars Program and the UCSF-University of Utrecht Health Professions Education doctoral program she teaches and mentors faculty and learners interested in education research and scholarship. At the San Francisco VA, she directs the Advanced Fellowship in Health Professions Education Evaluation and Research. In 2015 she was selected as one of five national Macy Faculty Scholars supported by the Josiah Macy Jr. Foundation and in 2021 she was selected as a KIPRIME Fellow at the Karolinska Instituet. She is a deputy editor for the journal Academic Medicine.\nDr. Ilene Harris, deceased, was professor and head, Department of Medical Education, University of Illinois at Chicago College of Medicine, Chicago, Illinois.\nDr. Thomas Beckman is professor of medicine and medical education, Department of Medicine, Mayo Clinic College of Medicine, Rochester, Minnesota.\nDr. Darcy Reed is associate professor of medicine and medical education, Department of Medicine, Mayo Clinic College of Medicine, Rochester, Minnesota.\nDr. David Cook is associate director, Mayo Clinic Online Learning, research chair, Mayo Multidisciplinary Simulation Center, and professor of medicine and medical education, Mayo Clinic College of Medicine, Rochester, Minnesota.\n\n\nHow was this guideline made?\nThe developers synthesised 40 sets of recommendations previously proposed by experts in qualitative methods. You can read about their development process here.\n\n\nWhat to do if asked to remove guideline related content\nIf a colleague or reviewer asks you to remove content that is related to this guideline, you can direct them to this guideline and the explanation for why that item is important. If they insist, consider moving the item to a supplement, table or figure.\n\n\nWhere can I get general writing training?\nThe EQUATOR Network provides in-person training for writing research articles.\nAuthorAID have resources, an online course, and mentoring to help authors.\n\n\n\nResearch paradigm\n\n\n\n\n\n\nThe set of beliefs and assumptions that guide the research process. These commonly include positivist, post-positivist, constructivist or interpretivist, and critical theory. Qualitative research generally draws from a post-positivist or constructivist/interpretivist paradigm.”\n\n\n\n\n\n\nInstruments\n\n\n\n\n\n\nData collection instruments include (but are not limited to) interview or focus group guides, observational protocols and prompts for field notes, and data extraction or coding protocols for selection and analysis of documents, photographs, videos, or other artifacts”\n\n\n\n\n\n\nBias\n\n\n\n\n\n\nA term drawn from quantitative research, bias technically means a systematic error, where a particular research finding deviates from a true finding. This might come about through errors in the manner of interviewing, or by errors in sampling. In qualitative research this is a problematic concept, since by definition the qualitative researcher is part of the process, and all researchers are different. This human factor has been said to be both the greatest strength and the greatest weakness of qualitative method. What can be done in commercial qualitative research, however, is to minimise obvious and avoidable sources of bias, for example by not confining all the fieldwork to one social group or geographic location, by taking steps to recognise the personal views of the researcher, (using techniques such as bracketing), and by working in teams.\n\n\n\n\n\n\nSampling strategy\n\n\n\n\n\n\nSeveral sampling strategies are commonly used in qualitative research, although most fall under the umbrella of purposeful (or purposive) sampling.\nPurposeful sampling means that participants, documents, or events are selected for their relevance to the research question, based on guiding theory or experiences and assumptions of the researchers. Over the course of the research process, researchers may determine that additional or different participants, documents, or events should be included to address the research question.\nOther sampling techniques, such as theoretical sampling (seeking examples of theoretical constructs), snowball sampling (using study participants to identify additional participants who meet study criteria), and convenience sampling (including any volunteers with no or minimal criteria for inclusion) may be appropriate depending on the question and approach, so long as the authors provide explanation and justification.\n\n\n\n\n\n\nApproach\n\n\n\n\n\n\nA qualitative approach is a general way of thinking about conducting qualitative research. It describes, either explicitly or implicitly, the purpose of the qualitative research, the role of the researcher(s), the stages of research, and the method of data analysis. Commonly used approaches include ethnography, grounded theory, case study, phenomenology, and narrative research.\n\n\n\n\n\n\nData collection methods\n\n\n\n\n\n\nData collection methods include (but are not limited to) interviews, focus groups, observations (direct or indirect via video), and review of written text, photographs, and other documents or materials.\n\n\n\n\n\n\nIterative\n\n\n\n\n\n\nQualitative research often occurs as an iterative process, meaning that researchers begin data analysis before they complete data collection. The data collection and analysis process may occur in phases or stages. As part of an iterative collection-analysis process, researchers will often alter their data collection methods to explore their preliminary impressions in greater depth and/or actively pursue confirming and disconfirming perspectives.\n\n\n\n\n\n\nStudy period\n\n\n\n\n\n\nThe start and end dates for data collection and analysis.\n\n\n\n\n\n\nEthnography\n\n\n\n\n\n\nThe scientific description of peoples and cultures with their customs, habits, and mutual differences.\nRead more\n\n\n\n\n\n\nGrounded theory\n\n\n\n\n\n\nA method consisting of a set of systematic, but flexible, guidelines for conducting inductive qualitative inquiry aimed toward theory construction. This method focuses squarely on the analytic phases of research, although both data collection and analysis inform and shape each other and are conducted in tandem.\nRead more\n\n\n\n\n\n\nDegree of participation\n\n\n\n\n\n\nFor example, if some participants were observed and interviewed and others only interviewed, or if some participants completed multiple interviews and others completed a single interview, these variations should be explained.\n\n\n\n\n\n\nUnit of analysis\n\n\n\n\n\n\nIn qualitative research, the unit of analysis is not necessarily the same as the unit of sampling (e.g., individual participants or events). Instead, some approaches use specific events as the unit of analysis, such as mentions of a particular topic or experience, or observations of a particular behavior or phenomenon, while others use groups rather than individual group participants. This specification has implications for how the data are organized and analyzed as well as the inferences drawn from the data.\n\n\n\n\n\n\nReflexivity\n\n\n\n\n\n\nReflexivity refers to intentional, systematic consideration of the potential or actual effects of the researcher(s) on all aspects of the study process.\n\n\n\n\n\n\nTransferability\n\n\n\n\n\n\nThe transferability of a research finding is the extent to which it can be applied in other contexts and studies. It is thus equivalent to or a replacement for the terms generalizability and external validity.\n\n\n\n\n\n\nGeneralizability\n\n\n\n\n\n\nThe appropriate scope for generalization of the findings beyond the study (e.g., to other settings, populations, time periods, circumstances)\n\n\n\n\n\n\nAnalytic findings\n\n\n\n\n\n\nAnalytic findings may include interpretations, inferences, narratives, themes, and models.\n\n\n\n\n\n\nFrequency counts\n\n\n\n\n\n\nThe frequency of specific themes or codes."
  },
  {
    "objectID": "guidelines/srqr/qscc.html",
    "href": "guidelines/srqr/qscc.html",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "",
    "text": "Full Title: Standards for Reporting Qualitative Research\nAuthors: Bridget O’Brien, Ilene Harris, Thomas Beckman, Darcy Reed, David Cook\nVersion: 1.1\nDOI: 10.1234/equator/1010101"
  },
  {
    "objectID": "guidelines/srqr/qscc.html#translations",
    "href": "guidelines/srqr/qscc.html#translations",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Translations",
    "text": "Translations\nThis guideline is also available in French."
  },
  {
    "objectID": "guidelines/srqr/qscc.html#about-this-guideline",
    "href": "guidelines/srqr/qscc.html#about-this-guideline",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "About this guideline",
    "text": "About this guideline\n\nWhen to use this guideline\n\nUse this guideline for writing qualitative research articles. You can use it when describing all kinds of qualitative approaches, methods, and designs.\n\n\nYou can also use this guideline for:\n\nwriting proposals or protocols (use the items within the Introduction and Method sections).\nreviewing the reporting of an article, but not for appraising its quality.\n\n\n\n\n\n\n\n\n\nDo not use this guidance for:\n\nwriting a qualitative evidence synthesis, use ENTREQ instead.\nappraising the quality of qualitative research, use an appraisal tool like CASP-Qual instead. \n\n\n\n\n\n\n\n\n\n\nRelated reporting guidelines:\n\nJARS Qual for writing qualitative, psychology manuscripts\nENTREQ for writing qualitative evidence syntheses\nFor writing studies involving interviews or focus groups, you can use this guideline or COREQ. \n\nFor appraising research consider:\n\nCASP-Qual\n\n\n\n\n\n\nHow to use this guideline\nReporting guidelines help me structure my drafts and develop as a researcher. I use them when teaching and hope that my students continue to use them. Manuel Silva - Researcher\nReporting guidelines are best used early. Read the full guideline below or use a tool when working:\n\nUse a template to draft manuscripts and avoid the blank page \nUse a log book to plan and record information whilst conducting research\nUse a checklist to double-check and reassure others that your article describes all the important details of your work\n\n\n\n\nFor drafting\n\nTemplate\n\n\n\nFor planning\n\nLog book\n\n\n\nFor checking\n\nChecklist\n\n\nThis reporting guideline does not prescribe a rigid format or standardized content. Consider each item and prioritize elements that are most relevant to your study, findings, context, and readers.\nYou may prefer to report an item in a different order, section, or in a table or figure. If you feel confident that an item is less important to your study, you could report it in an appendix or supplement. If you think an item is not applicable, state why. You don’t need to write your article in the order the items are presented.\nIn your methods section, state that you used the SRQR guideline when writing your article and cite it (see How to cite).\n\n\nWhy use this guideline\nFollowing this guideline will make it easier for other researchers to find, use, and synthesise your work. It won’t tell you how to do qualitative research, but it will help you explain your choices so others can understand what you did, why you did it, and what you found. At our journal, we ensure all qualitative articles follow SRQR as this makes them easier for readers to understand and faster to publish.Fiona Ludlow - Editor in Chief\nIt was written by experts from the qualitative research community, using a thorough development process. SRQR is endorsed by hundreds of journals, so using this guideline (and its checklist) will help you meet editors’ expectations."
  },
  {
    "objectID": "guidelines/srqr/qscc.html#guidance",
    "href": "guidelines/srqr/qscc.html#guidance",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Guidance",
    "text": "Guidance\nApprox. 17 min read\n\n\n1. Title\n\n\n\n\n\n\n\nDescribe the nature and topic of the study.\nIdentify the study as qualitative or indicate the approach or data collection methods.\n\n\n\n\n\n\nJustification, Examples, and Resources\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\nTraining and Resources\n\n\nWhy readers need this information\nThis allows readers to quickly identify the type of study. Your title will be indexed by search tools so descriptive words will make it easier for others to discover.\n\n\nExamples\n\nResidents learning from a narrative experience with dying patients: a qualitative study.\n\n\nMedical students’ perceptions of the factors influencing their academic performance: an exploratory interview study with high-achieving and re-sitting medical students.\n\n\nUndergraduate rural medical education program development: focus group consultation with the NRHA Rural Medical Educators Group.\n\n\n\n\nTraining and Resources\nSee this article for advice on writing titles that are interesting and descriptive.\nThe EQUATOR Network provides training for writing effective manuscript titles as part of their publication school.\nBack to top\n\n\n\n\n\n\n2. Abstract\n\n\n\n\n\n\n\nInformative key words in the title and abstract make manuscripts easier to find.Lorna Ellis - Evidence Synthesiser\nSummarise key elements of the study including:\n\nbackground about the problem or phenomenon of interest\ndescription of the study purpose or research question\nmethods, including the approach or perspective (e.g., general inductive, grounded theory), context (setting, time period), sample (number and key characteristics of participants, events, documents), data collection strategies (e g., observation, interview, focus group) and data analysis techniques\ndescription of main findings (e.g., themes or inferences) related to the study purpose and/or research question\nstudy implications\n\n\nInformation presented in the abstract should be consistent with the information presented in the full text.\nUse the format of your intended journal. 1\n\n\n\n\n\n\nJustification, Example, and Resources\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExample\nTraining and Resources\n\n\nWhy readers need this information\nA reader should be able to read the abstract independent of the manuscript and get a sense of the background, purpose, methods, main results and implications that will be described in greater detail in the manuscript.\nYour abstract will be indexed by search tools so descriptive words will make it easier for others to discover.\n\n\n\nExample\n\nPurpose: Although academic centers rely on assessments from medical trainees regarding the effectiveness of their faculty as teachers, little is known about how trainees conceptualize and approach their role as assessors of their clinical supervisorsMethod: In 2010, using a constructivist grounded theory approach, five focus group interviews were conducted with 19 residents from an internal medicine residency program. A constant comparative analysis of emergent themes was conducted.Results: Residents viewed clinical teaching assessment (CTA) as a time-consuming task with little reward. They reported struggling throughout the academic year to meet their CTA obligations and described several shortcut strategies they used to reduce their burden. Rather than conceptualizing their assessments as a conduit for both formative and summative feedback, residents perceived CTA as useful for the surveillance of clinical supervisors at the extremes of the spectrum of teaching effectiveness. They put the most effort, including the crafting of written comments, into the CTAs of these outliers. Trainees desired greater transparency in the CTA process and were sceptical regarding the anonymity and perceived validity of their faculty appraisals.Conclusions: Individual and system-based factors conspire to influence postgraduate medical trainees’ motivation for generating high-quality appraisals of clinical teaching. Academic centers need to address these factors if they want to maximize the usefulness of these assessments.\n\n\n\n\nTraining and Resources\nThe EQUATOR Network provides training for writing effective manuscript abstracts as part of their publication school.\nSee this article from the London School of Economics for general abstract writing tips.\nBack to top\n\n\n\n\n\nIntroduction\n\n\n3. Problem Formulation\n\n\n\n\n\n\n\nWhen the problem is described clearly I find it much easier to understand why a study was done, even if its from a different subject area, country, or decade.Nkinda Akaro - Researcher\nDescribe the theoretical and/or practical issues or concerns that make the study necessary, including:\n\nan overview of what is known about the problem\ngaps in current knowledge (the problem statement)\nthe scope of the research problem or phenomena addressed in the study (what will and will not be included)\ntheoretical and/or empirical work directly relevant to the problem or phenomena studied\nthe need for a qualitative approach.2\n\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExample\n\n\nWhy readers need this information\nThis information provides context for the research question or study purpose and situates the study in relation to other work in the field.\n\n\nExample\n\nRegulatory focus theory may therefore offer insight into the variability in responses to feedback, but how well do these experimental findings translate to real clinical situations in which the reality of responsiveness to feedback seems frustratingly complex? …. In order to better elaborate a theory to account for this variability in learner response to feedback, the present study was undertaken. We aimed, in this study, to determine how readily clinical learning events could be classified as activating a promotion or a prevention focus, and to explore, through a careful analysis of doctors’ descriptions of their feedback experiences, the predictive value of regulatory focus theory in the context of real clinical learning situations.\n\n\n\nBack to top\n\n\n\n\n\n\n4. Purpose or research question\n\n\n\n\n\n\n\nA clear purpose frames the article. Rahilah Zayn - Researcher\nInclude a statement of study intent. This can be framed as one or more research questions, purposes, goals, or objectives.3\n\n\n\n\n\n\nJustification, Example, and Resources\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExample\nTraining and Resources\n\n\nWhy readers need this information\nBy clearly stating the purpose of the study, authors set readers’ expectations for the methods, findings and discussion sections of the manuscript.\n\n\nExample\n\nThe purposes of this study were to investigate how medical students recognize, respond to and utilise feedback, and to determine whether there are maturational differences in understandings of the role of feedback across academic years in medical school.\n\n\n\nTraining and Resources\nSee this article for advice on writing qualitative research questions.\nBack to top\n\n\n\n\n\n\nMethods\n\n\n5. Qualitative approach and research paradigm\n\n\n\n\n\n\n\nDescribe your qualitative approach, your guiding theory (if appropriate), and identify the research paradigm.4 I like it when authors describe terms in their own words as well as using the proper terms.Rahilah Zayn - Researcher\nExplain why the selected approach is appropriate for the research question.\nProvide references to theories or traditions that guide the use of the approach as needed.\nI hadn’t heard the term approach before writing-up my study. I realise now that I did have an approach and that it might be different to other researcher’s approaches, so I need to describe things that I felt were obvious.Tim Westland - Researcher\n\nIf you don’t know what your approach or paradigm was, or you don’t think you had one, it’s OK to reflect on this after collecting data and you should still report it. Read this list to see what best describes your work.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nIdentifying the research paradigm helps readers understand whether the researcher assumes that there is a single, objective reality (positivist or post-positivist) and has thus designed the study to describe this reality or whether the researcher assumes multiple, subjective realities and designed the study to describe these multiple realities, with no attempt to merge or reconcile these realities (constructivist/interpretivist). The paradigm has implications for the study design, approach, methods, and techniques to ensure rigour and trustworthiness.\nSince the research paradigm does not necessarily dictate particular approaches or methods, the approach should also be clearly defined. Stating the approach provides readers the opportunity to evaluate the fidelity of the research approach to the research question(s) and consider the rationale for modifications and deviations from the selected approach. Qualitative research also includes an array of methods that can be used across paradigms and approaches. (See also Item 10).\n\n\nExamples\n\nThe study was performed from a constructivist point of view using an interpretative phenomenological epistemology. Based on the notion that social phenomena are constructed by the communal making of meaning about the underlying phenomena, we aimed to construct insightful accounts of lead consultants’ approaches to educational change, rather than to identify the true nature of these approaches. Because the management of change by lead consultants is an under-researched area, we conducted an exploratory qualitative study…\n\n\nGiven the relative dearth of explanatory theories about factors affecting medical students’ emotional reactions, we chose to develop one by applying methods associated with grounded theory, specifically constant comparative analysis, to qualitative data obtained from learning logs and interviews. Our approach was constructivist, deliberately using researchers’ own experiences and acquired knowledge to enhance theoretical sensitivity and enrich theory development.\n\nBack to top\n\n\n\n\n\n\n6. Researcher characteristics and reflexivity\n\n\n\n\n\n\n\nDescribe how roles and identities of research team members influence choice of research approach, data collection, and data analysis.5\nComing from quantitative research it felt strange to include such personal descriptions of the team. But I came to realise why it was useful to others, and that peer reviewers would expect it. Tim Westland - Researcher\nDescribe the perspectives, assumptions, prior knowledge, preliminary hypotheses, and/or motives (the stance) of the members of the research team.\nDescribe the researchers’ relationships to participants in the study and what decisions were made in light of these relationships.6\nIf your research was observational (e.g., ethnography), describe the role of the researcher along a spectrum from passive observer (no involvement in the activity studied) to participant-observer (ranging from some limited involvement in the activity to full involvement).\nThere is no expectation that the study could be precisely replicated; these characteristics and perspectives of the researcher should not be mentioned in the limitation section. (See also Item 14: Data Analysis)\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nIn positivist and post-positivist paradigms, personal characteristics and perspectives of researchers might be viewed as biases that limit the credibility of study findings, while in constructivist or interpretivist paradigms the characteristics and perspectives of the researchers are important contextual factors that are an accepted part of the study design, data collection, and data analysis. These characteristics and perspectives may explain how the researcher(s) obtained access to the site or participants included in the study or may add valuable insight during data analysis.\n\n\nExamples\n\nReflexivity was maintained by the research team through the analysis and writing by recording, discussing and challenging established assumptions. In addition both EH and SV kept reflexive diaries.\n\n\nThe first author conducted all interviews and discussion groups. Her own medical undergraduate training took place between 1995 and 2000. She was not known to the participants of this research prior to undertaking the study and deliberately did not undertake any clinical or teaching activities locally alongside this research. Whilst it was useful to know (from her own background) what the students were talking about medically (and in terms of detecting items of significance), as a researcher she made conscious efforts not to accept potentially common assumptions at face value.\n\nBack to top\n\n\n\n\n\n\n7. Context\n\n\n\n\n\n\n\nDescribe the setting/site(s) in which the study was conducted, the reason(s) why the setting/site(s) was selected, and the salient cultural, political, historical, economic and/or other external factors that influence the study.\nWhen synthesising evidence, context helps me work out how and why different studies and findings fit together (or don’t)Lorna Ellis - Evidence Synthesiser\nAdditional context may be reported with findings (i.e., the Results section) to add evidence for interpretations and to enhance discussion of transferability.\n\n\n\n\n\n\nJustification and Example\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExample\n\n\nWhy readers need this information\nThis helps readers interpret the meaning and significance of the study findings by situating them in social, cultural, temporal and other relevant contexts.\n\n\nExample\n\nWe conducted the study among hospital-based clinical teachers of students in years 4 to 6 of a six-year undergraduate medical program at Maastricht University Medical School. Years 4 to 6 are devoted to clerkships in the academic hospital and affiliated regional hospitals. Rotations differ in duration depending on the type of rotation and the discipline, and the sequence of rotations differs among students. During rotations, students spend time in the wards, the outpatient clinics, and the accident and emergency department. Clerkships in years 4 and 5 last between 4 and 10 weeks (regular clerkship), whereas students in year 6 undertake an 18-week senior clerkship in a discipline of their choice.\n\nBack to top\n\n\n\n\n\n\n8. Sampling strategy\n\n\n\n\n\n\n\nDescribe how and why research participants, sites, documents/artifacts, and/or events were selected for inclusion in (and, if appropriate, exclusion from) the study, along with a justification for this strategy.\nDescribe the sampling strategy rather than simply labeling it (e.g., purposive or snowball), since such labels do not have a universally accepted definition and, more importantly, procedures tend to be study- specific.\n\nDescribe how you established the final sample size:\n\nIf you used a flexible sampling strategy, then explain the criteria used to decide when no further sampling was necessary.\nIf data collection ended once saturation or sufficiency had been reached, then describe the specific criteria used to define saturation or sufficiency.\n\n\n\nDescribe procedures used to recruit participants, including:\n\nwho was involved in recruitment\nwhat their relationship was to participants\nhow and when recruitment occurred\nwhy these procedures were selected. (See also Item 6: Researcher characteristics and reflexivity)\n\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThis information helps readers understand the source of data / findings so they can consider the boundaries of the study and the relevance to their own context. It also gives readers insight into the researchers’ decisions, which can be important for critical appraisal.\n\n\nExamples\n\nAs students’ perceptions were previously shown to be related to gender, age, prior experience and place of attachment,[REF] we purposely selected respondents with different backgrounds. This sampling strategy led to the diversity of gender, age, prior patient experience and place of attachment shown in Table 1.\n\n\nPurposive sampling was directed towards achieving maximum variation in age and specialty, using a snowball approach (a non-probabilistic form of sampling in which persons initially chosen for the sample are used as informants to locate other persons having necessary characteristics making them eligible for the sample).[REF ]\n\n\nPotential participants were all medical students in Years 1 and 2 at the University of Toronto in 2004. Following research ethics board approval, recruitment was conducted via e-mail to class listservs. Participant responses were sent directly to the research assistant, who was unknown to participants, so that the principal investigators did not know who did or did not participate. This process was engaged to protect participants’ anonymity and to avoid any impression of coercion because the lead researcher (SG) was involved in the administration of the undergraduate curriculum at the time. Sample size was estimated to be sufficient based on the principle of theoretical saturation [REF] and our previous experience with this methodology (i.e., with a relatively homogeneous population, we expected to reach saturation with approximately 15 interviews per group). There were no exclusion criteria and we accepted the first 15 students from each class who volunteered.\n\nBack to top\n\n\n\n\n\n\n9. Ethical issues pertaining to human subjects\n\n\n\n\n\n\n\nReport approval for the study from an appropriate institutional review board for research associated with human subjects.\nIf you did not receive ethical approval, describe why.\nDescribe procedures used to protect participants, including:\n\ndata collection (e.g., recruitment and informed consent)\nanalysis (e.g., data security and integrity)\nand reporting of findings (e.g., anonymization of excerpts).\n\nIf you provided compensation or offered incentives to facilitate participation, describe this too.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nJournals will require an ethical approval statement.\nQualitative research often involves interaction between researchers and research participants. Correspondingly, researchers should ensure that participants are fully aware of their participation in a research study, the risks and benefits associated with the study, the steps and precautions the researchers will take to minimize risks, such as loss of privacy and confidentiality, and how the researchers plan to use the data.\n\n\n\nExamples\n\nEthical approval was granted by the University of Otago and student participants were invited to attend each focus group discussion by the university representatives of New Zealand Medical Students Association (NZMSA). Usually, a key concern when collecting data from students is that students may feel vulnerable when sharing their experiences with academic staff during a focus group discussion. However, this potential harm was removed as each group discussion was facilitated by a fellow student, the discussions were transcribed by a professional transcribing service, and only the primary researchers [Names] had access to the raw data.\n\nBack to top\n\n\n\n\n\n\n10. Data collection methods\n\n\n\n\n\n\n\nDescribe data collection methods and design in detail, and justify them in relation to the research question(s), paradigm, approach, and other methods.7\n\nIf data collection and analysis was iterative:\n\ndescribe the iterative process\nif changes occurred during the research process, describe how and why study procedures changed in response to evolving study findings.\n\nIdentify the study period.\nDescribe important characteristics of the individuals conducting interviews, observations or focus groups, and methods used to train these individuals.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThe study period helps readers place the study in temporal context and identify factors not mentioned by the authors that might affect findings, interpretation, and implications. (See Item 8 for ending data collection.)\n\nDescribing researcher characteristics clarifies the relationship between the individuals involved in data collection and the participants in the research and also explains what efforts were made to ensure consistency in the data collection process (See Items 6 and 15.)\n\n\n\n\nExamples\n\nFurther, it was decided that group interviews, also known as focus group discussions would be the best means of data collection. This is a method of data collection that enables group members to feed off each other’s ideas and an effective moderator will maintain group focus whilst at the same time permitting flexibility in the direction those aspects of the discussion might take. [REF]\n\n\nAdjustments to the interview protocol were made according to early experience and information participants had provided (i.e. redundant questions were eliminated; questions were reworded to improve flow and clarity; additional probes were included).\n\n\nFaculty staff were then interviewed individually by a trained study investigator in a 15-minute, semi-structured interview. This sequence was repeated with other video encounters. Table 2 presents examples of interview questions. Each faculty member was interviewed by at least three interviewers over their various interviews. Interviewers were chosen based on their experience in interviewing. All were trained during a half-day meeting to interpret and deliver the interview guide in the same manner in order to elicit information of a consistent type.\n\nBack to top\n\n\n\n\n\n\n11. Data collection instruments and technologies\n\n\n\n\n\n\n\nDescribe all data collection instruments, including their development, and if/how they changed over the course of the study. Cite relevant literatures, theories or conceptual frameworks as appropriate. Consider sharing the data collection instrument(s) or a detailed description of them in the article body, supplementary material, or published elsewhere. I publish my materials on the OSF and then describe and cite them in my articles.Tim Westland - Researcher\nDescribe the use of equipment for audio or video recording, reproduction of paper documents or computer files, or other processes in data collection.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nDescribing instruments and equipment helps readers understand the full context in which data collection occurred and how this context might have affected data collection (e.g., the influence of recording devices on participants’ behaviors; the nature of inferences drawn from live vs. recorded events).\n\n\nExamples\n\nTo facilitate the discussion and to maintain consistency over different sets of discussions, key trigger questions were devised prior to the discussion. The opening trigger question was: Thinking back to some of your best clinical learning placements in 4th and 5th year. What was it about those clinical placements that provided good opportunities for learning?\n\n\nInterviews included discussion of the expectations, processes and consequences of AEE [authentic early experiences]. The interview schedule was derived following identification of questions that could not be fully answered in a systematic review of previous empirical or theoretical literature. It comprised a sequence of topic areas including experiences in action, and areas of frustration in Medical Education such as the learning of content knowledge, achieving functional knowledge, and transfer of knowledge.[REF]… Interested readers can request a copy of the schedules from the corresponding author. Interviews lasted between 20 and 90 min. and discussion groups between 60 and 90 min. All interviews and discussion groups were conducted in private rooms at the participant’s workplace—the medical school for students and faculty, and individual places of work for workplace supervisors (except for one who chose to be interviewed at the medical school). All data were audio-recorded, and transcribed verbatim.\n\nBack to top\n\n\n\n\n\n\n12. Units of study\n\n\n\n\n\n\n\nDescribe the number of participants, documents, or events included in the study (the units of study).\nDescribe characteristics of the participants, documents or events that are relevant to the study purpose and research question(s).8\nInclude the dates or timeframe for participation.\nIf the actual sample differs from the target sample, describe:\n\nthe difference,\nwhy these differences may have occurred,\nhow this might affect the findings.\n\nIf the degree of participation varied among individuals, describe:\n\nthe different levels of participation,\nthe reasons for these differences (i.e., the researchers’ choice or the participants’ preferences),\nand how these different levels of participation were taken into account in the analysis.\n\nThis information could appear in the Methods section as part of the description of the sample, or at the beginning of the Results section to provide context for the findings presented.\n\n\n\n\n\n\nMore Information, Justification, and Example\n\n\n\n\n\nJump to:\n\nHow does this item differ to Sampling (Item 8)?\nWhy readers need this information\nExample\n\n\nHow does this item differ to Sampling (Item 8)?\nThe sampling item (Item 8) describes the target or ideal participants, documents, or events selected for the study. By contrast, this item focuses on description of the actual participants, documents or events included in the study.\n\n\nWhy readers need this information\nThis helps readers know whose experiences and perspective are and are not included.\n\n\n\nExample\n\nOf the 70 Mindful Communication program participants, 46 met the eligibility requirements to participate in the in-depth interviews. We randomly chose and then contacted 22 participants, of whom 20 agreed to be interviewed within six months of completing the program: 15 in person and 5 by telephone. Two declined for lack of time. On reaching saturation after 20 interviews, no further attempts to contact the remaining 24 participants were made.\n\n\n\n\n\nThere were 31 nursing handovers covering 137 patients, and 21 resident handovers covering 101 patients included in this study.\n\nBack to top\n\n\n\n\n\n\n13. Data processing\n\n\n\n\n\n\n\nDescribe the ways in which data are prepared for analysis and managed throughout the analysis process. These activities might include transcription, coding, data entry, and organization of data. (See footnote for audio or visual recordings9).\nDescribe the processes used to organize, compile, and categorize data (e.g., field notes, transcripts, documents, photographs, artifacts) for analysis.\nIf you used transcripts, describe procedures used to check accuracy.\nDescribe procedures used to maintain data security and protect the privacy of participants, as specified in the human subjects protocols (see footnote on anonymisation10 as an example).\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript.\n\n\n\nExamples\n\nInterviews were anonymised and each participant was given a code number.\n\n\nThe interviewers and another member of the research team (H.B.) reviewed transcripts for accuracy.\n\n\nWe collected data throughout the admission process through direct observation, audio- recording, and chart extraction. We audio-recorded, transcribed, and anonymized both the overnight and morning case review discussions. We also observed the morning case review discussions in person and collected field notes. For each case review discussion, we copied the admission notes from the patient’s record and de-identified all data.\n\nBack to top\n\n\n\n\n\n\n14. Data analysis\n\n\n\n\n\n\n\nDescribe your analytic process as transparently as possible.11\nIf you used an approach that has a well-defined process for data analysis (e.g., grounded theory, discourse analysis, phenomenography):\n\ncite the guiding literature\ndescribe your processes in sufficient detail so readers can judge the extent to which your processes align with the guiding approach.\nIf you modified or deviated from the guiding approach, explain and justify these modifications.\n\nSpecify the unit of analysis.12\nExplain the rationale underlying different decisions made throughout the data analysis process to provide as much transparency as possible.13\nIf observations that contrast or deviate from identified concepts or themes were important to your analysis, describe how these discrepancies were handled during the analysis.\nIf you drew upon a theoretical perspective or framework during analysis, describe theoretical or other influences on your analysis scheme or categories if they exist. If you identified a theoretical perspective or framework early in the conception of the study or after reviewing some or all of their data, consider referring to these as sensitizing concepts to acknowledge that the approach is inductive, but with influence from relevant theory, models, or organizational schemes. Alternatively, explain that themes were developed from the data with no external influences.\nDescribe which members of the research team are involved in data analysis and what perspective(s) they bring to the analysis.\nIf software was used to assist with data analysis14, describe how it was used. Simply stating that software was used is insufficient.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nTechniques used for data analysis will depend on the paradigm, approach, and/or data collection methods selected by the researchers. Correspondingly, authors should be as transparent as possible about the analytic process so that readers can follow the logic of inquiry from the research question(s) to the analysis and findings.\n\n\nExamples\n\n…we brought sensitizing concepts to the analysis while we conducted an open, inductive analysis.[REF]In this case the sensitizing concepts arose, a priori to analysis, from a framework derived from the literature [REF] (as described above), in which participants’ motivations to act are based on principles of professionalism, internal affect, or potential implications of their actions.[REF]\n\n\nThrough an iterative process of listening, discussing, and relistening, the team identified and consensually validated emerging themes[REF] and appended segments of dialogue supporting the proposed themes. Recruitment stopped when saturation was reached (no new themes were identified). The team systematically reviewed the themes and sorted them into content domains. The team used an analytic matrix to identify patterns and connections amongst the domains. Two of us not involved in the qualitative coding process (R.E., M.K.) audited the analytic matrix, choice of quotes, and thematic analysis.\n\n\nThe analysis started after the first interview. All data were analyzed with the aid of the audio- coding facility of the NVivo 8: QSR International Pty Ltd, Doncaster, Vic, Australia programme. First, [name] and [name] coded independently from one another, making sure to stay semantically close to the participants’ wording. Then we discussed these open codes and defined axial codes.[REF] New insights about the impact of CST were written down in memos.\n\n\nVideotapes were analysed using immersion/crystallisation methods of qualitative data analysis.[REF] With no pre-existing framework developed in advance for analysis, an inductive approach was used to discover patterns of NVB in the data. A team of six researchers met weekly for 18 months to view videos together. Using a consensus-building approach based on a combination of field notes, opportunistic interviews with the participants, and repeated viewing of the same material, sometimes many months apart, we eventually achieved consensus on verbal, non-verbal, and physical themes and patterns observed in the data. Finally, as a test of goodness-of-fit, we carefully reviewed the videotapes for any deviant cases that did not fit the categories we had developed.\n\n\nAll transcripts were coded thematically by four of the five authors, who met regularly to identify areas of convergence until full agreement was reached. One of the interviewers (P.M.) maintained an audit trail to track the team’s developing thinking. A process of dialectical empiricism[REF] was used to categorise the emergent themes into more abstract concepts…\n\nBack to top\n\n\n\n\n\n\n15. Techniques to enhance trustworthiness\n\n\n\n\n\n\n\nDescribe methods used to ensure trustworthiness and credibility throughout the data collection and analysis process. (See footnote on commonly used techniques15).\nExplain your choice of techniques and why these are appropriate for the particular study.\n\n\n\n\n\n\nJustification, Examples, and Resources\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\nTraining and resources\n\n\nWhy readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript.\n\n\n\nExamples\nMember checking\n\nMember checks [REF] with an external TBL expert (R.L.) supported the validity of these analyses.\n\nTriangulation of data types and data sources\n\nThe interview data were triangulated with the data of 11 student and supervisor focus groups of a previous study, and more specifically, with those data that concern in particular the influence of CST [Communication Skills Training] on the development of patient-centredness…. Triangulation with the focus group data allowed us to broaden the in-depth information from the interviews in the analysis and to share and compare this with information from students and doctors with varying levels of CST (no, limited, full programme) and from two universities (Universities of Antwerp and Ghent). Moreover, this enabled us to better explore the evolution over time, given that the focus groups included participants at different stages of their study: before clerkships (year 4, undergraduate), during clerkships (year 6, undergraduate), after clerkships (year 7, undergraduate) and postgraduate (general practice trainees, and supervising specialists and GPs; Table 1).\n\n\n\n\n\nFinally, as a test of goodness-of-fit, we carefully reviewed the videotapes for any deviant cases that did not fit the categories we had developed.\n\nTriangulation of Researchers + Audit trail\n\nTo ensure rigor and increase authenticity in our methodology, we used two kinds of triangulation—investigator triangulation and data triangulation.[REF] We sought analytical rigor using an audit trail and multiple coders; our coding team included an experienced clinician (M.G.) as well as a nonclinician with expertise in medical communication and team dynamics (L.L.).\n\n\n\nTraining and resources\nSee Lincoln and Guba’s Evaluative Criteria for trustworthiness.\nBack to top\n\n\n\n\n\n\nResults\n\n\n16. Synthesis and interpretation\n\n\n\n\n\n\n\nDescribe the main analytic findings.16\nIn most cases, report a synthesis of the data along with specific quotes, examples, or illustrations derived from the data.\n\nI like when authors use words like many, few, or all to describe frequency.Nkinda Akaro - Researcher\nConsider describing frequency, variety, representativeness, counter-examples, concrete details, contextualization, conditions, and qualifications related to the findings.\nFrequency counts play a limited role in qualitative research, and need not be reported unless they play a meaningful role in interpretation of the data.\nIf your findings include integration with prior literature or theory and/or the development of a theory, model or meta-narrative, consider using tables and figures to communicate these findings.\nItems 16 and 18 can be reported in Results or Discussion sections.17\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript.\n\n\n\nExamples\n\nWe identified four patterns of NVB (non-verbal behavior) that relate to handover quality and have dubbed them: (1) joint focus of attention; (2) the poker hand; (3) parallel play; and (4) kerbside consultation. Each pattern constitutes a transfix, or systematic way of participating non-verbally in the care transfer process. And, although there are variations in each pattern, we have been able to code virtually every handover we have observed in nursing, medicine and surgery into one of these four categories.\n\n\nBecause our participants came from similar educational backgrounds, had studied medicine as their tertiary course, were embedded in the culture of medicine, and were associated in meaningful ways with a single medical school, we approached their transcripts with the assumption that they belonged to a loosely formed discourse community. Although their graduation dates ranged over a period of 50 years and their collective sphere of practice included 10 different specialty areas, there were many similarities in their experiences of enculturation during and after medical school.\n\n\nTheir three major (often overlapping) areas of concern were epistemic (acquiring knowledge and skill), interpersonal (relating to patients, families, colleagues and administrators) and personal (achieving work–life balance). In each of these areas, medical enculturation was achieved by two overlapping processes, absorption and assimilation, each of which may have distinct implications for postgraduate medical education.\n\nBack to top\n\n\n\n\n\n\n17. Links to empirical data\n\n\n\n\n\n\n\nProvide evidence (e.g., quotes, field notes, text excerpts, photographs) to substantiate the more general and abstract concepts or inferences they present as findings.18 I love seeing evidence like this because it really brings results to life. Nkinda Akaro - Researcher\nYou could report this evidence in a table or figure, incorporated into a narrative description of findings, as a stand-alone narrative, or in text blocks embedded in the manuscript text. If you are constrained by word limits or media limitations (e.g.. video), consider sharing data via an appendix, supplemental material, or web-based repository.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript.\n\n\n\nExamples\nSee Frankel et al. for an excellent example of how to use photographs (or snapshots from video) to illustrate and provide supporting evidence for patterns of behavior identified in the analysis. http://qualitysafety.bmj.com/content/21/Suppl_1/i121.\n\nWe identified five interruption types: (1) probing for further data, (2) prompting for expected sequence, (3) teaching around the case, (4) thinking out loud, and (5) providing direction (see Table 1). Several interruption types served both goals of the case review discussions—teaching and patient care. For example, when thinking out loud, supervisors reasoned through problems and taught the team: So that’s the big question, did she have a mechanical fall, or did she have a medicine-related fall? (Case 2). Supervisors prompted for expected sequence, preventing presenters from skipping over information while simultaneously allowing the supervisor to instruct the team on presentation style: So now you can tell me what the rest of his test results are because I haven’t heard those (Case 16).\n\nBack to top\n\n\n\n\n\n\nDiscussion\n\n\n18. Integration with prior work, implications, transferability, and contribution(s) to the field\n\n\n\n\n\n\n\nDescribe how the findings and conclusions connect to, support, elaborate on, or challenge previous findings, experiences, theory, or a guiding paradigm or approach.19\nDescribe how the findings contribute to or advance the field.\nDescribe any implications of the work, such transferability or generalizability.\n\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThe short summary reminds readers of the main findings and may help them assess whether the subsequent interpretation and implications formulated by the authors are supported by the findings.\n\n\nExamples\n\nThis study contributes to the understanding and discussion of the complexity of involving patients in healthcare education. It shows that integrating patient-led teaching into initiatives that are partly faculty-led influences the way in which students perceive learning from and with PIs. What is not known, however, is whether perceptions are also affected by type of health profession and the students’ different orientation towards logics of care and science, and issues of authority and power relations.\n\nFor complete examples of Discussions, see:\n\nHenriksen & Ringsted, 2013\nWesterman et al., 2013.\n\nBack to top\n\n\n\n\n\n\n19. Limitations\n\n\n\n\n\n\n\nDescribe problems or gaps in their efforts to ensure trustworthiness and the potential implications.20 Authors should never feel nervous to report limitations. In fact, thoughtful discussion of limitations is a hallmark of good research!Charles Ruggle - Editor\nDescribe how the chosen paradigm, approach, and methods will influence the situations to which the findings may reasonably apply.21 (See also Item 18.)\nDescribe how specific decisions or events in the conduct of the study strengthen or weaken the rigor of the findings.\n\n\n\n\n\n\nJustification and Example\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExample\n\n\nWhy readers need this information\nAll research has limitations. Discussing them will help readers consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be). If you don’t address limitations, editors and peer reviewers may ask you about them which will delay publication.\n\n\nExample\n\nThe study has several limitations. One is that the focus group interview method reveals students’ perceptions rather than their actual behaviors. Observations of the patient-led teaching encounter may have illuminated an understanding of the relationship between patient instructors and medical students. Another limitation is that the PI-led teaching is optional rather than mandatory, which may have influenced students’ attitudes in a positive direction. Moreover, students who are eager to take on extra-curricular activities may not be representative of the whole population. That only 23 out of 39 students signed up for this study might also have influenced results if the missing group of students represented other perceptions than those present in the focus groups. However the received data from the focus groups were rich in information and diverse perceptions were present. Another limitation is the overrepresentation of women over men in our sample. Even though women are also overrepresented in medical school this might potentially have influenced results, but gender differences in perceptions were nevertheless not identified in the data.\n\nBack to top\n\n\n\n\n\n\nOther\n\n\n20. Conflicts of interest\n\n\n\n\n\n\n\nAuthors shouldn’t feel nervous reporting conflicts of interest. They are very common and rarely influence our decision to publish.Charles Ruggle - Editor\nDescribe any real or potential conflicts of interest that might have influenced or could appear to have influenced the research.\nDescribe:\n\nhow these conflicts were managed in the conduct of the study,\nthe potential impact on study findings and/or conclusions.\n\nSome aspects may be mentioned as part of reflexivity (see Item 6).\n\n\n\n\n\n\nJustification\n\n\n\n\n\n\nWhy readers need this information\nMany factors, including professional and personal relationships and activities, can influence the design, conduct, and reporting of the clinical science that informs health care decision. The potential for conflict of interest exists when these relationships and activities may bias judgment (1). Many stakeholders— editors, peer reviewers, clinicians, educators, policymakers, patients, and the public—rely on the disclosure of authors’ relationships and activities to inform their assessments. Trust in the transparency, consistency, and completeness of these disclosures is essential. - ICMJE\n\n\n\n\n\n\n\n21. Funding\n\n\n\n\n\n\n\nDescribe any sources of funding and other support for the study.\nDescribe the role of funders in data collection, data analysis, and reporting if applicable."
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-title-2",
    "href": "guidelines/srqr/qscc.html#sec-title-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis allows readers to quickly identify the type of study. Your title will be indexed by search tools so descriptive words will make it easier for others to discover."
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-title-3",
    "href": "guidelines/srqr/qscc.html#sec-title-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nResidents learning from a narrative experience with dying patients: a qualitative study.\n\n\nMedical students’ perceptions of the factors influencing their academic performance: an exploratory interview study with high-achieving and re-sitting medical students.\n\n\nUndergraduate rural medical education program development: focus group consultation with the NRHA Rural Medical Educators Group."
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-title-4",
    "href": "guidelines/srqr/qscc.html#sec-title-4",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Training and Resources",
    "text": "Training and Resources\nSee this article for advice on writing titles that are interesting and descriptive.\nThe EQUATOR Network provides training for writing effective manuscript titles as part of their publication school.\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-abstract-2",
    "href": "guidelines/srqr/qscc.html#sec-abstract-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nA reader should be able to read the abstract independent of the manuscript and get a sense of the background, purpose, methods, main results and implications that will be described in greater detail in the manuscript.\nYour abstract will be indexed by search tools so descriptive words will make it easier for others to discover."
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-abstract-3",
    "href": "guidelines/srqr/qscc.html#sec-abstract-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nPurpose: Although academic centers rely on assessments from medical trainees regarding the effectiveness of their faculty as teachers, little is known about how trainees conceptualize and approach their role as assessors of their clinical supervisorsMethod: In 2010, using a constructivist grounded theory approach, five focus group interviews were conducted with 19 residents from an internal medicine residency program. A constant comparative analysis of emergent themes was conducted.Results: Residents viewed clinical teaching assessment (CTA) as a time-consuming task with little reward. They reported struggling throughout the academic year to meet their CTA obligations and described several shortcut strategies they used to reduce their burden. Rather than conceptualizing their assessments as a conduit for both formative and summative feedback, residents perceived CTA as useful for the surveillance of clinical supervisors at the extremes of the spectrum of teaching effectiveness. They put the most effort, including the crafting of written comments, into the CTAs of these outliers. Trainees desired greater transparency in the CTA process and were sceptical regarding the anonymity and perceived validity of their faculty appraisals.Conclusions: Individual and system-based factors conspire to influence postgraduate medical trainees’ motivation for generating high-quality appraisals of clinical teaching. Academic centers need to address these factors if they want to maximize the usefulness of these assessments."
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-abstract-4",
    "href": "guidelines/srqr/qscc.html#sec-abstract-4",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Training and Resources",
    "text": "Training and Resources\nThe EQUATOR Network provides training for writing effective manuscript abstracts as part of their publication school.\nSee this article from the London School of Economics for general abstract writing tips.\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-problem-formulation-2",
    "href": "guidelines/srqr/qscc.html#sec-problem-formulation-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information provides context for the research question or study purpose and situates the study in relation to other work in the field."
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-problem-formulation-3",
    "href": "guidelines/srqr/qscc.html#sec-problem-formulation-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nRegulatory focus theory may therefore offer insight into the variability in responses to feedback, but how well do these experimental findings translate to real clinical situations in which the reality of responsiveness to feedback seems frustratingly complex? …. In order to better elaborate a theory to account for this variability in learner response to feedback, the present study was undertaken. We aimed, in this study, to determine how readily clinical learning events could be classified as activating a promotion or a prevention focus, and to explore, through a careful analysis of doctors’ descriptions of their feedback experiences, the predictive value of regulatory focus theory in the context of real clinical learning situations.\n\n\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-purpose-2",
    "href": "guidelines/srqr/qscc.html#sec-purpose-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nBy clearly stating the purpose of the study, authors set readers’ expectations for the methods, findings and discussion sections of the manuscript."
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-purpose-3",
    "href": "guidelines/srqr/qscc.html#sec-purpose-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nThe purposes of this study were to investigate how medical students recognize, respond to and utilise feedback, and to determine whether there are maturational differences in understandings of the role of feedback across academic years in medical school."
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-purpose-4",
    "href": "guidelines/srqr/qscc.html#sec-purpose-4",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Training and Resources",
    "text": "Training and Resources\nSee this article for advice on writing qualitative research questions.\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-qualitative-approach-2",
    "href": "guidelines/srqr/qscc.html#sec-qualitative-approach-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nIdentifying the research paradigm helps readers understand whether the researcher assumes that there is a single, objective reality (positivist or post-positivist) and has thus designed the study to describe this reality or whether the researcher assumes multiple, subjective realities and designed the study to describe these multiple realities, with no attempt to merge or reconcile these realities (constructivist/interpretivist). The paradigm has implications for the study design, approach, methods, and techniques to ensure rigour and trustworthiness.\nSince the research paradigm does not necessarily dictate particular approaches or methods, the approach should also be clearly defined. Stating the approach provides readers the opportunity to evaluate the fidelity of the research approach to the research question(s) and consider the rationale for modifications and deviations from the selected approach. Qualitative research also includes an array of methods that can be used across paradigms and approaches. (See also Item 10)."
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-qualitative-approach-3",
    "href": "guidelines/srqr/qscc.html#sec-qualitative-approach-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nThe study was performed from a constructivist point of view using an interpretative phenomenological epistemology. Based on the notion that social phenomena are constructed by the communal making of meaning about the underlying phenomena, we aimed to construct insightful accounts of lead consultants’ approaches to educational change, rather than to identify the true nature of these approaches. Because the management of change by lead consultants is an under-researched area, we conducted an exploratory qualitative study…\n\n\nGiven the relative dearth of explanatory theories about factors affecting medical students’ emotional reactions, we chose to develop one by applying methods associated with grounded theory, specifically constant comparative analysis, to qualitative data obtained from learning logs and interviews. Our approach was constructivist, deliberately using researchers’ own experiences and acquired knowledge to enhance theoretical sensitivity and enrich theory development.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-researcher-characteristics-and-reflexivity-2",
    "href": "guidelines/srqr/qscc.html#sec-researcher-characteristics-and-reflexivity-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nIn positivist and post-positivist paradigms, personal characteristics and perspectives of researchers might be viewed as biases that limit the credibility of study findings, while in constructivist or interpretivist paradigms the characteristics and perspectives of the researchers are important contextual factors that are an accepted part of the study design, data collection, and data analysis. These characteristics and perspectives may explain how the researcher(s) obtained access to the site or participants included in the study or may add valuable insight during data analysis."
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-researcher-characteristics-and-reflexivity-3",
    "href": "guidelines/srqr/qscc.html#sec-researcher-characteristics-and-reflexivity-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nReflexivity was maintained by the research team through the analysis and writing by recording, discussing and challenging established assumptions. In addition both EH and SV kept reflexive diaries.\n\n\nThe first author conducted all interviews and discussion groups. Her own medical undergraduate training took place between 1995 and 2000. She was not known to the participants of this research prior to undertaking the study and deliberately did not undertake any clinical or teaching activities locally alongside this research. Whilst it was useful to know (from her own background) what the students were talking about medically (and in terms of detecting items of significance), as a researcher she made conscious efforts not to accept potentially common assumptions at face value.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-context-2",
    "href": "guidelines/srqr/qscc.html#sec-context-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis helps readers interpret the meaning and significance of the study findings by situating them in social, cultural, temporal and other relevant contexts."
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-context-3",
    "href": "guidelines/srqr/qscc.html#sec-context-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nWe conducted the study among hospital-based clinical teachers of students in years 4 to 6 of a six-year undergraduate medical program at Maastricht University Medical School. Years 4 to 6 are devoted to clerkships in the academic hospital and affiliated regional hospitals. Rotations differ in duration depending on the type of rotation and the discipline, and the sequence of rotations differs among students. During rotations, students spend time in the wards, the outpatient clinics, and the accident and emergency department. Clerkships in years 4 and 5 last between 4 and 10 weeks (regular clerkship), whereas students in year 6 undertake an 18-week senior clerkship in a discipline of their choice.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-sampling-strategy-2",
    "href": "guidelines/srqr/qscc.html#sec-sampling-strategy-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information helps readers understand the source of data / findings so they can consider the boundaries of the study and the relevance to their own context. It also gives readers insight into the researchers’ decisions, which can be important for critical appraisal."
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-sampling-strategy-3",
    "href": "guidelines/srqr/qscc.html#sec-sampling-strategy-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nAs students’ perceptions were previously shown to be related to gender, age, prior experience and place of attachment,[REF] we purposely selected respondents with different backgrounds. This sampling strategy led to the diversity of gender, age, prior patient experience and place of attachment shown in Table 1.\n\n\nPurposive sampling was directed towards achieving maximum variation in age and specialty, using a snowball approach (a non-probabilistic form of sampling in which persons initially chosen for the sample are used as informants to locate other persons having necessary characteristics making them eligible for the sample).[REF ]\n\n\nPotential participants were all medical students in Years 1 and 2 at the University of Toronto in 2004. Following research ethics board approval, recruitment was conducted via e-mail to class listservs. Participant responses were sent directly to the research assistant, who was unknown to participants, so that the principal investigators did not know who did or did not participate. This process was engaged to protect participants’ anonymity and to avoid any impression of coercion because the lead researcher (SG) was involved in the administration of the undergraduate curriculum at the time. Sample size was estimated to be sufficient based on the principle of theoretical saturation [REF] and our previous experience with this methodology (i.e., with a relatively homogeneous population, we expected to reach saturation with approximately 15 interviews per group). There were no exclusion criteria and we accepted the first 15 students from each class who volunteered.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-ethics-2",
    "href": "guidelines/srqr/qscc.html#sec-ethics-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nJournals will require an ethical approval statement.\nQualitative research often involves interaction between researchers and research participants. Correspondingly, researchers should ensure that participants are fully aware of their participation in a research study, the risks and benefits associated with the study, the steps and precautions the researchers will take to minimize risks, such as loss of privacy and confidentiality, and how the researchers plan to use the data."
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-ethics-3",
    "href": "guidelines/srqr/qscc.html#sec-ethics-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nEthical approval was granted by the University of Otago and student participants were invited to attend each focus group discussion by the university representatives of New Zealand Medical Students Association (NZMSA). Usually, a key concern when collecting data from students is that students may feel vulnerable when sharing their experiences with academic staff during a focus group discussion. However, this potential harm was removed as each group discussion was facilitated by a fellow student, the discussions were transcribed by a professional transcribing service, and only the primary researchers [Names] had access to the raw data.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-data-collection-methods-2",
    "href": "guidelines/srqr/qscc.html#sec-data-collection-methods-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThe study period helps readers place the study in temporal context and identify factors not mentioned by the authors that might affect findings, interpretation, and implications. (See Item 8 for ending data collection.)\n\nDescribing researcher characteristics clarifies the relationship between the individuals involved in data collection and the participants in the research and also explains what efforts were made to ensure consistency in the data collection process (See Items 6 and 15.)"
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-data-collection-methods-3",
    "href": "guidelines/srqr/qscc.html#sec-data-collection-methods-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nFurther, it was decided that group interviews, also known as focus group discussions would be the best means of data collection. This is a method of data collection that enables group members to feed off each other’s ideas and an effective moderator will maintain group focus whilst at the same time permitting flexibility in the direction those aspects of the discussion might take. [REF]\n\n\nAdjustments to the interview protocol were made according to early experience and information participants had provided (i.e. redundant questions were eliminated; questions were reworded to improve flow and clarity; additional probes were included).\n\n\nFaculty staff were then interviewed individually by a trained study investigator in a 15-minute, semi-structured interview. This sequence was repeated with other video encounters. Table 2 presents examples of interview questions. Each faculty member was interviewed by at least three interviewers over their various interviews. Interviewers were chosen based on their experience in interviewing. All were trained during a half-day meeting to interpret and deliver the interview guide in the same manner in order to elicit information of a consistent type.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-data-collection-instruments-2",
    "href": "guidelines/srqr/qscc.html#sec-data-collection-instruments-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nDescribing instruments and equipment helps readers understand the full context in which data collection occurred and how this context might have affected data collection (e.g., the influence of recording devices on participants’ behaviors; the nature of inferences drawn from live vs. recorded events)."
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-data-collection-instruments-3",
    "href": "guidelines/srqr/qscc.html#sec-data-collection-instruments-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nTo facilitate the discussion and to maintain consistency over different sets of discussions, key trigger questions were devised prior to the discussion. The opening trigger question was: Thinking back to some of your best clinical learning placements in 4th and 5th year. What was it about those clinical placements that provided good opportunities for learning?\n\n\nInterviews included discussion of the expectations, processes and consequences of AEE [authentic early experiences]. The interview schedule was derived following identification of questions that could not be fully answered in a systematic review of previous empirical or theoretical literature. It comprised a sequence of topic areas including experiences in action, and areas of frustration in Medical Education such as the learning of content knowledge, achieving functional knowledge, and transfer of knowledge.[REF]… Interested readers can request a copy of the schedules from the corresponding author. Interviews lasted between 20 and 90 min. and discussion groups between 60 and 90 min. All interviews and discussion groups were conducted in private rooms at the participant’s workplace—the medical school for students and faculty, and individual places of work for workplace supervisors (except for one who chose to be interviewed at the medical school). All data were audio-recorded, and transcribed verbatim.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-units-of-study-2",
    "href": "guidelines/srqr/qscc.html#sec-units-of-study-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "How does this item differ to Sampling (Item 8)?",
    "text": "How does this item differ to Sampling (Item 8)?\nThe sampling item (Item 8) describes the target or ideal participants, documents, or events selected for the study. By contrast, this item focuses on description of the actual participants, documents or events included in the study."
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-units-of-study-3",
    "href": "guidelines/srqr/qscc.html#sec-units-of-study-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis helps readers know whose experiences and perspective are and are not included."
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-units-of-study-4",
    "href": "guidelines/srqr/qscc.html#sec-units-of-study-4",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nOf the 70 Mindful Communication program participants, 46 met the eligibility requirements to participate in the in-depth interviews. We randomly chose and then contacted 22 participants, of whom 20 agreed to be interviewed within six months of completing the program: 15 in person and 5 by telephone. Two declined for lack of time. On reaching saturation after 20 interviews, no further attempts to contact the remaining 24 participants were made.\n\n\n\n\n\nThere were 31 nursing handovers covering 137 patients, and 21 resident handovers covering 101 patients included in this study.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-data-processing-2",
    "href": "guidelines/srqr/qscc.html#sec-data-processing-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript."
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-data-processing-3",
    "href": "guidelines/srqr/qscc.html#sec-data-processing-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nInterviews were anonymised and each participant was given a code number.\n\n\nThe interviewers and another member of the research team (H.B.) reviewed transcripts for accuracy.\n\n\nWe collected data throughout the admission process through direct observation, audio- recording, and chart extraction. We audio-recorded, transcribed, and anonymized both the overnight and morning case review discussions. We also observed the morning case review discussions in person and collected field notes. For each case review discussion, we copied the admission notes from the patient’s record and de-identified all data.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-data-analysis-2",
    "href": "guidelines/srqr/qscc.html#sec-data-analysis-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nTechniques used for data analysis will depend on the paradigm, approach, and/or data collection methods selected by the researchers. Correspondingly, authors should be as transparent as possible about the analytic process so that readers can follow the logic of inquiry from the research question(s) to the analysis and findings."
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-data-analysis-3",
    "href": "guidelines/srqr/qscc.html#sec-data-analysis-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\n…we brought sensitizing concepts to the analysis while we conducted an open, inductive analysis.[REF]In this case the sensitizing concepts arose, a priori to analysis, from a framework derived from the literature [REF] (as described above), in which participants’ motivations to act are based on principles of professionalism, internal affect, or potential implications of their actions.[REF]\n\n\nThrough an iterative process of listening, discussing, and relistening, the team identified and consensually validated emerging themes[REF] and appended segments of dialogue supporting the proposed themes. Recruitment stopped when saturation was reached (no new themes were identified). The team systematically reviewed the themes and sorted them into content domains. The team used an analytic matrix to identify patterns and connections amongst the domains. Two of us not involved in the qualitative coding process (R.E., M.K.) audited the analytic matrix, choice of quotes, and thematic analysis.\n\n\nThe analysis started after the first interview. All data were analyzed with the aid of the audio- coding facility of the NVivo 8: QSR International Pty Ltd, Doncaster, Vic, Australia programme. First, [name] and [name] coded independently from one another, making sure to stay semantically close to the participants’ wording. Then we discussed these open codes and defined axial codes.[REF] New insights about the impact of CST were written down in memos.\n\n\nVideotapes were analysed using immersion/crystallisation methods of qualitative data analysis.[REF] With no pre-existing framework developed in advance for analysis, an inductive approach was used to discover patterns of NVB in the data. A team of six researchers met weekly for 18 months to view videos together. Using a consensus-building approach based on a combination of field notes, opportunistic interviews with the participants, and repeated viewing of the same material, sometimes many months apart, we eventually achieved consensus on verbal, non-verbal, and physical themes and patterns observed in the data. Finally, as a test of goodness-of-fit, we carefully reviewed the videotapes for any deviant cases that did not fit the categories we had developed.\n\n\nAll transcripts were coded thematically by four of the five authors, who met regularly to identify areas of convergence until full agreement was reached. One of the interviewers (P.M.) maintained an audit trail to track the team’s developing thinking. A process of dialectical empiricism[REF] was used to categorise the emergent themes into more abstract concepts…\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-trustworthiness-2",
    "href": "guidelines/srqr/qscc.html#sec-trustworthiness-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript."
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-trustworthiness-3",
    "href": "guidelines/srqr/qscc.html#sec-trustworthiness-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\nMember checking\n\nMember checks [REF] with an external TBL expert (R.L.) supported the validity of these analyses.\n\nTriangulation of data types and data sources\n\nThe interview data were triangulated with the data of 11 student and supervisor focus groups of a previous study, and more specifically, with those data that concern in particular the influence of CST [Communication Skills Training] on the development of patient-centredness…. Triangulation with the focus group data allowed us to broaden the in-depth information from the interviews in the analysis and to share and compare this with information from students and doctors with varying levels of CST (no, limited, full programme) and from two universities (Universities of Antwerp and Ghent). Moreover, this enabled us to better explore the evolution over time, given that the focus groups included participants at different stages of their study: before clerkships (year 4, undergraduate), during clerkships (year 6, undergraduate), after clerkships (year 7, undergraduate) and postgraduate (general practice trainees, and supervising specialists and GPs; Table 1).\n\n\n\n\n\nFinally, as a test of goodness-of-fit, we carefully reviewed the videotapes for any deviant cases that did not fit the categories we had developed.\n\nTriangulation of Researchers + Audit trail\n\nTo ensure rigor and increase authenticity in our methodology, we used two kinds of triangulation—investigator triangulation and data triangulation.[REF] We sought analytical rigor using an audit trail and multiple coders; our coding team included an experienced clinician (M.G.) as well as a nonclinician with expertise in medical communication and team dynamics (L.L.)."
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-trustworthiness-4",
    "href": "guidelines/srqr/qscc.html#sec-trustworthiness-4",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Training and resources",
    "text": "Training and resources\nSee Lincoln and Guba’s Evaluative Criteria for trustworthiness.\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-synthesis-and-interpretation-2",
    "href": "guidelines/srqr/qscc.html#sec-synthesis-and-interpretation-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript."
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-synthesis-and-interpretation-3",
    "href": "guidelines/srqr/qscc.html#sec-synthesis-and-interpretation-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nWe identified four patterns of NVB (non-verbal behavior) that relate to handover quality and have dubbed them: (1) joint focus of attention; (2) the poker hand; (3) parallel play; and (4) kerbside consultation. Each pattern constitutes a transfix, or systematic way of participating non-verbally in the care transfer process. And, although there are variations in each pattern, we have been able to code virtually every handover we have observed in nursing, medicine and surgery into one of these four categories.\n\n\nBecause our participants came from similar educational backgrounds, had studied medicine as their tertiary course, were embedded in the culture of medicine, and were associated in meaningful ways with a single medical school, we approached their transcripts with the assumption that they belonged to a loosely formed discourse community. Although their graduation dates ranged over a period of 50 years and their collective sphere of practice included 10 different specialty areas, there were many similarities in their experiences of enculturation during and after medical school.\n\n\nTheir three major (often overlapping) areas of concern were epistemic (acquiring knowledge and skill), interpersonal (relating to patients, families, colleagues and administrators) and personal (achieving work–life balance). In each of these areas, medical enculturation was achieved by two overlapping processes, absorption and assimilation, each of which may have distinct implications for postgraduate medical education.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-links-to-empirical-data-2",
    "href": "guidelines/srqr/qscc.html#sec-links-to-empirical-data-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript."
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-links-to-empirical-data-3",
    "href": "guidelines/srqr/qscc.html#sec-links-to-empirical-data-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\nSee Frankel et al. for an excellent example of how to use photographs (or snapshots from video) to illustrate and provide supporting evidence for patterns of behavior identified in the analysis. http://qualitysafety.bmj.com/content/21/Suppl_1/i121.\n\nWe identified five interruption types: (1) probing for further data, (2) prompting for expected sequence, (3) teaching around the case, (4) thinking out loud, and (5) providing direction (see Table 1). Several interruption types served both goals of the case review discussions—teaching and patient care. For example, when thinking out loud, supervisors reasoned through problems and taught the team: So that’s the big question, did she have a mechanical fall, or did she have a medicine-related fall? (Case 2). Supervisors prompted for expected sequence, preventing presenters from skipping over information while simultaneously allowing the supervisor to instruct the team on presentation style: So now you can tell me what the rest of his test results are because I haven’t heard those (Case 16).\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-integration-with-prior-work-2",
    "href": "guidelines/srqr/qscc.html#sec-integration-with-prior-work-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThe short summary reminds readers of the main findings and may help them assess whether the subsequent interpretation and implications formulated by the authors are supported by the findings."
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-integration-with-prior-work-3",
    "href": "guidelines/srqr/qscc.html#sec-integration-with-prior-work-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nThis study contributes to the understanding and discussion of the complexity of involving patients in healthcare education. It shows that integrating patient-led teaching into initiatives that are partly faculty-led influences the way in which students perceive learning from and with PIs. What is not known, however, is whether perceptions are also affected by type of health profession and the students’ different orientation towards logics of care and science, and issues of authority and power relations.\n\nFor complete examples of Discussions, see:\n\nHenriksen & Ringsted, 2013\nWesterman et al., 2013.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-limitations-2",
    "href": "guidelines/srqr/qscc.html#sec-limitations-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nAll research has limitations. Discussing them will help readers consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be). If you don’t address limitations, editors and peer reviewers may ask you about them which will delay publication."
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-limitations-3",
    "href": "guidelines/srqr/qscc.html#sec-limitations-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nThe study has several limitations. One is that the focus group interview method reveals students’ perceptions rather than their actual behaviors. Observations of the patient-led teaching encounter may have illuminated an understanding of the relationship between patient instructors and medical students. Another limitation is that the PI-led teaching is optional rather than mandatory, which may have influenced students’ attitudes in a positive direction. Moreover, students who are eager to take on extra-curricular activities may not be representative of the whole population. That only 23 out of 39 students signed up for this study might also have influenced results if the missing group of students represented other perceptions than those present in the focus groups. However the received data from the focus groups were rich in information and diverse perceptions were present. Another limitation is the overrepresentation of women over men in our sample. Even though women are also overrepresented in medical school this might potentially have influenced results, but gender differences in perceptions were nevertheless not identified in the data.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscc.html#sec-conflicts-of-interest-2",
    "href": "guidelines/srqr/qscc.html#sec-conflicts-of-interest-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nMany factors, including professional and personal relationships and activities, can influence the design, conduct, and reporting of the clinical science that informs health care decision. The potential for conflict of interest exists when these relationships and activities may bias judgment (1). Many stakeholders— editors, peer reviewers, clinicians, educators, policymakers, patients, and the public—rely on the disclosure of authors’ relationships and activities to inform their assessments. Trust in the transparency, consistency, and completeness of these disclosures is essential. - ICMJE"
  },
  {
    "objectID": "guidelines/srqr/qscc.html#ready-to-get-started",
    "href": "guidelines/srqr/qscc.html#ready-to-get-started",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Ready to get started?",
    "text": "Ready to get started?\nDownload resources"
  },
  {
    "objectID": "guidelines/srqr/qscc.html#how-to-cite",
    "href": "guidelines/srqr/qscc.html#how-to-cite",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "How to cite",
    "text": "How to cite\nFor attribution, please cite this guideline as:\nBridget O’Brien, Ilene Harris, Thomas Beckman, Darcy Reed, and David Cook. 2023. “The SRQR Guidelines for Writing Qualitative Research Articles version 1.1.” The EQUATOR Network Guideline Dissemination Platform. https://doi.org/10.1234/equator/1010101.\nYou can use your reference manager to save citation information for this webpage, or copy the BibTeX below."
  },
  {
    "objectID": "guidelines/srqr/qscc.html#faqs",
    "href": "guidelines/srqr/qscc.html#faqs",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "FAQs",
    "text": "FAQs\n\nWho made this guideline?\n\nBridget O’Brien, PhD has been a faculty member in the Department of Medicine, Division of General Internal Medicine, since 2008. She is a professor of medicine and an education scientist in the Office of Medical Education’s Center for Faculty Educators. As co-director of the Teaching Scholars Program and the UCSF-University of Utrecht Health Professions Education doctoral program she teaches and mentors faculty and learners interested in education research and scholarship. At the San Francisco VA, she directs the Advanced Fellowship in Health Professions Education Evaluation and Research. In 2015 she was selected as one of five national Macy Faculty Scholars supported by the Josiah Macy Jr. Foundation and in 2021 she was selected as a KIPRIME Fellow at the Karolinska Instituet. She is a deputy editor for the journal Academic Medicine.\nDr. Ilene Harris, deceased, was professor and head, Department of Medical Education, University of Illinois at Chicago College of Medicine, Chicago, Illinois.\nDr. Thomas Beckman is professor of medicine and medical education, Department of Medicine, Mayo Clinic College of Medicine, Rochester, Minnesota.\nDr. Darcy Reed is associate professor of medicine and medical education, Department of Medicine, Mayo Clinic College of Medicine, Rochester, Minnesota.\nDr. David Cook is associate director, Mayo Clinic Online Learning, research chair, Mayo Multidisciplinary Simulation Center, and professor of medicine and medical education, Mayo Clinic College of Medicine, Rochester, Minnesota.\n\n\nHow was this guideline made?\nThe developers synthesised 40 sets of recommendations previously proposed by experts in qualitative methods. You can read about their development process here.\n\n\nWhat to do if asked to remove guideline related content\nIf a colleague or reviewer asks you to remove content that is related to this guideline, you can direct them to this guideline and the explanation for why that item is important. If they insist, consider moving the item to a supplement, table or figure.\n\n\nWhere can I get general writing training?\nThe EQUATOR Network provides in-person training for writing research articles.\nAuthorAID have resources, an online course, and mentoring to help authors.\n\n\n\nResearch paradigm\n\n\n\n\n\n\nThe set of beliefs and assumptions that guide the research process. These commonly include positivist, post-positivist, constructivist or interpretivist, and critical theory. Qualitative research generally draws from a post-positivist or constructivist/interpretivist paradigm.”\n\n\n\n\n\n\nInstruments\n\n\n\n\n\n\nData collection instruments include (but are not limited to) interview or focus group guides, observational protocols and prompts for field notes, and data extraction or coding protocols for selection and analysis of documents, photographs, videos, or other artifacts”\n\n\n\n\n\n\nBias\n\n\n\n\n\n\nA term drawn from quantitative research, bias technically means a systematic error, where a particular research finding deviates from a true finding. This might come about through errors in the manner of interviewing, or by errors in sampling. In qualitative research this is a problematic concept, since by definition the qualitative researcher is part of the process, and all researchers are different. This human factor has been said to be both the greatest strength and the greatest weakness of qualitative method. What can be done in commercial qualitative research, however, is to minimise obvious and avoidable sources of bias, for example by not confining all the fieldwork to one social group or geographic location, by taking steps to recognise the personal views of the researcher, (using techniques such as bracketing), and by working in teams.\n\n\n\n\n\n\nSampling strategy\n\n\n\n\n\n\nSeveral sampling strategies are commonly used in qualitative research, although most fall under the umbrella of purposeful (or purposive) sampling.\nPurposeful sampling means that participants, documents, or events are selected for their relevance to the research question, based on guiding theory or experiences and assumptions of the researchers. Over the course of the research process, researchers may determine that additional or different participants, documents, or events should be included to address the research question.\nOther sampling techniques, such as theoretical sampling (seeking examples of theoretical constructs), snowball sampling (using study participants to identify additional participants who meet study criteria), and convenience sampling (including any volunteers with no or minimal criteria for inclusion) may be appropriate depending on the question and approach, so long as the authors provide explanation and justification.\n\n\n\n\n\n\nApproach\n\n\n\n\n\n\nA qualitative approach is a general way of thinking about conducting qualitative research. It describes, either explicitly or implicitly, the purpose of the qualitative research, the role of the researcher(s), the stages of research, and the method of data analysis. Commonly used approaches include ethnography, grounded theory, case study, phenomenology, and narrative research.\n\n\n\n\n\n\nData collection methods\n\n\n\n\n\n\nData collection methods include (but are not limited to) interviews, focus groups, observations (direct or indirect via video), and review of written text, photographs, and other documents or materials.\n\n\n\n\n\n\nIterative\n\n\n\n\n\n\nQualitative research often occurs as an iterative process, meaning that researchers begin data analysis before they complete data collection. The data collection and analysis process may occur in phases or stages. As part of an iterative collection-analysis process, researchers will often alter their data collection methods to explore their preliminary impressions in greater depth and/or actively pursue confirming and disconfirming perspectives.\n\n\n\n\n\n\nStudy period\n\n\n\n\n\n\nThe start and end dates for data collection and analysis.\n\n\n\n\n\n\nEthnography\n\n\n\n\n\n\nThe scientific description of peoples and cultures with their customs, habits, and mutual differences.\nRead more\n\n\n\n\n\n\nGrounded theory\n\n\n\n\n\n\nA method consisting of a set of systematic, but flexible, guidelines for conducting inductive qualitative inquiry aimed toward theory construction. This method focuses squarely on the analytic phases of research, although both data collection and analysis inform and shape each other and are conducted in tandem.\nRead more\n\n\n\n\n\n\nDegree of participation\n\n\n\n\n\n\nFor example, if some participants were observed and interviewed and others only interviewed, or if some participants completed multiple interviews and others completed a single interview, these variations should be explained.\n\n\n\n\n\n\nUnit of analysis\n\n\n\n\n\n\nIn qualitative research, the unit of analysis is not necessarily the same as the unit of sampling (e.g., individual participants or events). Instead, some approaches use specific events as the unit of analysis, such as mentions of a particular topic or experience, or observations of a particular behavior or phenomenon, while others use groups rather than individual group participants. This specification has implications for how the data are organized and analyzed as well as the inferences drawn from the data.\n\n\n\n\n\n\nReflexivity\n\n\n\n\n\n\nReflexivity refers to intentional, systematic consideration of the potential or actual effects of the researcher(s) on all aspects of the study process.\n\n\n\n\n\n\nTransferability\n\n\n\n\n\n\nThe transferability of a research finding is the extent to which it can be applied in other contexts and studies. It is thus equivalent to or a replacement for the terms generalizability and external validity.\n\n\n\n\n\n\nGeneralizability\n\n\n\n\n\n\nThe appropriate scope for generalization of the findings beyond the study (e.g., to other settings, populations, time periods, circumstances)\n\n\n\n\n\n\nAnalytic findings\n\n\n\n\n\n\nAnalytic findings may include interpretations, inferences, narratives, themes, and models.\n\n\n\n\n\n\nFrequency counts\n\n\n\n\n\n\nThe frequency of specific themes or codes."
  },
  {
    "objectID": "guidelines/srqr/qscd.html",
    "href": "guidelines/srqr/qscd.html",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "",
    "text": "Full Title: Standards for Reporting Qualitative Research\nAuthors: Bridget O’Brien, Ilene Harris, Thomas Beckman, Darcy Reed, David Cook\nVersion: 1.1\nDOI: 10.1234/equator/1010101"
  },
  {
    "objectID": "guidelines/srqr/qscd.html#translations",
    "href": "guidelines/srqr/qscd.html#translations",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Translations",
    "text": "Translations\nThis guideline is also available in French."
  },
  {
    "objectID": "guidelines/srqr/qscd.html#about-this-guideline",
    "href": "guidelines/srqr/qscd.html#about-this-guideline",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "About this guideline",
    "text": "About this guideline\n\nWhen to use this guideline\n\nUse this guideline for writing qualitative research articles. You can use it when describing all kinds of qualitative approaches, methods, and designs.\n\n\nYou can also use this guideline for:\n\nwriting proposals or protocols (use the items within the Introduction and Method sections).\nreviewing the reporting of an article, but not for appraising its quality.\n\n\n\n\n\n\n\n\n\nDo not use this guidance for:\n\nwriting a qualitative evidence synthesis, use ENTREQ instead.\nappraising the quality of qualitative research, use an appraisal tool like CASP-Qual instead. \n\n\n\n\n\n\n\n\n\n\nRelated reporting guidelines:\n\nJARS Qual for writing qualitative, psychology manuscripts\nENTREQ for writing qualitative evidence syntheses\nFor writing studies involving interviews or focus groups, you can use this guideline or COREQ. \n\nFor appraising research consider:\n\nCASP-Qual\n\n\n\n\n\n\nHow to use this guideline\nReporting guidelines help me structure my drafts and develop as a researcher. I use them when teaching and hope that my students continue to use them. Manuel Silva - Researcher\nReporting guidelines are best used early. Read the full guideline below or use a tool when working:\n\nUse a template to draft manuscripts and avoid the blank page \nUse a log book to plan and record information whilst conducting research\nUse a checklist to double-check and reassure others that your article describes all the important details of your work\n\n\n\n\nFor drafting\n\nTemplate\n\n\n\nFor planning\n\nLog book\n\n\n\nFor checking\n\nChecklist\n\n\nThis reporting guideline does not prescribe a rigid format or standardized content. Consider each item and prioritize elements that are most relevant to your study, findings, context, and readers.\nYou may prefer to report an item in a different order, section, or in a table or figure. If you feel confident that an item is less important to your study, you could report it in an appendix or supplement. If you think an item is not applicable, state why. You don’t need to write your article in the order the items are presented.\nIn your methods section, state that you used the SRQR guideline when writing your article and cite it (see How to cite).\n\n\nWhy use this guideline\nFollowing this guideline will make it easier for other researchers to find, use, and synthesise your work. It won’t tell you how to do qualitative research, but it will help you explain your choices so others can understand what you did, why you did it, and what you found. At our journal, we ensure all qualitative articles follow SRQR as this makes them easier for readers to understand and faster to publish.Fiona Ludlow - Editor in Chief\nIt was written by experts from the qualitative research community, using a thorough development process. SRQR is endorsed by hundreds of journals, so using this guideline (and its checklist) will help you meet editors’ expectations."
  },
  {
    "objectID": "guidelines/srqr/qscd.html#guidance",
    "href": "guidelines/srqr/qscd.html#guidance",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Guidance",
    "text": "Guidance\nApprox. 17 min read\n\n\n1. Title\n\n\n\n\n\n\n\nDescribe the nature and topic of the study.\nIdentify the study as qualitative or indicate the approach or data collection methods.\n\n\n\n\n\n\nJustification, Examples, and Resources\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\nTraining and Resources\n\n\nWhy readers need this information\nThis allows readers to quickly identify the type of study. Your title will be indexed by search tools so descriptive words will make it easier for others to discover.\n\n\nExamples\n\nResidents learning from a narrative experience with dying patients: a qualitative study.\n\n\nMedical students’ perceptions of the factors influencing their academic performance: an exploratory interview study with high-achieving and re-sitting medical students.\n\n\nUndergraduate rural medical education program development: focus group consultation with the NRHA Rural Medical Educators Group.\n\n\n\n\nTraining and Resources\nSee this article for advice on writing titles that are interesting and descriptive.\nThe EQUATOR Network provides training for writing effective manuscript titles as part of their publication school.\nBack to top\n\n\n\n\n\n\n2. Abstract\n\n\n\n\n\n\n\nInformative key words in the title and abstract make manuscripts easier to find.Lorna Ellis - Evidence Synthesiser\nSummarise key elements of the study including:\n\nbackground about the problem or phenomenon of interest\ndescription of the study purpose or research question\nmethods, including the approach or perspective (e.g., general inductive, grounded theory), context (setting, time period), sample (number and key characteristics of participants, events, documents), data collection strategies (e g., observation, interview, focus group) and data analysis techniques\ndescription of main findings (e.g., themes or inferences) related to the study purpose and/or research question\nstudy implications\n\n\nInformation presented in the abstract should be consistent with the information presented in the full text.\nUse the format of your intended journal. 1\n\n\n\n\n\n\nJustification, Example, and Resources\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExample\nTraining and Resources\n\n\nWhy readers need this information\nA reader should be able to read the abstract independent of the manuscript and get a sense of the background, purpose, methods, main results and implications that will be described in greater detail in the manuscript.\nYour abstract will be indexed by search tools so descriptive words will make it easier for others to discover.\n\n\n\nExample\n\nPurpose: Although academic centers rely on assessments from medical trainees regarding the effectiveness of their faculty as teachers, little is known about how trainees conceptualize and approach their role as assessors of their clinical supervisorsMethod: In 2010, using a constructivist grounded theory approach, five focus group interviews were conducted with 19 residents from an internal medicine residency program. A constant comparative analysis of emergent themes was conducted.Results: Residents viewed clinical teaching assessment (CTA) as a time-consuming task with little reward. They reported struggling throughout the academic year to meet their CTA obligations and described several shortcut strategies they used to reduce their burden. Rather than conceptualizing their assessments as a conduit for both formative and summative feedback, residents perceived CTA as useful for the surveillance of clinical supervisors at the extremes of the spectrum of teaching effectiveness. They put the most effort, including the crafting of written comments, into the CTAs of these outliers. Trainees desired greater transparency in the CTA process and were sceptical regarding the anonymity and perceived validity of their faculty appraisals.Conclusions: Individual and system-based factors conspire to influence postgraduate medical trainees’ motivation for generating high-quality appraisals of clinical teaching. Academic centers need to address these factors if they want to maximize the usefulness of these assessments.\n\n\n\n\nTraining and Resources\nThe EQUATOR Network provides training for writing effective manuscript abstracts as part of their publication school.\nSee this article from the London School of Economics for general abstract writing tips.\nBack to top\n\n\n\n\n\nIntroduction\n\n\n3. Problem Formulation\n\n\n\n\n\n\n\nWhen the problem is described clearly I find it much easier to understand why a study was done, even if its from a different subject area, country, or decade.Nkinda Akaro - Researcher\nDescribe the theoretical and/or practical issues or concerns that make the study necessary, including:\n\nan overview of what is known about the problem\ngaps in current knowledge (the problem statement)\nthe scope of the research problem or phenomena addressed in the study (what will and will not be included)\ntheoretical and/or empirical work directly relevant to the problem or phenomena studied\nthe need for a qualitative approach.2\n\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExample\n\n\nWhy readers need this information\nThis information provides context for the research question or study purpose and situates the study in relation to other work in the field.\n\n\nExample\n\nRegulatory focus theory may therefore offer insight into the variability in responses to feedback, but how well do these experimental findings translate to real clinical situations in which the reality of responsiveness to feedback seems frustratingly complex? …. In order to better elaborate a theory to account for this variability in learner response to feedback, the present study was undertaken. We aimed, in this study, to determine how readily clinical learning events could be classified as activating a promotion or a prevention focus, and to explore, through a careful analysis of doctors’ descriptions of their feedback experiences, the predictive value of regulatory focus theory in the context of real clinical learning situations.\n\n\n\nBack to top\n\n\n\n\n\n\n4. Purpose or research question\n\n\n\n\n\n\n\nA clear purpose frames the article. Rahilah Zayn - Researcher\nInclude a statement of study intent. This can be framed as one or more research questions, purposes, goals, or objectives.3\n\n\n\n\n\n\nJustification, Example, and Resources\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExample\nTraining and Resources\n\n\nWhy readers need this information\nBy clearly stating the purpose of the study, authors set readers’ expectations for the methods, findings and discussion sections of the manuscript.\n\n\nExample\n\nThe purposes of this study were to investigate how medical students recognize, respond to and utilise feedback, and to determine whether there are maturational differences in understandings of the role of feedback across academic years in medical school.\n\n\n\nTraining and Resources\nSee this article for advice on writing qualitative research questions.\nBack to top\n\n\n\n\n\n\nMethods\n\n\n5. Qualitative approach and research paradigm\n\n\n\n\n\n\n\nDescribe your qualitative approach, your guiding theory (if appropriate), and identify the research paradigm.4 I like it when authors describe terms in their own words as well as using the proper terms.Rahilah Zayn - Researcher\nExplain why the selected approach is appropriate for the research question.\nProvide references to theories or traditions that guide the use of the approach as needed.\nI hadn’t heard the term approach before writing-up my study. I realise now that I did have an approach and that it might be different to other researcher’s approaches, so I need to describe things that I felt were obvious.Tim Westland - Researcher\n\nIf you don’t know what your approach or paradigm was, or you don’t think you had one, it’s OK to reflect on this after collecting data and you should still report it. Read this list to see what best describes your work.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nIdentifying the research paradigm helps readers understand whether the researcher assumes that there is a single, objective reality (positivist or post-positivist) and has thus designed the study to describe this reality or whether the researcher assumes multiple, subjective realities and designed the study to describe these multiple realities, with no attempt to merge or reconcile these realities (constructivist/interpretivist). The paradigm has implications for the study design, approach, methods, and techniques to ensure rigour and trustworthiness.\nSince the research paradigm does not necessarily dictate particular approaches or methods, the approach should also be clearly defined. Stating the approach provides readers the opportunity to evaluate the fidelity of the research approach to the research question(s) and consider the rationale for modifications and deviations from the selected approach. Qualitative research also includes an array of methods that can be used across paradigms and approaches. (See also Item 10).\n\n\nExamples\n\nThe study was performed from a constructivist point of view using an interpretative phenomenological epistemology. Based on the notion that social phenomena are constructed by the communal making of meaning about the underlying phenomena, we aimed to construct insightful accounts of lead consultants’ approaches to educational change, rather than to identify the true nature of these approaches. Because the management of change by lead consultants is an under-researched area, we conducted an exploratory qualitative study…\n\n\nGiven the relative dearth of explanatory theories about factors affecting medical students’ emotional reactions, we chose to develop one by applying methods associated with grounded theory, specifically constant comparative analysis, to qualitative data obtained from learning logs and interviews. Our approach was constructivist, deliberately using researchers’ own experiences and acquired knowledge to enhance theoretical sensitivity and enrich theory development.\n\nBack to top\n\n\n\n\n\n\n6. Researcher characteristics and reflexivity\n\n\n\n\n\n\n\nDescribe how roles and identities of research team members influence choice of research approach, data collection, and data analysis.5\nComing from quantitative research it felt strange to include such personal descriptions of the team. But I came to realise why it was useful to others, and that peer reviewers would expect it. Tim Westland - Researcher\nDescribe the perspectives, assumptions, prior knowledge, preliminary hypotheses, and/or motives (the stance) of the members of the research team.\nDescribe the researchers’ relationships to participants in the study and what decisions were made in light of these relationships.6\nIf your research was observational (e.g., ethnography), describe the role of the researcher along a spectrum from passive observer (no involvement in the activity studied) to participant-observer (ranging from some limited involvement in the activity to full involvement).\nThere is no expectation that the study could be precisely replicated; these characteristics and perspectives of the researcher should not be mentioned in the limitation section. (See also Item 14: Data Analysis)\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nIn positivist and post-positivist paradigms, personal characteristics and perspectives of researchers might be viewed as biases that limit the credibility of study findings, while in constructivist or interpretivist paradigms the characteristics and perspectives of the researchers are important contextual factors that are an accepted part of the study design, data collection, and data analysis. These characteristics and perspectives may explain how the researcher(s) obtained access to the site or participants included in the study or may add valuable insight during data analysis.\n\n\nExamples\n\nReflexivity was maintained by the research team through the analysis and writing by recording, discussing and challenging established assumptions. In addition both EH and SV kept reflexive diaries.\n\n\nThe first author conducted all interviews and discussion groups. Her own medical undergraduate training took place between 1995 and 2000. She was not known to the participants of this research prior to undertaking the study and deliberately did not undertake any clinical or teaching activities locally alongside this research. Whilst it was useful to know (from her own background) what the students were talking about medically (and in terms of detecting items of significance), as a researcher she made conscious efforts not to accept potentially common assumptions at face value.\n\nBack to top\n\n\n\n\n\n\n7. Context\n\n\n\n\n\n\n\nDescribe the setting/site(s) in which the study was conducted, the reason(s) why the setting/site(s) was selected, and the salient cultural, political, historical, economic and/or other external factors that influence the study.\nWhen synthesising evidence, context helps me work out how and why different studies and findings fit together (or don’t)Lorna Ellis - Evidence Synthesiser\nAdditional context may be reported with findings (i.e., the Results section) to add evidence for interpretations and to enhance discussion of transferability.\n\n\n\n\n\n\nJustification and Example\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExample\n\n\nWhy readers need this information\nThis helps readers interpret the meaning and significance of the study findings by situating them in social, cultural, temporal and other relevant contexts.\n\n\nExample\n\nWe conducted the study among hospital-based clinical teachers of students in years 4 to 6 of a six-year undergraduate medical program at Maastricht University Medical School. Years 4 to 6 are devoted to clerkships in the academic hospital and affiliated regional hospitals. Rotations differ in duration depending on the type of rotation and the discipline, and the sequence of rotations differs among students. During rotations, students spend time in the wards, the outpatient clinics, and the accident and emergency department. Clerkships in years 4 and 5 last between 4 and 10 weeks (regular clerkship), whereas students in year 6 undertake an 18-week senior clerkship in a discipline of their choice.\n\nBack to top\n\n\n\n\n\n\n8. Sampling strategy\n\n\n\n\n\n\n\nDescribe how and why research participants, sites, documents/artifacts, and/or events were selected for inclusion in (and, if appropriate, exclusion from) the study, along with a justification for this strategy.\nDescribe the sampling strategy rather than simply labeling it (e.g., purposive or snowball), since such labels do not have a universally accepted definition and, more importantly, procedures tend to be study- specific.\n\nDescribe how you established the final sample size:\n\nIf you used a flexible sampling strategy, then explain the criteria used to decide when no further sampling was necessary.\nIf data collection ended once saturation or sufficiency had been reached, then describe the specific criteria used to define saturation or sufficiency.\n\n\n\nDescribe procedures used to recruit participants, including:\n\nwho was involved in recruitment\nwhat their relationship was to participants\nhow and when recruitment occurred\nwhy these procedures were selected. (See also Item 6: Researcher characteristics and reflexivity)\n\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThis information helps readers understand the source of data / findings so they can consider the boundaries of the study and the relevance to their own context. It also gives readers insight into the researchers’ decisions, which can be important for critical appraisal.\n\n\nExamples\n\nAs students’ perceptions were previously shown to be related to gender, age, prior experience and place of attachment,[REF] we purposely selected respondents with different backgrounds. This sampling strategy led to the diversity of gender, age, prior patient experience and place of attachment shown in Table 1.\n\n\nPurposive sampling was directed towards achieving maximum variation in age and specialty, using a snowball approach (a non-probabilistic form of sampling in which persons initially chosen for the sample are used as informants to locate other persons having necessary characteristics making them eligible for the sample).[REF ]\n\n\nPotential participants were all medical students in Years 1 and 2 at the University of Toronto in 2004. Following research ethics board approval, recruitment was conducted via e-mail to class listservs. Participant responses were sent directly to the research assistant, who was unknown to participants, so that the principal investigators did not know who did or did not participate. This process was engaged to protect participants’ anonymity and to avoid any impression of coercion because the lead researcher (SG) was involved in the administration of the undergraduate curriculum at the time. Sample size was estimated to be sufficient based on the principle of theoretical saturation [REF] and our previous experience with this methodology (i.e., with a relatively homogeneous population, we expected to reach saturation with approximately 15 interviews per group). There were no exclusion criteria and we accepted the first 15 students from each class who volunteered.\n\nBack to top\n\n\n\n\n\n\n9. Ethical issues pertaining to human subjects\n\n\n\n\n\n\n\nReport approval for the study from an appropriate institutional review board for research associated with human subjects.\nIf you did not receive ethical approval, describe why.\nDescribe procedures used to protect participants, including:\n\ndata collection (e.g., recruitment and informed consent)\nanalysis (e.g., data security and integrity)\nand reporting of findings (e.g., anonymization of excerpts).\n\nIf you provided compensation or offered incentives to facilitate participation, describe this too.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nJournals will require an ethical approval statement.\nQualitative research often involves interaction between researchers and research participants. Correspondingly, researchers should ensure that participants are fully aware of their participation in a research study, the risks and benefits associated with the study, the steps and precautions the researchers will take to minimize risks, such as loss of privacy and confidentiality, and how the researchers plan to use the data.\n\n\n\nExamples\n\nEthical approval was granted by the University of Otago and student participants were invited to attend each focus group discussion by the university representatives of New Zealand Medical Students Association (NZMSA). Usually, a key concern when collecting data from students is that students may feel vulnerable when sharing their experiences with academic staff during a focus group discussion. However, this potential harm was removed as each group discussion was facilitated by a fellow student, the discussions were transcribed by a professional transcribing service, and only the primary researchers [Names] had access to the raw data.\n\nBack to top\n\n\n\n\n\n\n10. Data collection methods\n\n\n\n\n\n\n\nDescribe data collection methods and design in detail, and justify them in relation to the research question(s), paradigm, approach, and other methods.7\n\nIf data collection and analysis was iterative:\n\ndescribe the iterative process\nif changes occurred during the research process, describe how and why study procedures changed in response to evolving study findings.\n\nIdentify the study period.\nDescribe important characteristics of the individuals conducting interviews, observations or focus groups, and methods used to train these individuals.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThe study period helps readers place the study in temporal context and identify factors not mentioned by the authors that might affect findings, interpretation, and implications. (See Item 8 for ending data collection.)\n\nDescribing researcher characteristics clarifies the relationship between the individuals involved in data collection and the participants in the research and also explains what efforts were made to ensure consistency in the data collection process (See Items 6 and 15.)\n\n\n\n\nExamples\n\nFurther, it was decided that group interviews, also known as focus group discussions would be the best means of data collection. This is a method of data collection that enables group members to feed off each other’s ideas and an effective moderator will maintain group focus whilst at the same time permitting flexibility in the direction those aspects of the discussion might take. [REF]\n\n\nAdjustments to the interview protocol were made according to early experience and information participants had provided (i.e. redundant questions were eliminated; questions were reworded to improve flow and clarity; additional probes were included).\n\n\nFaculty staff were then interviewed individually by a trained study investigator in a 15-minute, semi-structured interview. This sequence was repeated with other video encounters. Table 2 presents examples of interview questions. Each faculty member was interviewed by at least three interviewers over their various interviews. Interviewers were chosen based on their experience in interviewing. All were trained during a half-day meeting to interpret and deliver the interview guide in the same manner in order to elicit information of a consistent type.\n\nBack to top\n\n\n\n\n\n\n11. Data collection instruments and technologies\n\n\n\n\n\n\n\nDescribe all data collection instruments, including their development, and if/how they changed over the course of the study. Cite relevant literatures, theories or conceptual frameworks as appropriate. Consider sharing the data collection instrument(s) or a detailed description of them in the article body, supplementary material, or published elsewhere. I publish my materials on the OSF and then describe and cite them in my articles.Tim Westland - Researcher\nDescribe the use of equipment for audio or video recording, reproduction of paper documents or computer files, or other processes in data collection.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nDescribing instruments and equipment helps readers understand the full context in which data collection occurred and how this context might have affected data collection (e.g., the influence of recording devices on participants’ behaviors; the nature of inferences drawn from live vs. recorded events).\n\n\nExamples\n\nTo facilitate the discussion and to maintain consistency over different sets of discussions, key trigger questions were devised prior to the discussion. The opening trigger question was: Thinking back to some of your best clinical learning placements in 4th and 5th year. What was it about those clinical placements that provided good opportunities for learning?\n\n\nInterviews included discussion of the expectations, processes and consequences of AEE [authentic early experiences]. The interview schedule was derived following identification of questions that could not be fully answered in a systematic review of previous empirical or theoretical literature. It comprised a sequence of topic areas including experiences in action, and areas of frustration in Medical Education such as the learning of content knowledge, achieving functional knowledge, and transfer of knowledge.[REF]… Interested readers can request a copy of the schedules from the corresponding author. Interviews lasted between 20 and 90 min. and discussion groups between 60 and 90 min. All interviews and discussion groups were conducted in private rooms at the participant’s workplace—the medical school for students and faculty, and individual places of work for workplace supervisors (except for one who chose to be interviewed at the medical school). All data were audio-recorded, and transcribed verbatim.\n\nBack to top\n\n\n\n\n\n\n12. Units of study\n\n\n\n\n\n\n\nDescribe the number of participants, documents, or events included in the study (the units of study).\nDescribe characteristics of the participants, documents or events that are relevant to the study purpose and research question(s).8\nInclude the dates or timeframe for participation.\nIf the actual sample differs from the target sample, describe:\n\nthe difference,\nwhy these differences may have occurred,\nhow this might affect the findings.\n\nIf the degree of participation varied among individuals, describe:\n\nthe different levels of participation,\nthe reasons for these differences (i.e., the researchers’ choice or the participants’ preferences),\nand how these different levels of participation were taken into account in the analysis.\n\nThis information could appear in the Methods section as part of the description of the sample, or at the beginning of the Results section to provide context for the findings presented.\n\n\n\n\n\n\nMore Information, Justification, and Example\n\n\n\n\n\nJump to:\n\nHow does this item differ to Sampling (Item 8)?\nWhy readers need this information\nExample\n\n\nHow does this item differ to Sampling (Item 8)?\nThe sampling item (Item 8) describes the target or ideal participants, documents, or events selected for the study. By contrast, this item focuses on description of the actual participants, documents or events included in the study.\n\n\nWhy readers need this information\nThis helps readers know whose experiences and perspective are and are not included.\n\n\n\nExample\n\nOf the 70 Mindful Communication program participants, 46 met the eligibility requirements to participate in the in-depth interviews. We randomly chose and then contacted 22 participants, of whom 20 agreed to be interviewed within six months of completing the program: 15 in person and 5 by telephone. Two declined for lack of time. On reaching saturation after 20 interviews, no further attempts to contact the remaining 24 participants were made.\n\n\n\n\n\nThere were 31 nursing handovers covering 137 patients, and 21 resident handovers covering 101 patients included in this study.\n\nBack to top\n\n\n\n\n\n\n13. Data processing\n\n\n\n\n\n\n\nDescribe the ways in which data are prepared for analysis and managed throughout the analysis process. These activities might include transcription, coding, data entry, and organization of data. (See footnote for audio or visual recordings9).\nDescribe the processes used to organize, compile, and categorize data (e.g., field notes, transcripts, documents, photographs, artifacts) for analysis.\nIf you used transcripts, describe procedures used to check accuracy.\nDescribe procedures used to maintain data security and protect the privacy of participants, as specified in the human subjects protocols (see footnote on anonymisation10 as an example).\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript.\n\n\n\nExamples\n\nInterviews were anonymised and each participant was given a code number.\n\n\nThe interviewers and another member of the research team (H.B.) reviewed transcripts for accuracy.\n\n\nWe collected data throughout the admission process through direct observation, audio- recording, and chart extraction. We audio-recorded, transcribed, and anonymized both the overnight and morning case review discussions. We also observed the morning case review discussions in person and collected field notes. For each case review discussion, we copied the admission notes from the patient’s record and de-identified all data.\n\nBack to top\n\n\n\n\n\n\n14. Data analysis\n\n\n\n\n\n\n\nDescribe your analytic process as transparently as possible.11\nIf you used an approach that has a well-defined process for data analysis (e.g., grounded theory, discourse analysis, phenomenography):\n\ncite the guiding literature\ndescribe your processes in sufficient detail so readers can judge the extent to which your processes align with the guiding approach.\nIf you modified or deviated from the guiding approach, explain and justify these modifications.\n\nSpecify the unit of analysis.12\nExplain the rationale underlying different decisions made throughout the data analysis process to provide as much transparency as possible.13\nIf observations that contrast or deviate from identified concepts or themes were important to your analysis, describe how these discrepancies were handled during the analysis.\nIf you drew upon a theoretical perspective or framework during analysis, describe theoretical or other influences on your analysis scheme or categories if they exist. If you identified a theoretical perspective or framework early in the conception of the study or after reviewing some or all of their data, consider referring to these as sensitizing concepts to acknowledge that the approach is inductive, but with influence from relevant theory, models, or organizational schemes. Alternatively, explain that themes were developed from the data with no external influences.\nDescribe which members of the research team are involved in data analysis and what perspective(s) they bring to the analysis.\nIf software was used to assist with data analysis14, describe how it was used. Simply stating that software was used is insufficient.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nTechniques used for data analysis will depend on the paradigm, approach, and/or data collection methods selected by the researchers. Correspondingly, authors should be as transparent as possible about the analytic process so that readers can follow the logic of inquiry from the research question(s) to the analysis and findings.\n\n\nExamples\n\n…we brought sensitizing concepts to the analysis while we conducted an open, inductive analysis.[REF]In this case the sensitizing concepts arose, a priori to analysis, from a framework derived from the literature [REF] (as described above), in which participants’ motivations to act are based on principles of professionalism, internal affect, or potential implications of their actions.[REF]\n\n\nThrough an iterative process of listening, discussing, and relistening, the team identified and consensually validated emerging themes[REF] and appended segments of dialogue supporting the proposed themes. Recruitment stopped when saturation was reached (no new themes were identified). The team systematically reviewed the themes and sorted them into content domains. The team used an analytic matrix to identify patterns and connections amongst the domains. Two of us not involved in the qualitative coding process (R.E., M.K.) audited the analytic matrix, choice of quotes, and thematic analysis.\n\n\nThe analysis started after the first interview. All data were analyzed with the aid of the audio- coding facility of the NVivo 8: QSR International Pty Ltd, Doncaster, Vic, Australia programme. First, [name] and [name] coded independently from one another, making sure to stay semantically close to the participants’ wording. Then we discussed these open codes and defined axial codes.[REF] New insights about the impact of CST were written down in memos.\n\n\nVideotapes were analysed using immersion/crystallisation methods of qualitative data analysis.[REF] With no pre-existing framework developed in advance for analysis, an inductive approach was used to discover patterns of NVB in the data. A team of six researchers met weekly for 18 months to view videos together. Using a consensus-building approach based on a combination of field notes, opportunistic interviews with the participants, and repeated viewing of the same material, sometimes many months apart, we eventually achieved consensus on verbal, non-verbal, and physical themes and patterns observed in the data. Finally, as a test of goodness-of-fit, we carefully reviewed the videotapes for any deviant cases that did not fit the categories we had developed.\n\n\nAll transcripts were coded thematically by four of the five authors, who met regularly to identify areas of convergence until full agreement was reached. One of the interviewers (P.M.) maintained an audit trail to track the team’s developing thinking. A process of dialectical empiricism[REF] was used to categorise the emergent themes into more abstract concepts…\n\nBack to top\n\n\n\n\n\n\n15. Techniques to enhance trustworthiness\n\n\n\n\n\n\n\nDescribe methods used to ensure trustworthiness and credibility throughout the data collection and analysis process. (See footnote on commonly used techniques15).\nExplain your choice of techniques and why these are appropriate for the particular study.\n\n\n\n\n\n\nJustification, Examples, and Resources\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\nTraining and resources\n\n\nWhy readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript.\n\n\n\nExamples\nMember checking\n\nMember checks [REF] with an external TBL expert (R.L.) supported the validity of these analyses.\n\nTriangulation of data types and data sources\n\nThe interview data were triangulated with the data of 11 student and supervisor focus groups of a previous study, and more specifically, with those data that concern in particular the influence of CST [Communication Skills Training] on the development of patient-centredness…. Triangulation with the focus group data allowed us to broaden the in-depth information from the interviews in the analysis and to share and compare this with information from students and doctors with varying levels of CST (no, limited, full programme) and from two universities (Universities of Antwerp and Ghent). Moreover, this enabled us to better explore the evolution over time, given that the focus groups included participants at different stages of their study: before clerkships (year 4, undergraduate), during clerkships (year 6, undergraduate), after clerkships (year 7, undergraduate) and postgraduate (general practice trainees, and supervising specialists and GPs; Table 1).\n\n\n\n\n\nFinally, as a test of goodness-of-fit, we carefully reviewed the videotapes for any deviant cases that did not fit the categories we had developed.\n\nTriangulation of Researchers + Audit trail\n\nTo ensure rigor and increase authenticity in our methodology, we used two kinds of triangulation—investigator triangulation and data triangulation.[REF] We sought analytical rigor using an audit trail and multiple coders; our coding team included an experienced clinician (M.G.) as well as a nonclinician with expertise in medical communication and team dynamics (L.L.).\n\n\n\nTraining and resources\nSee Lincoln and Guba’s Evaluative Criteria for trustworthiness.\nBack to top\n\n\n\n\n\n\nResults\n\n\n16. Synthesis and interpretation\n\n\n\n\n\n\n\nDescribe the main analytic findings.16\nIn most cases, report a synthesis of the data along with specific quotes, examples, or illustrations derived from the data.\n\nI like when authors use words like many, few, or all to describe frequency.Nkinda Akaro - Researcher\nConsider describing frequency, variety, representativeness, counter-examples, concrete details, contextualization, conditions, and qualifications related to the findings.\nFrequency counts play a limited role in qualitative research, and need not be reported unless they play a meaningful role in interpretation of the data.\nIf your findings include integration with prior literature or theory and/or the development of a theory, model or meta-narrative, consider using tables and figures to communicate these findings.\nItems 16 and 18 can be reported in Results or Discussion sections.17\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript.\n\n\n\nExamples\n\nWe identified four patterns of NVB (non-verbal behavior) that relate to handover quality and have dubbed them: (1) joint focus of attention; (2) the poker hand; (3) parallel play; and (4) kerbside consultation. Each pattern constitutes a transfix, or systematic way of participating non-verbally in the care transfer process. And, although there are variations in each pattern, we have been able to code virtually every handover we have observed in nursing, medicine and surgery into one of these four categories.\n\n\nBecause our participants came from similar educational backgrounds, had studied medicine as their tertiary course, were embedded in the culture of medicine, and were associated in meaningful ways with a single medical school, we approached their transcripts with the assumption that they belonged to a loosely formed discourse community. Although their graduation dates ranged over a period of 50 years and their collective sphere of practice included 10 different specialty areas, there were many similarities in their experiences of enculturation during and after medical school.\n\n\nTheir three major (often overlapping) areas of concern were epistemic (acquiring knowledge and skill), interpersonal (relating to patients, families, colleagues and administrators) and personal (achieving work–life balance). In each of these areas, medical enculturation was achieved by two overlapping processes, absorption and assimilation, each of which may have distinct implications for postgraduate medical education.\n\nBack to top\n\n\n\n\n\n\n17. Links to empirical data\n\n\n\n\n\n\n\nProvide evidence (e.g., quotes, field notes, text excerpts, photographs) to substantiate the more general and abstract concepts or inferences they present as findings.18 I love seeing evidence like this because it really brings results to life. Nkinda Akaro - Researcher\nYou could report this evidence in a table or figure, incorporated into a narrative description of findings, as a stand-alone narrative, or in text blocks embedded in the manuscript text. If you are constrained by word limits or media limitations (e.g.. video), consider sharing data via an appendix, supplemental material, or web-based repository.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript.\n\n\n\nExamples\nSee Frankel et al. for an excellent example of how to use photographs (or snapshots from video) to illustrate and provide supporting evidence for patterns of behavior identified in the analysis. http://qualitysafety.bmj.com/content/21/Suppl_1/i121.\n\nWe identified five interruption types: (1) probing for further data, (2) prompting for expected sequence, (3) teaching around the case, (4) thinking out loud, and (5) providing direction (see Table 1). Several interruption types served both goals of the case review discussions—teaching and patient care. For example, when thinking out loud, supervisors reasoned through problems and taught the team: So that’s the big question, did she have a mechanical fall, or did she have a medicine-related fall? (Case 2). Supervisors prompted for expected sequence, preventing presenters from skipping over information while simultaneously allowing the supervisor to instruct the team on presentation style: So now you can tell me what the rest of his test results are because I haven’t heard those (Case 16).\n\nBack to top\n\n\n\n\n\n\nDiscussion\n\n\n18. Integration with prior work, implications, transferability, and contribution(s) to the field\n\n\n\n\n\n\n\nDescribe how the findings and conclusions connect to, support, elaborate on, or challenge previous findings, experiences, theory, or a guiding paradigm or approach.19\nDescribe how the findings contribute to or advance the field.\nDescribe any implications of the work, such transferability or generalizability.\n\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThe short summary reminds readers of the main findings and may help them assess whether the subsequent interpretation and implications formulated by the authors are supported by the findings.\n\n\nExamples\n\nThis study contributes to the understanding and discussion of the complexity of involving patients in healthcare education. It shows that integrating patient-led teaching into initiatives that are partly faculty-led influences the way in which students perceive learning from and with PIs. What is not known, however, is whether perceptions are also affected by type of health profession and the students’ different orientation towards logics of care and science, and issues of authority and power relations.\n\nFor complete examples of Discussions, see:\n\nHenriksen & Ringsted, 2013\nWesterman et al., 2013.\n\nBack to top\n\n\n\n\n\n\n19. Limitations\n\n\n\n\n\n\n\nDescribe problems or gaps in their efforts to ensure trustworthiness and the potential implications.20 Authors should never feel nervous to report limitations. In fact, thoughtful discussion of limitations is a hallmark of good research!Charles Ruggle - Editor\nDescribe how the chosen paradigm, approach, and methods will influence the situations to which the findings may reasonably apply.21 (See also Item 18.)\nDescribe how specific decisions or events in the conduct of the study strengthen or weaken the rigor of the findings.\n\n\n\n\n\n\nJustification and Example\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExample\n\n\nWhy readers need this information\nAll research has limitations. Discussing them will help readers consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be). If you don’t address limitations, editors and peer reviewers may ask you about them which will delay publication.\n\n\nExample\n\nThe study has several limitations. One is that the focus group interview method reveals students’ perceptions rather than their actual behaviors. Observations of the patient-led teaching encounter may have illuminated an understanding of the relationship between patient instructors and medical students. Another limitation is that the PI-led teaching is optional rather than mandatory, which may have influenced students’ attitudes in a positive direction. Moreover, students who are eager to take on extra-curricular activities may not be representative of the whole population. That only 23 out of 39 students signed up for this study might also have influenced results if the missing group of students represented other perceptions than those present in the focus groups. However the received data from the focus groups were rich in information and diverse perceptions were present. Another limitation is the overrepresentation of women over men in our sample. Even though women are also overrepresented in medical school this might potentially have influenced results, but gender differences in perceptions were nevertheless not identified in the data.\n\nBack to top\n\n\n\n\n\n\nOther\n\n\n20. Conflicts of interest\n\n\n\n\n\n\n\nAuthors shouldn’t feel nervous reporting conflicts of interest. They are very common and rarely influence our decision to publish.Charles Ruggle - Editor\nDescribe any real or potential conflicts of interest that might have influenced or could appear to have influenced the research.\nDescribe:\n\nhow these conflicts were managed in the conduct of the study,\nthe potential impact on study findings and/or conclusions.\n\nSome aspects may be mentioned as part of reflexivity (see Item 6).\n\n\n\n\n\n\nJustification\n\n\n\n\n\n\nWhy readers need this information\nMany factors, including professional and personal relationships and activities, can influence the design, conduct, and reporting of the clinical science that informs health care decision. The potential for conflict of interest exists when these relationships and activities may bias judgment (1). Many stakeholders— editors, peer reviewers, clinicians, educators, policymakers, patients, and the public—rely on the disclosure of authors’ relationships and activities to inform their assessments. Trust in the transparency, consistency, and completeness of these disclosures is essential. - ICMJE\n\n\n\n\n\n\n\n21. Funding\n\n\n\n\n\n\n\nDescribe any sources of funding and other support for the study.\nDescribe the role of funders in data collection, data analysis, and reporting if applicable."
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-title-2",
    "href": "guidelines/srqr/qscd.html#sec-title-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis allows readers to quickly identify the type of study. Your title will be indexed by search tools so descriptive words will make it easier for others to discover."
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-title-3",
    "href": "guidelines/srqr/qscd.html#sec-title-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nResidents learning from a narrative experience with dying patients: a qualitative study.\n\n\nMedical students’ perceptions of the factors influencing their academic performance: an exploratory interview study with high-achieving and re-sitting medical students.\n\n\nUndergraduate rural medical education program development: focus group consultation with the NRHA Rural Medical Educators Group."
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-title-4",
    "href": "guidelines/srqr/qscd.html#sec-title-4",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Training and Resources",
    "text": "Training and Resources\nSee this article for advice on writing titles that are interesting and descriptive.\nThe EQUATOR Network provides training for writing effective manuscript titles as part of their publication school.\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-abstract-2",
    "href": "guidelines/srqr/qscd.html#sec-abstract-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nA reader should be able to read the abstract independent of the manuscript and get a sense of the background, purpose, methods, main results and implications that will be described in greater detail in the manuscript.\nYour abstract will be indexed by search tools so descriptive words will make it easier for others to discover."
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-abstract-3",
    "href": "guidelines/srqr/qscd.html#sec-abstract-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nPurpose: Although academic centers rely on assessments from medical trainees regarding the effectiveness of their faculty as teachers, little is known about how trainees conceptualize and approach their role as assessors of their clinical supervisorsMethod: In 2010, using a constructivist grounded theory approach, five focus group interviews were conducted with 19 residents from an internal medicine residency program. A constant comparative analysis of emergent themes was conducted.Results: Residents viewed clinical teaching assessment (CTA) as a time-consuming task with little reward. They reported struggling throughout the academic year to meet their CTA obligations and described several shortcut strategies they used to reduce their burden. Rather than conceptualizing their assessments as a conduit for both formative and summative feedback, residents perceived CTA as useful for the surveillance of clinical supervisors at the extremes of the spectrum of teaching effectiveness. They put the most effort, including the crafting of written comments, into the CTAs of these outliers. Trainees desired greater transparency in the CTA process and were sceptical regarding the anonymity and perceived validity of their faculty appraisals.Conclusions: Individual and system-based factors conspire to influence postgraduate medical trainees’ motivation for generating high-quality appraisals of clinical teaching. Academic centers need to address these factors if they want to maximize the usefulness of these assessments."
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-abstract-4",
    "href": "guidelines/srqr/qscd.html#sec-abstract-4",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Training and Resources",
    "text": "Training and Resources\nThe EQUATOR Network provides training for writing effective manuscript abstracts as part of their publication school.\nSee this article from the London School of Economics for general abstract writing tips.\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-problem-formulation-2",
    "href": "guidelines/srqr/qscd.html#sec-problem-formulation-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information provides context for the research question or study purpose and situates the study in relation to other work in the field."
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-problem-formulation-3",
    "href": "guidelines/srqr/qscd.html#sec-problem-formulation-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nRegulatory focus theory may therefore offer insight into the variability in responses to feedback, but how well do these experimental findings translate to real clinical situations in which the reality of responsiveness to feedback seems frustratingly complex? …. In order to better elaborate a theory to account for this variability in learner response to feedback, the present study was undertaken. We aimed, in this study, to determine how readily clinical learning events could be classified as activating a promotion or a prevention focus, and to explore, through a careful analysis of doctors’ descriptions of their feedback experiences, the predictive value of regulatory focus theory in the context of real clinical learning situations.\n\n\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-purpose-2",
    "href": "guidelines/srqr/qscd.html#sec-purpose-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nBy clearly stating the purpose of the study, authors set readers’ expectations for the methods, findings and discussion sections of the manuscript."
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-purpose-3",
    "href": "guidelines/srqr/qscd.html#sec-purpose-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nThe purposes of this study were to investigate how medical students recognize, respond to and utilise feedback, and to determine whether there are maturational differences in understandings of the role of feedback across academic years in medical school."
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-purpose-4",
    "href": "guidelines/srqr/qscd.html#sec-purpose-4",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Training and Resources",
    "text": "Training and Resources\nSee this article for advice on writing qualitative research questions.\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-qualitative-approach-2",
    "href": "guidelines/srqr/qscd.html#sec-qualitative-approach-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nIdentifying the research paradigm helps readers understand whether the researcher assumes that there is a single, objective reality (positivist or post-positivist) and has thus designed the study to describe this reality or whether the researcher assumes multiple, subjective realities and designed the study to describe these multiple realities, with no attempt to merge or reconcile these realities (constructivist/interpretivist). The paradigm has implications for the study design, approach, methods, and techniques to ensure rigour and trustworthiness.\nSince the research paradigm does not necessarily dictate particular approaches or methods, the approach should also be clearly defined. Stating the approach provides readers the opportunity to evaluate the fidelity of the research approach to the research question(s) and consider the rationale for modifications and deviations from the selected approach. Qualitative research also includes an array of methods that can be used across paradigms and approaches. (See also Item 10)."
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-qualitative-approach-3",
    "href": "guidelines/srqr/qscd.html#sec-qualitative-approach-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nThe study was performed from a constructivist point of view using an interpretative phenomenological epistemology. Based on the notion that social phenomena are constructed by the communal making of meaning about the underlying phenomena, we aimed to construct insightful accounts of lead consultants’ approaches to educational change, rather than to identify the true nature of these approaches. Because the management of change by lead consultants is an under-researched area, we conducted an exploratory qualitative study…\n\n\nGiven the relative dearth of explanatory theories about factors affecting medical students’ emotional reactions, we chose to develop one by applying methods associated with grounded theory, specifically constant comparative analysis, to qualitative data obtained from learning logs and interviews. Our approach was constructivist, deliberately using researchers’ own experiences and acquired knowledge to enhance theoretical sensitivity and enrich theory development.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-researcher-characteristics-and-reflexivity-2",
    "href": "guidelines/srqr/qscd.html#sec-researcher-characteristics-and-reflexivity-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nIn positivist and post-positivist paradigms, personal characteristics and perspectives of researchers might be viewed as biases that limit the credibility of study findings, while in constructivist or interpretivist paradigms the characteristics and perspectives of the researchers are important contextual factors that are an accepted part of the study design, data collection, and data analysis. These characteristics and perspectives may explain how the researcher(s) obtained access to the site or participants included in the study or may add valuable insight during data analysis."
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-researcher-characteristics-and-reflexivity-3",
    "href": "guidelines/srqr/qscd.html#sec-researcher-characteristics-and-reflexivity-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nReflexivity was maintained by the research team through the analysis and writing by recording, discussing and challenging established assumptions. In addition both EH and SV kept reflexive diaries.\n\n\nThe first author conducted all interviews and discussion groups. Her own medical undergraduate training took place between 1995 and 2000. She was not known to the participants of this research prior to undertaking the study and deliberately did not undertake any clinical or teaching activities locally alongside this research. Whilst it was useful to know (from her own background) what the students were talking about medically (and in terms of detecting items of significance), as a researcher she made conscious efforts not to accept potentially common assumptions at face value.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-context-2",
    "href": "guidelines/srqr/qscd.html#sec-context-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis helps readers interpret the meaning and significance of the study findings by situating them in social, cultural, temporal and other relevant contexts."
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-context-3",
    "href": "guidelines/srqr/qscd.html#sec-context-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nWe conducted the study among hospital-based clinical teachers of students in years 4 to 6 of a six-year undergraduate medical program at Maastricht University Medical School. Years 4 to 6 are devoted to clerkships in the academic hospital and affiliated regional hospitals. Rotations differ in duration depending on the type of rotation and the discipline, and the sequence of rotations differs among students. During rotations, students spend time in the wards, the outpatient clinics, and the accident and emergency department. Clerkships in years 4 and 5 last between 4 and 10 weeks (regular clerkship), whereas students in year 6 undertake an 18-week senior clerkship in a discipline of their choice.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-sampling-strategy-2",
    "href": "guidelines/srqr/qscd.html#sec-sampling-strategy-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information helps readers understand the source of data / findings so they can consider the boundaries of the study and the relevance to their own context. It also gives readers insight into the researchers’ decisions, which can be important for critical appraisal."
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-sampling-strategy-3",
    "href": "guidelines/srqr/qscd.html#sec-sampling-strategy-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nAs students’ perceptions were previously shown to be related to gender, age, prior experience and place of attachment,[REF] we purposely selected respondents with different backgrounds. This sampling strategy led to the diversity of gender, age, prior patient experience and place of attachment shown in Table 1.\n\n\nPurposive sampling was directed towards achieving maximum variation in age and specialty, using a snowball approach (a non-probabilistic form of sampling in which persons initially chosen for the sample are used as informants to locate other persons having necessary characteristics making them eligible for the sample).[REF ]\n\n\nPotential participants were all medical students in Years 1 and 2 at the University of Toronto in 2004. Following research ethics board approval, recruitment was conducted via e-mail to class listservs. Participant responses were sent directly to the research assistant, who was unknown to participants, so that the principal investigators did not know who did or did not participate. This process was engaged to protect participants’ anonymity and to avoid any impression of coercion because the lead researcher (SG) was involved in the administration of the undergraduate curriculum at the time. Sample size was estimated to be sufficient based on the principle of theoretical saturation [REF] and our previous experience with this methodology (i.e., with a relatively homogeneous population, we expected to reach saturation with approximately 15 interviews per group). There were no exclusion criteria and we accepted the first 15 students from each class who volunteered.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-ethics-2",
    "href": "guidelines/srqr/qscd.html#sec-ethics-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nJournals will require an ethical approval statement.\nQualitative research often involves interaction between researchers and research participants. Correspondingly, researchers should ensure that participants are fully aware of their participation in a research study, the risks and benefits associated with the study, the steps and precautions the researchers will take to minimize risks, such as loss of privacy and confidentiality, and how the researchers plan to use the data."
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-ethics-3",
    "href": "guidelines/srqr/qscd.html#sec-ethics-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nEthical approval was granted by the University of Otago and student participants were invited to attend each focus group discussion by the university representatives of New Zealand Medical Students Association (NZMSA). Usually, a key concern when collecting data from students is that students may feel vulnerable when sharing their experiences with academic staff during a focus group discussion. However, this potential harm was removed as each group discussion was facilitated by a fellow student, the discussions were transcribed by a professional transcribing service, and only the primary researchers [Names] had access to the raw data.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-data-collection-methods-2",
    "href": "guidelines/srqr/qscd.html#sec-data-collection-methods-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThe study period helps readers place the study in temporal context and identify factors not mentioned by the authors that might affect findings, interpretation, and implications. (See Item 8 for ending data collection.)\n\nDescribing researcher characteristics clarifies the relationship between the individuals involved in data collection and the participants in the research and also explains what efforts were made to ensure consistency in the data collection process (See Items 6 and 15.)"
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-data-collection-methods-3",
    "href": "guidelines/srqr/qscd.html#sec-data-collection-methods-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nFurther, it was decided that group interviews, also known as focus group discussions would be the best means of data collection. This is a method of data collection that enables group members to feed off each other’s ideas and an effective moderator will maintain group focus whilst at the same time permitting flexibility in the direction those aspects of the discussion might take. [REF]\n\n\nAdjustments to the interview protocol were made according to early experience and information participants had provided (i.e. redundant questions were eliminated; questions were reworded to improve flow and clarity; additional probes were included).\n\n\nFaculty staff were then interviewed individually by a trained study investigator in a 15-minute, semi-structured interview. This sequence was repeated with other video encounters. Table 2 presents examples of interview questions. Each faculty member was interviewed by at least three interviewers over their various interviews. Interviewers were chosen based on their experience in interviewing. All were trained during a half-day meeting to interpret and deliver the interview guide in the same manner in order to elicit information of a consistent type.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-data-collection-instruments-2",
    "href": "guidelines/srqr/qscd.html#sec-data-collection-instruments-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nDescribing instruments and equipment helps readers understand the full context in which data collection occurred and how this context might have affected data collection (e.g., the influence of recording devices on participants’ behaviors; the nature of inferences drawn from live vs. recorded events)."
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-data-collection-instruments-3",
    "href": "guidelines/srqr/qscd.html#sec-data-collection-instruments-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nTo facilitate the discussion and to maintain consistency over different sets of discussions, key trigger questions were devised prior to the discussion. The opening trigger question was: Thinking back to some of your best clinical learning placements in 4th and 5th year. What was it about those clinical placements that provided good opportunities for learning?\n\n\nInterviews included discussion of the expectations, processes and consequences of AEE [authentic early experiences]. The interview schedule was derived following identification of questions that could not be fully answered in a systematic review of previous empirical or theoretical literature. It comprised a sequence of topic areas including experiences in action, and areas of frustration in Medical Education such as the learning of content knowledge, achieving functional knowledge, and transfer of knowledge.[REF]… Interested readers can request a copy of the schedules from the corresponding author. Interviews lasted between 20 and 90 min. and discussion groups between 60 and 90 min. All interviews and discussion groups were conducted in private rooms at the participant’s workplace—the medical school for students and faculty, and individual places of work for workplace supervisors (except for one who chose to be interviewed at the medical school). All data were audio-recorded, and transcribed verbatim.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-units-of-study-2",
    "href": "guidelines/srqr/qscd.html#sec-units-of-study-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "How does this item differ to Sampling (Item 8)?",
    "text": "How does this item differ to Sampling (Item 8)?\nThe sampling item (Item 8) describes the target or ideal participants, documents, or events selected for the study. By contrast, this item focuses on description of the actual participants, documents or events included in the study."
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-units-of-study-3",
    "href": "guidelines/srqr/qscd.html#sec-units-of-study-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis helps readers know whose experiences and perspective are and are not included."
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-units-of-study-4",
    "href": "guidelines/srqr/qscd.html#sec-units-of-study-4",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nOf the 70 Mindful Communication program participants, 46 met the eligibility requirements to participate in the in-depth interviews. We randomly chose and then contacted 22 participants, of whom 20 agreed to be interviewed within six months of completing the program: 15 in person and 5 by telephone. Two declined for lack of time. On reaching saturation after 20 interviews, no further attempts to contact the remaining 24 participants were made.\n\n\n\n\n\nThere were 31 nursing handovers covering 137 patients, and 21 resident handovers covering 101 patients included in this study.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-data-processing-2",
    "href": "guidelines/srqr/qscd.html#sec-data-processing-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript."
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-data-processing-3",
    "href": "guidelines/srqr/qscd.html#sec-data-processing-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nInterviews were anonymised and each participant was given a code number.\n\n\nThe interviewers and another member of the research team (H.B.) reviewed transcripts for accuracy.\n\n\nWe collected data throughout the admission process through direct observation, audio- recording, and chart extraction. We audio-recorded, transcribed, and anonymized both the overnight and morning case review discussions. We also observed the morning case review discussions in person and collected field notes. For each case review discussion, we copied the admission notes from the patient’s record and de-identified all data.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-data-analysis-2",
    "href": "guidelines/srqr/qscd.html#sec-data-analysis-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nTechniques used for data analysis will depend on the paradigm, approach, and/or data collection methods selected by the researchers. Correspondingly, authors should be as transparent as possible about the analytic process so that readers can follow the logic of inquiry from the research question(s) to the analysis and findings."
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-data-analysis-3",
    "href": "guidelines/srqr/qscd.html#sec-data-analysis-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\n…we brought sensitizing concepts to the analysis while we conducted an open, inductive analysis.[REF]In this case the sensitizing concepts arose, a priori to analysis, from a framework derived from the literature [REF] (as described above), in which participants’ motivations to act are based on principles of professionalism, internal affect, or potential implications of their actions.[REF]\n\n\nThrough an iterative process of listening, discussing, and relistening, the team identified and consensually validated emerging themes[REF] and appended segments of dialogue supporting the proposed themes. Recruitment stopped when saturation was reached (no new themes were identified). The team systematically reviewed the themes and sorted them into content domains. The team used an analytic matrix to identify patterns and connections amongst the domains. Two of us not involved in the qualitative coding process (R.E., M.K.) audited the analytic matrix, choice of quotes, and thematic analysis.\n\n\nThe analysis started after the first interview. All data were analyzed with the aid of the audio- coding facility of the NVivo 8: QSR International Pty Ltd, Doncaster, Vic, Australia programme. First, [name] and [name] coded independently from one another, making sure to stay semantically close to the participants’ wording. Then we discussed these open codes and defined axial codes.[REF] New insights about the impact of CST were written down in memos.\n\n\nVideotapes were analysed using immersion/crystallisation methods of qualitative data analysis.[REF] With no pre-existing framework developed in advance for analysis, an inductive approach was used to discover patterns of NVB in the data. A team of six researchers met weekly for 18 months to view videos together. Using a consensus-building approach based on a combination of field notes, opportunistic interviews with the participants, and repeated viewing of the same material, sometimes many months apart, we eventually achieved consensus on verbal, non-verbal, and physical themes and patterns observed in the data. Finally, as a test of goodness-of-fit, we carefully reviewed the videotapes for any deviant cases that did not fit the categories we had developed.\n\n\nAll transcripts were coded thematically by four of the five authors, who met regularly to identify areas of convergence until full agreement was reached. One of the interviewers (P.M.) maintained an audit trail to track the team’s developing thinking. A process of dialectical empiricism[REF] was used to categorise the emergent themes into more abstract concepts…\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-trustworthiness-2",
    "href": "guidelines/srqr/qscd.html#sec-trustworthiness-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript."
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-trustworthiness-3",
    "href": "guidelines/srqr/qscd.html#sec-trustworthiness-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\nMember checking\n\nMember checks [REF] with an external TBL expert (R.L.) supported the validity of these analyses.\n\nTriangulation of data types and data sources\n\nThe interview data were triangulated with the data of 11 student and supervisor focus groups of a previous study, and more specifically, with those data that concern in particular the influence of CST [Communication Skills Training] on the development of patient-centredness…. Triangulation with the focus group data allowed us to broaden the in-depth information from the interviews in the analysis and to share and compare this with information from students and doctors with varying levels of CST (no, limited, full programme) and from two universities (Universities of Antwerp and Ghent). Moreover, this enabled us to better explore the evolution over time, given that the focus groups included participants at different stages of their study: before clerkships (year 4, undergraduate), during clerkships (year 6, undergraduate), after clerkships (year 7, undergraduate) and postgraduate (general practice trainees, and supervising specialists and GPs; Table 1).\n\n\n\n\n\nFinally, as a test of goodness-of-fit, we carefully reviewed the videotapes for any deviant cases that did not fit the categories we had developed.\n\nTriangulation of Researchers + Audit trail\n\nTo ensure rigor and increase authenticity in our methodology, we used two kinds of triangulation—investigator triangulation and data triangulation.[REF] We sought analytical rigor using an audit trail and multiple coders; our coding team included an experienced clinician (M.G.) as well as a nonclinician with expertise in medical communication and team dynamics (L.L.)."
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-trustworthiness-4",
    "href": "guidelines/srqr/qscd.html#sec-trustworthiness-4",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Training and resources",
    "text": "Training and resources\nSee Lincoln and Guba’s Evaluative Criteria for trustworthiness.\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-synthesis-and-interpretation-2",
    "href": "guidelines/srqr/qscd.html#sec-synthesis-and-interpretation-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript."
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-synthesis-and-interpretation-3",
    "href": "guidelines/srqr/qscd.html#sec-synthesis-and-interpretation-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nWe identified four patterns of NVB (non-verbal behavior) that relate to handover quality and have dubbed them: (1) joint focus of attention; (2) the poker hand; (3) parallel play; and (4) kerbside consultation. Each pattern constitutes a transfix, or systematic way of participating non-verbally in the care transfer process. And, although there are variations in each pattern, we have been able to code virtually every handover we have observed in nursing, medicine and surgery into one of these four categories.\n\n\nBecause our participants came from similar educational backgrounds, had studied medicine as their tertiary course, were embedded in the culture of medicine, and were associated in meaningful ways with a single medical school, we approached their transcripts with the assumption that they belonged to a loosely formed discourse community. Although their graduation dates ranged over a period of 50 years and their collective sphere of practice included 10 different specialty areas, there were many similarities in their experiences of enculturation during and after medical school.\n\n\nTheir three major (often overlapping) areas of concern were epistemic (acquiring knowledge and skill), interpersonal (relating to patients, families, colleagues and administrators) and personal (achieving work–life balance). In each of these areas, medical enculturation was achieved by two overlapping processes, absorption and assimilation, each of which may have distinct implications for postgraduate medical education.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-links-to-empirical-data-2",
    "href": "guidelines/srqr/qscd.html#sec-links-to-empirical-data-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript."
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-links-to-empirical-data-3",
    "href": "guidelines/srqr/qscd.html#sec-links-to-empirical-data-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\nSee Frankel et al. for an excellent example of how to use photographs (or snapshots from video) to illustrate and provide supporting evidence for patterns of behavior identified in the analysis. http://qualitysafety.bmj.com/content/21/Suppl_1/i121.\n\nWe identified five interruption types: (1) probing for further data, (2) prompting for expected sequence, (3) teaching around the case, (4) thinking out loud, and (5) providing direction (see Table 1). Several interruption types served both goals of the case review discussions—teaching and patient care. For example, when thinking out loud, supervisors reasoned through problems and taught the team: So that’s the big question, did she have a mechanical fall, or did she have a medicine-related fall? (Case 2). Supervisors prompted for expected sequence, preventing presenters from skipping over information while simultaneously allowing the supervisor to instruct the team on presentation style: So now you can tell me what the rest of his test results are because I haven’t heard those (Case 16).\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-integration-with-prior-work-2",
    "href": "guidelines/srqr/qscd.html#sec-integration-with-prior-work-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThe short summary reminds readers of the main findings and may help them assess whether the subsequent interpretation and implications formulated by the authors are supported by the findings."
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-integration-with-prior-work-3",
    "href": "guidelines/srqr/qscd.html#sec-integration-with-prior-work-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nThis study contributes to the understanding and discussion of the complexity of involving patients in healthcare education. It shows that integrating patient-led teaching into initiatives that are partly faculty-led influences the way in which students perceive learning from and with PIs. What is not known, however, is whether perceptions are also affected by type of health profession and the students’ different orientation towards logics of care and science, and issues of authority and power relations.\n\nFor complete examples of Discussions, see:\n\nHenriksen & Ringsted, 2013\nWesterman et al., 2013.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-limitations-2",
    "href": "guidelines/srqr/qscd.html#sec-limitations-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nAll research has limitations. Discussing them will help readers consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be). If you don’t address limitations, editors and peer reviewers may ask you about them which will delay publication."
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-limitations-3",
    "href": "guidelines/srqr/qscd.html#sec-limitations-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nThe study has several limitations. One is that the focus group interview method reveals students’ perceptions rather than their actual behaviors. Observations of the patient-led teaching encounter may have illuminated an understanding of the relationship between patient instructors and medical students. Another limitation is that the PI-led teaching is optional rather than mandatory, which may have influenced students’ attitudes in a positive direction. Moreover, students who are eager to take on extra-curricular activities may not be representative of the whole population. That only 23 out of 39 students signed up for this study might also have influenced results if the missing group of students represented other perceptions than those present in the focus groups. However the received data from the focus groups were rich in information and diverse perceptions were present. Another limitation is the overrepresentation of women over men in our sample. Even though women are also overrepresented in medical school this might potentially have influenced results, but gender differences in perceptions were nevertheless not identified in the data.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qscd.html#sec-conflicts-of-interest-2",
    "href": "guidelines/srqr/qscd.html#sec-conflicts-of-interest-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nMany factors, including professional and personal relationships and activities, can influence the design, conduct, and reporting of the clinical science that informs health care decision. The potential for conflict of interest exists when these relationships and activities may bias judgment (1). Many stakeholders— editors, peer reviewers, clinicians, educators, policymakers, patients, and the public—rely on the disclosure of authors’ relationships and activities to inform their assessments. Trust in the transparency, consistency, and completeness of these disclosures is essential. - ICMJE"
  },
  {
    "objectID": "guidelines/srqr/qscd.html#ready-to-get-started",
    "href": "guidelines/srqr/qscd.html#ready-to-get-started",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Ready to get started?",
    "text": "Ready to get started?\nDownload resources"
  },
  {
    "objectID": "guidelines/srqr/qscd.html#how-to-cite",
    "href": "guidelines/srqr/qscd.html#how-to-cite",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "How to cite",
    "text": "How to cite\nFor attribution, please cite this guideline as:\nBridget O’Brien, Ilene Harris, Thomas Beckman, Darcy Reed, and David Cook. 2023. “The SRQR Guidelines for Writing Qualitative Research Articles version 1.1.” The EQUATOR Network Guideline Dissemination Platform. https://doi.org/10.1234/equator/1010101.\nYou can use your reference manager to save citation information for this webpage, or copy the BibTeX below."
  },
  {
    "objectID": "guidelines/srqr/qscd.html#faqs",
    "href": "guidelines/srqr/qscd.html#faqs",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "FAQs",
    "text": "FAQs\n\nWho made this guideline?\n\nBridget O’Brien, PhD has been a faculty member in the Department of Medicine, Division of General Internal Medicine, since 2008. She is a professor of medicine and an education scientist in the Office of Medical Education’s Center for Faculty Educators. As co-director of the Teaching Scholars Program and the UCSF-University of Utrecht Health Professions Education doctoral program she teaches and mentors faculty and learners interested in education research and scholarship. At the San Francisco VA, she directs the Advanced Fellowship in Health Professions Education Evaluation and Research. In 2015 she was selected as one of five national Macy Faculty Scholars supported by the Josiah Macy Jr. Foundation and in 2021 she was selected as a KIPRIME Fellow at the Karolinska Instituet. She is a deputy editor for the journal Academic Medicine.\nDr. Ilene Harris, deceased, was professor and head, Department of Medical Education, University of Illinois at Chicago College of Medicine, Chicago, Illinois.\nDr. Thomas Beckman is professor of medicine and medical education, Department of Medicine, Mayo Clinic College of Medicine, Rochester, Minnesota.\nDr. Darcy Reed is associate professor of medicine and medical education, Department of Medicine, Mayo Clinic College of Medicine, Rochester, Minnesota.\nDr. David Cook is associate director, Mayo Clinic Online Learning, research chair, Mayo Multidisciplinary Simulation Center, and professor of medicine and medical education, Mayo Clinic College of Medicine, Rochester, Minnesota.\n\n\nHow was this guideline made?\nThe developers synthesised 40 sets of recommendations previously proposed by experts in qualitative methods. You can read about their development process here.\n\n\nWhat to do if asked to remove guideline related content\nIf a colleague or reviewer asks you to remove content that is related to this guideline, you can direct them to this guideline and the explanation for why that item is important. If they insist, consider moving the item to a supplement, table or figure.\n\n\nWhere can I get general writing training?\nThe EQUATOR Network provides in-person training for writing research articles.\nAuthorAID have resources, an online course, and mentoring to help authors.\n\n\n\nResearch paradigm\n\n\n\n\n\n\nThe set of beliefs and assumptions that guide the research process. These commonly include positivist, post-positivist, constructivist or interpretivist, and critical theory. Qualitative research generally draws from a post-positivist or constructivist/interpretivist paradigm.”\n\n\n\n\n\n\nInstruments\n\n\n\n\n\n\nData collection instruments include (but are not limited to) interview or focus group guides, observational protocols and prompts for field notes, and data extraction or coding protocols for selection and analysis of documents, photographs, videos, or other artifacts”\n\n\n\n\n\n\nBias\n\n\n\n\n\n\nA term drawn from quantitative research, bias technically means a systematic error, where a particular research finding deviates from a true finding. This might come about through errors in the manner of interviewing, or by errors in sampling. In qualitative research this is a problematic concept, since by definition the qualitative researcher is part of the process, and all researchers are different. This human factor has been said to be both the greatest strength and the greatest weakness of qualitative method. What can be done in commercial qualitative research, however, is to minimise obvious and avoidable sources of bias, for example by not confining all the fieldwork to one social group or geographic location, by taking steps to recognise the personal views of the researcher, (using techniques such as bracketing), and by working in teams.\n\n\n\n\n\n\nSampling strategy\n\n\n\n\n\n\nSeveral sampling strategies are commonly used in qualitative research, although most fall under the umbrella of purposeful (or purposive) sampling.\nPurposeful sampling means that participants, documents, or events are selected for their relevance to the research question, based on guiding theory or experiences and assumptions of the researchers. Over the course of the research process, researchers may determine that additional or different participants, documents, or events should be included to address the research question.\nOther sampling techniques, such as theoretical sampling (seeking examples of theoretical constructs), snowball sampling (using study participants to identify additional participants who meet study criteria), and convenience sampling (including any volunteers with no or minimal criteria for inclusion) may be appropriate depending on the question and approach, so long as the authors provide explanation and justification.\n\n\n\n\n\n\nApproach\n\n\n\n\n\n\nA qualitative approach is a general way of thinking about conducting qualitative research. It describes, either explicitly or implicitly, the purpose of the qualitative research, the role of the researcher(s), the stages of research, and the method of data analysis. Commonly used approaches include ethnography, grounded theory, case study, phenomenology, and narrative research.\n\n\n\n\n\n\nData collection methods\n\n\n\n\n\n\nData collection methods include (but are not limited to) interviews, focus groups, observations (direct or indirect via video), and review of written text, photographs, and other documents or materials.\n\n\n\n\n\n\nIterative\n\n\n\n\n\n\nQualitative research often occurs as an iterative process, meaning that researchers begin data analysis before they complete data collection. The data collection and analysis process may occur in phases or stages. As part of an iterative collection-analysis process, researchers will often alter their data collection methods to explore their preliminary impressions in greater depth and/or actively pursue confirming and disconfirming perspectives.\n\n\n\n\n\n\nStudy period\n\n\n\n\n\n\nThe start and end dates for data collection and analysis.\n\n\n\n\n\n\nEthnography\n\n\n\n\n\n\nThe scientific description of peoples and cultures with their customs, habits, and mutual differences.\nRead more\n\n\n\n\n\n\nGrounded theory\n\n\n\n\n\n\nA method consisting of a set of systematic, but flexible, guidelines for conducting inductive qualitative inquiry aimed toward theory construction. This method focuses squarely on the analytic phases of research, although both data collection and analysis inform and shape each other and are conducted in tandem.\nRead more\n\n\n\n\n\n\nDegree of participation\n\n\n\n\n\n\nFor example, if some participants were observed and interviewed and others only interviewed, or if some participants completed multiple interviews and others completed a single interview, these variations should be explained.\n\n\n\n\n\n\nUnit of analysis\n\n\n\n\n\n\nIn qualitative research, the unit of analysis is not necessarily the same as the unit of sampling (e.g., individual participants or events). Instead, some approaches use specific events as the unit of analysis, such as mentions of a particular topic or experience, or observations of a particular behavior or phenomenon, while others use groups rather than individual group participants. This specification has implications for how the data are organized and analyzed as well as the inferences drawn from the data.\n\n\n\n\n\n\nReflexivity\n\n\n\n\n\n\nReflexivity refers to intentional, systematic consideration of the potential or actual effects of the researcher(s) on all aspects of the study process.\n\n\n\n\n\n\nTransferability\n\n\n\n\n\n\nThe transferability of a research finding is the extent to which it can be applied in other contexts and studies. It is thus equivalent to or a replacement for the terms generalizability and external validity.\n\n\n\n\n\n\nGeneralizability\n\n\n\n\n\n\nThe appropriate scope for generalization of the findings beyond the study (e.g., to other settings, populations, time periods, circumstances)\n\n\n\n\n\n\nAnalytic findings\n\n\n\n\n\n\nAnalytic findings may include interpretations, inferences, narratives, themes, and models.\n\n\n\n\n\n\nFrequency counts\n\n\n\n\n\n\nThe frequency of specific themes or codes."
  },
  {
    "objectID": "guidelines/srqr/qsce.html",
    "href": "guidelines/srqr/qsce.html",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "",
    "text": "Full Title: Standards for Reporting Qualitative Research\nAuthors: Bridget O’Brien, Ilene Harris, Thomas Beckman, Darcy Reed, David Cook\nVersion: 1.1\nDOI: 10.1234/equator/1010101"
  },
  {
    "objectID": "guidelines/srqr/qsce.html#translations",
    "href": "guidelines/srqr/qsce.html#translations",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Translations",
    "text": "Translations\nThis guideline is also available in French."
  },
  {
    "objectID": "guidelines/srqr/qsce.html#about-this-guideline",
    "href": "guidelines/srqr/qsce.html#about-this-guideline",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "About this guideline",
    "text": "About this guideline\n\nWhen to use this guideline\n\nUse this guideline for writing qualitative research articles. You can use it when describing all kinds of qualitative approaches, methods, and designs.\n\n\nYou can also use this guideline for:\n\nwriting proposals or protocols (use the items within the Introduction and Method sections).\nreviewing the reporting of an article, but not for appraising its quality.\n\n\n\n\n\n\n\n\n\nDo not use this guidance for:\n\nwriting a qualitative evidence synthesis, use ENTREQ instead.\nappraising the quality of qualitative research, use an appraisal tool like CASP-Qual instead. \n\n\n\n\n\n\n\n\n\n\nRelated reporting guidelines:\n\nJARS Qual for writing qualitative, psychology manuscripts\nENTREQ for writing qualitative evidence syntheses\nFor writing studies involving interviews or focus groups, you can use this guideline or COREQ. \n\nFor appraising research consider:\n\nCASP-Qual\n\n\n\n\n\n\nHow to use this guideline\nReporting guidelines help me structure my drafts and develop as a researcher. I use them when teaching and hope that my students continue to use them. Manuel Silva - Researcher\nReporting guidelines are best used early. Read the full guideline below or use a tool when working:\n\nUse a template to draft manuscripts and avoid the blank page \nUse a log book to plan and record information whilst conducting research\nUse a checklist to double-check and reassure others that your article describes all the important details of your work\n\n\n\n\nFor drafting\n\nTemplate\n\n\n\nFor planning\n\nLog book\n\n\n\nFor checking\n\nChecklist\n\n\nThis reporting guideline does not prescribe a rigid format or standardized content. Consider each item and prioritize elements that are most relevant to your study, findings, context, and readers.\nYou may prefer to report an item in a different order, section, or in a table or figure. If you feel confident that an item is less important to your study, you could report it in an appendix or supplement. If you think an item is not applicable, state why. You don’t need to write your article in the order the items are presented.\nIn your methods section, state that you used the SRQR guideline when writing your article and cite it (see How to cite).\n\n\nWhy use this guideline\nFollowing this guideline will make it easier for other researchers to find, use, and synthesise your work. It won’t tell you how to do qualitative research, but it will help you explain your choices so others can understand what you did, why you did it, and what you found. At our journal, we ensure all qualitative articles follow SRQR as this makes them easier for readers to understand and faster to publish.Fiona Ludlow - Editor in Chief\nIt was written by experts from the qualitative research community, using a thorough development process. SRQR is endorsed by hundreds of journals, so using this guideline (and its checklist) will help you meet editors’ expectations."
  },
  {
    "objectID": "guidelines/srqr/qsce.html#guidance",
    "href": "guidelines/srqr/qsce.html#guidance",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Guidance",
    "text": "Guidance\nApprox. 17 min read\n\n\n1. Title\n\n\n\n\n\n\n\nDescribe the nature and topic of the study.\nIdentify the study as qualitative or indicate the approach or data collection methods.\n\n\n\n\n\n\nJustification, Examples, and Resources\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\nTraining and Resources\n\n\nWhy readers need this information\nThis allows readers to quickly identify the type of study. Your title will be indexed by search tools so descriptive words will make it easier for others to discover.\n\n\nExamples\n\nResidents learning from a narrative experience with dying patients: a qualitative study.\n\n\nMedical students’ perceptions of the factors influencing their academic performance: an exploratory interview study with high-achieving and re-sitting medical students.\n\n\nUndergraduate rural medical education program development: focus group consultation with the NRHA Rural Medical Educators Group.\n\n\n\n\nTraining and Resources\nSee this article for advice on writing titles that are interesting and descriptive.\nThe EQUATOR Network provides training for writing effective manuscript titles as part of their publication school.\nBack to top\n\n\n\n\n\n\n2. Abstract\n\n\n\n\n\n\n\nInformative key words in the title and abstract make manuscripts easier to find.Lorna Ellis - Evidence Synthesiser\nSummarise key elements of the study including:\n\nbackground about the problem or phenomenon of interest\ndescription of the study purpose or research question\nmethods, including the approach or perspective (e.g., general inductive, grounded theory), context (setting, time period), sample (number and key characteristics of participants, events, documents), data collection strategies (e g., observation, interview, focus group) and data analysis techniques\ndescription of main findings (e.g., themes or inferences) related to the study purpose and/or research question\nstudy implications\n\n\nInformation presented in the abstract should be consistent with the information presented in the full text.\nUse the format of your intended journal. 1\n\n\n\n\n\n\nJustification, Example, and Resources\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExample\nTraining and Resources\n\n\nWhy readers need this information\nA reader should be able to read the abstract independent of the manuscript and get a sense of the background, purpose, methods, main results and implications that will be described in greater detail in the manuscript.\nYour abstract will be indexed by search tools so descriptive words will make it easier for others to discover.\n\n\n\nExample\n\nPurpose: Although academic centers rely on assessments from medical trainees regarding the effectiveness of their faculty as teachers, little is known about how trainees conceptualize and approach their role as assessors of their clinical supervisorsMethod: In 2010, using a constructivist grounded theory approach, five focus group interviews were conducted with 19 residents from an internal medicine residency program. A constant comparative analysis of emergent themes was conducted.Results: Residents viewed clinical teaching assessment (CTA) as a time-consuming task with little reward. They reported struggling throughout the academic year to meet their CTA obligations and described several shortcut strategies they used to reduce their burden. Rather than conceptualizing their assessments as a conduit for both formative and summative feedback, residents perceived CTA as useful for the surveillance of clinical supervisors at the extremes of the spectrum of teaching effectiveness. They put the most effort, including the crafting of written comments, into the CTAs of these outliers. Trainees desired greater transparency in the CTA process and were sceptical regarding the anonymity and perceived validity of their faculty appraisals.Conclusions: Individual and system-based factors conspire to influence postgraduate medical trainees’ motivation for generating high-quality appraisals of clinical teaching. Academic centers need to address these factors if they want to maximize the usefulness of these assessments.\n\n\n\n\nTraining and Resources\nThe EQUATOR Network provides training for writing effective manuscript abstracts as part of their publication school.\nSee this article from the London School of Economics for general abstract writing tips.\nBack to top\n\n\n\n\n\nIntroduction\n\n\n3. Problem Formulation\n\n\n\n\n\n\n\nWhen the problem is described clearly I find it much easier to understand why a study was done, even if its from a different subject area, country, or decade.Nkinda Akaro - Researcher\nDescribe the theoretical and/or practical issues or concerns that make the study necessary, including:\n\nan overview of what is known about the problem\ngaps in current knowledge (the problem statement)\nthe scope of the research problem or phenomena addressed in the study (what will and will not be included)\ntheoretical and/or empirical work directly relevant to the problem or phenomena studied\nthe need for a qualitative approach.2\n\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExample\n\n\nWhy readers need this information\nThis information provides context for the research question or study purpose and situates the study in relation to other work in the field.\n\n\nExample\n\nRegulatory focus theory may therefore offer insight into the variability in responses to feedback, but how well do these experimental findings translate to real clinical situations in which the reality of responsiveness to feedback seems frustratingly complex? …. In order to better elaborate a theory to account for this variability in learner response to feedback, the present study was undertaken. We aimed, in this study, to determine how readily clinical learning events could be classified as activating a promotion or a prevention focus, and to explore, through a careful analysis of doctors’ descriptions of their feedback experiences, the predictive value of regulatory focus theory in the context of real clinical learning situations.\n\n\n\nBack to top\n\n\n\n\n\n\n4. Purpose or research question\n\n\n\n\n\n\n\nA clear purpose frames the article. Rahilah Zayn - Researcher\nInclude a statement of study intent. This can be framed as one or more research questions, purposes, goals, or objectives.3\n\n\n\n\n\n\nJustification, Example, and Resources\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExample\nTraining and Resources\n\n\nWhy readers need this information\nBy clearly stating the purpose of the study, authors set readers’ expectations for the methods, findings and discussion sections of the manuscript.\n\n\nExample\n\nThe purposes of this study were to investigate how medical students recognize, respond to and utilise feedback, and to determine whether there are maturational differences in understandings of the role of feedback across academic years in medical school.\n\n\n\nTraining and Resources\nSee this article for advice on writing qualitative research questions.\nBack to top\n\n\n\n\n\n\nMethods\n\n\n5. Qualitative approach and research paradigm\n\n\n\n\n\n\n\nDescribe your qualitative approach, your guiding theory (if appropriate), and identify the research paradigm.4 I like it when authors describe terms in their own words as well as using the proper terms.Rahilah Zayn - Researcher\nExplain why the selected approach is appropriate for the research question.\nProvide references to theories or traditions that guide the use of the approach as needed.\nI hadn’t heard the term approach before writing-up my study. I realise now that I did have an approach and that it might be different to other researcher’s approaches, so I need to describe things that I felt were obvious.Tim Westland - Researcher\n\nIf you don’t know what your approach or paradigm was, or you don’t think you had one, it’s OK to reflect on this after collecting data and you should still report it. Read this list to see what best describes your work.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nIdentifying the research paradigm helps readers understand whether the researcher assumes that there is a single, objective reality (positivist or post-positivist) and has thus designed the study to describe this reality or whether the researcher assumes multiple, subjective realities and designed the study to describe these multiple realities, with no attempt to merge or reconcile these realities (constructivist/interpretivist). The paradigm has implications for the study design, approach, methods, and techniques to ensure rigour and trustworthiness.\nSince the research paradigm does not necessarily dictate particular approaches or methods, the approach should also be clearly defined. Stating the approach provides readers the opportunity to evaluate the fidelity of the research approach to the research question(s) and consider the rationale for modifications and deviations from the selected approach. Qualitative research also includes an array of methods that can be used across paradigms and approaches. (See also Item 10).\n\n\nExamples\n\nThe study was performed from a constructivist point of view using an interpretative phenomenological epistemology. Based on the notion that social phenomena are constructed by the communal making of meaning about the underlying phenomena, we aimed to construct insightful accounts of lead consultants’ approaches to educational change, rather than to identify the true nature of these approaches. Because the management of change by lead consultants is an under-researched area, we conducted an exploratory qualitative study…\n\n\nGiven the relative dearth of explanatory theories about factors affecting medical students’ emotional reactions, we chose to develop one by applying methods associated with grounded theory, specifically constant comparative analysis, to qualitative data obtained from learning logs and interviews. Our approach was constructivist, deliberately using researchers’ own experiences and acquired knowledge to enhance theoretical sensitivity and enrich theory development.\n\nBack to top\n\n\n\n\n\n\n6. Researcher characteristics and reflexivity\n\n\n\n\n\n\n\nDescribe how roles and identities of research team members influence choice of research approach, data collection, and data analysis.5\nComing from quantitative research it felt strange to include such personal descriptions of the team. But I came to realise why it was useful to others, and that peer reviewers would expect it. Tim Westland - Researcher\nDescribe the perspectives, assumptions, prior knowledge, preliminary hypotheses, and/or motives (the stance) of the members of the research team.\nDescribe the researchers’ relationships to participants in the study and what decisions were made in light of these relationships.6\nIf your research was observational (e.g., ethnography), describe the role of the researcher along a spectrum from passive observer (no involvement in the activity studied) to participant-observer (ranging from some limited involvement in the activity to full involvement).\nThere is no expectation that the study could be precisely replicated; these characteristics and perspectives of the researcher should not be mentioned in the limitation section. (See also Item 14: Data Analysis)\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nIn positivist and post-positivist paradigms, personal characteristics and perspectives of researchers might be viewed as biases that limit the credibility of study findings, while in constructivist or interpretivist paradigms the characteristics and perspectives of the researchers are important contextual factors that are an accepted part of the study design, data collection, and data analysis. These characteristics and perspectives may explain how the researcher(s) obtained access to the site or participants included in the study or may add valuable insight during data analysis.\n\n\nExamples\n\nReflexivity was maintained by the research team through the analysis and writing by recording, discussing and challenging established assumptions. In addition both EH and SV kept reflexive diaries.\n\n\nThe first author conducted all interviews and discussion groups. Her own medical undergraduate training took place between 1995 and 2000. She was not known to the participants of this research prior to undertaking the study and deliberately did not undertake any clinical or teaching activities locally alongside this research. Whilst it was useful to know (from her own background) what the students were talking about medically (and in terms of detecting items of significance), as a researcher she made conscious efforts not to accept potentially common assumptions at face value.\n\nBack to top\n\n\n\n\n\n\n7. Context\n\n\n\n\n\n\n\nDescribe the setting/site(s) in which the study was conducted, the reason(s) why the setting/site(s) was selected, and the salient cultural, political, historical, economic and/or other external factors that influence the study.\nWhen synthesising evidence, context helps me work out how and why different studies and findings fit together (or don’t)Lorna Ellis - Evidence Synthesiser\nAdditional context may be reported with findings (i.e., the Results section) to add evidence for interpretations and to enhance discussion of transferability.\n\n\n\n\n\n\nJustification and Example\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExample\n\n\nWhy readers need this information\nThis helps readers interpret the meaning and significance of the study findings by situating them in social, cultural, temporal and other relevant contexts.\n\n\nExample\n\nWe conducted the study among hospital-based clinical teachers of students in years 4 to 6 of a six-year undergraduate medical program at Maastricht University Medical School. Years 4 to 6 are devoted to clerkships in the academic hospital and affiliated regional hospitals. Rotations differ in duration depending on the type of rotation and the discipline, and the sequence of rotations differs among students. During rotations, students spend time in the wards, the outpatient clinics, and the accident and emergency department. Clerkships in years 4 and 5 last between 4 and 10 weeks (regular clerkship), whereas students in year 6 undertake an 18-week senior clerkship in a discipline of their choice.\n\nBack to top\n\n\n\n\n\n\n8. Sampling strategy\n\n\n\n\n\n\n\nDescribe how and why research participants, sites, documents/artifacts, and/or events were selected for inclusion in (and, if appropriate, exclusion from) the study, along with a justification for this strategy.\nDescribe the sampling strategy rather than simply labeling it (e.g., purposive or snowball), since such labels do not have a universally accepted definition and, more importantly, procedures tend to be study- specific.\n\nDescribe how you established the final sample size:\n\nIf you used a flexible sampling strategy, then explain the criteria used to decide when no further sampling was necessary.\nIf data collection ended once saturation or sufficiency had been reached, then describe the specific criteria used to define saturation or sufficiency.\n\n\n\nDescribe procedures used to recruit participants, including:\n\nwho was involved in recruitment\nwhat their relationship was to participants\nhow and when recruitment occurred\nwhy these procedures were selected. (See also Item 6: Researcher characteristics and reflexivity)\n\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThis information helps readers understand the source of data / findings so they can consider the boundaries of the study and the relevance to their own context. It also gives readers insight into the researchers’ decisions, which can be important for critical appraisal.\n\n\nExamples\n\nAs students’ perceptions were previously shown to be related to gender, age, prior experience and place of attachment,[REF] we purposely selected respondents with different backgrounds. This sampling strategy led to the diversity of gender, age, prior patient experience and place of attachment shown in Table 1.\n\n\nPurposive sampling was directed towards achieving maximum variation in age and specialty, using a snowball approach (a non-probabilistic form of sampling in which persons initially chosen for the sample are used as informants to locate other persons having necessary characteristics making them eligible for the sample).[REF ]\n\n\nPotential participants were all medical students in Years 1 and 2 at the University of Toronto in 2004. Following research ethics board approval, recruitment was conducted via e-mail to class listservs. Participant responses were sent directly to the research assistant, who was unknown to participants, so that the principal investigators did not know who did or did not participate. This process was engaged to protect participants’ anonymity and to avoid any impression of coercion because the lead researcher (SG) was involved in the administration of the undergraduate curriculum at the time. Sample size was estimated to be sufficient based on the principle of theoretical saturation [REF] and our previous experience with this methodology (i.e., with a relatively homogeneous population, we expected to reach saturation with approximately 15 interviews per group). There were no exclusion criteria and we accepted the first 15 students from each class who volunteered.\n\nBack to top\n\n\n\n\n\n\n9. Ethical issues pertaining to human subjects\n\n\n\n\n\n\n\nReport approval for the study from an appropriate institutional review board for research associated with human subjects.\nIf you did not receive ethical approval, describe why.\nDescribe procedures used to protect participants, including:\n\ndata collection (e.g., recruitment and informed consent)\nanalysis (e.g., data security and integrity)\nand reporting of findings (e.g., anonymization of excerpts).\n\nIf you provided compensation or offered incentives to facilitate participation, describe this too.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nJournals will require an ethical approval statement.\nQualitative research often involves interaction between researchers and research participants. Correspondingly, researchers should ensure that participants are fully aware of their participation in a research study, the risks and benefits associated with the study, the steps and precautions the researchers will take to minimize risks, such as loss of privacy and confidentiality, and how the researchers plan to use the data.\n\n\n\nExamples\n\nEthical approval was granted by the University of Otago and student participants were invited to attend each focus group discussion by the university representatives of New Zealand Medical Students Association (NZMSA). Usually, a key concern when collecting data from students is that students may feel vulnerable when sharing their experiences with academic staff during a focus group discussion. However, this potential harm was removed as each group discussion was facilitated by a fellow student, the discussions were transcribed by a professional transcribing service, and only the primary researchers [Names] had access to the raw data.\n\nBack to top\n\n\n\n\n\n\n10. Data collection methods\n\n\n\n\n\n\n\nDescribe data collection methods and design in detail, and justify them in relation to the research question(s), paradigm, approach, and other methods.7\n\nIf data collection and analysis was iterative:\n\ndescribe the iterative process\nif changes occurred during the research process, describe how and why study procedures changed in response to evolving study findings.\n\nIdentify the study period.\nDescribe important characteristics of the individuals conducting interviews, observations or focus groups, and methods used to train these individuals.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThe study period helps readers place the study in temporal context and identify factors not mentioned by the authors that might affect findings, interpretation, and implications. (See Item 8 for ending data collection.)\n\nDescribing researcher characteristics clarifies the relationship between the individuals involved in data collection and the participants in the research and also explains what efforts were made to ensure consistency in the data collection process (See Items 6 and 15.)\n\n\n\n\nExamples\n\nFurther, it was decided that group interviews, also known as focus group discussions would be the best means of data collection. This is a method of data collection that enables group members to feed off each other’s ideas and an effective moderator will maintain group focus whilst at the same time permitting flexibility in the direction those aspects of the discussion might take. [REF]\n\n\nAdjustments to the interview protocol were made according to early experience and information participants had provided (i.e. redundant questions were eliminated; questions were reworded to improve flow and clarity; additional probes were included).\n\n\nFaculty staff were then interviewed individually by a trained study investigator in a 15-minute, semi-structured interview. This sequence was repeated with other video encounters. Table 2 presents examples of interview questions. Each faculty member was interviewed by at least three interviewers over their various interviews. Interviewers were chosen based on their experience in interviewing. All were trained during a half-day meeting to interpret and deliver the interview guide in the same manner in order to elicit information of a consistent type.\n\nBack to top\n\n\n\n\n\n\n11. Data collection instruments and technologies\n\n\n\n\n\n\n\nDescribe all data collection instruments, including their development, and if/how they changed over the course of the study. Cite relevant literatures, theories or conceptual frameworks as appropriate. Consider sharing the data collection instrument(s) or a detailed description of them in the article body, supplementary material, or published elsewhere. I publish my materials on the OSF and then describe and cite them in my articles.Tim Westland - Researcher\nDescribe the use of equipment for audio or video recording, reproduction of paper documents or computer files, or other processes in data collection.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nDescribing instruments and equipment helps readers understand the full context in which data collection occurred and how this context might have affected data collection (e.g., the influence of recording devices on participants’ behaviors; the nature of inferences drawn from live vs. recorded events).\n\n\nExamples\n\nTo facilitate the discussion and to maintain consistency over different sets of discussions, key trigger questions were devised prior to the discussion. The opening trigger question was: Thinking back to some of your best clinical learning placements in 4th and 5th year. What was it about those clinical placements that provided good opportunities for learning?\n\n\nInterviews included discussion of the expectations, processes and consequences of AEE [authentic early experiences]. The interview schedule was derived following identification of questions that could not be fully answered in a systematic review of previous empirical or theoretical literature. It comprised a sequence of topic areas including experiences in action, and areas of frustration in Medical Education such as the learning of content knowledge, achieving functional knowledge, and transfer of knowledge.[REF]… Interested readers can request a copy of the schedules from the corresponding author. Interviews lasted between 20 and 90 min. and discussion groups between 60 and 90 min. All interviews and discussion groups were conducted in private rooms at the participant’s workplace—the medical school for students and faculty, and individual places of work for workplace supervisors (except for one who chose to be interviewed at the medical school). All data were audio-recorded, and transcribed verbatim.\n\nBack to top\n\n\n\n\n\n\n12. Units of study\n\n\n\n\n\n\n\nDescribe the number of participants, documents, or events included in the study (the units of study).\nDescribe characteristics of the participants, documents or events that are relevant to the study purpose and research question(s).8\nInclude the dates or timeframe for participation.\nIf the actual sample differs from the target sample, describe:\n\nthe difference,\nwhy these differences may have occurred,\nhow this might affect the findings.\n\nIf the degree of participation varied among individuals, describe:\n\nthe different levels of participation,\nthe reasons for these differences (i.e., the researchers’ choice or the participants’ preferences),\nand how these different levels of participation were taken into account in the analysis.\n\nThis information could appear in the Methods section as part of the description of the sample, or at the beginning of the Results section to provide context for the findings presented.\n\n\n\n\n\n\nMore Information, Justification, and Example\n\n\n\n\n\nJump to:\n\nHow does this item differ to Sampling (Item 8)?\nWhy readers need this information\nExample\n\n\nHow does this item differ to Sampling (Item 8)?\nThe sampling item (Item 8) describes the target or ideal participants, documents, or events selected for the study. By contrast, this item focuses on description of the actual participants, documents or events included in the study.\n\n\nWhy readers need this information\nThis helps readers know whose experiences and perspective are and are not included.\n\n\n\nExample\n\nOf the 70 Mindful Communication program participants, 46 met the eligibility requirements to participate in the in-depth interviews. We randomly chose and then contacted 22 participants, of whom 20 agreed to be interviewed within six months of completing the program: 15 in person and 5 by telephone. Two declined for lack of time. On reaching saturation after 20 interviews, no further attempts to contact the remaining 24 participants were made.\n\n\n\n\n\nThere were 31 nursing handovers covering 137 patients, and 21 resident handovers covering 101 patients included in this study.\n\nBack to top\n\n\n\n\n\n\n13. Data processing\n\n\n\n\n\n\n\nDescribe the ways in which data are prepared for analysis and managed throughout the analysis process. These activities might include transcription, coding, data entry, and organization of data. (See footnote for audio or visual recordings9).\nDescribe the processes used to organize, compile, and categorize data (e.g., field notes, transcripts, documents, photographs, artifacts) for analysis.\nIf you used transcripts, describe procedures used to check accuracy.\nDescribe procedures used to maintain data security and protect the privacy of participants, as specified in the human subjects protocols (see footnote on anonymisation10 as an example).\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript.\n\n\n\nExamples\n\nInterviews were anonymised and each participant was given a code number.\n\n\nThe interviewers and another member of the research team (H.B.) reviewed transcripts for accuracy.\n\n\nWe collected data throughout the admission process through direct observation, audio- recording, and chart extraction. We audio-recorded, transcribed, and anonymized both the overnight and morning case review discussions. We also observed the morning case review discussions in person and collected field notes. For each case review discussion, we copied the admission notes from the patient’s record and de-identified all data.\n\nBack to top\n\n\n\n\n\n\n14. Data analysis\n\n\n\n\n\n\n\nDescribe your analytic process as transparently as possible.11\nIf you used an approach that has a well-defined process for data analysis (e.g., grounded theory, discourse analysis, phenomenography):\n\ncite the guiding literature\ndescribe your processes in sufficient detail so readers can judge the extent to which your processes align with the guiding approach.\nIf you modified or deviated from the guiding approach, explain and justify these modifications.\n\nSpecify the unit of analysis.12\nExplain the rationale underlying different decisions made throughout the data analysis process to provide as much transparency as possible.13\nIf observations that contrast or deviate from identified concepts or themes were important to your analysis, describe how these discrepancies were handled during the analysis.\nIf you drew upon a theoretical perspective or framework during analysis, describe theoretical or other influences on your analysis scheme or categories if they exist. If you identified a theoretical perspective or framework early in the conception of the study or after reviewing some or all of their data, consider referring to these as sensitizing concepts to acknowledge that the approach is inductive, but with influence from relevant theory, models, or organizational schemes. Alternatively, explain that themes were developed from the data with no external influences.\nDescribe which members of the research team are involved in data analysis and what perspective(s) they bring to the analysis.\nIf software was used to assist with data analysis14, describe how it was used. Simply stating that software was used is insufficient.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nTechniques used for data analysis will depend on the paradigm, approach, and/or data collection methods selected by the researchers. Correspondingly, authors should be as transparent as possible about the analytic process so that readers can follow the logic of inquiry from the research question(s) to the analysis and findings.\n\n\nExamples\n\n…we brought sensitizing concepts to the analysis while we conducted an open, inductive analysis.[REF]In this case the sensitizing concepts arose, a priori to analysis, from a framework derived from the literature [REF] (as described above), in which participants’ motivations to act are based on principles of professionalism, internal affect, or potential implications of their actions.[REF]\n\n\nThrough an iterative process of listening, discussing, and relistening, the team identified and consensually validated emerging themes[REF] and appended segments of dialogue supporting the proposed themes. Recruitment stopped when saturation was reached (no new themes were identified). The team systematically reviewed the themes and sorted them into content domains. The team used an analytic matrix to identify patterns and connections amongst the domains. Two of us not involved in the qualitative coding process (R.E., M.K.) audited the analytic matrix, choice of quotes, and thematic analysis.\n\n\nThe analysis started after the first interview. All data were analyzed with the aid of the audio- coding facility of the NVivo 8: QSR International Pty Ltd, Doncaster, Vic, Australia programme. First, [name] and [name] coded independently from one another, making sure to stay semantically close to the participants’ wording. Then we discussed these open codes and defined axial codes.[REF] New insights about the impact of CST were written down in memos.\n\n\nVideotapes were analysed using immersion/crystallisation methods of qualitative data analysis.[REF] With no pre-existing framework developed in advance for analysis, an inductive approach was used to discover patterns of NVB in the data. A team of six researchers met weekly for 18 months to view videos together. Using a consensus-building approach based on a combination of field notes, opportunistic interviews with the participants, and repeated viewing of the same material, sometimes many months apart, we eventually achieved consensus on verbal, non-verbal, and physical themes and patterns observed in the data. Finally, as a test of goodness-of-fit, we carefully reviewed the videotapes for any deviant cases that did not fit the categories we had developed.\n\n\nAll transcripts were coded thematically by four of the five authors, who met regularly to identify areas of convergence until full agreement was reached. One of the interviewers (P.M.) maintained an audit trail to track the team’s developing thinking. A process of dialectical empiricism[REF] was used to categorise the emergent themes into more abstract concepts…\n\nBack to top\n\n\n\n\n\n\n15. Techniques to enhance trustworthiness\n\n\n\n\n\n\n\nDescribe methods used to ensure trustworthiness and credibility throughout the data collection and analysis process. (See footnote on commonly used techniques15).\nExplain your choice of techniques and why these are appropriate for the particular study.\n\n\n\n\n\n\nJustification, Examples, and Resources\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\nTraining and resources\n\n\nWhy readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript.\n\n\n\nExamples\nMember checking\n\nMember checks [REF] with an external TBL expert (R.L.) supported the validity of these analyses.\n\nTriangulation of data types and data sources\n\nThe interview data were triangulated with the data of 11 student and supervisor focus groups of a previous study, and more specifically, with those data that concern in particular the influence of CST [Communication Skills Training] on the development of patient-centredness…. Triangulation with the focus group data allowed us to broaden the in-depth information from the interviews in the analysis and to share and compare this with information from students and doctors with varying levels of CST (no, limited, full programme) and from two universities (Universities of Antwerp and Ghent). Moreover, this enabled us to better explore the evolution over time, given that the focus groups included participants at different stages of their study: before clerkships (year 4, undergraduate), during clerkships (year 6, undergraduate), after clerkships (year 7, undergraduate) and postgraduate (general practice trainees, and supervising specialists and GPs; Table 1).\n\n\n\n\n\nFinally, as a test of goodness-of-fit, we carefully reviewed the videotapes for any deviant cases that did not fit the categories we had developed.\n\nTriangulation of Researchers + Audit trail\n\nTo ensure rigor and increase authenticity in our methodology, we used two kinds of triangulation—investigator triangulation and data triangulation.[REF] We sought analytical rigor using an audit trail and multiple coders; our coding team included an experienced clinician (M.G.) as well as a nonclinician with expertise in medical communication and team dynamics (L.L.).\n\n\n\nTraining and resources\nSee Lincoln and Guba’s Evaluative Criteria for trustworthiness.\nBack to top\n\n\n\n\n\n\nResults\n\n\n16. Synthesis and interpretation\n\n\n\n\n\n\n\nDescribe the main analytic findings.16\nIn most cases, report a synthesis of the data along with specific quotes, examples, or illustrations derived from the data.\n\nI like when authors use words like many, few, or all to describe frequency.Nkinda Akaro - Researcher\nConsider describing frequency, variety, representativeness, counter-examples, concrete details, contextualization, conditions, and qualifications related to the findings.\nFrequency counts play a limited role in qualitative research, and need not be reported unless they play a meaningful role in interpretation of the data.\nIf your findings include integration with prior literature or theory and/or the development of a theory, model or meta-narrative, consider using tables and figures to communicate these findings.\nItems 16 and 18 can be reported in Results or Discussion sections.17\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript.\n\n\n\nExamples\n\nWe identified four patterns of NVB (non-verbal behavior) that relate to handover quality and have dubbed them: (1) joint focus of attention; (2) the poker hand; (3) parallel play; and (4) kerbside consultation. Each pattern constitutes a transfix, or systematic way of participating non-verbally in the care transfer process. And, although there are variations in each pattern, we have been able to code virtually every handover we have observed in nursing, medicine and surgery into one of these four categories.\n\n\nBecause our participants came from similar educational backgrounds, had studied medicine as their tertiary course, were embedded in the culture of medicine, and were associated in meaningful ways with a single medical school, we approached their transcripts with the assumption that they belonged to a loosely formed discourse community. Although their graduation dates ranged over a period of 50 years and their collective sphere of practice included 10 different specialty areas, there were many similarities in their experiences of enculturation during and after medical school.\n\n\nTheir three major (often overlapping) areas of concern were epistemic (acquiring knowledge and skill), interpersonal (relating to patients, families, colleagues and administrators) and personal (achieving work–life balance). In each of these areas, medical enculturation was achieved by two overlapping processes, absorption and assimilation, each of which may have distinct implications for postgraduate medical education.\n\nBack to top\n\n\n\n\n\n\n17. Links to empirical data\n\n\n\n\n\n\n\nProvide evidence (e.g., quotes, field notes, text excerpts, photographs) to substantiate the more general and abstract concepts or inferences they present as findings.18 I love seeing evidence like this because it really brings results to life. Nkinda Akaro - Researcher\nYou could report this evidence in a table or figure, incorporated into a narrative description of findings, as a stand-alone narrative, or in text blocks embedded in the manuscript text. If you are constrained by word limits or media limitations (e.g.. video), consider sharing data via an appendix, supplemental material, or web-based repository.\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript.\n\n\n\nExamples\nSee Frankel et al. for an excellent example of how to use photographs (or snapshots from video) to illustrate and provide supporting evidence for patterns of behavior identified in the analysis. http://qualitysafety.bmj.com/content/21/Suppl_1/i121.\n\nWe identified five interruption types: (1) probing for further data, (2) prompting for expected sequence, (3) teaching around the case, (4) thinking out loud, and (5) providing direction (see Table 1). Several interruption types served both goals of the case review discussions—teaching and patient care. For example, when thinking out loud, supervisors reasoned through problems and taught the team: So that’s the big question, did she have a mechanical fall, or did she have a medicine-related fall? (Case 2). Supervisors prompted for expected sequence, preventing presenters from skipping over information while simultaneously allowing the supervisor to instruct the team on presentation style: So now you can tell me what the rest of his test results are because I haven’t heard those (Case 16).\n\nBack to top\n\n\n\n\n\n\nDiscussion\n\n\n18. Integration with prior work, implications, transferability, and contribution(s) to the field\n\n\n\n\n\n\n\nDescribe how the findings and conclusions connect to, support, elaborate on, or challenge previous findings, experiences, theory, or a guiding paradigm or approach.19\nDescribe how the findings contribute to or advance the field.\nDescribe any implications of the work, such transferability or generalizability.\n\n\n\n\n\n\n\nJustification and Examples\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExamples\n\n\nWhy readers need this information\nThe short summary reminds readers of the main findings and may help them assess whether the subsequent interpretation and implications formulated by the authors are supported by the findings.\n\n\nExamples\n\nThis study contributes to the understanding and discussion of the complexity of involving patients in healthcare education. It shows that integrating patient-led teaching into initiatives that are partly faculty-led influences the way in which students perceive learning from and with PIs. What is not known, however, is whether perceptions are also affected by type of health profession and the students’ different orientation towards logics of care and science, and issues of authority and power relations.\n\nFor complete examples of Discussions, see:\n\nHenriksen & Ringsted, 2013\nWesterman et al., 2013.\n\nBack to top\n\n\n\n\n\n\n19. Limitations\n\n\n\n\n\n\n\nDescribe problems or gaps in their efforts to ensure trustworthiness and the potential implications.20 Authors should never feel nervous to report limitations. In fact, thoughtful discussion of limitations is a hallmark of good research!Charles Ruggle - Editor\nDescribe how the chosen paradigm, approach, and methods will influence the situations to which the findings may reasonably apply.21 (See also Item 18.)\nDescribe how specific decisions or events in the conduct of the study strengthen or weaken the rigor of the findings.\n\n\n\n\n\n\nJustification and Example\n\n\n\n\n\nJump to:\n\nWhy readers need this information\nExample\n\n\nWhy readers need this information\nAll research has limitations. Discussing them will help readers consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be). If you don’t address limitations, editors and peer reviewers may ask you about them which will delay publication.\n\n\nExample\n\nThe study has several limitations. One is that the focus group interview method reveals students’ perceptions rather than their actual behaviors. Observations of the patient-led teaching encounter may have illuminated an understanding of the relationship between patient instructors and medical students. Another limitation is that the PI-led teaching is optional rather than mandatory, which may have influenced students’ attitudes in a positive direction. Moreover, students who are eager to take on extra-curricular activities may not be representative of the whole population. That only 23 out of 39 students signed up for this study might also have influenced results if the missing group of students represented other perceptions than those present in the focus groups. However the received data from the focus groups were rich in information and diverse perceptions were present. Another limitation is the overrepresentation of women over men in our sample. Even though women are also overrepresented in medical school this might potentially have influenced results, but gender differences in perceptions were nevertheless not identified in the data.\n\nBack to top\n\n\n\n\n\n\nOther\n\n\n20. Conflicts of interest\n\n\n\n\n\n\n\nAuthors shouldn’t feel nervous reporting conflicts of interest. They are very common and rarely influence our decision to publish.Charles Ruggle - Editor\nDescribe any real or potential conflicts of interest that might have influenced or could appear to have influenced the research.\nDescribe:\n\nhow these conflicts were managed in the conduct of the study,\nthe potential impact on study findings and/or conclusions.\n\nSome aspects may be mentioned as part of reflexivity (see Item 6).\n\n\n\n\n\n\nJustification\n\n\n\n\n\n\nWhy readers need this information\nMany factors, including professional and personal relationships and activities, can influence the design, conduct, and reporting of the clinical science that informs health care decision. The potential for conflict of interest exists when these relationships and activities may bias judgment (1). Many stakeholders— editors, peer reviewers, clinicians, educators, policymakers, patients, and the public—rely on the disclosure of authors’ relationships and activities to inform their assessments. Trust in the transparency, consistency, and completeness of these disclosures is essential. - ICMJE\n\n\n\n\n\n\n\n21. Funding\n\n\n\n\n\n\n\nDescribe any sources of funding and other support for the study.\nDescribe the role of funders in data collection, data analysis, and reporting if applicable."
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-title-2",
    "href": "guidelines/srqr/qsce.html#sec-title-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis allows readers to quickly identify the type of study. Your title will be indexed by search tools so descriptive words will make it easier for others to discover."
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-title-3",
    "href": "guidelines/srqr/qsce.html#sec-title-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nResidents learning from a narrative experience with dying patients: a qualitative study.\n\n\nMedical students’ perceptions of the factors influencing their academic performance: an exploratory interview study with high-achieving and re-sitting medical students.\n\n\nUndergraduate rural medical education program development: focus group consultation with the NRHA Rural Medical Educators Group."
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-title-4",
    "href": "guidelines/srqr/qsce.html#sec-title-4",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Training and Resources",
    "text": "Training and Resources\nSee this article for advice on writing titles that are interesting and descriptive.\nThe EQUATOR Network provides training for writing effective manuscript titles as part of their publication school.\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-abstract-2",
    "href": "guidelines/srqr/qsce.html#sec-abstract-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nA reader should be able to read the abstract independent of the manuscript and get a sense of the background, purpose, methods, main results and implications that will be described in greater detail in the manuscript.\nYour abstract will be indexed by search tools so descriptive words will make it easier for others to discover."
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-abstract-3",
    "href": "guidelines/srqr/qsce.html#sec-abstract-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nPurpose: Although academic centers rely on assessments from medical trainees regarding the effectiveness of their faculty as teachers, little is known about how trainees conceptualize and approach their role as assessors of their clinical supervisorsMethod: In 2010, using a constructivist grounded theory approach, five focus group interviews were conducted with 19 residents from an internal medicine residency program. A constant comparative analysis of emergent themes was conducted.Results: Residents viewed clinical teaching assessment (CTA) as a time-consuming task with little reward. They reported struggling throughout the academic year to meet their CTA obligations and described several shortcut strategies they used to reduce their burden. Rather than conceptualizing their assessments as a conduit for both formative and summative feedback, residents perceived CTA as useful for the surveillance of clinical supervisors at the extremes of the spectrum of teaching effectiveness. They put the most effort, including the crafting of written comments, into the CTAs of these outliers. Trainees desired greater transparency in the CTA process and were sceptical regarding the anonymity and perceived validity of their faculty appraisals.Conclusions: Individual and system-based factors conspire to influence postgraduate medical trainees’ motivation for generating high-quality appraisals of clinical teaching. Academic centers need to address these factors if they want to maximize the usefulness of these assessments."
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-abstract-4",
    "href": "guidelines/srqr/qsce.html#sec-abstract-4",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Training and Resources",
    "text": "Training and Resources\nThe EQUATOR Network provides training for writing effective manuscript abstracts as part of their publication school.\nSee this article from the London School of Economics for general abstract writing tips.\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-problem-formulation-2",
    "href": "guidelines/srqr/qsce.html#sec-problem-formulation-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information provides context for the research question or study purpose and situates the study in relation to other work in the field."
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-problem-formulation-3",
    "href": "guidelines/srqr/qsce.html#sec-problem-formulation-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nRegulatory focus theory may therefore offer insight into the variability in responses to feedback, but how well do these experimental findings translate to real clinical situations in which the reality of responsiveness to feedback seems frustratingly complex? …. In order to better elaborate a theory to account for this variability in learner response to feedback, the present study was undertaken. We aimed, in this study, to determine how readily clinical learning events could be classified as activating a promotion or a prevention focus, and to explore, through a careful analysis of doctors’ descriptions of their feedback experiences, the predictive value of regulatory focus theory in the context of real clinical learning situations.\n\n\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-purpose-2",
    "href": "guidelines/srqr/qsce.html#sec-purpose-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nBy clearly stating the purpose of the study, authors set readers’ expectations for the methods, findings and discussion sections of the manuscript."
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-purpose-3",
    "href": "guidelines/srqr/qsce.html#sec-purpose-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nThe purposes of this study were to investigate how medical students recognize, respond to and utilise feedback, and to determine whether there are maturational differences in understandings of the role of feedback across academic years in medical school."
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-purpose-4",
    "href": "guidelines/srqr/qsce.html#sec-purpose-4",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Training and Resources",
    "text": "Training and Resources\nSee this article for advice on writing qualitative research questions.\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-qualitative-approach-2",
    "href": "guidelines/srqr/qsce.html#sec-qualitative-approach-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nIdentifying the research paradigm helps readers understand whether the researcher assumes that there is a single, objective reality (positivist or post-positivist) and has thus designed the study to describe this reality or whether the researcher assumes multiple, subjective realities and designed the study to describe these multiple realities, with no attempt to merge or reconcile these realities (constructivist/interpretivist). The paradigm has implications for the study design, approach, methods, and techniques to ensure rigour and trustworthiness.\nSince the research paradigm does not necessarily dictate particular approaches or methods, the approach should also be clearly defined. Stating the approach provides readers the opportunity to evaluate the fidelity of the research approach to the research question(s) and consider the rationale for modifications and deviations from the selected approach. Qualitative research also includes an array of methods that can be used across paradigms and approaches. (See also Item 10)."
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-qualitative-approach-3",
    "href": "guidelines/srqr/qsce.html#sec-qualitative-approach-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nThe study was performed from a constructivist point of view using an interpretative phenomenological epistemology. Based on the notion that social phenomena are constructed by the communal making of meaning about the underlying phenomena, we aimed to construct insightful accounts of lead consultants’ approaches to educational change, rather than to identify the true nature of these approaches. Because the management of change by lead consultants is an under-researched area, we conducted an exploratory qualitative study…\n\n\nGiven the relative dearth of explanatory theories about factors affecting medical students’ emotional reactions, we chose to develop one by applying methods associated with grounded theory, specifically constant comparative analysis, to qualitative data obtained from learning logs and interviews. Our approach was constructivist, deliberately using researchers’ own experiences and acquired knowledge to enhance theoretical sensitivity and enrich theory development.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-researcher-characteristics-and-reflexivity-2",
    "href": "guidelines/srqr/qsce.html#sec-researcher-characteristics-and-reflexivity-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nIn positivist and post-positivist paradigms, personal characteristics and perspectives of researchers might be viewed as biases that limit the credibility of study findings, while in constructivist or interpretivist paradigms the characteristics and perspectives of the researchers are important contextual factors that are an accepted part of the study design, data collection, and data analysis. These characteristics and perspectives may explain how the researcher(s) obtained access to the site or participants included in the study or may add valuable insight during data analysis."
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-researcher-characteristics-and-reflexivity-3",
    "href": "guidelines/srqr/qsce.html#sec-researcher-characteristics-and-reflexivity-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nReflexivity was maintained by the research team through the analysis and writing by recording, discussing and challenging established assumptions. In addition both EH and SV kept reflexive diaries.\n\n\nThe first author conducted all interviews and discussion groups. Her own medical undergraduate training took place between 1995 and 2000. She was not known to the participants of this research prior to undertaking the study and deliberately did not undertake any clinical or teaching activities locally alongside this research. Whilst it was useful to know (from her own background) what the students were talking about medically (and in terms of detecting items of significance), as a researcher she made conscious efforts not to accept potentially common assumptions at face value.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-context-2",
    "href": "guidelines/srqr/qsce.html#sec-context-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis helps readers interpret the meaning and significance of the study findings by situating them in social, cultural, temporal and other relevant contexts."
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-context-3",
    "href": "guidelines/srqr/qsce.html#sec-context-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nWe conducted the study among hospital-based clinical teachers of students in years 4 to 6 of a six-year undergraduate medical program at Maastricht University Medical School. Years 4 to 6 are devoted to clerkships in the academic hospital and affiliated regional hospitals. Rotations differ in duration depending on the type of rotation and the discipline, and the sequence of rotations differs among students. During rotations, students spend time in the wards, the outpatient clinics, and the accident and emergency department. Clerkships in years 4 and 5 last between 4 and 10 weeks (regular clerkship), whereas students in year 6 undertake an 18-week senior clerkship in a discipline of their choice.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-sampling-strategy-2",
    "href": "guidelines/srqr/qsce.html#sec-sampling-strategy-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information helps readers understand the source of data / findings so they can consider the boundaries of the study and the relevance to their own context. It also gives readers insight into the researchers’ decisions, which can be important for critical appraisal."
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-sampling-strategy-3",
    "href": "guidelines/srqr/qsce.html#sec-sampling-strategy-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nAs students’ perceptions were previously shown to be related to gender, age, prior experience and place of attachment,[REF] we purposely selected respondents with different backgrounds. This sampling strategy led to the diversity of gender, age, prior patient experience and place of attachment shown in Table 1.\n\n\nPurposive sampling was directed towards achieving maximum variation in age and specialty, using a snowball approach (a non-probabilistic form of sampling in which persons initially chosen for the sample are used as informants to locate other persons having necessary characteristics making them eligible for the sample).[REF ]\n\n\nPotential participants were all medical students in Years 1 and 2 at the University of Toronto in 2004. Following research ethics board approval, recruitment was conducted via e-mail to class listservs. Participant responses were sent directly to the research assistant, who was unknown to participants, so that the principal investigators did not know who did or did not participate. This process was engaged to protect participants’ anonymity and to avoid any impression of coercion because the lead researcher (SG) was involved in the administration of the undergraduate curriculum at the time. Sample size was estimated to be sufficient based on the principle of theoretical saturation [REF] and our previous experience with this methodology (i.e., with a relatively homogeneous population, we expected to reach saturation with approximately 15 interviews per group). There were no exclusion criteria and we accepted the first 15 students from each class who volunteered.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-ethics-2",
    "href": "guidelines/srqr/qsce.html#sec-ethics-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nJournals will require an ethical approval statement.\nQualitative research often involves interaction between researchers and research participants. Correspondingly, researchers should ensure that participants are fully aware of their participation in a research study, the risks and benefits associated with the study, the steps and precautions the researchers will take to minimize risks, such as loss of privacy and confidentiality, and how the researchers plan to use the data."
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-ethics-3",
    "href": "guidelines/srqr/qsce.html#sec-ethics-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nEthical approval was granted by the University of Otago and student participants were invited to attend each focus group discussion by the university representatives of New Zealand Medical Students Association (NZMSA). Usually, a key concern when collecting data from students is that students may feel vulnerable when sharing their experiences with academic staff during a focus group discussion. However, this potential harm was removed as each group discussion was facilitated by a fellow student, the discussions were transcribed by a professional transcribing service, and only the primary researchers [Names] had access to the raw data.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-data-collection-methods-2",
    "href": "guidelines/srqr/qsce.html#sec-data-collection-methods-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThe study period helps readers place the study in temporal context and identify factors not mentioned by the authors that might affect findings, interpretation, and implications. (See Item 8 for ending data collection.)\n\nDescribing researcher characteristics clarifies the relationship between the individuals involved in data collection and the participants in the research and also explains what efforts were made to ensure consistency in the data collection process (See Items 6 and 15.)"
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-data-collection-methods-3",
    "href": "guidelines/srqr/qsce.html#sec-data-collection-methods-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nFurther, it was decided that group interviews, also known as focus group discussions would be the best means of data collection. This is a method of data collection that enables group members to feed off each other’s ideas and an effective moderator will maintain group focus whilst at the same time permitting flexibility in the direction those aspects of the discussion might take. [REF]\n\n\nAdjustments to the interview protocol were made according to early experience and information participants had provided (i.e. redundant questions were eliminated; questions were reworded to improve flow and clarity; additional probes were included).\n\n\nFaculty staff were then interviewed individually by a trained study investigator in a 15-minute, semi-structured interview. This sequence was repeated with other video encounters. Table 2 presents examples of interview questions. Each faculty member was interviewed by at least three interviewers over their various interviews. Interviewers were chosen based on their experience in interviewing. All were trained during a half-day meeting to interpret and deliver the interview guide in the same manner in order to elicit information of a consistent type.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-data-collection-instruments-2",
    "href": "guidelines/srqr/qsce.html#sec-data-collection-instruments-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nDescribing instruments and equipment helps readers understand the full context in which data collection occurred and how this context might have affected data collection (e.g., the influence of recording devices on participants’ behaviors; the nature of inferences drawn from live vs. recorded events)."
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-data-collection-instruments-3",
    "href": "guidelines/srqr/qsce.html#sec-data-collection-instruments-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nTo facilitate the discussion and to maintain consistency over different sets of discussions, key trigger questions were devised prior to the discussion. The opening trigger question was: Thinking back to some of your best clinical learning placements in 4th and 5th year. What was it about those clinical placements that provided good opportunities for learning?\n\n\nInterviews included discussion of the expectations, processes and consequences of AEE [authentic early experiences]. The interview schedule was derived following identification of questions that could not be fully answered in a systematic review of previous empirical or theoretical literature. It comprised a sequence of topic areas including experiences in action, and areas of frustration in Medical Education such as the learning of content knowledge, achieving functional knowledge, and transfer of knowledge.[REF]… Interested readers can request a copy of the schedules from the corresponding author. Interviews lasted between 20 and 90 min. and discussion groups between 60 and 90 min. All interviews and discussion groups were conducted in private rooms at the participant’s workplace—the medical school for students and faculty, and individual places of work for workplace supervisors (except for one who chose to be interviewed at the medical school). All data were audio-recorded, and transcribed verbatim.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-units-of-study-2",
    "href": "guidelines/srqr/qsce.html#sec-units-of-study-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "How does this item differ to Sampling (Item 8)?",
    "text": "How does this item differ to Sampling (Item 8)?\nThe sampling item (Item 8) describes the target or ideal participants, documents, or events selected for the study. By contrast, this item focuses on description of the actual participants, documents or events included in the study."
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-units-of-study-3",
    "href": "guidelines/srqr/qsce.html#sec-units-of-study-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis helps readers know whose experiences and perspective are and are not included."
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-units-of-study-4",
    "href": "guidelines/srqr/qsce.html#sec-units-of-study-4",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nOf the 70 Mindful Communication program participants, 46 met the eligibility requirements to participate in the in-depth interviews. We randomly chose and then contacted 22 participants, of whom 20 agreed to be interviewed within six months of completing the program: 15 in person and 5 by telephone. Two declined for lack of time. On reaching saturation after 20 interviews, no further attempts to contact the remaining 24 participants were made.\n\n\n\n\n\nThere were 31 nursing handovers covering 137 patients, and 21 resident handovers covering 101 patients included in this study.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-data-processing-2",
    "href": "guidelines/srqr/qsce.html#sec-data-processing-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript."
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-data-processing-3",
    "href": "guidelines/srqr/qsce.html#sec-data-processing-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nInterviews were anonymised and each participant was given a code number.\n\n\nThe interviewers and another member of the research team (H.B.) reviewed transcripts for accuracy.\n\n\nWe collected data throughout the admission process through direct observation, audio- recording, and chart extraction. We audio-recorded, transcribed, and anonymized both the overnight and morning case review discussions. We also observed the morning case review discussions in person and collected field notes. For each case review discussion, we copied the admission notes from the patient’s record and de-identified all data.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-data-analysis-2",
    "href": "guidelines/srqr/qsce.html#sec-data-analysis-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nTechniques used for data analysis will depend on the paradigm, approach, and/or data collection methods selected by the researchers. Correspondingly, authors should be as transparent as possible about the analytic process so that readers can follow the logic of inquiry from the research question(s) to the analysis and findings."
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-data-analysis-3",
    "href": "guidelines/srqr/qsce.html#sec-data-analysis-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\n…we brought sensitizing concepts to the analysis while we conducted an open, inductive analysis.[REF]In this case the sensitizing concepts arose, a priori to analysis, from a framework derived from the literature [REF] (as described above), in which participants’ motivations to act are based on principles of professionalism, internal affect, or potential implications of their actions.[REF]\n\n\nThrough an iterative process of listening, discussing, and relistening, the team identified and consensually validated emerging themes[REF] and appended segments of dialogue supporting the proposed themes. Recruitment stopped when saturation was reached (no new themes were identified). The team systematically reviewed the themes and sorted them into content domains. The team used an analytic matrix to identify patterns and connections amongst the domains. Two of us not involved in the qualitative coding process (R.E., M.K.) audited the analytic matrix, choice of quotes, and thematic analysis.\n\n\nThe analysis started after the first interview. All data were analyzed with the aid of the audio- coding facility of the NVivo 8: QSR International Pty Ltd, Doncaster, Vic, Australia programme. First, [name] and [name] coded independently from one another, making sure to stay semantically close to the participants’ wording. Then we discussed these open codes and defined axial codes.[REF] New insights about the impact of CST were written down in memos.\n\n\nVideotapes were analysed using immersion/crystallisation methods of qualitative data analysis.[REF] With no pre-existing framework developed in advance for analysis, an inductive approach was used to discover patterns of NVB in the data. A team of six researchers met weekly for 18 months to view videos together. Using a consensus-building approach based on a combination of field notes, opportunistic interviews with the participants, and repeated viewing of the same material, sometimes many months apart, we eventually achieved consensus on verbal, non-verbal, and physical themes and patterns observed in the data. Finally, as a test of goodness-of-fit, we carefully reviewed the videotapes for any deviant cases that did not fit the categories we had developed.\n\n\nAll transcripts were coded thematically by four of the five authors, who met regularly to identify areas of convergence until full agreement was reached. One of the interviewers (P.M.) maintained an audit trail to track the team’s developing thinking. A process of dialectical empiricism[REF] was used to categorise the emergent themes into more abstract concepts…\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-trustworthiness-2",
    "href": "guidelines/srqr/qsce.html#sec-trustworthiness-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript."
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-trustworthiness-3",
    "href": "guidelines/srqr/qsce.html#sec-trustworthiness-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\nMember checking\n\nMember checks [REF] with an external TBL expert (R.L.) supported the validity of these analyses.\n\nTriangulation of data types and data sources\n\nThe interview data were triangulated with the data of 11 student and supervisor focus groups of a previous study, and more specifically, with those data that concern in particular the influence of CST [Communication Skills Training] on the development of patient-centredness…. Triangulation with the focus group data allowed us to broaden the in-depth information from the interviews in the analysis and to share and compare this with information from students and doctors with varying levels of CST (no, limited, full programme) and from two universities (Universities of Antwerp and Ghent). Moreover, this enabled us to better explore the evolution over time, given that the focus groups included participants at different stages of their study: before clerkships (year 4, undergraduate), during clerkships (year 6, undergraduate), after clerkships (year 7, undergraduate) and postgraduate (general practice trainees, and supervising specialists and GPs; Table 1).\n\n\n\n\n\nFinally, as a test of goodness-of-fit, we carefully reviewed the videotapes for any deviant cases that did not fit the categories we had developed.\n\nTriangulation of Researchers + Audit trail\n\nTo ensure rigor and increase authenticity in our methodology, we used two kinds of triangulation—investigator triangulation and data triangulation.[REF] We sought analytical rigor using an audit trail and multiple coders; our coding team included an experienced clinician (M.G.) as well as a nonclinician with expertise in medical communication and team dynamics (L.L.)."
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-trustworthiness-4",
    "href": "guidelines/srqr/qsce.html#sec-trustworthiness-4",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Training and resources",
    "text": "Training and resources\nSee Lincoln and Guba’s Evaluative Criteria for trustworthiness.\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-synthesis-and-interpretation-2",
    "href": "guidelines/srqr/qsce.html#sec-synthesis-and-interpretation-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript."
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-synthesis-and-interpretation-3",
    "href": "guidelines/srqr/qsce.html#sec-synthesis-and-interpretation-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nWe identified four patterns of NVB (non-verbal behavior) that relate to handover quality and have dubbed them: (1) joint focus of attention; (2) the poker hand; (3) parallel play; and (4) kerbside consultation. Each pattern constitutes a transfix, or systematic way of participating non-verbally in the care transfer process. And, although there are variations in each pattern, we have been able to code virtually every handover we have observed in nursing, medicine and surgery into one of these four categories.\n\n\nBecause our participants came from similar educational backgrounds, had studied medicine as their tertiary course, were embedded in the culture of medicine, and were associated in meaningful ways with a single medical school, we approached their transcripts with the assumption that they belonged to a loosely formed discourse community. Although their graduation dates ranged over a period of 50 years and their collective sphere of practice included 10 different specialty areas, there were many similarities in their experiences of enculturation during and after medical school.\n\n\nTheir three major (often overlapping) areas of concern were epistemic (acquiring knowledge and skill), interpersonal (relating to patients, families, colleagues and administrators) and personal (achieving work–life balance). In each of these areas, medical enculturation was achieved by two overlapping processes, absorption and assimilation, each of which may have distinct implications for postgraduate medical education.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-links-to-empirical-data-2",
    "href": "guidelines/srqr/qsce.html#sec-links-to-empirical-data-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThis information helps readers know what decisions the researchers made and why so the reader can 1) consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be) and 2) evaluate or critically appraise the manuscript."
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-links-to-empirical-data-3",
    "href": "guidelines/srqr/qsce.html#sec-links-to-empirical-data-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\nSee Frankel et al. for an excellent example of how to use photographs (or snapshots from video) to illustrate and provide supporting evidence for patterns of behavior identified in the analysis. http://qualitysafety.bmj.com/content/21/Suppl_1/i121.\n\nWe identified five interruption types: (1) probing for further data, (2) prompting for expected sequence, (3) teaching around the case, (4) thinking out loud, and (5) providing direction (see Table 1). Several interruption types served both goals of the case review discussions—teaching and patient care. For example, when thinking out loud, supervisors reasoned through problems and taught the team: So that’s the big question, did she have a mechanical fall, or did she have a medicine-related fall? (Case 2). Supervisors prompted for expected sequence, preventing presenters from skipping over information while simultaneously allowing the supervisor to instruct the team on presentation style: So now you can tell me what the rest of his test results are because I haven’t heard those (Case 16).\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-integration-with-prior-work-2",
    "href": "guidelines/srqr/qsce.html#sec-integration-with-prior-work-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nThe short summary reminds readers of the main findings and may help them assess whether the subsequent interpretation and implications formulated by the authors are supported by the findings."
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-integration-with-prior-work-3",
    "href": "guidelines/srqr/qsce.html#sec-integration-with-prior-work-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Examples",
    "text": "Examples\n\nThis study contributes to the understanding and discussion of the complexity of involving patients in healthcare education. It shows that integrating patient-led teaching into initiatives that are partly faculty-led influences the way in which students perceive learning from and with PIs. What is not known, however, is whether perceptions are also affected by type of health profession and the students’ different orientation towards logics of care and science, and issues of authority and power relations.\n\nFor complete examples of Discussions, see:\n\nHenriksen & Ringsted, 2013\nWesterman et al., 2013.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-limitations-2",
    "href": "guidelines/srqr/qsce.html#sec-limitations-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nAll research has limitations. Discussing them will help readers consider the relevance to their context and the resonance with their own experience or observations (or lack of resonance and why that might be). If you don’t address limitations, editors and peer reviewers may ask you about them which will delay publication."
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-limitations-3",
    "href": "guidelines/srqr/qsce.html#sec-limitations-3",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Example",
    "text": "Example\n\nThe study has several limitations. One is that the focus group interview method reveals students’ perceptions rather than their actual behaviors. Observations of the patient-led teaching encounter may have illuminated an understanding of the relationship between patient instructors and medical students. Another limitation is that the PI-led teaching is optional rather than mandatory, which may have influenced students’ attitudes in a positive direction. Moreover, students who are eager to take on extra-curricular activities may not be representative of the whole population. That only 23 out of 39 students signed up for this study might also have influenced results if the missing group of students represented other perceptions than those present in the focus groups. However the received data from the focus groups were rich in information and diverse perceptions were present. Another limitation is the overrepresentation of women over men in our sample. Even though women are also overrepresented in medical school this might potentially have influenced results, but gender differences in perceptions were nevertheless not identified in the data.\n\nBack to top"
  },
  {
    "objectID": "guidelines/srqr/qsce.html#sec-conflicts-of-interest-2",
    "href": "guidelines/srqr/qsce.html#sec-conflicts-of-interest-2",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Why readers need this information",
    "text": "Why readers need this information\nMany factors, including professional and personal relationships and activities, can influence the design, conduct, and reporting of the clinical science that informs health care decision. The potential for conflict of interest exists when these relationships and activities may bias judgment (1). Many stakeholders— editors, peer reviewers, clinicians, educators, policymakers, patients, and the public—rely on the disclosure of authors’ relationships and activities to inform their assessments. Trust in the transparency, consistency, and completeness of these disclosures is essential. - ICMJE"
  },
  {
    "objectID": "guidelines/srqr/qsce.html#ready-to-get-started",
    "href": "guidelines/srqr/qsce.html#ready-to-get-started",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "Ready to get started?",
    "text": "Ready to get started?\nDownload resources"
  },
  {
    "objectID": "guidelines/srqr/qsce.html#how-to-cite",
    "href": "guidelines/srqr/qsce.html#how-to-cite",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "How to cite",
    "text": "How to cite\nFor attribution, please cite this guideline as:\nBridget O’Brien, Ilene Harris, Thomas Beckman, Darcy Reed, and David Cook. 2023. “The SRQR Guidelines for Writing Qualitative Research Articles version 1.1.” The EQUATOR Network Guideline Dissemination Platform. https://doi.org/10.1234/equator/1010101.\nYou can use your reference manager to save citation information for this webpage, or copy the BibTeX below."
  },
  {
    "objectID": "guidelines/srqr/qsce.html#faqs",
    "href": "guidelines/srqr/qsce.html#faqs",
    "title": "The SRQR guidelines for writing qualitative research articles",
    "section": "FAQs",
    "text": "FAQs\n\nWho made this guideline?\n\nBridget O’Brien, PhD has been a faculty member in the Department of Medicine, Division of General Internal Medicine, since 2008. She is a professor of medicine and an education scientist in the Office of Medical Education’s Center for Faculty Educators. As co-director of the Teaching Scholars Program and the UCSF-University of Utrecht Health Professions Education doctoral program she teaches and mentors faculty and learners interested in education research and scholarship. At the San Francisco VA, she directs the Advanced Fellowship in Health Professions Education Evaluation and Research. In 2015 she was selected as one of five national Macy Faculty Scholars supported by the Josiah Macy Jr. Foundation and in 2021 she was selected as a KIPRIME Fellow at the Karolinska Instituet. She is a deputy editor for the journal Academic Medicine.\nDr. Ilene Harris, deceased, was professor and head, Department of Medical Education, University of Illinois at Chicago College of Medicine, Chicago, Illinois.\nDr. Thomas Beckman is professor of medicine and medical education, Department of Medicine, Mayo Clinic College of Medicine, Rochester, Minnesota.\nDr. Darcy Reed is associate professor of medicine and medical education, Department of Medicine, Mayo Clinic College of Medicine, Rochester, Minnesota.\nDr. David Cook is associate director, Mayo Clinic Online Learning, research chair, Mayo Multidisciplinary Simulation Center, and professor of medicine and medical education, Mayo Clinic College of Medicine, Rochester, Minnesota.\n\n\nHow was this guideline made?\nThe developers synthesised 40 sets of recommendations previously proposed by experts in qualitative methods. You can read about their development process here.\n\n\nWhat to do if asked to remove guideline related content\nIf a colleague or reviewer asks you to remove content that is related to this guideline, you can direct them to this guideline and the explanation for why that item is important. If they insist, consider moving the item to a supplement, table or figure.\n\n\nWhere can I get general writing training?\nThe EQUATOR Network provides in-person training for writing research articles.\nAuthorAID have resources, an online course, and mentoring to help authors.\n\n\n\nResearch paradigm\n\n\n\n\n\n\nThe set of beliefs and assumptions that guide the research process. These commonly include positivist, post-positivist, constructivist or interpretivist, and critical theory. Qualitative research generally draws from a post-positivist or constructivist/interpretivist paradigm.”\n\n\n\n\n\n\nInstruments\n\n\n\n\n\n\nData collection instruments include (but are not limited to) interview or focus group guides, observational protocols and prompts for field notes, and data extraction or coding protocols for selection and analysis of documents, photographs, videos, or other artifacts”\n\n\n\n\n\n\nBias\n\n\n\n\n\n\nA term drawn from quantitative research, bias technically means a systematic error, where a particular research finding deviates from a true finding. This might come about through errors in the manner of interviewing, or by errors in sampling. In qualitative research this is a problematic concept, since by definition the qualitative researcher is part of the process, and all researchers are different. This human factor has been said to be both the greatest strength and the greatest weakness of qualitative method. What can be done in commercial qualitative research, however, is to minimise obvious and avoidable sources of bias, for example by not confining all the fieldwork to one social group or geographic location, by taking steps to recognise the personal views of the researcher, (using techniques such as bracketing), and by working in teams.\n\n\n\n\n\n\nSampling strategy\n\n\n\n\n\n\nSeveral sampling strategies are commonly used in qualitative research, although most fall under the umbrella of purposeful (or purposive) sampling.\nPurposeful sampling means that participants, documents, or events are selected for their relevance to the research question, based on guiding theory or experiences and assumptions of the researchers. Over the course of the research process, researchers may determine that additional or different participants, documents, or events should be included to address the research question.\nOther sampling techniques, such as theoretical sampling (seeking examples of theoretical constructs), snowball sampling (using study participants to identify additional participants who meet study criteria), and convenience sampling (including any volunteers with no or minimal criteria for inclusion) may be appropriate depending on the question and approach, so long as the authors provide explanation and justification.\n\n\n\n\n\n\nApproach\n\n\n\n\n\n\nA qualitative approach is a general way of thinking about conducting qualitative research. It describes, either explicitly or implicitly, the purpose of the qualitative research, the role of the researcher(s), the stages of research, and the method of data analysis. Commonly used approaches include ethnography, grounded theory, case study, phenomenology, and narrative research.\n\n\n\n\n\n\nData collection methods\n\n\n\n\n\n\nData collection methods include (but are not limited to) interviews, focus groups, observations (direct or indirect via video), and review of written text, photographs, and other documents or materials.\n\n\n\n\n\n\nIterative\n\n\n\n\n\n\nQualitative research often occurs as an iterative process, meaning that researchers begin data analysis before they complete data collection. The data collection and analysis process may occur in phases or stages. As part of an iterative collection-analysis process, researchers will often alter their data collection methods to explore their preliminary impressions in greater depth and/or actively pursue confirming and disconfirming perspectives.\n\n\n\n\n\n\nStudy period\n\n\n\n\n\n\nThe start and end dates for data collection and analysis.\n\n\n\n\n\n\nEthnography\n\n\n\n\n\n\nThe scientific description of peoples and cultures with their customs, habits, and mutual differences.\nRead more\n\n\n\n\n\n\nGrounded theory\n\n\n\n\n\n\nA method consisting of a set of systematic, but flexible, guidelines for conducting inductive qualitative inquiry aimed toward theory construction. This method focuses squarely on the analytic phases of research, although both data collection and analysis inform and shape each other and are conducted in tandem.\nRead more\n\n\n\n\n\n\nDegree of participation\n\n\n\n\n\n\nFor example, if some participants were observed and interviewed and others only interviewed, or if some participants completed multiple interviews and others completed a single interview, these variations should be explained.\n\n\n\n\n\n\nUnit of analysis\n\n\n\n\n\n\nIn qualitative research, the unit of analysis is not necessarily the same as the unit of sampling (e.g., individual participants or events). Instead, some approaches use specific events as the unit of analysis, such as mentions of a particular topic or experience, or observations of a particular behavior or phenomenon, while others use groups rather than individual group participants. This specification has implications for how the data are organized and analyzed as well as the inferences drawn from the data.\n\n\n\n\n\n\nReflexivity\n\n\n\n\n\n\nReflexivity refers to intentional, systematic consideration of the potential or actual effects of the researcher(s) on all aspects of the study process.\n\n\n\n\n\n\nTransferability\n\n\n\n\n\n\nThe transferability of a research finding is the extent to which it can be applied in other contexts and studies. It is thus equivalent to or a replacement for the terms generalizability and external validity.\n\n\n\n\n\n\nGeneralizability\n\n\n\n\n\n\nThe appropriate scope for generalization of the findings beyond the study (e.g., to other settings, populations, time periods, circumstances)\n\n\n\n\n\n\nAnalytic findings\n\n\n\n\n\n\nAnalytic findings may include interpretations, inferences, narratives, themes, and models.\n\n\n\n\n\n\nFrequency counts\n\n\n\n\n\n\nThe frequency of specific themes or codes."
  },
  {
    "objectID": "guidelines/stard/index.html#about-this-guideline",
    "href": "guidelines/stard/index.html#about-this-guideline",
    "title": "The STARD guideline for writing a diagnostic test accuracy study.",
    "section": "About this guideline",
    "text": "About this guideline\n\nThis guideline is relevant to studies of diagnostic accuracy"
  },
  {
    "objectID": "guidelines/stard/index.html#download-resources",
    "href": "guidelines/stard/index.html#download-resources",
    "title": "The STARD guideline for writing a diagnostic test accuracy study.",
    "section": "Download Resources",
    "text": "Download Resources\nReporting guidelines are best used early:\n\nUse a template to write quickly and avoid the blank page\nUse a checklist to double-check and reassure others that your work is described transparently\n\n\n\n\nFor drafting\n\nTemplate\n\n\n\nFor checking\n\nChecklist\n\n\n\nFor planning\n\nTo-do list"
  },
  {
    "objectID": "guidelines/stard/index.html#guidance",
    "href": "guidelines/stard/index.html#guidance",
    "title": "The STARD guideline for writing a diagnostic test accuracy study.",
    "section": "Guidance",
    "text": "Guidance\nApprox. 2 min read\n\nTitle or abstract\n\n\n1. Title or abstract\n\n\n\n\n\n\n\nIdentification as a study of diagnostic accuracy using at least one measure of accuracy (such as sensitivity, specificity, predictive values, or AUC)\n\n\nAbstract\n\n\n2. Abstract\n\n\n\n\n\n\n\nStructured summary of study design, methods, results, and conclusions (for specific guidance, see STARD for Abstracts)\n\n\nIntroduction\n\n\n3. Introduction\n\n\n\n\n\n\n\nScientific and clinical background, including the intended use and clinical role of the index test\n\n\n4. Introduction\n\n\n\n\n\n\n\nStudy objectives and hypotheses\n\n\nMethods\n\n\n5. Study design\n\n\n\n\n\n\n\nWhether data collection was planned before the index test and reference standard were performed (prospective study) or after (retrospective study)\n\n\n6. Participants\n\n\n\n\n\n\n\nEligibility criteria\n\n\n7. Participants\n\n\n\n\n\n\n\nOn what basis potentially eligible participants were identified (such as symptoms, results from previous tests, inclusion in registry)\n\n\n8. Participants\n\n\n\n\n\n\n\nWhere and when potentially eligible participants were identified (setting, location and dates)\n\n\n9. Participants\n\n\n\n\n\n\n\nWhether participants formed a consecutive, random or convenience series\n\n\n10a. Test methods\n\n\n\n\n\n\n\nIndex test, in sufficient detail to allow replication\n\n\n10b. Test methods\n\n\n\n\n\n\n\nReference standard, in sufficient detail to allow replication\n\n\n11. Test methods\n\n\n\n\n\n\n\nRationale for choosing the reference standard (if alternatives exist)\n\n\n12a. Test methods\n\n\n\n\n\n\n\nDefinition of and rationale for test positivity cut-offs or result categories of the index test, distinguishing pre-specified from exploratory\n\n\n12b. Test methods\n\n\n\n\n\n\n\nDefinition of and rationale for test positivity cut-offs or result categories of the reference standard, distinguishing pre-specified from exploratory\n\n\n13a. Test methods\n\n\n\n\n\n\n\nWhether clinical information and reference standard results were available to the performers / readers of the index test\n\n\n13b. Test methods\n\n\n\n\n\n\n\nWhether clinical information and index test results were available to the assessors of the reference standard\n\n\n14. Analysis\n\n\n\n\n\n\n\nMethods for estimating or comparing measures of diagnostic accuracy\n\n\n15. Analysis\n\n\n\n\n\n\n\nHow indeterminate index test or reference standard results were handled\n\n\n16. Analysis\n\n\n\n\n\n\n\nHow missing data on the index test and reference standard were handled\n\n\n17. Analysis\n\n\n\n\n\n\n\nAny analyses of variability in diagnostic accuracy, distinguishing pre-specified from exploratory\n\n\n18. Analysis\n\n\n\n\n\n\n\nIntended sample size and how it was determined\n\n\nResults\n\n\n19. Participants\n\n\n\n\n\n\n\nFlow of participants, using a diagram\n\n\n20. Participants\n\n\n\n\n\n\n\nBaseline demographic and clinical characteristics of participants\n\n\n21a. Participants\n\n\n\n\n\n\n\nDistribution of severity of disease in those with the target condition\n\n\n21b. Participants\n\n\n\n\n\n\n\nDistribution of alternative diagnoses in those without the target condition\n\n\n22. Participants\n\n\n\n\n\n\n\nTime interval and any clinical interventions between index test and reference standard\n\n\n23. Test results\n\n\n\n\n\n\n\nCross tabulation of the index test results (or their distribution) by the results of the reference standard\n\n\n24. Test results\n\n\n\n\n\n\n\nEstimates of diagnostic accuracy and their precision (such as 95% confidence intervals)\n\n\n25. Test results\n\n\n\n\n\n\n\nAny adverse events from performing the index test or the reference standard\n\n\nDiscussion\n\n\n26. Discussion\n\n\n\n\n\n\n\nStudy limitations, including sources of potential bias, statistical uncertainty, and generalisability\n\n\n27. Discussion\n\n\n\n\n\n\n\nImplications for practice, including the intended use and clinical role of the index test\n\n\nOther information\n\n\n28. Other information\n\n\n\n\n\n\n\nRegistration number and name of registry\n\n\n29. Other information\n\n\n\n\n\n\n\nWhere the full study protocol can be accessed\n\n\n30. Other information\n\n\n\n\n\n\n\nSources of funding and other support; role of funders"
  },
  {
    "objectID": "guidelines/strega/index.html#about-this-guideline",
    "href": "guidelines/strega/index.html#about-this-guideline",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "About this guideline",
    "text": "About this guideline\n\nThis guideline is relevant to studies reporting genetic associations"
  },
  {
    "objectID": "guidelines/strega/index.html#download-resources",
    "href": "guidelines/strega/index.html#download-resources",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Download Resources",
    "text": "Download Resources\nReporting guidelines are best used early:\n\nUse a template to write quickly and avoid the blank page\nUse a checklist to double-check and reassure others that your work is described transparently\n\n\n\n\nFor drafting\n\nTemplate\n\n\n\nFor checking\n\nChecklist\n\n\n\nFor planning\n\nTo-do list"
  },
  {
    "objectID": "guidelines/strega/index.html#guidance",
    "href": "guidelines/strega/index.html#guidance",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Guidance",
    "text": "Guidance\nApprox. 5 min read\n\nTitle and abstract\n\n\n1a. Title\n\n\n\n\n\n\n\nIndicate the study’s design with a commonly used term in the title or the abstract\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nReaders should be able to easily identify the design that was used from the title or abstract. An explicit, commonly used term for the study design also helps ensure correct indexing of articles in electronic databases\n\n\n\n\n\n\n1b. Abstract\n\n\n\n\n\n\n\nProvide in the abstract an informative and balanced summary of what was done and what was found\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nThe abstract provides key information that enables readers to understand a study and decide whether to read the article. Typical components include a statement of the research question, a short description of methods and results, and a conclusion. Abstracts should summarize key details of studies and should only present information that is provided in the article. We advise presenting key results in a numerical form that includes numbers of participants, estimates of associations and appropriate measures of variability and uncertainty (e.g., odds ratios with confidence intervals). We regard it insufficient to state only that an exposure is or is not significantly associated with an outcome.\nA series of headings pertaining to the background, design, conduct, and analysis of a study may help readers acquire the essential information rapidly. Many journals require such structured abstracts, which tend to be of higher quality and more readily informative than unstructured summaries.\n\n\n\n\n\n\nBackground/rationale\n\n\n2. Background/rationale\n\n\n\n\n\n\n\nExplain the scientific background and rationale for the investigation being reported\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nThe scientific background of the study provides important context for readers. It sets the stage for the study and describes its focus. It gives an overview of what is known on a topic and what gaps in current knowledge are addressed by the study. Background material should note recent pertinent studies and any systematic reviews of pertinent studies.\n\n\n\n\n\n\nObjectives\n\n\n3. Objectives\n\n\n\n\n\n\n\nState specific objectives, including any prespecified hypotheses. State if the study is the first report of a genetic association, a replication effort, or both.\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nObjectives are the detailed aims of the study. Well crafted objectives specify populations, exposures and outcomes, and parameters that will be estimated. They may be formulated as specific hypotheses or as questions that the study was designed to address. In some situations objectives may be less specific, for example, in early discovery phases. Regardless, the report should clearly reflect the investigators’ intentions. For example, if important subgroups or additional analyses were not the original aim of the study but arose during data analysis, they should be described accordingly (see also items 4, 17 and 20).\n\n\n\n\n\n\nStudy design\n\n\n4. Study design\n\n\n\n\n\n\n\nPresent key elements of study design early in the paper\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nWe advise presenting key elements of study design early in the methods section (or at the end of the introduction) so that readers can understand the basics of the study. For example, authors should indicate that the study was a cohort study, which followed people over a particular time period, and describe the group of persons that comprised the cohort and their exposure status. Similarly, if the investigation used a case-control design, the cases and controls and their source population should be described. If the study was a cross-sectional survey, the population and the point in time at which the cross-section was taken should be mentioned. When a study is a variant of the three main study types, there is an additional need for clarity. For instance, for a case-crossover study, one of the variants of the case-control design, a succinct description of the principles was given in the example above [28].\nWe recommend that authors refrain from simply calling a study prospective or retrospective because these terms are ill defined [29]. One usage sees cohort and prospective as synonymous and reserves the word retrospective for case-control studies [30]. A second usage distinguishes prospective and retrospective cohort studies according to the timing of data collection relative to when the idea for the study was developed [31]. A third usage distinguishes prospective and retrospective case-control studies depending on whether the data about the exposure of interest existed when cases were selected [32]. Some advise against using these terms [33], or adopting the alternatives concurrent and historical for describing cohort studies [34]. In STROBE, we do not use the words prospective and retrospective, nor alternatives such as concurrent and historical. We recommend that, whenever authors use these words, they define what they mean. Most importantly, we recommend that authors describe exactly how and when data collection took place.\nThe first part of the methods section might also be the place to mention whether the report is one of several from a study. If a new report is in line with the original aims of the study, this is usually indicated by referring to an earlier publication and by briefly restating the salient features of the study. However, the aims of a study may also evolve over time. Researchers often use data for purposes for which they were not originally intended, including, for example, official vital statistics that were collected primarily for administrative purposes, items in questionnaires that originally were only included for completeness, or blood samples that were collected for another purpose. For example, the Physicians’ Health Study, a randomized controlled trial of aspirin and carotene, was later used to demonstrate that a point mutation in the factor V gene was associated with an increased risk of venous thrombosis, but not of myocardial infarction or stroke [35]. The secondary use of existing data is a creative part of observational research and does not necessarily make results less credible or less important. However, briefly restating the original aims might help readers understand the context of the research and possible limitations in the data.\n\n\n\n\n\n\nSetting\n\n\n5. Setting\n\n\n\n\n\n\n\nDescribe the setting, locations, and relevant dates, including periods of recruitment, exposure, follow-up, and data collection\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nReaders need information on setting and locations to assess the context and generalisability of a study’s results. Exposures such as environmental factors and therapies can change over time. Also, study methods may evolve over time. Knowing when a study took place and over what period participants were recruited and followed up places the study in historical context and is important for the interpretation of results.\nInformation about setting includes recruitment sites or sources (e.g., electoral roll, outpatient clinic, cancer registry, or tertiary care centre). Information about location may refer to the countries, towns, hospitals or practices where the investigation took place. We advise stating dates rather than only describing the length of time periods. There may be different sets of dates for exposure, disease occurrence, recruitment, beginning and end of follow-up, and data collection. Of note, nearly 80% of 132 reports in oncology journals that used survival analysis included the starting and ending dates for accrual of patients, but only 24% also reported the date on which follow-up ended [37].\n\n\n\n\n\n\nEligibility criteria\n\n\n6a. Eligibility criteria\n\n\n\n\n\n\n\nCohort study – Give the eligibility criteria, and the sources and methods of selection of participants. Describe methods of follow-up.\nCase-control study – Give the eligibility criteria, and the sources and methods of case ascertainment and control selection. Give the rationale for the choice of cases and controls.\nCross-sectional study – Give the eligibility criteria, and the sources and methods of selection of participants.\nGive information on the criteria and methods for selection of subsets of participants from a larger study, when relevant.\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nDetailed descriptions of the study participants help readers understand the applicability of the results. Investigators usually restrict a study population by defining clinical, demographic and other characteristics of eligible participants. Typical eligibility criteria relate to age, gender, diagnosis and comorbid conditions. Despite their importance, eligibility criteria often are not reported adequately. In a survey of observational stroke research, 17 of 49 reports (35%) did not specify eligibility criteria [5].\nEligibility criteria may be presented as inclusion and exclusion criteria, although this distinction is not always necessary or useful. Regardless, we advise authors to report all eligibility criteria and also to describe the group from which the study population was selected (e.g., the general population of a region or country), and the method of recruitment (e.g., referral or self-selection through advertisements).\nKnowing details about follow-up procedures, including whether procedures minimized non-response and loss to follow-up and whether the procedures were similar for all participants, informs judgments about the validity of results. For example, in a study that used IgM antibodies to detect acute infections, readers needed to know the interval between blood tests for IgM antibodies so that they could judge whether some infections likely were missed because the interval between blood tests was too long [41]. In other studies where follow-up procedures differed between exposed and unexposed groups, readers might recognize substantial bias due to unequal ascertainment of events or differences in non-response or loss to follow-up [42]. Accordingly, we advise that researchers describe the methods used for following participants and whether those methods were the same for all participants, and that they describe the completeness of ascertainment of variables (see also item 14).\nIn case-control studies, the choice of cases and controls is crucial to interpreting the results, and the method of their selection has major implications for study validity. In general, controls should reflect the population from which the cases arose. Various methods are used to sample controls, all with advantages and disadvantages: for cases that arise from a general population, population roster sampling, random digit dialling, neighbourhood or friend controls are used. Neighbourhood or friend controls may present intrinsic matching on exposure [17]. Controls with other diseases may have advantages over population-based controls, in particular for hospital-based cases, because they better reflect the catchment population of a hospital, have greater comparability of recall and ease of recruitment. However, they can present problems if the exposure of interest affects the risk of developing or being hospitalized for the control condition(s) [43,44]. To remedy this problem often a mixture of the best defensible control diseases is used [45].\n\n\n\n\n\n\n6b. Eligibility criteria\n\n\n\n\n\n\n\nCohort study – For matched studies, give matching criteria and number of exposed and unexposed.\nCase-control study – For matched studies, give matching criteria and the number of controls per case.\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nMatching is much more common in case-control studies, but occasionally, investigators use matching in cohort studies to make groups comparable at the start of follow-up. Matching in cohort studies makes groups directly comparable for potential confounders and presents fewer intricacies than with case-control studies. For example, it is not necessary to take the matching into account for the estimation of the relative risk [48]. Because matching in cohort studies may increase statistical precision investigators might allow for the matching in their analyses and thus obtain narrower confidence intervals.\nIn case-control studies matching is done to increase a study’s efficiency by ensuring similarity in the distribution of variables between cases and controls, in particular the distribution of potential confounding variables [48,49]. Because matching can be done in various ways, with one or more controls per case, the rationale for the choice of matching variables and the details of the method used should be described. Commonly used forms of matching are frequency matching (also called group matching) and individual matching. In frequency matching, investigators choose controls so that the distribution of matching variables becomes identical or similar to that of cases. Individual matching involves matching one or several controls to each case. Although intuitively appealing and sometimes useful, matching in case-control studies has a number of disadvantages, is not always appropriate, and needs to be taken into account in the analysis\n\n\n\n\n\n\nVariables\n\n\n7a. Variables\n\n\n\n\n\n\n\nClearly define all outcomes, exposures, predictors, potential confounders, and effect modifiers. Give diagnostic criteria, if applicable\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nAuthors should define all variables considered for and included in the analysis, including outcomes, exposures, predictors, potential confounders and potential effect modifiers. Disease outcomes require adequately detailed description of the diagnostic criteria. This applies to criteria for cases in a case-control study, disease events during follow-up in a cohort study and prevalent disease in a cross-sectional study. Clear definitions and steps taken to adhere to them are particularly important for any disease condition of primary interest in the study.\nFor some studies, determinant or predictor may be appropriate terms for exposure variables and outcomes may be called endpoints. In multivariable models, authors sometimes use dependent variable for an outcome and independent variable or explanatory variable for exposure and confounding variables. The latter is not precise as it does not distinguish exposures from confounders.\nIf many variables have been measured and included in exploratory analyses in an early discovery phase, consider providing a list with details on each variable in an appendix, additional table or separate publication. Of note, the International Journal of Epidemiology recently launched a new section with cohort profiles, that includes detailed information on what was measured at different points in time in particular studies [56,57]. Finally, we advise that authors declare all candidate variables considered for statistical analysis, rather than selectively reporting only those included in the final models (see also item 16a) [58,59]\n\n\n\n\n\n\n7b. Variables\n\n\n\n\n\n\n\nClearly define genetic exposures (genetic variants) using a widely-used nomenclature system. Identify variables likely to be associated with population stratification (confounding by ethnic origin).\n\n\nData sources/measurement\n\n\n8a. Data sources/measurement\n\n\n\n\n\n\n\nFor each variable of interest give sources of data and details of methods of assessment (measurement). Describe comparability of assessment methods if there is more than one group. Give information separately for for exposed and unexposed groups if applicable.\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nThe way in which exposures, confounders and outcomes were measured affects the reliability and validity of a study. Measurement error and misclassification of exposures or outcomes can make it more difficult to detect cause-effect relationships, or may produce spurious relationships. Error in measurement of potential confounders can increase the risk of residual confounding [62,63]. It is helpful, therefore, if authors report the findings of any studies of the validity or reliability of assessments or measurements, including details of the reference standard that was used. Rather than simply citing validation studies (as in the first example), we advise that authors give the estimated validity or reliability, which can then be used for measurement error adjustment or sensitivity analyses (see items 12e and 17).\nIn addition, it is important to know if groups being compared differed with respect to the way in which the data were collected. This may be important for laboratory examinations (as in the second example) and other situations. For instance, if an interviewer first questions all the cases and then the controls, or vice versa, bias is possible because of the learning curve; solutions such as randomising the order of interviewing may avoid this problem. Information bias may also arise if the compared groups are not given the same diagnostic tests or if one group receives more tests of the same kind than another (see also item 9).\n\n\n\n\n\n\n8b. Data sources/measurement\n\n\n\n\n\n\n\nDescribe laboratory methods, including source and storage of DNA, genotyping methods and platforms (including the allele calling algorithm used, and its version), error rates and call rates. State the laboratory / centre where genotyping was done. Describe comparability of laboratory methods if there is more than one group. Specify whether genotypes were assigned using all of the data from the study simultaneously or in smaller batches.\n\n\nBias\n\n\n9a. Bias\n\n\n\n\n\n\n\nDescribe any efforts to address potential sources of bias\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nBiased studies produce results that differ systematically from the truth (see also Box 3). It is important for a reader to know what measures were taken during the conduct of a study to reduce the potential of bias. Ideally, investigators carefully consider potential sources of bias when they plan their study. At the stage of reporting, we recommend that authors always assess the likelihood of relevant biases. Specifically, the direction and magnitude of bias should be discussed and, if possible, estimated. For instance, in case-control studies information bias can occur, but may be reduced by selecting an appropriate control group, as in the first example [64]. Differences in the medical surveillance of participants were a problem in the second example [65]. Consequently, the authors provide more detail about the additional data they collected to tackle this problem. When investigators have set up quality control programs for data collection to counter a possible drift in measurements of variables in longitudinal studies, or to keep variability at a minimum when multiple observers are used, these should be described.\nUnfortunately, authors often do not address important biases when reporting their results. Among 43 case-control and cohort studies published from 1990 to 1994 that investigated the risk of second cancers in patients with a history of cancer, medical surveillance bias was mentioned in only 5 articles [66]. A survey of reports of mental health research published during 1998 in three psychiatric journals found that only 13% of 392 articles mentioned response bias [67]. A survey of cohort studies in stroke research found that 14 of 49 (28%) articles published from 1999 to 2003 addressed potential selection bias in the recruitment of study participants and 35 (71%) mentioned the possibility that any type of bias may have affected results\n\n\n\n\n\n\n9b. Bias\n\n\n\n\n\n\n\nDescribe any efforts to address potential sources of bias\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nFor quantitative outcome variables, specify if any investigation of potential bias resulting from pharmacotherapy was undertaken. If relevant, describe the nature and magnitude of the potential bias, and explain what approach was used to deal with this.\n\n\n\n\n\n\nStudy size\n\n\n10. Study size\n\n\n\n\n\n\n\nExplain how the study size was arrived at\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nA study should be large enough to obtain a point estimate with a sufficiently narrow confidence interval to meaningfully answer a research question. Large samples are needed to distinguish a small association from no association. Small studies often provide valuable information, but wide confidence intervals may indicate that they contribute less to current knowledge in comparison with studies providing estimates with narrower confidence intervals. Also, small studies that show interesting or statistically significant associations are published more frequently than small studies that do not have significant findings. While these studies may provide an early signal in the context of discovery, readers should be informed of their potential weaknesses.\nThe importance of sample size determination in observational studies depends on the context. If an analysis is performed on data that were already available for other purposes, the main question is whether the analysis of the data will produce results with sufficient statistical precision to contribute substantially to the literature, and sample size considerations will be informal. Formal, a priori calculation of sample size may be useful when planning a new study. Such calculations are associated with more uncertainty than implied by the single number that is generally produced. For example, estimates of the rate of the event of interest or other assumptions central to calculations are commonly imprecise, if not guesswork. The precision obtained in the final analysis can often not be determined beforehand because it will be reduced by inclusion of confounding variables in multivariable analyses, the degree of precision with which key variables can be measured, and the exclusion of some individuals.\nFew epidemiological studies explain or report deliberations about sample size. We encourage investigators to report pertinent formal sample size calculations if they were done. In other situations they should indicate the considerations that determined the study size (e.g., a fixed available sample, as in the first example above). If the observational study was stopped early when statistical significance was achieved, readers should be told. Do not bother readers with post hoc justifications for study size or retrospective power calculations. From the point of view of the reader, confidence intervals indicate the statistical precision that was ultimately obtained. It should be realized that confidence intervals reflect statistical uncertainty only, and not all uncertainty that may be present in a study (see item 20).\n\n\n\n\n\n\nQuantitative variables\n\n\n11. Quantitative variables\n\n\n\n\n\n\n\nExplain how quantitative variables were handled in the analyses. If applicable, describe which groupings were chosen, and why. If applicable, describe how effects of treatment were dealt with.\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nInvestigators make choices regarding how to collect and analyse quantitative data about exposures, effect modifiers and confounders. For example, they may group a continuous exposure variable to create a new categorical variable. Grouping choices may have important consequences for later analyses. We advise that authors explain why and how they grouped quantitative data, including the number of categories, the cut-points, and category mean or median values. Whenever data are reported in tabular form, the counts of cases, controls, persons at risk, person-time at risk, etc. should be given for each category. Tables should not consist solely of effect-measure estimates or results of model fitting.\nInvestigators might model an exposure as continuous in order to retain all the information. In making this choice, one needs to consider the nature of the relationship of the exposure to the outcome. As it may be wrong to assume a linear relation automatically, possible departures from linearity should be investigated. Authors could mention alternative models they explored during analyses (e.g., using log transformation, quadratic terms or spline functions). Several methods exist for fitting a non-linear relation between the exposure and outcome. Also, it may be informative to present both continuous and grouped analyses for a quantitative exposure of prime interest.\nIn a recent survey, two thirds of epidemiological publications studied quantitative exposure variables. In 42 of 50 articles (84%) exposures were grouped into several ordered categories, but often without any stated rationale for the choices made. Fifteen articles used linear associations to model continuous exposure but only two reported checking for linearity. In another survey, of the psychological literature, dichotomization was justified in only 22 of 110 articles (20%)\n\n\n\n\n\n\nStatistical methods\n\n\n12a. Statistical methods\n\n\n\n\n\n\n\nDescribe all statistical methods, including those used to control for confounding. State software version used and options (or settings) chosen.\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nIn general, there is no one correct statistical analysis but, rather, several possibilities that may address the same question, but make different assumptions. Regardless, investigators should pre-determine analyses at least for the primary study objectives in a study protocol. Often additional analyses are needed, either instead of, or as well as, those originally envisaged, and these may sometimes be motivated by the data. When a study is reported, authors should tell readers whether particular analyses were suggested by data inspection. Even though the distinction between pre-specified and exploratory analyses may sometimes be blurred, authors should clarify reasons for particular analyses.\nIf groups being compared are not similar with regard to some characteristics, adjustment should be made for possible confounding variables by stratification or by multivariable regression (see Box 5) [94]. Often, the study design determines which type of regression analysis is chosen. For instance, Cox proportional hazard regression is commonly used in cohort studies [95]. whereas logistic regression is often the method of choice in case-control studies [96,97]. Analysts should fully describe specific procedures for variable selection and not only present results from the final model [98,99]. If model comparisons are made to narrow down a list of potential confounders for inclusion in a final model, this process should be described. It is helpful to tell readers if one or two covariates are responsible for a great deal of the apparent confounding in a data analysis. Other statistical analyses such as imputation procedures, data transformation, and calculations of attributable risks should also be described. Non-standard or novel approaches should be referenced and the statistical software used reported. As a guiding principle, we advise statistical methods be described with enough detail to enable a knowledgeable reader with access to the original data to verify the reported results [100].\nIn an empirical study, only 93 of 169 articles (55%) reporting adjustment for confounding clearly stated how continuous and multi-category variables were entered into the statistical model [101]. Another study found that among 67 articles in which statistical analyses were adjusted for confounders, it was mostly unclear how confounders were chosen\n\n\n\n\n\n\n12b. Statistical methods\n\n\n\n\n\n\n\nDescribe any methods used to examine subgroups and interactions\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nAs discussed in detail under item 17, many debate the use and value of analyses restricted to subgroups of the study population [4,104]. Subgroup analyses are nevertheless often done [4]. Readers need to know which subgroup analyses were planned in advance, and which arose while analysing the data. Also, it is important to explain what methods were used to examine whether effects or associations differed across groups (see item 17).\nInteraction relates to the situation when one factor modifies the effect of another (therefore also called effect modification). The joint action of two factors can be characterized in two ways: on an additive scale, in terms of risk differences; or on a multiplicative scale, in terms of relative risk (see Box 8). Many authors and readers may have their own preference about the way interactions should be analysed. Still, they may be interested to know to what extent the joint effect of exposures differs from the separate effects. There is consensus that the additive scale, which uses absolute risks, is more appropriate for public health and clinical decision making [105]. Whatever view is taken, this should be clearly presented to the reader, as is done in the example above [103]. A lay-out presenting separate effects of both exposures as well as their joint effect, each relative to no exposure, might be most informative. It is presented in the example for interaction under item 17, and the calculations on the different scales are explained in Box 8\n\n\n\n\n\n\n12c. Statistical methods\n\n\n\n\n\n\n\nExplain how missing data were addressed\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nMissing data are common in observational research. Questionnaires posted to study participants are not always filled in completely, participants may not attend all follow-up visits and routine data sources and clinical databases are often incomplete. Despite its ubiquity and importance, few papers report in detail on the problem of missing data [5,107]. Investigators may use any of several approaches to address missing data. We describe some strengths and limitations of various approaches in Box 6. We advise that authors report the number of missing values for each variable of interest (exposures, outcomes, confounders) and for each step in the analysis. Authors should give reasons for missing values if possible, and indicate how many individuals were excluded because of missing data when describing the flow of participants through the study (see also item 13). For analyses that account for missing data, authors should describe the nature of the analysis (e.g., multiple imputation) and the assumptions that were made (e.g., missing at random, see Box 6).\n\n\n\n\n\n\n12d. Statistical methods\n\n\n\n\n\n\n\nIf applicable, explain how loss to follow-up was addressed\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nCohort studies are analysed using life table methods or other approaches that are based on the person-time of follow-up and time to developing the disease of interest. Among individuals who remain free of the disease at the end of their observation period, the amount of follow-up time is assumed to be unrelated to the probability of developing the outcome. This will be the case if follow-up ends on a fixed date or at a particular age. Loss to follow-up occurs when participants withdraw from a study before that date. This may hamper the validity of a study if loss to follow-up occurs selectively in exposed individuals, or in persons at high risk of developing the disease (informative censoring). In the example above, patients lost to follow-up in treatment programmes with no active follow-up had fewer CD4 helper cells than those remaining under observation and were therefore at higher risk of dying [116].\nIt is important to distinguish persons who reach the end of the study from those lost to follow-up. Unfortunately, statistical software usually does not distinguish between the two situations: in both cases follow-up time is automatically truncated (censored) at the end of the observation period. Investigators therefore need to decide, ideally at the stage of planning the study, how they will deal with loss to follow-up. When few patients are lost, investigators may either exclude individuals with incomplete follow-up, or treat them as if they withdrew alive at either the date of loss to follow-up or the end of the study. We advise authors to report how many patients were lost to follow-up and what censoring strategies they used\n\n\n\n\n\n\n12e. Statistical methods\n\n\n\n\n\n\n\nDescribe any sensitivity analyses\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nSensitivity analyses are useful to investigate whether or not the main results are consistent with those obtained with alternative analysis strategies or assumptions [121]. Issues that may be examined include the criteria for inclusion in analyses, the definitions of exposures or outcomes [122], which confounding variables merit adjustment, the handling of missing data [120,123], possible selection bias or bias from inaccurate or inconsistent measurement of exposure, disease and other variables, and specific analysis choices, such as the treatment of quantitative variables (see item 11). Sophisticated methods are increasingly used to simultaneously model the influence of several biases or assumptions [124–126].\nIn 1959 Cornfield et al. famously showed that a relative risk of 9 for cigarette smoking and lung cancer was extremely unlikely to be due to any conceivable confounder, since the confounder would need to be at least nine times as prevalent in smokers as in non-smokers [127]. This analysis did not rule out the possibility that such a factor was present, but it did identify the prevalence such a factor would need to have. The same approach was recently used to identify plausible confounding factors that could explain the association between childhood leukaemia and living near electric power lines [128]. More generally, sensitivity analyses can be used to identify the degree of confounding, selection bias, or information bias required to distort an association. One important, perhaps under recognised, use of sensitivity analysis is when a study shows little or no association between an exposure and an outcome and it is plausible that confounding or other biases toward the null are present\n\n\n\n\n\n\n12f. Statistical methods\n\n\n\n\n\n\n\nState whether Hardy-Weinberg equilibrium was considered and, if so, how.\n\n\n12g. Statistical methods\n\n\n\n\n\n\n\nDescribe any methods used for inferring genotypes or haplotypes\n\n\n12h. Statistical methods\n\n\n\n\n\n\n\nDescribe any methods used to assess or address population stratification.\n\n\n12i. Statistical methods\n\n\n\n\n\n\n\nDescribe any methods used to address multiple comparisons or to control risk of false positive findings.\n\n\n12j. Statistical methods\n\n\n\n\n\n\n\nDescribe any methods used to address and correct for relatedness among subjects\n\n\nParticipants\n\n\n13a. Participants\n\n\n\n\n\n\n\nReport numbers of individuals at each stage of study—eg numbers potentially eligible, examined for eligibility, confirmed eligible, included in the study, completing follow-up, and analysed. Give information separately for for exposed and unexposed groups if applicable. Report numbers of individuals in whom genotyping was attempted and numbers of individuals in whom genotyping was successful.\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nDetailed information on the process of recruiting study participants is important for several reasons. Those included in a study often differ in relevant ways from the target population to which results are applied. This may result in estimates of prevalence or incidence that do not reflect the experience of the target population. For example, people who agreed to participate in a postal survey of sexual behaviour attended church less often, had less conservative sexual attitudes and earlier age at first sexual intercourse, and were more likely to smoke cigarettes and drink alcohol than people who refused [130]. These differences suggest that postal surveys may overestimate sexual liberalism and activity in the population. Such response bias (see Box 3) can distort exposure-disease associations if associations differ between those eligible for the study and those included in the study. As another example, the association between young maternal age and leukaemia in offspring, which has been observed in some case-control studies [131,132], was explained by differential participation of young women in case and control groups. Young women with healthy children were less likely to participate than those with unhealthy children [133]. Although low participation does not necessarily compromise the validity of a study, transparent information on participation and reasons for non-participation is essential. Also, as there are no universally agreed definitions for participation, response or follow-up rates, readers need to understand how authors calculated such proportions [134].\nIdeally, investigators should give an account of the numbers of individuals considered at each stage of recruiting study participants, from the choice of a target population to the inclusion of participants’ data in the analysis. Depending on the type of study, this may include the number of individuals considered to be potentially eligible, the number assessed for eligibility, the number found to be eligible, the number included in the study, the number examined, the number followed up and the number included in the analysis. Information on different sampling units may be required, if sampling of study participants is carried out in two or more stages as in the example above (multistage sampling). In case-control studies, we advise that authors describe the flow of participants separately for case and control groups [135]. Controls can sometimes be selected from several sources, including, for example, hospitalised patients and community dwellers. In this case, we recommend a separate account of the numbers of participants for each type of control group. Olson and colleagues proposed useful reporting guidelines for controls recruited through random-digit dialling and other methods [136].\nA recent survey of epidemiological studies published in 10 general epidemiology, public health and medical journals found that some information regarding participation was provided in 47 of 107 case-control studies (59%), 49 of 154 cohort studies (32%), and 51 of 86 cross-sectional studies (59%) [137]. Incomplete or absent reporting of participation and non-participation in epidemiological studies was also documented in two other surveys of the literature [4,5]. Finally, there is evidence that participation in epidemiological studies may have declined in recent decades [137,138], which underscores the need for transparent reporting\n\n\n\n\n\n\n13b. Participants\n\n\n\n\n\n\n\nGive reasons for non-participation at each stage\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nExplaining the reasons why people no longer participated in a study or why they were excluded from statistical analyses helps readers judge whether the study population was representative of the target population and whether bias was possibly introduced. For example, in a cross-sectional health survey, non-participation due to reasons unlikely to be related to health status (for example, the letter of invitation was not delivered because of an incorrect address) will affect the precision of estimates but will probably not introduce bias. Conversely, if many individuals opt out of the survey because of illness, or perceived good health, results may underestimate or overestimate the prevalence of ill health in the population\n\n\n\n\n\n\n13c. Participants\n\n\n\n\n\n\n\nConsider use of a flow diagram\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nAn informative and well-structured flow diagram can readily and transparently convey information that might otherwise require a lengthy description [142], as in the example above. The diagram may usefully include the main results, such as the number of events for the primary outcome. While we recommend the use of a flow diagram, particularly for complex observational studies, we do not propose a specific format for the diagram\n\n\n\n\n\n\nDescriptive data\n\n\n14a. Descriptive data\n\n\n\n\n\n\n\nGive characteristics of study participants (eg demographic, clinical, social) and information on exposures and potential confounders. Give information separately for exposed and unexposed groups if applicable. Consider giving information by genotype\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nReaders need descriptions of study participants and their exposures to judge the generalisability of the findings. Information about potential confounders, including whether and how they were measured, influences judgments about study validity. We advise authors to summarize continuous variables for each study group by giving the mean and standard deviation, or when the data have an asymmetrical distribution, as is often the case, the median and percentile range (e.g., 25th and 75th percentiles). Variables that make up a small number of ordered categories (such as stages of disease I to IV) should not be presented as continuous variables; it is preferable to give numbers and proportions for each category (see also Box 4). In studies that compare groups, the descriptive characteristics and numbers should be given by group, as in the example above.\nInferential measures such as standard errors and confidence intervals should not be used to describe the variability of characteristics, and significance tests should be avoided in descriptive tables. Also, P values are not an appropriate criterion for selecting which confounders to adjust for in analysis; even small differences in a confounder that has a strong effect on the outcome can be important.\nIn cohort studies, it may be useful to document how an exposure relates to other characteristics and potential confounders. Authors could present this information in a table with columns for participants in two or more exposure categories, which permits to judge the differences in confounders between these categories.\nIn case-control studies potential confounders cannot be judged by comparing cases and controls. Control persons represent the source population and will usually be different from the cases in many respects. For example, in a study of oral contraceptives and myocardial infarction, a sample of young women with infarction more often had risk factors for that disease, such as high serum cholesterol, smoking and a positive family history, than the control group [146]. This does not influence the assessment of the effect of oral contraceptives, as long as the prescription of oral contraceptives was not guided by the presence of these risk factors—e.g., because the risk factors were only established after the event (see also Box 5). In case-control studies the equivalent of comparing exposed and non-exposed for the presence of potential confounders (as is done in cohorts) can be achieved by exploring the source population of the cases: if the control group is large enough and represents the source population, exposed and unexposed controls can be compared for potential confounders [121,147].\n\n\n\n\n\n\n14b. Descriptive data\n\n\n\n\n\n\n\nIndicate number of participants with missing data for each variable of interest\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nAs missing data may bias or affect generalisability of results, authors should tell readers amounts of missing data for exposures, potential confounders, and other important characteristics of patients (see also item 12c and Box 6). In a cohort study, authors should report the extent of loss to follow-up (with reasons), since incomplete follow-up may bias findings (see also items 12d and 13). We advise authors to use their tables and figures to enumerate amounts of missing data.\n\n\n\n\n\n\n14c. Descriptive data\n\n\n\n\n\n\n\nCohort study – Summarize follow-up time, e.g. average and total amount.\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nReaders need to know the duration and extent of follow-up for the available outcome data. Authors can present a summary of the average follow-up with either the mean or median follow-up time or both. The mean allows a reader to calculate the total number of person-years by multiplying it with the number of study participants. Authors also may present minimum and maximum times or percentiles of the distribution to show readers the spread of follow-up times. They may report total person-years of follow-up or some indication of the proportion of potential data that was captured [148]. All such information may be presented separately for participants in two or more exposure categories. Almost half of 132 articles in cancer journals (mostly cohort studies) did not give any summary of length of follow-up\n\n\n\n\n\n\nOutcome data\n\n\n15. Outcome data\n\n\n\n\n\n\n\nCohort study Report numbers of outcome events or summary measures over time.Give information separately for exposed and unexposed groups if applicable. Report outcomes (phenotypes) for each genotype category over time\nCase-control study – Report numbers in each exposure category, or summary measures of exposure.Give information separately for cases and controls . Report numbers in each genotype category. Cross-sectional study – Report numbers of outcome events or summary measures. Give information separately for exposed and unexposed groups if applicable. Report outcomes (phenotypes) for each genotype category\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nBefore addressing the possible association between exposures (risk factors) and outcomes, authors should report relevant descriptive data. It may be possible and meaningful to present measures of association in the same table that presents the descriptive data (see item 14a). In a cohort study with events as outcomes, report the numbers of events for each outcome of interest. Consider reporting the event rate per person-year of follow-up. If the risk of an event changes over follow-up time, present the numbers and rates of events in appropriate intervals of follow-up or as a Kaplan-Meier life table or plot. It might be preferable to show plots as cumulative incidence that go up from 0% rather than down from 100%, especially if the event rate is lower than, say, 30% [153]. Consider presenting such information separately for participants in different exposure categories of interest. If a cohort study is investigating other time-related outcomes (e.g., quantitative disease markers such as blood pressure), present appropriate summary measures (e.g., means and standard deviations) over time, perhaps in a table or figure.\nFor cross-sectional studies, we recommend presenting the same type of information on prevalent outcome events or summary measures. For case-control studies, the focus will be on reporting exposures separately for cases and controls as frequencies or quantitative summaries [154]. For all designs, it may be helpful also to tabulate continuous outcomes or exposures in categories, even if the data are not analysed as such\n\n\n\n\n\n\nMain results\n\n\n16a. Main results\n\n\n\n\n\n\n\nGive unadjusted estimates and, if applicable, confounder-adjusted estimates and their precision (eg, 95% confidence interval). Make clear which confounders were adjusted for and why they were included\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nIn many situations, authors may present the results of unadjusted or minimally adjusted analyses and those from fully adjusted analyses. We advise giving the unadjusted analyses together with the main data, for example the number of cases and controls that were exposed or not. This allows the reader to understand the data behind the measures of association (see also item 15). For adjusted analyses, report the number of persons in the analysis, as this number may differ because of missing values in covariates (see also item 12c). Estimates should be given with confidence intervals.\nReaders can compare unadjusted measures of association with those adjusted for potential confounders and judge by how much, and in what direction, they changed. Readers may think that adjusted results equal the causal part of the measure of association, but adjusted results are not necessarily free of random sampling error, selection bias, information bias, or residual confounding (see Box 5). Thus, great care should be exercised when interpreting adjusted results, as the validity of results often depends crucially on complete knowledge of important confounders, their precise measurement, and appropriate specification in the statistical model (see also item 20) [157,158].\nAuthors should explain all potential confounders considered, and the criteria for excluding or including variables in statistical models. Decisions about excluding or including variables should be guided by knowledge, or explicit assumptions, on causal relations. Inappropriate decisions may introduce bias, for example by including variables that are in the causal pathway between exposure and disease (unless the aim is to asses how much of the effect is carried by the intermediary variable). If the decision to include a variable in the model was based on the change in the estimate, it is important to report what change was considered sufficiently important to justify its inclusion. If a backward deletion or forward inclusion strategy was used to select confounders, explain that process and give the significance level for rejecting the null hypothesis of no confounding. Of note, we and others do not advise selecting confounders based solely on statistical significance testing [147,159,160].\nRecent studies of the quality of reporting of epidemiological studies found that confidence intervals were reported in most articles [4]. However, few authors explained their choice of confounding variables [4,5].\n\n\n\n\n\n\n16b. Main results\n\n\n\n\n\n\n\nReport category boundaries when continuous variables were categorized\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nCategorizing continuous data has several important implications for analysis (see Box 4) and also affects the presentation of results. In tables, outcomes should be given for each exposure category, for example as counts of persons at risk, person-time at risk, if relevant separately for each group (e.g., cases and controls). Details of the categories used may aid comparison of studies and meta-analysis. If data were grouped using conventional cut-points, such as body mass index thresholds [162], group boundaries (i.e., range of values) can be derived easily, except for the highest and lowest categories. If quantile-derived categories are used, the category boundaries cannot be inferred from the data. As a minimum, authors should report the category boundaries; it is helpful also to report the range of the data and the mean or median values within categories.\n\n\n\n\n\n\n16c. Main results\n\n\n\n\n\n\n\nIf relevant, consider translating estimates of relative risk into absolute risk for a meaningful time period\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nThe results from studies examining the association between an exposure and a disease are commonly reported in relative terms, as ratios of risks, rates or odds (see Box 8). Relative measures capture the strength of the association between an exposure and disease. If the relative risk is a long way from 1 it is less likely that the association is due to confounding [164,165]. Relative effects or associations tend to be more consistent across studies and populations than absolute measures, but what often tends to be the case may be irrelevant in a particular instance. For example, similar relative risks were obtained for the classic cardiovascular risk factors for men living in Northern Ireland, France, the USA and Germany, despite the fact that the underlying risk of coronary heart disease varies substantially between these countries [166,167]. In contrast, in a study of hypertension as a risk factor for cardiovascular disease mortality, the data were more compatible with a constant rate difference than with a constant rate ratio [168].\nWidely used statistical models, including logistic [169] and proportional hazards (Cox) regression [170] are based on ratio measures. In these models, only departures from constancy of ratio effect measures are easily discerned. Nevertheless, measures which assess departures from additivity of risk differences, such as the Relative Excess Risk from Interaction (RERI, see item 12b and Box 8), can be estimated in models based on ratio measures.\nIn many circumstances, the absolute risk associated with an exposure is of greater interest than the relative risk. For example, if the focus is on adverse effects of a drug, one will want to know the number of additional cases per unit time of use (e.g., days, weeks, or years). The example gives the additional number of breast cancer cases per 1000 women who used hormone-replacement therapy for 10 years [163]. Measures such as the attributable risk or population attributable fraction may be useful to gauge how much disease can be prevented if the exposure is eliminated. They should preferably be presented together with a measure of statistical uncertainty (e.g., confidence intervals as in the example). Authors should be aware of the strong assumptions made in this context, including a causal relationship between a risk factor and disease (also see Box 7) [171]. Because of the semantic ambiguity and complexities involved, authors should report in detail what methods were used to calculate attributable risks, ideally giving the formulae used [172].\nA recent survey of abstracts of 222 articles published in leading medical journals found that in 62% of abstracts of randomised trials including a ratio measure absolute risks were given, but only in 21% of abstracts of cohort studies [173]. A free text search of Medline 1966 to 1997 showed that 619 items mentioned attributable risks in the title or abstract, compared to 18,955 using relative risk or odds ratio, for a ratio of 1 to 31 [174].\n\n\n\n\n\n\n16d. Main results\n\n\n\n\n\n\n\nReport results of any adjustments for multiple comparisons\n\n\nOther analyses\n\n\n17a. Other analyses\n\n\n\n\n\n\n\nReport other analyses done—e.g., analyses of subgroups and interactions, and sensitivity analyses\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nIn addition to the main analysis other analyses are often done in observational studies. They may address specific subgroups, the potential interaction between risk factors, the calculation of attributable risks, or use alternative definitions of study variables in sensitivity analyses.\nThere is debate about the dangers associated with subgroup analyses, and multiplicity of analyses in general [4,104]. In our opinion, there is too great a tendency to look for evidence of subgroup-specific associations, or effect-measure modification, when overall results appear to suggest little or no effect. On the other hand, there is value in exploring whether an overall association appears consistent across several, preferably pre-specified subgroups especially when a study is large enough to have sufficient data in each subgroup. A second area of debate is about interesting subgroups that arose during the data analysis. They might be important findings, but might also arise by chance. Some argue that it is neither possible nor necessary to inform the reader about all subgroup analyses done as future analyses of other data will tell to what extent the early exciting findings stand the test of time [9]. We advise authors to report which analyses were planned, and which were not (see also items 4, 12b and 20). This will allow readers to judge the implications of multiplicity, taking into account the study’s position on the continuum from discovery to verification or refutation.\nA third area of debate is how joint effects and interactions between risk factors should be evaluated: on additive or multiplicative scales, or should the scale be determined by the statistical model that fits best (see also item 12b and Box 8)? A sensible approach is to report the separate effect of each exposure as well as the joint effect—if possible in a table, as in the first example above [183], or in the study by Martinelli et al. [185]. Such a table gives the reader sufficient information to evaluate additive as well as multiplicative interaction (how these calculations are done is shown in Box 8). Confidence intervals for separate and joint effects may help the reader to judge the strength of the data. In addition, confidence intervals around measures of interaction, such as the Relative Excess Risk from Interaction (RERI) relate to tests of interaction or homogeneity tests. One recurrent problem is that authors use comparisons of P-values across subgroups, which lead to erroneous claims about an effect modifier. For instance, a statistically significant association in one category (e.g., men), but not in the other (e.g., women) does not in itself provide evidence of effect modification. Similarly, the confidence intervals for each point estimate are sometimes inappropriately used to infer that there is no interaction when intervals overlap. A more valid inference is achieved by directly evaluating whether the magnitude of an association differs across subgroups.\nSensitivity analyses are helpful to investigate the influence of choices made in the statistical analysis, or to investigate the robustness of the findings to missing data or possible biases (see also item 12b). Judgement is needed regarding the level of reporting of such analyses. If many sensitivity analyses were performed, it may be impractical to present detailed findings for them all. It may sometimes be sufficient to report that sensitivity analyses were carried out and that they were consistent with the main results presented. Detailed presentation is more appropriate if the issue investigated is of major concern, or if effect estimates vary considerably [59,186].\nPocock and colleagues found that 43 out of 73 articles reporting observational studies contained subgroup analyses. The majority claimed differences across groups but only eight articles reported a formal evaluation of interaction (see item 12b) [4].\n\n\n\n\n\n\n17b. Other analyses\n\n\n\n\n\n\n\nReport other analyses done—e.g., analyses of subgroups and interactions, and sensitivity analyses\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nIf numerous genetic exposures (genetic variants) were examined, summarize results from all analyses undertaken.\n\n\n\n\n\n\n17c. Other analyses\n\n\n\n\n\n\n\nReport other analyses done—e.g., analyses of subgroups and interactions, and sensitivity analyses\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nIf detailed results are available elsewhere, state how they can be accessed.\n\n\n\n\n\n\nKey results\n\n\n18. Key results\n\n\n\n\n\n\n\nSummarise key results with reference to study objectives\n\n\nLimitations\n\n\n19. Limitations\n\n\n\n\n\n\n\nDiscuss limitations of the study, taking into account sources of potential bias or imprecision. Discuss both direction and magnitude of any potential bias.\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nThe identification and discussion of the limitations of a study are an essential part of scientific reporting. It is important not only to identify the sources of bias and confounding that could have affected results, but also to discuss the relative importance of different biases, including the likely direction and magnitude of any potential bias (see also item 9 and Box 3).\nAuthors should also discuss any imprecision of the results. Imprecision may arise in connection with several aspects of a study, including the study size (item 10) and the measurement of exposures, confounders and outcomes (item 8). The inability to precisely measure true values of an exposure tends to result in bias towards unity: the less precisely a risk factor is measured, the greater the bias. This effect has been described as attenuation [201,202], or more recently as regression dilution bias [203]. However, when correlated risk factors are measured with different degrees of imprecision, the adjusted relative risk associated with them can be biased towards or away from unity [204–206].\nWhen discussing limitations, authors may compare the study being presented with other studies in the literature in terms of validity, generalisability and precision. In this approach, each study can be viewed as contribution to the literature, not as a stand-alone basis for inference and action [207]. Surprisingly, the discussion of important limitations of a study is sometimes omitted from published reports. A survey of authors who had published original research articles in The Lancet found that important weaknesses of the study were reported by the investigators in the survey questionnaires, but not in the published article\n\n\n\n\n\n\nInterpretation\n\n\n20. Interpretation\n\n\n\n\n\n\n\nGive a cautious overall interpretation considering objectives, limitations, multiplicity of analyses, results from similar studies, and other relevant evidence.\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nThe heart of the discussion section is the interpretation of a study’s results. Over-interpretation is common and human: even when we try hard to give an objective assessment, reviewers often rightly point out that we went too far in some respects. When interpreting results, authors should consider the nature of the study on the discovery to verification continuum and potential sources of bias, including loss to follow-up and non-participation (see also items 9, 12 and 19). Due consideration should be given to confounding (item 16a), the results of relevant sensitivity analyses, and to the issue of multiplicity and subgroup analyses (item 17). Authors should also consider residual confounding due to unmeasured variables or imprecise measurement of confounders. For example, socioeconomic status (SES) is associated with many health outcomes and often differs between groups being compared. Variables used to measure SES (income, education, or occupation) are surrogates for other undefined and unmeasured exposures, and the true confounder will by definition be measured with error [208]. Authors should address the real range of uncertainty in estimates, which is larger than the statistical uncertainty reflected in confidence intervals. The latter do not take into account other uncertainties that arise from a study’s design, implementation, and methods of measurement [209].\nTo guide thinking and conclusions about causality, some may find criteria proposed by Bradford Hill in 1965 helpful [164]. How strong is the association with the exposure? Did it precede the onset of disease? Is the association consistently observed in different studies and settings? Is there supporting evidence from experimental studies, including laboratory and animal studies? How specific is the exposure’s putative effect, and is there a dose-response relationship? Is the association biologically plausible? These criteria should not, however, be applied mechanically. For example, some have argued that relative risks below 2 or 3 should be ignored [210,211]. This is a reversal of the point by Cornfield et al. about the strength of large relative risks (see item 12b) [127]. Although a causal effect is more likely with a relative risk of 9, it does not follow that one below 3 is necessarily spurious. For instance, the small increase in the risk of childhood leukaemia after intrauterine irradiation is credible because it concerns an adverse effect of a medical procedure for which no alternative explanations are obvious [212]. Moreover, the carcinogenic effects of radiation are well established. The doubling in the risk of ovarian cancer associated with eating 2 to 4 eggs per week is not immediately credible, since dietary habits are associated with a large number of lifestyle factors as well as SES [213]. In contrast, the credibility of much debated epidemiologic findings of a difference in thrombosis risk between different types of oral contraceptives was greatly enhanced by the differences in coagulation found in a randomised cross-over trial [214]. A discussion of the existing external evidence, from different types of studies, should always be included, but may be particularly important for studies reporting small increases in risk. Further, authors should put their results in context with similar studies and explain how the new study affects the existing body of evidence, ideally by referring to a systematic review.\n\n\n\n\n\n\nGeneralisability\n\n\n21. Generalisability\n\n\n\n\n\n\n\nDiscuss the generalisability (external validity) of the study results\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nGeneralisability, also called external validity or applicability, is the extent to which the results of a study can be applied to other circumstances [216]. There is no external validity per se; the term is meaningful only with regard to clearly specified conditions [217]. Can results be applied to an individual, groups or populations that differ from those enrolled in the study with regard to age, sex, ethnicity, severity of disease, and co-morbid conditions? Are the nature and level of exposures comparable, and the definitions of outcomes relevant to another setting or population? Are data that were collected in longitudinal studies many years ago still relevant today? Are results from health services research in one country applicable to health systems in other countries?\nThe question of whether the results of a study have external validity is often a matter of judgment that depends on the study setting, the characteristics of the participants, the exposures examined, and the outcomes assessed. Thus, it is crucial that authors provide readers with adequate information about the setting and locations, eligibility criteria, the exposures and how they were measured, the definition of outcomes, and the period of recruitment and follow-up. The degree of non-participation and the proportion of unexposed participants in whom the outcome develops are also relevant. Knowledge of the absolute risk and prevalence of the exposure, which will often vary across populations, are helpful when applying results to other settings and populations (see Box 7)\n\n\n\n\n\n\nFunding\n\n\n22. Funding\n\n\n\n\n\n\n\nGive the source of funding and the role of the funders for the present study and, if applicable, for the original study on which the present article is based\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nSome journals require authors to disclose the presence or absence of financial and other conflicts of interest [100,218]. Several investigations show strong associations between the source of funding and the conclusions of research articles [219–222]. The conclusions in randomised trials recommended the experimental drug as the drug of choice much more often (odds ratio 5.3) if the trial was funded by for-profit organisations, even after adjustment for the effect size [223]. Other studies document the influence of the tobacco and telecommunication industries on the research they funded [224–227]. There are also examples of undue influence when the sponsor is governmental or a non-profit organisation.\nAuthors or funders may have conflicts of interest that influence any of the following: the design of the study [228]; choice of exposures [228,229], outcomes [230], statistical methods [231], and selective publication of outcomes [230] and studies [232]. Consequently, the role of the funders should be described in detail: in what part of the study they took direct responsibility (e.g., design, data collection, analysis, drafting of manuscript, decision to publish) [100]. Other sources of undue influence include employers (e.g., university administrators for academic researchers and government supervisors, especially political appointees, for government researchers), advisory committees, litigants, and special interest groups"
  },
  {
    "objectID": "guidelines/strega/index.html#sec-1a_title-1",
    "href": "guidelines/strega/index.html#sec-1a_title-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nReaders should be able to easily identify the design that was used from the title or abstract. An explicit, commonly used term for the study design also helps ensure correct indexing of articles in electronic databases"
  },
  {
    "objectID": "guidelines/strega/index.html#sec-1b_abstract-1",
    "href": "guidelines/strega/index.html#sec-1b_abstract-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nThe abstract provides key information that enables readers to understand a study and decide whether to read the article. Typical components include a statement of the research question, a short description of methods and results, and a conclusion. Abstracts should summarize key details of studies and should only present information that is provided in the article. We advise presenting key results in a numerical form that includes numbers of participants, estimates of associations and appropriate measures of variability and uncertainty (e.g., odds ratios with confidence intervals). We regard it insufficient to state only that an exposure is or is not significantly associated with an outcome.\nA series of headings pertaining to the background, design, conduct, and analysis of a study may help readers acquire the essential information rapidly. Many journals require such structured abstracts, which tend to be of higher quality and more readily informative than unstructured summaries."
  },
  {
    "objectID": "guidelines/strega/index.html#sec-2-1",
    "href": "guidelines/strega/index.html#sec-2-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nThe scientific background of the study provides important context for readers. It sets the stage for the study and describes its focus. It gives an overview of what is known on a topic and what gaps in current knowledge are addressed by the study. Background material should note recent pertinent studies and any systematic reviews of pertinent studies."
  },
  {
    "objectID": "guidelines/strega/index.html#sec-3-1",
    "href": "guidelines/strega/index.html#sec-3-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nObjectives are the detailed aims of the study. Well crafted objectives specify populations, exposures and outcomes, and parameters that will be estimated. They may be formulated as specific hypotheses or as questions that the study was designed to address. In some situations objectives may be less specific, for example, in early discovery phases. Regardless, the report should clearly reflect the investigators’ intentions. For example, if important subgroups or additional analyses were not the original aim of the study but arose during data analysis, they should be described accordingly (see also items 4, 17 and 20)."
  },
  {
    "objectID": "guidelines/strega/index.html#sec-4-1",
    "href": "guidelines/strega/index.html#sec-4-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nWe advise presenting key elements of study design early in the methods section (or at the end of the introduction) so that readers can understand the basics of the study. For example, authors should indicate that the study was a cohort study, which followed people over a particular time period, and describe the group of persons that comprised the cohort and their exposure status. Similarly, if the investigation used a case-control design, the cases and controls and their source population should be described. If the study was a cross-sectional survey, the population and the point in time at which the cross-section was taken should be mentioned. When a study is a variant of the three main study types, there is an additional need for clarity. For instance, for a case-crossover study, one of the variants of the case-control design, a succinct description of the principles was given in the example above [28].\nWe recommend that authors refrain from simply calling a study prospective or retrospective because these terms are ill defined [29]. One usage sees cohort and prospective as synonymous and reserves the word retrospective for case-control studies [30]. A second usage distinguishes prospective and retrospective cohort studies according to the timing of data collection relative to when the idea for the study was developed [31]. A third usage distinguishes prospective and retrospective case-control studies depending on whether the data about the exposure of interest existed when cases were selected [32]. Some advise against using these terms [33], or adopting the alternatives concurrent and historical for describing cohort studies [34]. In STROBE, we do not use the words prospective and retrospective, nor alternatives such as concurrent and historical. We recommend that, whenever authors use these words, they define what they mean. Most importantly, we recommend that authors describe exactly how and when data collection took place.\nThe first part of the methods section might also be the place to mention whether the report is one of several from a study. If a new report is in line with the original aims of the study, this is usually indicated by referring to an earlier publication and by briefly restating the salient features of the study. However, the aims of a study may also evolve over time. Researchers often use data for purposes for which they were not originally intended, including, for example, official vital statistics that were collected primarily for administrative purposes, items in questionnaires that originally were only included for completeness, or blood samples that were collected for another purpose. For example, the Physicians’ Health Study, a randomized controlled trial of aspirin and carotene, was later used to demonstrate that a point mutation in the factor V gene was associated with an increased risk of venous thrombosis, but not of myocardial infarction or stroke [35]. The secondary use of existing data is a creative part of observational research and does not necessarily make results less credible or less important. However, briefly restating the original aims might help readers understand the context of the research and possible limitations in the data."
  },
  {
    "objectID": "guidelines/strega/index.html#sec-5-1",
    "href": "guidelines/strega/index.html#sec-5-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nReaders need information on setting and locations to assess the context and generalisability of a study’s results. Exposures such as environmental factors and therapies can change over time. Also, study methods may evolve over time. Knowing when a study took place and over what period participants were recruited and followed up places the study in historical context and is important for the interpretation of results.\nInformation about setting includes recruitment sites or sources (e.g., electoral roll, outpatient clinic, cancer registry, or tertiary care centre). Information about location may refer to the countries, towns, hospitals or practices where the investigation took place. We advise stating dates rather than only describing the length of time periods. There may be different sets of dates for exposure, disease occurrence, recruitment, beginning and end of follow-up, and data collection. Of note, nearly 80% of 132 reports in oncology journals that used survival analysis included the starting and ending dates for accrual of patients, but only 24% also reported the date on which follow-up ended [37]."
  },
  {
    "objectID": "guidelines/strega/index.html#sec-6a_-1",
    "href": "guidelines/strega/index.html#sec-6a_-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nDetailed descriptions of the study participants help readers understand the applicability of the results. Investigators usually restrict a study population by defining clinical, demographic and other characteristics of eligible participants. Typical eligibility criteria relate to age, gender, diagnosis and comorbid conditions. Despite their importance, eligibility criteria often are not reported adequately. In a survey of observational stroke research, 17 of 49 reports (35%) did not specify eligibility criteria [5].\nEligibility criteria may be presented as inclusion and exclusion criteria, although this distinction is not always necessary or useful. Regardless, we advise authors to report all eligibility criteria and also to describe the group from which the study population was selected (e.g., the general population of a region or country), and the method of recruitment (e.g., referral or self-selection through advertisements).\nKnowing details about follow-up procedures, including whether procedures minimized non-response and loss to follow-up and whether the procedures were similar for all participants, informs judgments about the validity of results. For example, in a study that used IgM antibodies to detect acute infections, readers needed to know the interval between blood tests for IgM antibodies so that they could judge whether some infections likely were missed because the interval between blood tests was too long [41]. In other studies where follow-up procedures differed between exposed and unexposed groups, readers might recognize substantial bias due to unequal ascertainment of events or differences in non-response or loss to follow-up [42]. Accordingly, we advise that researchers describe the methods used for following participants and whether those methods were the same for all participants, and that they describe the completeness of ascertainment of variables (see also item 14).\nIn case-control studies, the choice of cases and controls is crucial to interpreting the results, and the method of their selection has major implications for study validity. In general, controls should reflect the population from which the cases arose. Various methods are used to sample controls, all with advantages and disadvantages: for cases that arise from a general population, population roster sampling, random digit dialling, neighbourhood or friend controls are used. Neighbourhood or friend controls may present intrinsic matching on exposure [17]. Controls with other diseases may have advantages over population-based controls, in particular for hospital-based cases, because they better reflect the catchment population of a hospital, have greater comparability of recall and ease of recruitment. However, they can present problems if the exposure of interest affects the risk of developing or being hospitalized for the control condition(s) [43,44]. To remedy this problem often a mixture of the best defensible control diseases is used [45]."
  },
  {
    "objectID": "guidelines/strega/index.html#sec-6b_-1",
    "href": "guidelines/strega/index.html#sec-6b_-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nMatching is much more common in case-control studies, but occasionally, investigators use matching in cohort studies to make groups comparable at the start of follow-up. Matching in cohort studies makes groups directly comparable for potential confounders and presents fewer intricacies than with case-control studies. For example, it is not necessary to take the matching into account for the estimation of the relative risk [48]. Because matching in cohort studies may increase statistical precision investigators might allow for the matching in their analyses and thus obtain narrower confidence intervals.\nIn case-control studies matching is done to increase a study’s efficiency by ensuring similarity in the distribution of variables between cases and controls, in particular the distribution of potential confounding variables [48,49]. Because matching can be done in various ways, with one or more controls per case, the rationale for the choice of matching variables and the details of the method used should be described. Commonly used forms of matching are frequency matching (also called group matching) and individual matching. In frequency matching, investigators choose controls so that the distribution of matching variables becomes identical or similar to that of cases. Individual matching involves matching one or several controls to each case. Although intuitively appealing and sometimes useful, matching in case-control studies has a number of disadvantages, is not always appropriate, and needs to be taken into account in the analysis"
  },
  {
    "objectID": "guidelines/strega/index.html#sec-7a_-1",
    "href": "guidelines/strega/index.html#sec-7a_-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nAuthors should define all variables considered for and included in the analysis, including outcomes, exposures, predictors, potential confounders and potential effect modifiers. Disease outcomes require adequately detailed description of the diagnostic criteria. This applies to criteria for cases in a case-control study, disease events during follow-up in a cohort study and prevalent disease in a cross-sectional study. Clear definitions and steps taken to adhere to them are particularly important for any disease condition of primary interest in the study.\nFor some studies, determinant or predictor may be appropriate terms for exposure variables and outcomes may be called endpoints. In multivariable models, authors sometimes use dependent variable for an outcome and independent variable or explanatory variable for exposure and confounding variables. The latter is not precise as it does not distinguish exposures from confounders.\nIf many variables have been measured and included in exploratory analyses in an early discovery phase, consider providing a list with details on each variable in an appendix, additional table or separate publication. Of note, the International Journal of Epidemiology recently launched a new section with cohort profiles, that includes detailed information on what was measured at different points in time in particular studies [56,57]. Finally, we advise that authors declare all candidate variables considered for statistical analysis, rather than selectively reporting only those included in the final models (see also item 16a) [58,59]"
  },
  {
    "objectID": "guidelines/strega/index.html#sec-8a_-1",
    "href": "guidelines/strega/index.html#sec-8a_-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nThe way in which exposures, confounders and outcomes were measured affects the reliability and validity of a study. Measurement error and misclassification of exposures or outcomes can make it more difficult to detect cause-effect relationships, or may produce spurious relationships. Error in measurement of potential confounders can increase the risk of residual confounding [62,63]. It is helpful, therefore, if authors report the findings of any studies of the validity or reliability of assessments or measurements, including details of the reference standard that was used. Rather than simply citing validation studies (as in the first example), we advise that authors give the estimated validity or reliability, which can then be used for measurement error adjustment or sensitivity analyses (see items 12e and 17).\nIn addition, it is important to know if groups being compared differed with respect to the way in which the data were collected. This may be important for laboratory examinations (as in the second example) and other situations. For instance, if an interviewer first questions all the cases and then the controls, or vice versa, bias is possible because of the learning curve; solutions such as randomising the order of interviewing may avoid this problem. Information bias may also arise if the compared groups are not given the same diagnostic tests or if one group receives more tests of the same kind than another (see also item 9)."
  },
  {
    "objectID": "guidelines/strega/index.html#sec-9a_-1",
    "href": "guidelines/strega/index.html#sec-9a_-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nBiased studies produce results that differ systematically from the truth (see also Box 3). It is important for a reader to know what measures were taken during the conduct of a study to reduce the potential of bias. Ideally, investigators carefully consider potential sources of bias when they plan their study. At the stage of reporting, we recommend that authors always assess the likelihood of relevant biases. Specifically, the direction and magnitude of bias should be discussed and, if possible, estimated. For instance, in case-control studies information bias can occur, but may be reduced by selecting an appropriate control group, as in the first example [64]. Differences in the medical surveillance of participants were a problem in the second example [65]. Consequently, the authors provide more detail about the additional data they collected to tackle this problem. When investigators have set up quality control programs for data collection to counter a possible drift in measurements of variables in longitudinal studies, or to keep variability at a minimum when multiple observers are used, these should be described.\nUnfortunately, authors often do not address important biases when reporting their results. Among 43 case-control and cohort studies published from 1990 to 1994 that investigated the risk of second cancers in patients with a history of cancer, medical surveillance bias was mentioned in only 5 articles [66]. A survey of reports of mental health research published during 1998 in three psychiatric journals found that only 13% of 392 articles mentioned response bias [67]. A survey of cohort studies in stroke research found that 14 of 49 (28%) articles published from 1999 to 2003 addressed potential selection bias in the recruitment of study participants and 35 (71%) mentioned the possibility that any type of bias may have affected results"
  },
  {
    "objectID": "guidelines/strega/index.html#sec-9b_-1",
    "href": "guidelines/strega/index.html#sec-9b_-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nFor quantitative outcome variables, specify if any investigation of potential bias resulting from pharmacotherapy was undertaken. If relevant, describe the nature and magnitude of the potential bias, and explain what approach was used to deal with this."
  },
  {
    "objectID": "guidelines/strega/index.html#sec-10-1",
    "href": "guidelines/strega/index.html#sec-10-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nA study should be large enough to obtain a point estimate with a sufficiently narrow confidence interval to meaningfully answer a research question. Large samples are needed to distinguish a small association from no association. Small studies often provide valuable information, but wide confidence intervals may indicate that they contribute less to current knowledge in comparison with studies providing estimates with narrower confidence intervals. Also, small studies that show interesting or statistically significant associations are published more frequently than small studies that do not have significant findings. While these studies may provide an early signal in the context of discovery, readers should be informed of their potential weaknesses.\nThe importance of sample size determination in observational studies depends on the context. If an analysis is performed on data that were already available for other purposes, the main question is whether the analysis of the data will produce results with sufficient statistical precision to contribute substantially to the literature, and sample size considerations will be informal. Formal, a priori calculation of sample size may be useful when planning a new study. Such calculations are associated with more uncertainty than implied by the single number that is generally produced. For example, estimates of the rate of the event of interest or other assumptions central to calculations are commonly imprecise, if not guesswork. The precision obtained in the final analysis can often not be determined beforehand because it will be reduced by inclusion of confounding variables in multivariable analyses, the degree of precision with which key variables can be measured, and the exclusion of some individuals.\nFew epidemiological studies explain or report deliberations about sample size. We encourage investigators to report pertinent formal sample size calculations if they were done. In other situations they should indicate the considerations that determined the study size (e.g., a fixed available sample, as in the first example above). If the observational study was stopped early when statistical significance was achieved, readers should be told. Do not bother readers with post hoc justifications for study size or retrospective power calculations. From the point of view of the reader, confidence intervals indicate the statistical precision that was ultimately obtained. It should be realized that confidence intervals reflect statistical uncertainty only, and not all uncertainty that may be present in a study (see item 20)."
  },
  {
    "objectID": "guidelines/strega/index.html#sec-11-1",
    "href": "guidelines/strega/index.html#sec-11-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nInvestigators make choices regarding how to collect and analyse quantitative data about exposures, effect modifiers and confounders. For example, they may group a continuous exposure variable to create a new categorical variable. Grouping choices may have important consequences for later analyses. We advise that authors explain why and how they grouped quantitative data, including the number of categories, the cut-points, and category mean or median values. Whenever data are reported in tabular form, the counts of cases, controls, persons at risk, person-time at risk, etc. should be given for each category. Tables should not consist solely of effect-measure estimates or results of model fitting.\nInvestigators might model an exposure as continuous in order to retain all the information. In making this choice, one needs to consider the nature of the relationship of the exposure to the outcome. As it may be wrong to assume a linear relation automatically, possible departures from linearity should be investigated. Authors could mention alternative models they explored during analyses (e.g., using log transformation, quadratic terms or spline functions). Several methods exist for fitting a non-linear relation between the exposure and outcome. Also, it may be informative to present both continuous and grouped analyses for a quantitative exposure of prime interest.\nIn a recent survey, two thirds of epidemiological publications studied quantitative exposure variables. In 42 of 50 articles (84%) exposures were grouped into several ordered categories, but often without any stated rationale for the choices made. Fifteen articles used linear associations to model continuous exposure but only two reported checking for linearity. In another survey, of the psychological literature, dichotomization was justified in only 22 of 110 articles (20%)"
  },
  {
    "objectID": "guidelines/strega/index.html#sec-12a_-1",
    "href": "guidelines/strega/index.html#sec-12a_-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nIn general, there is no one correct statistical analysis but, rather, several possibilities that may address the same question, but make different assumptions. Regardless, investigators should pre-determine analyses at least for the primary study objectives in a study protocol. Often additional analyses are needed, either instead of, or as well as, those originally envisaged, and these may sometimes be motivated by the data. When a study is reported, authors should tell readers whether particular analyses were suggested by data inspection. Even though the distinction between pre-specified and exploratory analyses may sometimes be blurred, authors should clarify reasons for particular analyses.\nIf groups being compared are not similar with regard to some characteristics, adjustment should be made for possible confounding variables by stratification or by multivariable regression (see Box 5) [94]. Often, the study design determines which type of regression analysis is chosen. For instance, Cox proportional hazard regression is commonly used in cohort studies [95]. whereas logistic regression is often the method of choice in case-control studies [96,97]. Analysts should fully describe specific procedures for variable selection and not only present results from the final model [98,99]. If model comparisons are made to narrow down a list of potential confounders for inclusion in a final model, this process should be described. It is helpful to tell readers if one or two covariates are responsible for a great deal of the apparent confounding in a data analysis. Other statistical analyses such as imputation procedures, data transformation, and calculations of attributable risks should also be described. Non-standard or novel approaches should be referenced and the statistical software used reported. As a guiding principle, we advise statistical methods be described with enough detail to enable a knowledgeable reader with access to the original data to verify the reported results [100].\nIn an empirical study, only 93 of 169 articles (55%) reporting adjustment for confounding clearly stated how continuous and multi-category variables were entered into the statistical model [101]. Another study found that among 67 articles in which statistical analyses were adjusted for confounders, it was mostly unclear how confounders were chosen"
  },
  {
    "objectID": "guidelines/strega/index.html#sec-12b_-1",
    "href": "guidelines/strega/index.html#sec-12b_-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nAs discussed in detail under item 17, many debate the use and value of analyses restricted to subgroups of the study population [4,104]. Subgroup analyses are nevertheless often done [4]. Readers need to know which subgroup analyses were planned in advance, and which arose while analysing the data. Also, it is important to explain what methods were used to examine whether effects or associations differed across groups (see item 17).\nInteraction relates to the situation when one factor modifies the effect of another (therefore also called effect modification). The joint action of two factors can be characterized in two ways: on an additive scale, in terms of risk differences; or on a multiplicative scale, in terms of relative risk (see Box 8). Many authors and readers may have their own preference about the way interactions should be analysed. Still, they may be interested to know to what extent the joint effect of exposures differs from the separate effects. There is consensus that the additive scale, which uses absolute risks, is more appropriate for public health and clinical decision making [105]. Whatever view is taken, this should be clearly presented to the reader, as is done in the example above [103]. A lay-out presenting separate effects of both exposures as well as their joint effect, each relative to no exposure, might be most informative. It is presented in the example for interaction under item 17, and the calculations on the different scales are explained in Box 8"
  },
  {
    "objectID": "guidelines/strega/index.html#sec-12c_-1",
    "href": "guidelines/strega/index.html#sec-12c_-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nMissing data are common in observational research. Questionnaires posted to study participants are not always filled in completely, participants may not attend all follow-up visits and routine data sources and clinical databases are often incomplete. Despite its ubiquity and importance, few papers report in detail on the problem of missing data [5,107]. Investigators may use any of several approaches to address missing data. We describe some strengths and limitations of various approaches in Box 6. We advise that authors report the number of missing values for each variable of interest (exposures, outcomes, confounders) and for each step in the analysis. Authors should give reasons for missing values if possible, and indicate how many individuals were excluded because of missing data when describing the flow of participants through the study (see also item 13). For analyses that account for missing data, authors should describe the nature of the analysis (e.g., multiple imputation) and the assumptions that were made (e.g., missing at random, see Box 6)."
  },
  {
    "objectID": "guidelines/strega/index.html#sec-12d_-1",
    "href": "guidelines/strega/index.html#sec-12d_-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nCohort studies are analysed using life table methods or other approaches that are based on the person-time of follow-up and time to developing the disease of interest. Among individuals who remain free of the disease at the end of their observation period, the amount of follow-up time is assumed to be unrelated to the probability of developing the outcome. This will be the case if follow-up ends on a fixed date or at a particular age. Loss to follow-up occurs when participants withdraw from a study before that date. This may hamper the validity of a study if loss to follow-up occurs selectively in exposed individuals, or in persons at high risk of developing the disease (informative censoring). In the example above, patients lost to follow-up in treatment programmes with no active follow-up had fewer CD4 helper cells than those remaining under observation and were therefore at higher risk of dying [116].\nIt is important to distinguish persons who reach the end of the study from those lost to follow-up. Unfortunately, statistical software usually does not distinguish between the two situations: in both cases follow-up time is automatically truncated (censored) at the end of the observation period. Investigators therefore need to decide, ideally at the stage of planning the study, how they will deal with loss to follow-up. When few patients are lost, investigators may either exclude individuals with incomplete follow-up, or treat them as if they withdrew alive at either the date of loss to follow-up or the end of the study. We advise authors to report how many patients were lost to follow-up and what censoring strategies they used"
  },
  {
    "objectID": "guidelines/strega/index.html#sec-12e_-1",
    "href": "guidelines/strega/index.html#sec-12e_-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nSensitivity analyses are useful to investigate whether or not the main results are consistent with those obtained with alternative analysis strategies or assumptions [121]. Issues that may be examined include the criteria for inclusion in analyses, the definitions of exposures or outcomes [122], which confounding variables merit adjustment, the handling of missing data [120,123], possible selection bias or bias from inaccurate or inconsistent measurement of exposure, disease and other variables, and specific analysis choices, such as the treatment of quantitative variables (see item 11). Sophisticated methods are increasingly used to simultaneously model the influence of several biases or assumptions [124–126].\nIn 1959 Cornfield et al. famously showed that a relative risk of 9 for cigarette smoking and lung cancer was extremely unlikely to be due to any conceivable confounder, since the confounder would need to be at least nine times as prevalent in smokers as in non-smokers [127]. This analysis did not rule out the possibility that such a factor was present, but it did identify the prevalence such a factor would need to have. The same approach was recently used to identify plausible confounding factors that could explain the association between childhood leukaemia and living near electric power lines [128]. More generally, sensitivity analyses can be used to identify the degree of confounding, selection bias, or information bias required to distort an association. One important, perhaps under recognised, use of sensitivity analysis is when a study shows little or no association between an exposure and an outcome and it is plausible that confounding or other biases toward the null are present"
  },
  {
    "objectID": "guidelines/strega/index.html#sec-13a_-1",
    "href": "guidelines/strega/index.html#sec-13a_-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nDetailed information on the process of recruiting study participants is important for several reasons. Those included in a study often differ in relevant ways from the target population to which results are applied. This may result in estimates of prevalence or incidence that do not reflect the experience of the target population. For example, people who agreed to participate in a postal survey of sexual behaviour attended church less often, had less conservative sexual attitudes and earlier age at first sexual intercourse, and were more likely to smoke cigarettes and drink alcohol than people who refused [130]. These differences suggest that postal surveys may overestimate sexual liberalism and activity in the population. Such response bias (see Box 3) can distort exposure-disease associations if associations differ between those eligible for the study and those included in the study. As another example, the association between young maternal age and leukaemia in offspring, which has been observed in some case-control studies [131,132], was explained by differential participation of young women in case and control groups. Young women with healthy children were less likely to participate than those with unhealthy children [133]. Although low participation does not necessarily compromise the validity of a study, transparent information on participation and reasons for non-participation is essential. Also, as there are no universally agreed definitions for participation, response or follow-up rates, readers need to understand how authors calculated such proportions [134].\nIdeally, investigators should give an account of the numbers of individuals considered at each stage of recruiting study participants, from the choice of a target population to the inclusion of participants’ data in the analysis. Depending on the type of study, this may include the number of individuals considered to be potentially eligible, the number assessed for eligibility, the number found to be eligible, the number included in the study, the number examined, the number followed up and the number included in the analysis. Information on different sampling units may be required, if sampling of study participants is carried out in two or more stages as in the example above (multistage sampling). In case-control studies, we advise that authors describe the flow of participants separately for case and control groups [135]. Controls can sometimes be selected from several sources, including, for example, hospitalised patients and community dwellers. In this case, we recommend a separate account of the numbers of participants for each type of control group. Olson and colleagues proposed useful reporting guidelines for controls recruited through random-digit dialling and other methods [136].\nA recent survey of epidemiological studies published in 10 general epidemiology, public health and medical journals found that some information regarding participation was provided in 47 of 107 case-control studies (59%), 49 of 154 cohort studies (32%), and 51 of 86 cross-sectional studies (59%) [137]. Incomplete or absent reporting of participation and non-participation in epidemiological studies was also documented in two other surveys of the literature [4,5]. Finally, there is evidence that participation in epidemiological studies may have declined in recent decades [137,138], which underscores the need for transparent reporting"
  },
  {
    "objectID": "guidelines/strega/index.html#sec-13b_-1",
    "href": "guidelines/strega/index.html#sec-13b_-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nExplaining the reasons why people no longer participated in a study or why they were excluded from statistical analyses helps readers judge whether the study population was representative of the target population and whether bias was possibly introduced. For example, in a cross-sectional health survey, non-participation due to reasons unlikely to be related to health status (for example, the letter of invitation was not delivered because of an incorrect address) will affect the precision of estimates but will probably not introduce bias. Conversely, if many individuals opt out of the survey because of illness, or perceived good health, results may underestimate or overestimate the prevalence of ill health in the population"
  },
  {
    "objectID": "guidelines/strega/index.html#sec-13c_-1",
    "href": "guidelines/strega/index.html#sec-13c_-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nAn informative and well-structured flow diagram can readily and transparently convey information that might otherwise require a lengthy description [142], as in the example above. The diagram may usefully include the main results, such as the number of events for the primary outcome. While we recommend the use of a flow diagram, particularly for complex observational studies, we do not propose a specific format for the diagram"
  },
  {
    "objectID": "guidelines/strega/index.html#sec-14a_-1",
    "href": "guidelines/strega/index.html#sec-14a_-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nReaders need descriptions of study participants and their exposures to judge the generalisability of the findings. Information about potential confounders, including whether and how they were measured, influences judgments about study validity. We advise authors to summarize continuous variables for each study group by giving the mean and standard deviation, or when the data have an asymmetrical distribution, as is often the case, the median and percentile range (e.g., 25th and 75th percentiles). Variables that make up a small number of ordered categories (such as stages of disease I to IV) should not be presented as continuous variables; it is preferable to give numbers and proportions for each category (see also Box 4). In studies that compare groups, the descriptive characteristics and numbers should be given by group, as in the example above.\nInferential measures such as standard errors and confidence intervals should not be used to describe the variability of characteristics, and significance tests should be avoided in descriptive tables. Also, P values are not an appropriate criterion for selecting which confounders to adjust for in analysis; even small differences in a confounder that has a strong effect on the outcome can be important.\nIn cohort studies, it may be useful to document how an exposure relates to other characteristics and potential confounders. Authors could present this information in a table with columns for participants in two or more exposure categories, which permits to judge the differences in confounders between these categories.\nIn case-control studies potential confounders cannot be judged by comparing cases and controls. Control persons represent the source population and will usually be different from the cases in many respects. For example, in a study of oral contraceptives and myocardial infarction, a sample of young women with infarction more often had risk factors for that disease, such as high serum cholesterol, smoking and a positive family history, than the control group [146]. This does not influence the assessment of the effect of oral contraceptives, as long as the prescription of oral contraceptives was not guided by the presence of these risk factors—e.g., because the risk factors were only established after the event (see also Box 5). In case-control studies the equivalent of comparing exposed and non-exposed for the presence of potential confounders (as is done in cohorts) can be achieved by exploring the source population of the cases: if the control group is large enough and represents the source population, exposed and unexposed controls can be compared for potential confounders [121,147]."
  },
  {
    "objectID": "guidelines/strega/index.html#sec-14b_-1",
    "href": "guidelines/strega/index.html#sec-14b_-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nAs missing data may bias or affect generalisability of results, authors should tell readers amounts of missing data for exposures, potential confounders, and other important characteristics of patients (see also item 12c and Box 6). In a cohort study, authors should report the extent of loss to follow-up (with reasons), since incomplete follow-up may bias findings (see also items 12d and 13). We advise authors to use their tables and figures to enumerate amounts of missing data."
  },
  {
    "objectID": "guidelines/strega/index.html#sec-14c_-1",
    "href": "guidelines/strega/index.html#sec-14c_-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nReaders need to know the duration and extent of follow-up for the available outcome data. Authors can present a summary of the average follow-up with either the mean or median follow-up time or both. The mean allows a reader to calculate the total number of person-years by multiplying it with the number of study participants. Authors also may present minimum and maximum times or percentiles of the distribution to show readers the spread of follow-up times. They may report total person-years of follow-up or some indication of the proportion of potential data that was captured [148]. All such information may be presented separately for participants in two or more exposure categories. Almost half of 132 articles in cancer journals (mostly cohort studies) did not give any summary of length of follow-up"
  },
  {
    "objectID": "guidelines/strega/index.html#sec-15-1",
    "href": "guidelines/strega/index.html#sec-15-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nBefore addressing the possible association between exposures (risk factors) and outcomes, authors should report relevant descriptive data. It may be possible and meaningful to present measures of association in the same table that presents the descriptive data (see item 14a). In a cohort study with events as outcomes, report the numbers of events for each outcome of interest. Consider reporting the event rate per person-year of follow-up. If the risk of an event changes over follow-up time, present the numbers and rates of events in appropriate intervals of follow-up or as a Kaplan-Meier life table or plot. It might be preferable to show plots as cumulative incidence that go up from 0% rather than down from 100%, especially if the event rate is lower than, say, 30% [153]. Consider presenting such information separately for participants in different exposure categories of interest. If a cohort study is investigating other time-related outcomes (e.g., quantitative disease markers such as blood pressure), present appropriate summary measures (e.g., means and standard deviations) over time, perhaps in a table or figure.\nFor cross-sectional studies, we recommend presenting the same type of information on prevalent outcome events or summary measures. For case-control studies, the focus will be on reporting exposures separately for cases and controls as frequencies or quantitative summaries [154]. For all designs, it may be helpful also to tabulate continuous outcomes or exposures in categories, even if the data are not analysed as such"
  },
  {
    "objectID": "guidelines/strega/index.html#sec-16a_-1",
    "href": "guidelines/strega/index.html#sec-16a_-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nIn many situations, authors may present the results of unadjusted or minimally adjusted analyses and those from fully adjusted analyses. We advise giving the unadjusted analyses together with the main data, for example the number of cases and controls that were exposed or not. This allows the reader to understand the data behind the measures of association (see also item 15). For adjusted analyses, report the number of persons in the analysis, as this number may differ because of missing values in covariates (see also item 12c). Estimates should be given with confidence intervals.\nReaders can compare unadjusted measures of association with those adjusted for potential confounders and judge by how much, and in what direction, they changed. Readers may think that adjusted results equal the causal part of the measure of association, but adjusted results are not necessarily free of random sampling error, selection bias, information bias, or residual confounding (see Box 5). Thus, great care should be exercised when interpreting adjusted results, as the validity of results often depends crucially on complete knowledge of important confounders, their precise measurement, and appropriate specification in the statistical model (see also item 20) [157,158].\nAuthors should explain all potential confounders considered, and the criteria for excluding or including variables in statistical models. Decisions about excluding or including variables should be guided by knowledge, or explicit assumptions, on causal relations. Inappropriate decisions may introduce bias, for example by including variables that are in the causal pathway between exposure and disease (unless the aim is to asses how much of the effect is carried by the intermediary variable). If the decision to include a variable in the model was based on the change in the estimate, it is important to report what change was considered sufficiently important to justify its inclusion. If a backward deletion or forward inclusion strategy was used to select confounders, explain that process and give the significance level for rejecting the null hypothesis of no confounding. Of note, we and others do not advise selecting confounders based solely on statistical significance testing [147,159,160].\nRecent studies of the quality of reporting of epidemiological studies found that confidence intervals were reported in most articles [4]. However, few authors explained their choice of confounding variables [4,5]."
  },
  {
    "objectID": "guidelines/strega/index.html#sec-16b_-1",
    "href": "guidelines/strega/index.html#sec-16b_-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nCategorizing continuous data has several important implications for analysis (see Box 4) and also affects the presentation of results. In tables, outcomes should be given for each exposure category, for example as counts of persons at risk, person-time at risk, if relevant separately for each group (e.g., cases and controls). Details of the categories used may aid comparison of studies and meta-analysis. If data were grouped using conventional cut-points, such as body mass index thresholds [162], group boundaries (i.e., range of values) can be derived easily, except for the highest and lowest categories. If quantile-derived categories are used, the category boundaries cannot be inferred from the data. As a minimum, authors should report the category boundaries; it is helpful also to report the range of the data and the mean or median values within categories."
  },
  {
    "objectID": "guidelines/strega/index.html#sec-16c_-1",
    "href": "guidelines/strega/index.html#sec-16c_-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nThe results from studies examining the association between an exposure and a disease are commonly reported in relative terms, as ratios of risks, rates or odds (see Box 8). Relative measures capture the strength of the association between an exposure and disease. If the relative risk is a long way from 1 it is less likely that the association is due to confounding [164,165]. Relative effects or associations tend to be more consistent across studies and populations than absolute measures, but what often tends to be the case may be irrelevant in a particular instance. For example, similar relative risks were obtained for the classic cardiovascular risk factors for men living in Northern Ireland, France, the USA and Germany, despite the fact that the underlying risk of coronary heart disease varies substantially between these countries [166,167]. In contrast, in a study of hypertension as a risk factor for cardiovascular disease mortality, the data were more compatible with a constant rate difference than with a constant rate ratio [168].\nWidely used statistical models, including logistic [169] and proportional hazards (Cox) regression [170] are based on ratio measures. In these models, only departures from constancy of ratio effect measures are easily discerned. Nevertheless, measures which assess departures from additivity of risk differences, such as the Relative Excess Risk from Interaction (RERI, see item 12b and Box 8), can be estimated in models based on ratio measures.\nIn many circumstances, the absolute risk associated with an exposure is of greater interest than the relative risk. For example, if the focus is on adverse effects of a drug, one will want to know the number of additional cases per unit time of use (e.g., days, weeks, or years). The example gives the additional number of breast cancer cases per 1000 women who used hormone-replacement therapy for 10 years [163]. Measures such as the attributable risk or population attributable fraction may be useful to gauge how much disease can be prevented if the exposure is eliminated. They should preferably be presented together with a measure of statistical uncertainty (e.g., confidence intervals as in the example). Authors should be aware of the strong assumptions made in this context, including a causal relationship between a risk factor and disease (also see Box 7) [171]. Because of the semantic ambiguity and complexities involved, authors should report in detail what methods were used to calculate attributable risks, ideally giving the formulae used [172].\nA recent survey of abstracts of 222 articles published in leading medical journals found that in 62% of abstracts of randomised trials including a ratio measure absolute risks were given, but only in 21% of abstracts of cohort studies [173]. A free text search of Medline 1966 to 1997 showed that 619 items mentioned attributable risks in the title or abstract, compared to 18,955 using relative risk or odds ratio, for a ratio of 1 to 31 [174]."
  },
  {
    "objectID": "guidelines/strega/index.html#sec-17a_-1",
    "href": "guidelines/strega/index.html#sec-17a_-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nIn addition to the main analysis other analyses are often done in observational studies. They may address specific subgroups, the potential interaction between risk factors, the calculation of attributable risks, or use alternative definitions of study variables in sensitivity analyses.\nThere is debate about the dangers associated with subgroup analyses, and multiplicity of analyses in general [4,104]. In our opinion, there is too great a tendency to look for evidence of subgroup-specific associations, or effect-measure modification, when overall results appear to suggest little or no effect. On the other hand, there is value in exploring whether an overall association appears consistent across several, preferably pre-specified subgroups especially when a study is large enough to have sufficient data in each subgroup. A second area of debate is about interesting subgroups that arose during the data analysis. They might be important findings, but might also arise by chance. Some argue that it is neither possible nor necessary to inform the reader about all subgroup analyses done as future analyses of other data will tell to what extent the early exciting findings stand the test of time [9]. We advise authors to report which analyses were planned, and which were not (see also items 4, 12b and 20). This will allow readers to judge the implications of multiplicity, taking into account the study’s position on the continuum from discovery to verification or refutation.\nA third area of debate is how joint effects and interactions between risk factors should be evaluated: on additive or multiplicative scales, or should the scale be determined by the statistical model that fits best (see also item 12b and Box 8)? A sensible approach is to report the separate effect of each exposure as well as the joint effect—if possible in a table, as in the first example above [183], or in the study by Martinelli et al. [185]. Such a table gives the reader sufficient information to evaluate additive as well as multiplicative interaction (how these calculations are done is shown in Box 8). Confidence intervals for separate and joint effects may help the reader to judge the strength of the data. In addition, confidence intervals around measures of interaction, such as the Relative Excess Risk from Interaction (RERI) relate to tests of interaction or homogeneity tests. One recurrent problem is that authors use comparisons of P-values across subgroups, which lead to erroneous claims about an effect modifier. For instance, a statistically significant association in one category (e.g., men), but not in the other (e.g., women) does not in itself provide evidence of effect modification. Similarly, the confidence intervals for each point estimate are sometimes inappropriately used to infer that there is no interaction when intervals overlap. A more valid inference is achieved by directly evaluating whether the magnitude of an association differs across subgroups.\nSensitivity analyses are helpful to investigate the influence of choices made in the statistical analysis, or to investigate the robustness of the findings to missing data or possible biases (see also item 12b). Judgement is needed regarding the level of reporting of such analyses. If many sensitivity analyses were performed, it may be impractical to present detailed findings for them all. It may sometimes be sufficient to report that sensitivity analyses were carried out and that they were consistent with the main results presented. Detailed presentation is more appropriate if the issue investigated is of major concern, or if effect estimates vary considerably [59,186].\nPocock and colleagues found that 43 out of 73 articles reporting observational studies contained subgroup analyses. The majority claimed differences across groups but only eight articles reported a formal evaluation of interaction (see item 12b) [4]."
  },
  {
    "objectID": "guidelines/strega/index.html#sec-17b_-1",
    "href": "guidelines/strega/index.html#sec-17b_-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nIf numerous genetic exposures (genetic variants) were examined, summarize results from all analyses undertaken."
  },
  {
    "objectID": "guidelines/strega/index.html#sec-17c_-1",
    "href": "guidelines/strega/index.html#sec-17c_-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nIf detailed results are available elsewhere, state how they can be accessed."
  },
  {
    "objectID": "guidelines/strega/index.html#sec-19-1",
    "href": "guidelines/strega/index.html#sec-19-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nThe identification and discussion of the limitations of a study are an essential part of scientific reporting. It is important not only to identify the sources of bias and confounding that could have affected results, but also to discuss the relative importance of different biases, including the likely direction and magnitude of any potential bias (see also item 9 and Box 3).\nAuthors should also discuss any imprecision of the results. Imprecision may arise in connection with several aspects of a study, including the study size (item 10) and the measurement of exposures, confounders and outcomes (item 8). The inability to precisely measure true values of an exposure tends to result in bias towards unity: the less precisely a risk factor is measured, the greater the bias. This effect has been described as attenuation [201,202], or more recently as regression dilution bias [203]. However, when correlated risk factors are measured with different degrees of imprecision, the adjusted relative risk associated with them can be biased towards or away from unity [204–206].\nWhen discussing limitations, authors may compare the study being presented with other studies in the literature in terms of validity, generalisability and precision. In this approach, each study can be viewed as contribution to the literature, not as a stand-alone basis for inference and action [207]. Surprisingly, the discussion of important limitations of a study is sometimes omitted from published reports. A survey of authors who had published original research articles in The Lancet found that important weaknesses of the study were reported by the investigators in the survey questionnaires, but not in the published article"
  },
  {
    "objectID": "guidelines/strega/index.html#sec-20-1",
    "href": "guidelines/strega/index.html#sec-20-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nThe heart of the discussion section is the interpretation of a study’s results. Over-interpretation is common and human: even when we try hard to give an objective assessment, reviewers often rightly point out that we went too far in some respects. When interpreting results, authors should consider the nature of the study on the discovery to verification continuum and potential sources of bias, including loss to follow-up and non-participation (see also items 9, 12 and 19). Due consideration should be given to confounding (item 16a), the results of relevant sensitivity analyses, and to the issue of multiplicity and subgroup analyses (item 17). Authors should also consider residual confounding due to unmeasured variables or imprecise measurement of confounders. For example, socioeconomic status (SES) is associated with many health outcomes and often differs between groups being compared. Variables used to measure SES (income, education, or occupation) are surrogates for other undefined and unmeasured exposures, and the true confounder will by definition be measured with error [208]. Authors should address the real range of uncertainty in estimates, which is larger than the statistical uncertainty reflected in confidence intervals. The latter do not take into account other uncertainties that arise from a study’s design, implementation, and methods of measurement [209].\nTo guide thinking and conclusions about causality, some may find criteria proposed by Bradford Hill in 1965 helpful [164]. How strong is the association with the exposure? Did it precede the onset of disease? Is the association consistently observed in different studies and settings? Is there supporting evidence from experimental studies, including laboratory and animal studies? How specific is the exposure’s putative effect, and is there a dose-response relationship? Is the association biologically plausible? These criteria should not, however, be applied mechanically. For example, some have argued that relative risks below 2 or 3 should be ignored [210,211]. This is a reversal of the point by Cornfield et al. about the strength of large relative risks (see item 12b) [127]. Although a causal effect is more likely with a relative risk of 9, it does not follow that one below 3 is necessarily spurious. For instance, the small increase in the risk of childhood leukaemia after intrauterine irradiation is credible because it concerns an adverse effect of a medical procedure for which no alternative explanations are obvious [212]. Moreover, the carcinogenic effects of radiation are well established. The doubling in the risk of ovarian cancer associated with eating 2 to 4 eggs per week is not immediately credible, since dietary habits are associated with a large number of lifestyle factors as well as SES [213]. In contrast, the credibility of much debated epidemiologic findings of a difference in thrombosis risk between different types of oral contraceptives was greatly enhanced by the differences in coagulation found in a randomised cross-over trial [214]. A discussion of the existing external evidence, from different types of studies, should always be included, but may be particularly important for studies reporting small increases in risk. Further, authors should put their results in context with similar studies and explain how the new study affects the existing body of evidence, ideally by referring to a systematic review."
  },
  {
    "objectID": "guidelines/strega/index.html#sec-21-1",
    "href": "guidelines/strega/index.html#sec-21-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nGeneralisability, also called external validity or applicability, is the extent to which the results of a study can be applied to other circumstances [216]. There is no external validity per se; the term is meaningful only with regard to clearly specified conditions [217]. Can results be applied to an individual, groups or populations that differ from those enrolled in the study with regard to age, sex, ethnicity, severity of disease, and co-morbid conditions? Are the nature and level of exposures comparable, and the definitions of outcomes relevant to another setting or population? Are data that were collected in longitudinal studies many years ago still relevant today? Are results from health services research in one country applicable to health systems in other countries?\nThe question of whether the results of a study have external validity is often a matter of judgment that depends on the study setting, the characteristics of the participants, the exposures examined, and the outcomes assessed. Thus, it is crucial that authors provide readers with adequate information about the setting and locations, eligibility criteria, the exposures and how they were measured, the definition of outcomes, and the period of recruitment and follow-up. The degree of non-participation and the proportion of unexposed participants in whom the outcome develops are also relevant. Knowledge of the absolute risk and prevalence of the exposure, which will often vary across populations, are helpful when applying results to other settings and populations (see Box 7)"
  },
  {
    "objectID": "guidelines/strega/index.html#sec-22-1",
    "href": "guidelines/strega/index.html#sec-22-1",
    "title": "The STREGA guideline for writing a genetic association study.",
    "section": "Read More",
    "text": "Read More\nSome journals require authors to disclose the presence or absence of financial and other conflicts of interest [100,218]. Several investigations show strong associations between the source of funding and the conclusions of research articles [219–222]. The conclusions in randomised trials recommended the experimental drug as the drug of choice much more often (odds ratio 5.3) if the trial was funded by for-profit organisations, even after adjustment for the effect size [223]. Other studies document the influence of the tobacco and telecommunication industries on the research they funded [224–227]. There are also examples of undue influence when the sponsor is governmental or a non-profit organisation.\nAuthors or funders may have conflicts of interest that influence any of the following: the design of the study [228]; choice of exposures [228,229], outcomes [230], statistical methods [231], and selective publication of outcomes [230] and studies [232]. Consequently, the role of the funders should be described in detail: in what part of the study they took direct responsibility (e.g., design, data collection, analysis, drafting of manuscript, decision to publish) [100]. Other sources of undue influence include employers (e.g., university administrators for academic researchers and government supervisors, especially political appointees, for government researchers), advisory committees, litigants, and special interest groups"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#about-this-guideline",
    "href": "guidelines/strobe-case-control/index.html#about-this-guideline",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "About this guideline",
    "text": "About this guideline\n\nThis guideline is relevant to studies reporting case-control studies"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#download-resources",
    "href": "guidelines/strobe-case-control/index.html#download-resources",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Download Resources",
    "text": "Download Resources\nReporting guidelines are best used early:\n\nUse a template to write quickly and avoid the blank page\nUse a checklist to double-check and reassure others that your work is described transparently\n\n\n\n\nFor drafting\n\nTemplate\n\n\n\nFor checking\n\nChecklist\n\n\n\nFor planning\n\nTo-do list"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#guidance",
    "href": "guidelines/strobe-case-control/index.html#guidance",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Guidance",
    "text": "Guidance\nApprox. 2 min read\n\nTitle and abstract\n\n\n1a. Title\n\n\n\n\n\n\n\nIndicate the study’s design with a commonly used term in the title or the abstract\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nReaders should be able to easily identify the design that was used from the title or abstract. An explicit, commonly used term for the study design also helps ensure correct indexing of articles in electronic databases\n\n\nExamples\n\nLeukaemia incidence among workers in the shoe and boot manufacturing industry: a case-control study\n\nBack to top\n\n\n\n\n\n\n1b. Abstract\n\n\n\n\n\n\n\nProvide in the abstract an informative and balanced summary of what was done and what was found\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe abstract provides key information that enables readers to understand a study and decide whether to read the article. Typical components include a statement of the research question, a short description of methods and results, and a conclusion. Abstracts should summarize key details of studies and should only present information that is provided in the article. We advise presenting key results in a numerical form that includes numbers of participants, estimates of associations and appropriate measures of variability and uncertainty (e.g., odds ratios with confidence intervals). We regard it insufficient to state only that an exposure is or is not significantly associated with an outcome.\nA series of headings pertaining to the background, design, conduct, and analysis of a study may help readers acquire the essential information rapidly. Many journals require such structured abstracts, which tend to be of higher quality and more readily informative than unstructured summaries.\n\n\nExamples\n\nBackground: The expected survival of HIV-infected patients is of major public health interest.\n\nObjective: To estimate survival time and age-specific mortality rates of an HIV-infected population compared with that of the general population.\nDesign: Population-based cohort study.\nSetting: All HIV-infected persons receiving care in Denmark from 1995 to 2005.\nPatients: Each member of the nationwide Danish HIV Cohort Study was matched with as many as 99 persons from the general population according to sex, date of birth, and municipality of residence.\nMeasurements: The authors computed Kaplan-Meier life tables with age as the time scale to estimate survival from age 25 years. Patients with HIV infection and corresponding persons from the general population were observed from the date of the patient’s HIV diagnosis until death, emigration, or 1 May 2005.\nResults: 3990 HIV-infected patients and 379,872 persons from the general population were included in the study, yielding 22,744 (median, 5.8 y/person) and 2,689,287 (median, 8.4 years/person) person-years of observation. Three percent of participants were lost to follow-up. From age 25 years, the median survival was 19.9 years (95% CI, 18.5 to 21.3) among patients with HIV infection and 51.1 years (CI, 50.9 to 51.5) among the general population. For HIV-infected patients, survival increased to 32.5 years (CI, 29.4 to 34.7) during the 2000 to 2005 period. In the subgroup that excluded persons with known hepatitis C coinfection (16%), median survival was 38.9 years (CI, 35.4 to 40.1) during this same period. The relative mortality rates for patients with HIV infection compared with those for the general population decreased with increasing age, whereas the excess mortality rate increased with increasing age.\nLimitations: The observed mortality rates are assumed to apply beyond the current maximum observation time of 10 years.\nConclusions: The estimated median survival is more than 35 years for a young person diagnosed with HIV infection in the late highly active antiretroviral therapy era. However, an ongoing effort is still needed to further reduce mortality rates for these persons compared with the general population\nBack to top\n\n\n\n\n\n\nIntroduction\n\n\n2. Background / rationale\n\n\n\n\n\n\n\nExplain the scientific background and rationale for the investigation being reported\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe scientific background of the study provides important context for readers. It sets the stage for the study and describes its focus. It gives an overview of what is known on a topic and what gaps in current knowledge are addressed by the study. Background material should note recent pertinent studies and any systematic reviews of pertinent studies.\n\n\nExamples\n\nConcerns about the rising prevalence of obesity in children and adolescents have focused on the well documented associations between childhood obesity and increased cardiovascular risk1 and mortality in adulthood.2 Childhood obesity has considerable social and psychological consequences within childhood and adolescence,3 yet little is known about social, socioeconomic, and psychological consequences in adult life.\n\nA recent systematic review found no longitudinal studies on the outcomes of childhood obesity other than physical health outcomes3 and only two longitudinal studies of the socioeconomic effects of obesity in adolescence. Gortmaker et al found that US women who had been obese in late adolescence in 1981 were less likely to be married and had lower incomes seven years later than women who had not been overweight, while men who had been overweight were less likely to be married.4 Sargent et al found that UK women, but not men, who had been obese at 16 years in 1974 earned 7.4% less than their non-obese peers at age 23.5\nThe study of adult outcomes of childhood obesity is difficult because obesity often continues into adult life and therefore poorer socioeconomic and educational outcomes may actually reflect confounding by adult obesity. Yet identifying outcomes related to obesity confined childhood is important in determining whether people who are obese in childhood and who later lose weight remain at risk for adult adversity and inequalities.\nWe used longitudinal data from the 1970 British birth cohort to examine the adult socioeconomic, educational, social, and psychological outcomes of childhood obesity. We hypothesised that obesity limited to childhood has fewer adverse adult outcomes than obesity that persists into adult life\nBack to top\n\n\n\n\n\n\n3. Objectives\n\n\n\n\n\n\n\nState specific objectives, including any prespecified hypotheses\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nObjectives are the detailed aims of the study. Well crafted objectives specify populations, exposures and outcomes, and parameters that will be estimated. They may be formulated as specific hypotheses or as questions that the study was designed to address. In some situations objectives may be less specific, for example, in early discovery phases. Regardless, the report should clearly reflect the investigators’ intentions. For example, if important subgroups or additional analyses were not the original aim of the study but arose during data analysis, they should be described accordingly (see also items 4, 17 and 20).\n\n\nExamples\n\nOur primary objectives were to 1) determine the prevalence of domestic violence among female patients presenting to four community-based, primary care, adult medicine practices that serve patients of diverse socioeconomic background and 2) identify demographic and clinical differences between currently abused patients and patients not currently being abused\n\nBack to top\n\n\n\n\n\n\nMethods\n\n\n4. Study design\n\n\n\n\n\n\n\nPresent key elements of study design early in the paper\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nWe advise presenting key elements of study design early in the methods section (or at the end of the introduction) so that readers can understand the basics of the study. For example, authors should indicate that the study was a cohort study, which followed people over a particular time period, and describe the group of persons that comprised the cohort and their exposure status. Similarly, if the investigation used a case-control design, the cases and controls and their source population should be described. If the study was a cross-sectional survey, the population and the point in time at which the cross-section was taken should be mentioned. When a study is a variant of the three main study types, there is an additional need for clarity. For instance, for a case-crossover study, one of the variants of the case-control design, a succinct description of the principles was given in the example above [28].\nWe recommend that authors refrain from simply calling a study prospective or retrospective because these terms are ill defined [29]. One usage sees cohort and prospective as synonymous and reserves the word retrospective for case-control studies [30]. A second usage distinguishes prospective and retrospective cohort studies according to the timing of data collection relative to when the idea for the study was developed [31]. A third usage distinguishes prospective and retrospective case-control studies depending on whether the data about the exposure of interest existed when cases were selected [32]. Some advise against using these terms [33], or adopting the alternatives concurrent and historical for describing cohort studies [34]. In STROBE, we do not use the words prospective and retrospective, nor alternatives such as concurrent and historical. We recommend that, whenever authors use these words, they define what they mean. Most importantly, we recommend that authors describe exactly how and when data collection took place.\nThe first part of the methods section might also be the place to mention whether the report is one of several from a study. If a new report is in line with the original aims of the study, this is usually indicated by referring to an earlier publication and by briefly restating the salient features of the study. However, the aims of a study may also evolve over time. Researchers often use data for purposes for which they were not originally intended, including, for example, official vital statistics that were collected primarily for administrative purposes, items in questionnaires that originally were only included for completeness, or blood samples that were collected for another purpose. For example, the Physicians’ Health Study, a randomized controlled trial of aspirin and carotene, was later used to demonstrate that a point mutation in the factor V gene was associated with an increased risk of venous thrombosis, but not of myocardial infarction or stroke [35]. The secondary use of existing data is a creative part of observational research and does not necessarily make results less credible or less important. However, briefly restating the original aims might help readers understand the context of the research and possible limitations in the data.\n\n\nExamples\n\nWe used a case-crossover design, a variation of a case-control design that is appropriate when a brief exposure (driver’s phone use) causes a transient rise in the risk of a rare outcome (a crash). We compared a driver’s use of a mobile phone at the estimated time of a crash with the same driver’s use during another suitable time period. Because drivers are their own controls, the design controls for characteristics of the driver that may affect the risk of a crash but do not change over a short period of time. As it is important that risks during control periods and crash trips are similar, we compared phone activity during the hazard interval (time immediately before the crash) with phone activity during control intervals (equivalent times during which participants were driving but did not crash) in the previous week\n\nBack to top\n\n\n\n\n\n\n5. Setting\n\n\n\n\n\n\n\nDescribe the setting, locations, and relevant dates, including periods of recruitment, exposure, follow-up, and data collection\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nReaders need information on setting and locations to assess the context and generalisability of a study’s results. Exposures such as environmental factors and therapies can change over time. Also, study methods may evolve over time. Knowing when a study took place and over what period participants were recruited and followed up places the study in historical context and is important for the interpretation of results.\nInformation about setting includes recruitment sites or sources (e.g., electoral roll, outpatient clinic, cancer registry, or tertiary care centre). Information about location may refer to the countries, towns, hospitals or practices where the investigation took place. We advise stating dates rather than only describing the length of time periods. There may be different sets of dates for exposure, disease occurrence, recruitment, beginning and end of follow-up, and data collection. Of note, nearly 80% of 132 reports in oncology journals that used survival analysis included the starting and ending dates for accrual of patients, but only 24% also reported the date on which follow-up ended [37].\n\n\nExamples\n\nThe Pasitos Cohort Study recruited pregnant women from Women, Infant and Child (WIC) clinics in Socorro and San Elizario, El Paso County, Texas and maternal-child clinics of the Mexican Social Security Institute (IMSS) in Ciudad Juarez, Mexico from April 1998 to October 2000. At baseline, prior to the birth of the enrolled cohort children, staff interviewed mothers regarding the household environment. In this ongoing cohort study, we target follow-up exams at 6-month intervals beginning at age 6 months\n\nBack to top\n\n\n\n\n\n\n6a. Eligibility criteria\n\n\n\n\n\n\n\nGive the eligibility criteria, and the sources and methods of case ascertainment and control selection. Give the rationale for the choice of cases and controls. For matched studies, give matching criteria and the number of controls per case\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nDetailed descriptions of the study participants help readers understand the applicability of the results. Investigators usually restrict a study population by defining clinical, demographic and other characteristics of eligible participants. Typical eligibility criteria relate to age, gender, diagnosis and comorbid conditions. Despite their importance, eligibility criteria often are not reported adequately. In a survey of observational stroke research, 17 of 49 reports (35%) did not specify eligibility criteria [5].\nEligibility criteria may be presented as inclusion and exclusion criteria, although this distinction is not always necessary or useful. Regardless, we advise authors to report all eligibility criteria and also to describe the group from which the study population was selected (e.g., the general population of a region or country), and the method of recruitment (e.g., referral or self-selection through advertisements).\nKnowing details about follow-up procedures, including whether procedures minimized non-response and loss to follow-up and whether the procedures were similar for all participants, informs judgments about the validity of results. For example, in a study that used IgM antibodies to detect acute infections, readers needed to know the interval between blood tests for IgM antibodies so that they could judge whether some infections likely were missed because the interval between blood tests was too long [41]. In other studies where follow-up procedures differed between exposed and unexposed groups, readers might recognize substantial bias due to unequal ascertainment of events or differences in non-response or loss to follow-up [42]. Accordingly, we advise that researchers describe the methods used for following participants and whether those methods were the same for all participants, and that they describe the completeness of ascertainment of variables (see also item 14).\nIn case-control studies, the choice of cases and controls is crucial to interpreting the results, and the method of their selection has major implications for study validity. In general, controls should reflect the population from which the cases arose. Various methods are used to sample controls, all with advantages and disadvantages: for cases that arise from a general population, population roster sampling, random digit dialling, neighbourhood or friend controls are used. Neighbourhood or friend controls may present intrinsic matching on exposure [17]. Controls with other diseases may have advantages over population-based controls, in particular for hospital-based cases, because they better reflect the catchment population of a hospital, have greater comparability of recall and ease of recruitment. However, they can present problems if the exposure of interest affects the risk of developing or being hospitalized for the control condition(s) [43,44]. To remedy this problem often a mixture of the best defensible control diseases is used [45].\n\n\nExamples\n\nCutaneous melanoma cases diagnosed in 1999 and 2000 were ascertained through the Iowa Cancer Registry (…). Controls, also identified through the Iowa Cancer Registry, were colorectal cancer patients diagnosed during the same time. Colorectal cancer controls were selected because they are common and have a relatively long survival, and because arsenic exposure has not been conclusively linked to the incidence of colorectal cancer\n\nBack to top\n\n\n\n\n\n\n6b. Eligibility criteria\n\n\n\n\n\n\n\nFor matched studies, give matching criteria and the number of controls per case\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nMatching is much more common in case-control studies, but occasionally, investigators use matching in cohort studies to make groups comparable at the start of follow-up. Matching in cohort studies makes groups directly comparable for potential confounders and presents fewer intricacies than with case-control studies. For example, it is not necessary to take the matching into account for the estimation of the relative risk [48]. Because matching in cohort studies may increase statistical precision investigators might allow for the matching in their analyses and thus obtain narrower confidence intervals.\nIn case-control studies matching is done to increase a study’s efficiency by ensuring similarity in the distribution of variables between cases and controls, in particular the distribution of potential confounding variables [48,49]. Because matching can be done in various ways, with one or more controls per case, the rationale for the choice of matching variables and the details of the method used should be described. Commonly used forms of matching are frequency matching (also called group matching) and individual matching. In frequency matching, investigators choose controls so that the distribution of matching variables becomes identical or similar to that of cases. Individual matching involves matching one or several controls to each case. Although intuitively appealing and sometimes useful, matching in case-control studies has a number of disadvantages, is not always appropriate, and needs to be taken into account in the analysis\n\n\nExamples\n\nWe aimed to select five controls for every case from among individuals in the study population who had no diagnosis of autism or other pervasive developmental disorders (PDD) recorded in their general practice record and who were alive and registered with a participating practice on the date of the PDD diagnosis in the case. Controls were individually matched to cases by year of birth (up to 1 year older or younger), sex, and general practice. For each of 300 cases, five controls could be identified who met all the matching criteria. For the remaining 994, one or more controls was excluded…\n\nBack to top\n\n\n\n\n\n\n7.\n\n\n\n\n\n\n\nClearly define all outcomes, exposures, predictors, potential confounders, and effect modifiers. Give diagnostic criteria, if applicable\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAuthors should define all variables considered for and included in the analysis, including outcomes, exposures, predictors, potential confounders and potential effect modifiers. Disease outcomes require adequately detailed description of the diagnostic criteria. This applies to criteria for cases in a case-control study, disease events during follow-up in a cohort study and prevalent disease in a cross-sectional study. Clear definitions and steps taken to adhere to them are particularly important for any disease condition of primary interest in the study.\nFor some studies, determinant or predictor may be appropriate terms for exposure variables and outcomes may be called endpoints. In multivariable models, authors sometimes use dependent variable for an outcome and independent variable or explanatory variable for exposure and confounding variables. The latter is not precise as it does not distinguish exposures from confounders.\nIf many variables have been measured and included in exploratory analyses in an early discovery phase, consider providing a list with details on each variable in an appendix, additional table or separate publication. Of note, the International Journal of Epidemiology recently launched a new section with cohort profiles, that includes detailed information on what was measured at different points in time in particular studies [56,57]. Finally, we advise that authors declare all candidate variables considered for statistical analysis, rather than selectively reporting only those included in the final models (see also item 16a) [58,59]\n\n\nExamples\n\nOnly major congenital malformations were included in the analyses. Minor anomalies were excluded according to the exclusion list of European Registration of Congenital Anomalies (EUROCAT). If a child had more than one major congenital malformation of one organ system, those malformations were treated as one outcome in the analyses by organ system (…) In the statistical analyses, factors considered potential confounders were maternal age at delivery and number of previous parities. Factors considered potential effect modifiers were maternal age at reimbursement for antiepileptic medication and maternal age at delivery\n\nBack to top\n\n\n\n\n\n\n8. Data sources / measurement\n\n\n\n\n\n\n\nFor each variable of interest give sources of data and details of methods of assessment (measurement). Describe comparability of assessment methods if there is more than one group. Give information separately for cases and controls.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe way in which exposures, confounders and outcomes were measured affects the reliability and validity of a study. Measurement error and misclassification of exposures or outcomes can make it more difficult to detect cause-effect relationships, or may produce spurious relationships. Error in measurement of potential confounders can increase the risk of residual confounding [62,63]. It is helpful, therefore, if authors report the findings of any studies of the validity or reliability of assessments or measurements, including details of the reference standard that was used. Rather than simply citing validation studies (as in the first example), we advise that authors give the estimated validity or reliability, which can then be used for measurement error adjustment or sensitivity analyses (see items 12e and 17).\nIn addition, it is important to know if groups being compared differed with respect to the way in which the data were collected. This may be important for laboratory examinations (as in the second example) and other situations. For instance, if an interviewer first questions all the cases and then the controls, or vice versa, bias is possible because of the learning curve; solutions such as randomising the order of interviewing may avoid this problem. Information bias may also arise if the compared groups are not given the same diagnostic tests or if one group receives more tests of the same kind than another (see also item 9).\n\n\nExamples\n\nSamples pertaining to matched cases and controls were always analyzed together in the same batch and laboratory personnel were unable to distinguish among cases and controls\n\nBack to top\n\n\n\n\n\n\n9. Bias\n\n\n\n\n\n\n\nDescribe any efforts to address potential sources of bias\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nBiased studies produce results that differ systematically from the truth (see also Box 3). It is important for a reader to know what measures were taken during the conduct of a study to reduce the potential of bias. Ideally, investigators carefully consider potential sources of bias when they plan their study. At the stage of reporting, we recommend that authors always assess the likelihood of relevant biases. Specifically, the direction and magnitude of bias should be discussed and, if possible, estimated. For instance, in case-control studies information bias can occur, but may be reduced by selecting an appropriate control group, as in the first example [64]. Differences in the medical surveillance of participants were a problem in the second example [65]. Consequently, the authors provide more detail about the additional data they collected to tackle this problem. When investigators have set up quality control programs for data collection to counter a possible drift in measurements of variables in longitudinal studies, or to keep variability at a minimum when multiple observers are used, these should be described.\nUnfortunately, authors often do not address important biases when reporting their results. Among 43 case-control and cohort studies published from 1990 to 1994 that investigated the risk of second cancers in patients with a history of cancer, medical surveillance bias was mentioned in only 5 articles [66]. A survey of reports of mental health research published during 1998 in three psychiatric journals found that only 13% of 392 articles mentioned response bias [67]. A survey of cohort studies in stroke research found that 14 of 49 (28%) articles published from 1999 to 2003 addressed potential selection bias in the recruitment of study participants and 35 (71%) mentioned the possibility that any type of bias may have affected results\n\n\nExamples\n\nIn most case-control studies of suicide, the control group comprises living individuals but we decided to have a control group of people who had died of other causes (…). With a control group of deceased individuals, the sources of information used to assess risk factors are informants who have recently experienced the death of a family member or close associate - and are therefore more comparable to the sources of information in the suicide group than if living controls were used\n\nBack to top\n\n\n\n\n\n\n10. Study size\n\n\n\n\n\n\n\nExplain how the study size was arrived at\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nA study should be large enough to obtain a point estimate with a sufficiently narrow confidence interval to meaningfully answer a research question. Large samples are needed to distinguish a small association from no association. Small studies often provide valuable information, but wide confidence intervals may indicate that they contribute less to current knowledge in comparison with studies providing estimates with narrower confidence intervals. Also, small studies that show interesting or statistically significant associations are published more frequently than small studies that do not have significant findings. While these studies may provide an early signal in the context of discovery, readers should be informed of their potential weaknesses.\nThe importance of sample size determination in observational studies depends on the context. If an analysis is performed on data that were already available for other purposes, the main question is whether the analysis of the data will produce results with sufficient statistical precision to contribute substantially to the literature, and sample size considerations will be informal. Formal, a priori calculation of sample size may be useful when planning a new study. Such calculations are associated with more uncertainty than implied by the single number that is generally produced. For example, estimates of the rate of the event of interest or other assumptions central to calculations are commonly imprecise, if not guesswork. The precision obtained in the final analysis can often not be determined beforehand because it will be reduced by inclusion of confounding variables in multivariable analyses, the degree of precision with which key variables can be measured, and the exclusion of some individuals.\nFew epidemiological studies explain or report deliberations about sample size. We encourage investigators to report pertinent formal sample size calculations if they were done. In other situations they should indicate the considerations that determined the study size (e.g., a fixed available sample, as in the first example above). If the observational study was stopped early when statistical significance was achieved, readers should be told. Do not bother readers with post hoc justifications for study size or retrospective power calculations. From the point of view of the reader, confidence intervals indicate the statistical precision that was ultimately obtained. It should be realized that confidence intervals reflect statistical uncertainty only, and not all uncertainty that may be present in a study (see item 20).\n\n\nExamples\n\nA survey of postnatal depression in the region had documented a prevalence of 19.8%. Assuming depression in mothers with normal weight children to be 20% and an odds ratio of 3 for depression in mothers with a malnourished child we needed 72 case-control sets (one case to one control) with an 80% power and 5% significance\n\n\nThe number of cases in the area during the study period determined the sample size\n\nBack to top\n\n\n\n\n\n\n11. Quantitative variables\n\n\n\n\n\n\n\nExplain how quantitative variables were handled in the analyses. If applicable, describe which groupings were chosen, and why\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nInvestigators make choices regarding how to collect and analyse quantitative data about exposures, effect modifiers and confounders. For example, they may group a continuous exposure variable to create a new categorical variable. Grouping choices may have important consequences for later analyses. We advise that authors explain why and how they grouped quantitative data, including the number of categories, the cut-points, and category mean or median values. Whenever data are reported in tabular form, the counts of cases, controls, persons at risk, person-time at risk, etc. should be given for each category. Tables should not consist solely of effect-measure estimates or results of model fitting.\nInvestigators might model an exposure as continuous in order to retain all the information. In making this choice, one needs to consider the nature of the relationship of the exposure to the outcome. As it may be wrong to assume a linear relation automatically, possible departures from linearity should be investigated. Authors could mention alternative models they explored during analyses (e.g., using log transformation, quadratic terms or spline functions). Several methods exist for fitting a non-linear relation between the exposure and outcome. Also, it may be informative to present both continuous and grouped analyses for a quantitative exposure of prime interest.\nIn a recent survey, two thirds of epidemiological publications studied quantitative exposure variables. In 42 of 50 articles (84%) exposures were grouped into several ordered categories, but often without any stated rationale for the choices made. Fifteen articles used linear associations to model continuous exposure but only two reported checking for linearity. In another survey, of the psychological literature, dichotomization was justified in only 22 of 110 articles (20%)\n\n\nExamples\n\nPatients with a Glasgow Coma Scale less than 8 are considered to be seriously injured. A GCS of 9 or more indicates less serious brain injury. We examined the association of GCS in these two categories with the occurrence of death within 12 months from injury\n\nBack to top\n\n\n\n\n\n\n12a. Statistical methods\n\n\n\n\n\n\n\nDescribe all statistical methods, including those used to control for confounding\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nIn general, there is no one correct statistical analysis but, rather, several possibilities that may address the same question, but make different assumptions. Regardless, investigators should pre-determine analyses at least for the primary study objectives in a study protocol. Often additional analyses are needed, either instead of, or as well as, those originally envisaged, and these may sometimes be motivated by the data. When a study is reported, authors should tell readers whether particular analyses were suggested by data inspection. Even though the distinction between pre-specified and exploratory analyses may sometimes be blurred, authors should clarify reasons for particular analyses.\nIf groups being compared are not similar with regard to some characteristics, adjustment should be made for possible confounding variables by stratification or by multivariable regression (see Box 5) [94]. Often, the study design determines which type of regression analysis is chosen. For instance, Cox proportional hazard regression is commonly used in cohort studies [95]. whereas logistic regression is often the method of choice in case-control studies [96,97]. Analysts should fully describe specific procedures for variable selection and not only present results from the final model [98,99]. If model comparisons are made to narrow down a list of potential confounders for inclusion in a final model, this process should be described. It is helpful to tell readers if one or two covariates are responsible for a great deal of the apparent confounding in a data analysis. Other statistical analyses such as imputation procedures, data transformation, and calculations of attributable risks should also be described. Non-standard or novel approaches should be referenced and the statistical software used reported. As a guiding principle, we advise statistical methods be described with enough detail to enable a knowledgeable reader with access to the original data to verify the reported results [100].\nIn an empirical study, only 93 of 169 articles (55%) reporting adjustment for confounding clearly stated how continuous and multi-category variables were entered into the statistical model [101]. Another study found that among 67 articles in which statistical analyses were adjusted for confounders, it was mostly unclear how confounders were chosen\n\n\nExamples\n\nThe adjusted relative risk was calculated using the Mantel-Haenszel technique, when evaluating if confounding by age or gender was present in the groups compared. The 95% confidence interval (CI) was computed around the adjusted relative risk, using the variance according to Greenland and Robins and Robins et al.\n\nBack to top\n\n\n\n\n\n\n12b. Statistical methods\n\n\n\n\n\n\n\nDescribe any methods used to examine subgroups and interactions\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAs discussed in detail under item 17, many debate the use and value of analyses restricted to subgroups of the study population [4,104]. Subgroup analyses are nevertheless often done [4]. Readers need to know which subgroup analyses were planned in advance, and which arose while analysing the data. Also, it is important to explain what methods were used to examine whether effects or associations differed across groups (see item 17).\nInteraction relates to the situation when one factor modifies the effect of another (therefore also called effect modification). The joint action of two factors can be characterized in two ways: on an additive scale, in terms of risk differences; or on a multiplicative scale, in terms of relative risk (see Box 8). Many authors and readers may have their own preference about the way interactions should be analysed. Still, they may be interested to know to what extent the joint effect of exposures differs from the separate effects. There is consensus that the additive scale, which uses absolute risks, is more appropriate for public health and clinical decision making [105]. Whatever view is taken, this should be clearly presented to the reader, as is done in the example above [103]. A lay-out presenting separate effects of both exposures as well as their joint effect, each relative to no exposure, might be most informative. It is presented in the example for interaction under item 17, and the calculations on the different scales are explained in Box 8\n\n\nExamples\n\nSex differences in susceptibility to the 3 lifestyle-related risk factors studied were explored by testing for biological interaction according to Rothman: a new composite variable with 4 categories (a−b−, a−b+, a+b−, and a+b+) was redefined for sex and a dichotomous exposure of interest where a− and b− denote absence of exposure. RR was calculated for each category after adjustment for age. An interaction effect is defined as departure from additivity of absolute effects, and excess RR caused by interaction (RERI) was calculated:\n\n[calculation]\nwhere RR(a+b+) denotes RR among those exposed to both factors where RR(a−b−) is used as reference category (RR = 1.0). Ninety-five percent CIs were calculated as proposed by Hosmer and Lemeshow. RERI of 0 means no interaction\nBack to top\n\n\n\n\n\n\n12c. Statistical methods\n\n\n\n\n\n\n\nExplain how missing data were addressed\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nMissing data are common in observational research. Questionnaires posted to study participants are not always filled in completely, participants may not attend all follow-up visits and routine data sources and clinical databases are often incomplete. Despite its ubiquity and importance, few papers report in detail on the problem of missing data [5,107]. Investigators may use any of several approaches to address missing data. We describe some strengths and limitations of various approaches in Box 6. We advise that authors report the number of missing values for each variable of interest (exposures, outcomes, confounders) and for each step in the analysis. Authors should give reasons for missing values if possible, and indicate how many individuals were excluded because of missing data when describing the flow of participants through the study (see also item 13). For analyses that account for missing data, authors should describe the nature of the analysis (e.g., multiple imputation) and the assumptions that were made (e.g., missing at random, see Box 6).\n\n\nExamples\n\nOur missing data analysis procedures used missing at random (MAR) assumptions. We used the MICE (multivariate imputation by chained equations) method of multiple multivariate imputation in STATA. We independently analysed 10 copies of the data, each with missing values suitably imputed, in the multivariate logistic regression analyses. We averaged estimates of the variables to give a single mean estimate and adjusted standard errors according to Rubin’s rules\n\nBack to top\n\n\n\n\n\n\n12d. Statistical methods\n\n\n\n\n\n\n\nIf applicable, explain how matching of cases and controls was addressed\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nIn individually matched case-control studies a crude analysis of the odds ratio, ignoring the matching, usually leads to an estimation that is biased towards unity (see Box 2). A matched analysis is therefore often necessary. This can intuitively be understood as a stratified analysis: each case is seen as one stratum with his or her set of matched controls. The analysis rests on considering whether the case is more often exposed than the controls, despite having made them alike regarding the matching variables. Investigators can do such a stratified analysis using the Mantel-Haenszel method on a matched 2 by 2 table. In its simplest form the odds ratio becomes the ratio of pairs that are discordant for the exposure variable. If matching was done for variables like age and sex that are universal attributes, the analysis needs not retain the individual, person-to-person matching: a simple analysis in categories of age and sex is sufficient [50]. For other matching variables, such as neighbourhood, sibship, or friendship, however, each matched set should be considered its own stratum.\nIn individually matched studies, the most widely used method of analysis is conditional logistic regression, in which each case and their controls are considered together. The conditional method is necessary when the number of controls varies among cases, and when, in addition to the matching variables, other variables need to be adjusted for. To allow readers to judge whether the matched design was appropriately taken into account in the analysis, we recommend that authors describe in detail what statistical methods were used to analyse the data. If taking the matching into account does have little effect on the estimates, authors may choose to present an unmatched analysis\n\n\nExamples\n\nWe used McNemar’s test, paired t test, and conditional logistic regression analysis to compare dementia patients with their matched controls for cardiovascular risk factors, the occurrence of spontaneous cerebral emboli, carotid disease, and venous to arterial circulation shunt\n\nBack to top\n\n\n\n\n\n\n12e. Statistical methods\n\n\n\n\n\n\n\nDescribe any sensitivity analyses\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nSensitivity analyses are useful to investigate whether or not the main results are consistent with those obtained with alternative analysis strategies or assumptions [121]. Issues that may be examined include the criteria for inclusion in analyses, the definitions of exposures or outcomes [122], which confounding variables merit adjustment, the handling of missing data [120,123], possible selection bias or bias from inaccurate or inconsistent measurement of exposure, disease and other variables, and specific analysis choices, such as the treatment of quantitative variables (see item 11). Sophisticated methods are increasingly used to simultaneously model the influence of several biases or assumptions [124–126].\nIn 1959 Cornfield et al. famously showed that a relative risk of 9 for cigarette smoking and lung cancer was extremely unlikely to be due to any conceivable confounder, since the confounder would need to be at least nine times as prevalent in smokers as in non-smokers [127]. This analysis did not rule out the possibility that such a factor was present, but it did identify the prevalence such a factor would need to have. The same approach was recently used to identify plausible confounding factors that could explain the association between childhood leukaemia and living near electric power lines [128]. More generally, sensitivity analyses can be used to identify the degree of confounding, selection bias, or information bias required to distort an association. One important, perhaps under recognised, use of sensitivity analysis is when a study shows little or no association between an exposure and an outcome and it is plausible that confounding or other biases toward the null are present\n\n\nExamples\n\nBecause we had a relatively higher proportion of missing dead patients with insufficient data (38/148=25.7%) as compared to live patients (15/437=3.4%) (…), it is possible that this might have biased the results. We have, therefore, carried out a sensitivity analysis. We have assumed that the proportion of women using oral contraceptives in the study group applies to the whole (19.1% for dead, and 11.4% for live patients), and then applied two extreme scenarios: either all the exposed missing patients used second generation pills or they all used third-generation pills\n\nBack to top\n\n\n\n\n\n\nResults\n\n\n13a. Participants\n\n\n\n\n\n\n\nReport numbers of individuals at each stage of study—eg numbers potentially eligible, examined for eligibility, confirmed eligible, included in the study, completing follow-up, and analysed. Give information separately for cases and controls.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nDetailed information on the process of recruiting study participants is important for several reasons. Those included in a study often differ in relevant ways from the target population to which results are applied. This may result in estimates of prevalence or incidence that do not reflect the experience of the target population. For example, people who agreed to participate in a postal survey of sexual behaviour attended church less often, had less conservative sexual attitudes and earlier age at first sexual intercourse, and were more likely to smoke cigarettes and drink alcohol than people who refused [130]. These differences suggest that postal surveys may overestimate sexual liberalism and activity in the population. Such response bias (see Box 3) can distort exposure-disease associations if associations differ between those eligible for the study and those included in the study. As another example, the association between young maternal age and leukaemia in offspring, which has been observed in some case-control studies [131,132], was explained by differential participation of young women in case and control groups. Young women with healthy children were less likely to participate than those with unhealthy children [133]. Although low participation does not necessarily compromise the validity of a study, transparent information on participation and reasons for non-participation is essential. Also, as there are no universally agreed definitions for participation, response or follow-up rates, readers need to understand how authors calculated such proportions [134].\nIdeally, investigators should give an account of the numbers of individuals considered at each stage of recruiting study participants, from the choice of a target population to the inclusion of participants’ data in the analysis. Depending on the type of study, this may include the number of individuals considered to be potentially eligible, the number assessed for eligibility, the number found to be eligible, the number included in the study, the number examined, the number followed up and the number included in the analysis. Information on different sampling units may be required, if sampling of study participants is carried out in two or more stages as in the example above (multistage sampling). In case-control studies, we advise that authors describe the flow of participants separately for case and control groups [135]. Controls can sometimes be selected from several sources, including, for example, hospitalised patients and community dwellers. In this case, we recommend a separate account of the numbers of participants for each type of control group. Olson and colleagues proposed useful reporting guidelines for controls recruited through random-digit dialling and other methods [136].\nA recent survey of epidemiological studies published in 10 general epidemiology, public health and medical journals found that some information regarding participation was provided in 47 of 107 case-control studies (59%), 49 of 154 cohort studies (32%), and 51 of 86 cross-sectional studies (59%) [137]. Incomplete or absent reporting of participation and non-participation in epidemiological studies was also documented in two other surveys of the literature [4,5]. Finally, there is evidence that participation in epidemiological studies may have declined in recent decades [137,138], which underscores the need for transparent reporting\n\n\nExamples\n\nOf the 105 freestanding bars and taverns sampled, 13 establishments were no longer in business and 9 were located in restaurants, leaving 83 eligible businesses. In 22 cases, the owner could not be reached by telephone despite 6 or more attempts. The owners of 36 bars declined study participation. (…) The 25 participating bars and taverns employed 124 bartenders, with 67 bartenders working at least 1 weekly daytime shift. Fifty-four of the daytime bartenders (81%) completed baseline interviews and spirometry; 53 of these subjects (98%) completed follow-up\n\nBack to top\n\n\n\n\n\n\n13b. Participants\n\n\n\n\n\n\n\nGive reasons for non-participation at each stage\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nExplaining the reasons why people no longer participated in a study or why they were excluded from statistical analyses helps readers judge whether the study population was representative of the target population and whether bias was possibly introduced. For example, in a cross-sectional health survey, non-participation due to reasons unlikely to be related to health status (for example, the letter of invitation was not delivered because of an incorrect address) will affect the precision of estimates but will probably not introduce bias. Conversely, if many individuals opt out of the survey because of illness, or perceived good health, results may underestimate or overestimate the prevalence of ill health in the population\n\n\nExamples\n\nThe main reasons for non-participation were the participant was too ill or had died before interview (cases 30%, controls < 1%), nonresponse (cases 2%, controls 21%), refusal (cases 10%, controls 29%), and other reasons (refusal by consultant or general practitioner, non-English speaking, mental impairment) (cases 7%, controls 5%)\n\nBack to top\n\n\n\n\n\n\n13c. Participants\n\n\n\n\n\n\n\nConsider use of a flow diagram\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAn informative and well-structured flow diagram can readily and transparently convey information that might otherwise require a lengthy description [142], as in the example above. The diagram may usefully include the main results, such as the number of events for the primary outcome. While we recommend the use of a flow diagram, particularly for complex observational studies, we do not propose a specific format for the diagram\n\n\nExamples\n\nFlow diagram from Hay et al. https://doi.org/10.1371/journal.pmed.0040297.g001\n\nBack to top\n\n\n\n\n\n\n14a. Descriptive data\n\n\n\n\n\n\n\nGive characteristics of study participants (eg demographic, clinical, social) and information on exposures and potential confounders. Give information separately for cases and controls\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nReaders need descriptions of study participants and their exposures to judge the generalisability of the findings. Information about potential confounders, including whether and how they were measured, influences judgments about study validity. We advise authors to summarize continuous variables for each study group by giving the mean and standard deviation, or when the data have an asymmetrical distribution, as is often the case, the median and percentile range (e.g., 25th and 75th percentiles). Variables that make up a small number of ordered categories (such as stages of disease I to IV) should not be presented as continuous variables; it is preferable to give numbers and proportions for each category (see also Box 4). In studies that compare groups, the descriptive characteristics and numbers should be given by group, as in the example above.\nInferential measures such as standard errors and confidence intervals should not be used to describe the variability of characteristics, and significance tests should be avoided in descriptive tables. Also, P values are not an appropriate criterion for selecting which confounders to adjust for in analysis; even small differences in a confounder that has a strong effect on the outcome can be important.\nIn cohort studies, it may be useful to document how an exposure relates to other characteristics and potential confounders. Authors could present this information in a table with columns for participants in two or more exposure categories, which permits to judge the differences in confounders between these categories.\nIn case-control studies potential confounders cannot be judged by comparing cases and controls. Control persons represent the source population and will usually be different from the cases in many respects. For example, in a study of oral contraceptives and myocardial infarction, a sample of young women with infarction more often had risk factors for that disease, such as high serum cholesterol, smoking and a positive family history, than the control group [146]. This does not influence the assessment of the effect of oral contraceptives, as long as the prescription of oral contraceptives was not guided by the presence of these risk factors—e.g., because the risk factors were only established after the event (see also Box 5). In case-control studies the equivalent of comparing exposed and non-exposed for the presence of potential confounders (as is done in cohorts) can be achieved by exploring the source population of the cases: if the control group is large enough and represents the source population, exposed and unexposed controls can be compared for potential confounders [121,147]\n\n\nExamples\n\nCharacteristics of the Study Base at Enrolment, Castellana G (Italy), 1985–1986\n\nhttps://doi.org/10.1371/journal.pmed.0040297.t002\nBack to top\n\n\n\n\n\n\n14b. Descriptive data\n\n\n\n\n\n\n\nIndicate number of participants with missing data for each variable of interest\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAs missing data may bias or affect generalisability of results, authors should tell readers amounts of missing data for exposures, potential confounders, and other important characteristics of patients (see also item 12c and Box 6). In a cohort study, authors should report the extent of loss to follow-up (with reasons), since incomplete follow-up may bias findings (see also items 12d and 13). We advise authors to use their tables and figures to enumerate amounts of missing data.\n\n\nExamples\n\nTable. Symptom End Points Used in Survival Analysis\n\nhttps://doi.org/10.1371/journal.pmed.0040297.t003\nBack to top\n\n\n\n\n\n\n15. Outcome data\n\n\n\n\n\n\n\nReport numbers in each exposure category, or summary measures of exposure. Give information separately for cases and controls\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nBefore addressing the possible association between exposures (risk factors) and outcomes, authors should report relevant descriptive data. It may be possible and meaningful to present measures of association in the same table that presents the descriptive data (see item 14a). In a cohort study with events as outcomes, report the numbers of events for each outcome of interest. Consider reporting the event rate per person-year of follow-up. If the risk of an event changes over follow-up time, present the numbers and rates of events in appropriate intervals of follow-up or as a Kaplan-Meier life table or plot. It might be preferable to show plots as cumulative incidence that go up from 0% rather than down from 100%, especially if the event rate is lower than, say, 30% [153]. Consider presenting such information separately for participants in different exposure categories of interest. If a cohort study is investigating other time-related outcomes (e.g., quantitative disease markers such as blood pressure), present appropriate summary measures (e.g., means and standard deviations) over time, perhaps in a table or figure.\nFor cross-sectional studies, we recommend presenting the same type of information on prevalent outcome events or summary measures. For case-control studies, the focus will be on reporting exposures separately for cases and controls as frequencies or quantitative summaries [154]. For all designs, it may be helpful also to tabulate continuous outcomes or exposures in categories, even if the data are not analysed as such\n\n\nExamples\n\nTable. Exposure among Liver Cirrhosis Cases and Controls https://doi.org/10.1371/journal.pmed.0040297.t006\n\nBack to top\n\n\n\n\n\n\n16a. Main results\n\n\n\n\n\n\n\nGive unadjusted estimates and, if applicable, confounder-adjusted estimates and their precision (eg, 95% confidence interval). Make clear which confounders were adjusted for and why they were included\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nIn many situations, authors may present the results of unadjusted or minimally adjusted analyses and those from fully adjusted analyses. We advise giving the unadjusted analyses together with the main data, for example the number of cases and controls that were exposed or not. This allows the reader to understand the data behind the measures of association (see also item 15). For adjusted analyses, report the number of persons in the analysis, as this number may differ because of missing values in covariates (see also item 12c). Estimates should be given with confidence intervals.\nReaders can compare unadjusted measures of association with those adjusted for potential confounders and judge by how much, and in what direction, they changed. Readers may think that adjusted results equal the causal part of the measure of association, but adjusted results are not necessarily free of random sampling error, selection bias, information bias, or residual confounding (see Box 5). Thus, great care should be exercised when interpreting adjusted results, as the validity of results often depends crucially on complete knowledge of important confounders, their precise measurement, and appropriate specification in the statistical model (see also item 20) [157,158].\nAuthors should explain all potential confounders considered, and the criteria for excluding or including variables in statistical models. Decisions about excluding or including variables should be guided by knowledge, or explicit assumptions, on causal relations. Inappropriate decisions may introduce bias, for example by including variables that are in the causal pathway between exposure and disease (unless the aim is to asses how much of the effect is carried by the intermediary variable). If the decision to include a variable in the model was based on the change in the estimate, it is important to report what change was considered sufficiently important to justify its inclusion. If a backward deletion or forward inclusion strategy was used to select confounders, explain that process and give the significance level for rejecting the null hypothesis of no confounding. Of note, we and others do not advise selecting confounders based solely on statistical significance testing [147,159,160].\nRecent studies of the quality of reporting of epidemiological studies found that confidence intervals were reported in most articles [4]. However, few authors explained their choice of confounding variables [4,5].\n\n\nExamples\n\nTable. Relative Rates of Rehospitalisation by Treatment in Patients in Community Care after First Hospitalisation due to Schizophrenia and Schizoaffective Disorder https://doi.org/10.1371/journal.pmed.0040297.t008\n\nBack to top\n\n\n\n\n\n\n16b. Main results\n\n\n\n\n\n\n\nReport category boundaries when continuous variables were categorized\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nCategorizing continuous data has several important implications for analysis (see Box 4) and also affects the presentation of results. In tables, outcomes should be given for each exposure category, for example as counts of persons at risk, person-time at risk, if relevant separately for each group (e.g., cases and controls). Details of the categories used may aid comparison of studies and meta-analysis. If data were grouped using conventional cut-points, such as body mass index thresholds [162], group boundaries (i.e., range of values) can be derived easily, except for the highest and lowest categories. If quantile-derived categories are used, the category boundaries cannot be inferred from the data. As a minimum, authors should report the category boundaries; it is helpful also to report the range of the data and the mean or median values within categories.\n\n\nExamples\n\nTable. Polychlorinated Biphenyls in Cord Serum\n\nhttps://doi.org/10.1371/journal.pmed.0040297.t005\nBack to top\n\n\n\n\n\n\n16c. Main results\n\n\n\n\n\n\n\nIf relevant, consider translating estimates of relative risk into absolute risk for a meaningful time period\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe results from studies examining the association between an exposure and a disease are commonly reported in relative terms, as ratios of risks, rates or odds (see Box 8). Relative measures capture the strength of the association between an exposure and disease. If the relative risk is a long way from 1 it is less likely that the association is due to confounding [164,165]. Relative effects or associations tend to be more consistent across studies and populations than absolute measures, but what often tends to be the case may be irrelevant in a particular instance. For example, similar relative risks were obtained for the classic cardiovascular risk factors for men living in Northern Ireland, France, the USA and Germany, despite the fact that the underlying risk of coronary heart disease varies substantially between these countries [166,167]. In contrast, in a study of hypertension as a risk factor for cardiovascular disease mortality, the data were more compatible with a constant rate difference than with a constant rate ratio [168].\nWidely used statistical models, including logistic [169] and proportional hazards (Cox) regression [170] are based on ratio measures. In these models, only departures from constancy of ratio effect measures are easily discerned. Nevertheless, measures which assess departures from additivity of risk differences, such as the Relative Excess Risk from Interaction (RERI, see item 12b and Box 8), can be estimated in models based on ratio measures.\nIn many circumstances, the absolute risk associated with an exposure is of greater interest than the relative risk. For example, if the focus is on adverse effects of a drug, one will want to know the number of additional cases per unit time of use (e.g., days, weeks, or years). The example gives the additional number of breast cancer cases per 1000 women who used hormone-replacement therapy for 10 years [163]. Measures such as the attributable risk or population attributable fraction may be useful to gauge how much disease can be prevented if the exposure is eliminated. They should preferably be presented together with a measure of statistical uncertainty (e.g., confidence intervals as in the example). Authors should be aware of the strong assumptions made in this context, including a causal relationship between a risk factor and disease (also see Box 7) [171]. Because of the semantic ambiguity and complexities involved, authors should report in detail what methods were used to calculate attributable risks, ideally giving the formulae used [172].\nA recent survey of abstracts of 222 articles published in leading medical journals found that in 62% of abstracts of randomised trials including a ratio measure absolute risks were given, but only in 21% of abstracts of cohort studies [173]. A free text search of Medline 1966 to 1997 showed that 619 items mentioned attributable risks in the title or abstract, compared to 18,955 using relative risk or odds ratio, for a ratio of 1 to 31 [174].\n\n\nExamples\n\n10 years’ use of HRT [hormone replacement therapy] is estimated to result in five (95% CI 3–7) additional breast cancers per 1000 users of oestrogen-only preparations and 19 (15–23) additional cancers per 1000 users of oestrogen-progestagen combinations\n\nBack to top\n\n\n\n\n\n\n17. Other analyses\n\n\n\n\n\n\n\nReport other analyses done—e.g., analyses of subgroups and interactions, and sensitivity analyses\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nIn addition to the main analysis other analyses are often done in observational studies. They may address specific subgroups, the potential interaction between risk factors, the calculation of attributable risks, or use alternative definitions of study variables in sensitivity analyses.\nThere is debate about the dangers associated with subgroup analyses, and multiplicity of analyses in general [4,104]. In our opinion, there is too great a tendency to look for evidence of subgroup-specific associations, or effect-measure modification, when overall results appear to suggest little or no effect. On the other hand, there is value in exploring whether an overall association appears consistent across several, preferably pre-specified subgroups especially when a study is large enough to have sufficient data in each subgroup. A second area of debate is about interesting subgroups that arose during the data analysis. They might be important findings, but might also arise by chance. Some argue that it is neither possible nor necessary to inform the reader about all subgroup analyses done as future analyses of other data will tell to what extent the early exciting findings stand the test of time [9]. We advise authors to report which analyses were planned, and which were not (see also items 4, 12b and 20). This will allow readers to judge the implications of multiplicity, taking into account the study’s position on the continuum from discovery to verification or refutation.\nA third area of debate is how joint effects and interactions between risk factors should be evaluated: on additive or multiplicative scales, or should the scale be determined by the statistical model that fits best (see also item 12b and Box 8)? A sensible approach is to report the separate effect of each exposure as well as the joint effect—if possible in a table, as in the first example above [183], or in the study by Martinelli et al. [185]. Such a table gives the reader sufficient information to evaluate additive as well as multiplicative interaction (how these calculations are done is shown in Box 8). Confidence intervals for separate and joint effects may help the reader to judge the strength of the data. In addition, confidence intervals around measures of interaction, such as the Relative Excess Risk from Interaction (RERI) relate to tests of interaction or homogeneity tests. One recurrent problem is that authors use comparisons of P-values across subgroups, which lead to erroneous claims about an effect modifier. For instance, a statistically significant association in one category (e.g., men), but not in the other (e.g., women) does not in itself provide evidence of effect modification. Similarly, the confidence intervals for each point estimate are sometimes inappropriately used to infer that there is no interaction when intervals overlap. A more valid inference is achieved by directly evaluating whether the magnitude of an association differs across subgroups.\nSensitivity analyses are helpful to investigate the influence of choices made in the statistical analysis, or to investigate the robustness of the findings to missing data or possible biases (see also item 12b). Judgement is needed regarding the level of reporting of such analyses. If many sensitivity analyses were performed, it may be impractical to present detailed findings for them all. It may sometimes be sufficient to report that sensitivity analyses were carried out and that they were consistent with the main results presented. Detailed presentation is more appropriate if the issue investigated is of major concern, or if effect estimates vary considerably [59,186].\nPocock and colleagues found that 43 out of 73 articles reporting observational studies contained subgroup analyses. The majority claimed differences across groups but only eight articles reported a formal evaluation of interaction (see item 12b) [4].\n\n\nExamples\n\nTable. Analysis of Oral Contraceptive Use, Presence of Factor V Leiden Allele, and Risk for Venous Thromboembolism\n\nhttps://doi.org/10.1371/journal.pmed.0040297.t009\n\nTable. Sensitivity of the Rate Ratio for Cardiovascular Outcome to an Unmeasured Confounder https://doi.org/10.1371/journal.pmed.0040297.t010\n\nBack to top\n\n\n\n\n\n\nDiscussion\n\n\n18. Key results\n\n\n\n\n\n\n\nSummarise key results with reference to study objectives\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nIt is good practice to begin the discussion with a short summary of the main findings of the study. The short summary reminds readers of the main findings and may help them assess whether the subsequent interpretation and implications offered by the authors are supported by the findings\n\n\nExamples\n\nWe hypothesized that ethnic minority status would be associated with higher levels of cardiovascular disease (CVD) risk factors, but that the associations would be explained substantially by socioeconomic status (SES). Our hypothesis was not confirmed. After adjustment for age and SES, highly significant differences in body mass index, blood pressure, diabetes, and physical inactivity remained between white women and both black and Mexican American women. In addition, we found large differences in CVD risk factors by SES, a finding that illustrates the high-risk status of both ethnic minority women as well as white women with low SES\n\nBack to top\n\n\n\n\n\n\n19. Limitations\n\n\n\n\n\n\n\nDiscuss limitations of the study, taking into account sources of potential bias or imprecision. Discuss both direction and magnitude of any potential bias.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe identification and discussion of the limitations of a study are an essential part of scientific reporting. It is important not only to identify the sources of bias and confounding that could have affected results, but also to discuss the relative importance of different biases, including the likely direction and magnitude of any potential bias (see also item 9 and Box 3).\nAuthors should also discuss any imprecision of the results. Imprecision may arise in connection with several aspects of a study, including the study size (item 10) and the measurement of exposures, confounders and outcomes (item 8). The inability to precisely measure true values of an exposure tends to result in bias towards unity: the less precisely a risk factor is measured, the greater the bias. This effect has been described as attenuation [201,202], or more recently as regression dilution bias [203]. However, when correlated risk factors are measured with different degrees of imprecision, the adjusted relative risk associated with them can be biased towards or away from unity [204–206].\nWhen discussing limitations, authors may compare the study being presented with other studies in the literature in terms of validity, generalisability and precision. In this approach, each study can be viewed as contribution to the literature, not as a stand-alone basis for inference and action [207]. Surprisingly, the discussion of important limitations of a study is sometimes omitted from published reports. A survey of authors who had published original research articles in The Lancet found that important weaknesses of the study were reported by the investigators in the survey questionnaires, but not in the published article\n\n\nExamples\n\nSince the prevalence of counseling increases with increasing levels of obesity, our estimates may overestimate the true prevalence. Telephone surveys also may overestimate the true prevalence of counseling. Although persons without telephones have similar levels of overweight as persons with telephones, persons without telephones tend to be less educated, a factor associated with lower levels of counseling in our study. Also, of concern is the potential bias caused by those who refused to participate as well as those who refused to respond to questions about weight. Furthermore, because data were collected cross-sectionally, we cannot infer that counseling preceded a patient’s attempt to lose weight\n\nBack to top\n\n\n\n\n\n\n20. Interpretation\n\n\n\n\n\n\n\nGive a cautious overall interpretation considering objectives, limitations, multiplicity of analyses, results from similar studies, and other relevant evidence.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe heart of the discussion section is the interpretation of a study’s results. Over-interpretation is common and human: even when we try hard to give an objective assessment, reviewers often rightly point out that we went too far in some respects. When interpreting results, authors should consider the nature of the study on the discovery to verification continuum and potential sources of bias, including loss to follow-up and non-participation (see also items 9, 12 and 19). Due consideration should be given to confounding (item 16a), the results of relevant sensitivity analyses, and to the issue of multiplicity and subgroup analyses (item 17). Authors should also consider residual confounding due to unmeasured variables or imprecise measurement of confounders. For example, socioeconomic status (SES) is associated with many health outcomes and often differs between groups being compared. Variables used to measure SES (income, education, or occupation) are surrogates for other undefined and unmeasured exposures, and the true confounder will by definition be measured with error [208]. Authors should address the real range of uncertainty in estimates, which is larger than the statistical uncertainty reflected in confidence intervals. The latter do not take into account other uncertainties that arise from a study’s design, implementation, and methods of measurement [209].\nTo guide thinking and conclusions about causality, some may find criteria proposed by Bradford Hill in 1965 helpful [164]. How strong is the association with the exposure? Did it precede the onset of disease? Is the association consistently observed in different studies and settings? Is there supporting evidence from experimental studies, including laboratory and animal studies? How specific is the exposure’s putative effect, and is there a dose-response relationship? Is the association biologically plausible? These criteria should not, however, be applied mechanically. For example, some have argued that relative risks below 2 or 3 should be ignored [210,211]. This is a reversal of the point by Cornfield et al. about the strength of large relative risks (see item 12b) [127]. Although a causal effect is more likely with a relative risk of 9, it does not follow that one below 3 is necessarily spurious. For instance, the small increase in the risk of childhood leukaemia after intrauterine irradiation is credible because it concerns an adverse effect of a medical procedure for which no alternative explanations are obvious [212]. Moreover, the carcinogenic effects of radiation are well established. The doubling in the risk of ovarian cancer associated with eating 2 to 4 eggs per week is not immediately credible, since dietary habits are associated with a large number of lifestyle factors as well as SES [213]. In contrast, the credibility of much debated epidemiologic findings of a difference in thrombosis risk between different types of oral contraceptives was greatly enhanced by the differences in coagulation found in a randomised cross-over trial [214]. A discussion of the existing external evidence, from different types of studies, should always be included, but may be particularly important for studies reporting small increases in risk. Further, authors should put their results in context with similar studies and explain how the new study affects the existing body of evidence, ideally by referring to a systematic review.\n\n\nExamples\n\nAny explanation for an association between death from myocardial infarction and use of second generation oral contraceptives must be conjectural. There is no published evidence to suggest a direct biologic mechanism, and there are no other epidemiologic studies with relevant results. (…) The increase in absolute risk is very small and probably applies predominantly to smokers. Due to the lack of corroborative evidence, and because the analysis is based on relatively small numbers, more evidence on the subject is needed. We would not recommend any change in prescribing practice on the strength of these results\n\nBack to top\n\n\n\n\n\n\n21. Generalisability\n\n\n\n\n\n\n\nDiscuss the generalisability (external validity) of the study results\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nGeneralisability, also called external validity or applicability, is the extent to which the results of a study can be applied to other circumstances [216]. There is no external validity per se; the term is meaningful only with regard to clearly specified conditions [217]. Can results be applied to an individual, groups or populations that differ from those enrolled in the study with regard to age, sex, ethnicity, severity of disease, and co-morbid conditions? Are the nature and level of exposures comparable, and the definitions of outcomes relevant to another setting or population? Are data that were collected in longitudinal studies many years ago still relevant today? Are results from health services research in one country applicable to health systems in other countries?\nThe question of whether the results of a study have external validity is often a matter of judgment that depends on the study setting, the characteristics of the participants, the exposures examined, and the outcomes assessed. Thus, it is crucial that authors provide readers with adequate information about the setting and locations, eligibility criteria, the exposures and how they were measured, the definition of outcomes, and the period of recruitment and follow-up. The degree of non-participation and the proportion of unexposed participants in whom the outcome develops are also relevant. Knowledge of the absolute risk and prevalence of the exposure, which will often vary across populations, are helpful when applying results to other settings and populations (see Box 7)\n\n\nExamples\n\nHow applicable are our estimates to other HIV-1-infected patients? This is an important question because the accuracy of prognostic models tends to be lower when applied to data other than those used to develop them. We addressed this issue by penalising model complexity, and by choosing models that generalised best to cohorts omitted from the estimation procedure. Our database included patients from many countries from Europe and North America, who were treated in different settings. The range of patients was broad: men and women, from teenagers to elderly people were included, and the major exposure categories were well represented. The severity of immunodeficiency at baseline ranged from not measureable to very severe, and viral load from undetectable to extremely high\n\nBack to top\n\n\n\n\n\n\nOther Information\n\n\n22. Funding\n\n\n\n\n\n\n\nGive the source of funding and the role of the funders for the present study and, if applicable, for the original study on which the present article is based\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nSome journals require authors to disclose the presence or absence of financial and other conflicts of interest [100,218]. Several investigations show strong associations between the source of funding and the conclusions of research articles [219–222]. The conclusions in randomised trials recommended the experimental drug as the drug of choice much more often (odds ratio 5.3) if the trial was funded by for-profit organisations, even after adjustment for the effect size [223]. Other studies document the influence of the tobacco and telecommunication industries on the research they funded [224–227]. There are also examples of undue influence when the sponsor is governmental or a non-profit organisation.\nAuthors or funders may have conflicts of interest that influence any of the following: the design of the study [228]; choice of exposures [228,229], outcomes [230], statistical methods [231], and selective publication of outcomes [230] and studies [232]. Consequently, the role of the funders should be described in detail: in what part of the study they took direct responsibility (e.g., design, data collection, analysis, drafting of manuscript, decision to publish) [100]. Other sources of undue influence include employers (e.g., university administrators for academic researchers and government supervisors, especially political appointees, for government researchers), advisory committees, litigants, and special interest groups"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-1a_title-1",
    "href": "guidelines/strobe-case-control/index.html#sec-1a_title-1",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Read More",
    "text": "Read More\nReaders should be able to easily identify the design that was used from the title or abstract. An explicit, commonly used term for the study design also helps ensure correct indexing of articles in electronic databases"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-1a_title-2",
    "href": "guidelines/strobe-case-control/index.html#sec-1a_title-2",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Examples",
    "text": "Examples\n\nLeukaemia incidence among workers in the shoe and boot manufacturing industry: a case-control study\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-1b_abstract-1",
    "href": "guidelines/strobe-case-control/index.html#sec-1b_abstract-1",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Read More",
    "text": "Read More\nThe abstract provides key information that enables readers to understand a study and decide whether to read the article. Typical components include a statement of the research question, a short description of methods and results, and a conclusion. Abstracts should summarize key details of studies and should only present information that is provided in the article. We advise presenting key results in a numerical form that includes numbers of participants, estimates of associations and appropriate measures of variability and uncertainty (e.g., odds ratios with confidence intervals). We regard it insufficient to state only that an exposure is or is not significantly associated with an outcome.\nA series of headings pertaining to the background, design, conduct, and analysis of a study may help readers acquire the essential information rapidly. Many journals require such structured abstracts, which tend to be of higher quality and more readily informative than unstructured summaries."
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-1b_abstract-2",
    "href": "guidelines/strobe-case-control/index.html#sec-1b_abstract-2",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Examples",
    "text": "Examples\n\nBackground: The expected survival of HIV-infected patients is of major public health interest.\n\nObjective: To estimate survival time and age-specific mortality rates of an HIV-infected population compared with that of the general population.\nDesign: Population-based cohort study.\nSetting: All HIV-infected persons receiving care in Denmark from 1995 to 2005.\nPatients: Each member of the nationwide Danish HIV Cohort Study was matched with as many as 99 persons from the general population according to sex, date of birth, and municipality of residence.\nMeasurements: The authors computed Kaplan-Meier life tables with age as the time scale to estimate survival from age 25 years. Patients with HIV infection and corresponding persons from the general population were observed from the date of the patient’s HIV diagnosis until death, emigration, or 1 May 2005.\nResults: 3990 HIV-infected patients and 379,872 persons from the general population were included in the study, yielding 22,744 (median, 5.8 y/person) and 2,689,287 (median, 8.4 years/person) person-years of observation. Three percent of participants were lost to follow-up. From age 25 years, the median survival was 19.9 years (95% CI, 18.5 to 21.3) among patients with HIV infection and 51.1 years (CI, 50.9 to 51.5) among the general population. For HIV-infected patients, survival increased to 32.5 years (CI, 29.4 to 34.7) during the 2000 to 2005 period. In the subgroup that excluded persons with known hepatitis C coinfection (16%), median survival was 38.9 years (CI, 35.4 to 40.1) during this same period. The relative mortality rates for patients with HIV infection compared with those for the general population decreased with increasing age, whereas the excess mortality rate increased with increasing age.\nLimitations: The observed mortality rates are assumed to apply beyond the current maximum observation time of 10 years.\nConclusions: The estimated median survival is more than 35 years for a young person diagnosed with HIV infection in the late highly active antiretroviral therapy era. However, an ongoing effort is still needed to further reduce mortality rates for these persons compared with the general population\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-2_background__rationale-1",
    "href": "guidelines/strobe-case-control/index.html#sec-2_background__rationale-1",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Read More",
    "text": "Read More\nThe scientific background of the study provides important context for readers. It sets the stage for the study and describes its focus. It gives an overview of what is known on a topic and what gaps in current knowledge are addressed by the study. Background material should note recent pertinent studies and any systematic reviews of pertinent studies."
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-2_background__rationale-2",
    "href": "guidelines/strobe-case-control/index.html#sec-2_background__rationale-2",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Examples",
    "text": "Examples\n\nConcerns about the rising prevalence of obesity in children and adolescents have focused on the well documented associations between childhood obesity and increased cardiovascular risk1 and mortality in adulthood.2 Childhood obesity has considerable social and psychological consequences within childhood and adolescence,3 yet little is known about social, socioeconomic, and psychological consequences in adult life.\n\nA recent systematic review found no longitudinal studies on the outcomes of childhood obesity other than physical health outcomes3 and only two longitudinal studies of the socioeconomic effects of obesity in adolescence. Gortmaker et al found that US women who had been obese in late adolescence in 1981 were less likely to be married and had lower incomes seven years later than women who had not been overweight, while men who had been overweight were less likely to be married.4 Sargent et al found that UK women, but not men, who had been obese at 16 years in 1974 earned 7.4% less than their non-obese peers at age 23.5\nThe study of adult outcomes of childhood obesity is difficult because obesity often continues into adult life and therefore poorer socioeconomic and educational outcomes may actually reflect confounding by adult obesity. Yet identifying outcomes related to obesity confined childhood is important in determining whether people who are obese in childhood and who later lose weight remain at risk for adult adversity and inequalities.\nWe used longitudinal data from the 1970 British birth cohort to examine the adult socioeconomic, educational, social, and psychological outcomes of childhood obesity. We hypothesised that obesity limited to childhood has fewer adverse adult outcomes than obesity that persists into adult life\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-3_objectives-1",
    "href": "guidelines/strobe-case-control/index.html#sec-3_objectives-1",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Read More",
    "text": "Read More\nObjectives are the detailed aims of the study. Well crafted objectives specify populations, exposures and outcomes, and parameters that will be estimated. They may be formulated as specific hypotheses or as questions that the study was designed to address. In some situations objectives may be less specific, for example, in early discovery phases. Regardless, the report should clearly reflect the investigators’ intentions. For example, if important subgroups or additional analyses were not the original aim of the study but arose during data analysis, they should be described accordingly (see also items 4, 17 and 20)."
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-3_objectives-2",
    "href": "guidelines/strobe-case-control/index.html#sec-3_objectives-2",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Examples",
    "text": "Examples\n\nOur primary objectives were to 1) determine the prevalence of domestic violence among female patients presenting to four community-based, primary care, adult medicine practices that serve patients of diverse socioeconomic background and 2) identify demographic and clinical differences between currently abused patients and patients not currently being abused\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-4_study_design-1",
    "href": "guidelines/strobe-case-control/index.html#sec-4_study_design-1",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Read More",
    "text": "Read More\nWe advise presenting key elements of study design early in the methods section (or at the end of the introduction) so that readers can understand the basics of the study. For example, authors should indicate that the study was a cohort study, which followed people over a particular time period, and describe the group of persons that comprised the cohort and their exposure status. Similarly, if the investigation used a case-control design, the cases and controls and their source population should be described. If the study was a cross-sectional survey, the population and the point in time at which the cross-section was taken should be mentioned. When a study is a variant of the three main study types, there is an additional need for clarity. For instance, for a case-crossover study, one of the variants of the case-control design, a succinct description of the principles was given in the example above [28].\nWe recommend that authors refrain from simply calling a study prospective or retrospective because these terms are ill defined [29]. One usage sees cohort and prospective as synonymous and reserves the word retrospective for case-control studies [30]. A second usage distinguishes prospective and retrospective cohort studies according to the timing of data collection relative to when the idea for the study was developed [31]. A third usage distinguishes prospective and retrospective case-control studies depending on whether the data about the exposure of interest existed when cases were selected [32]. Some advise against using these terms [33], or adopting the alternatives concurrent and historical for describing cohort studies [34]. In STROBE, we do not use the words prospective and retrospective, nor alternatives such as concurrent and historical. We recommend that, whenever authors use these words, they define what they mean. Most importantly, we recommend that authors describe exactly how and when data collection took place.\nThe first part of the methods section might also be the place to mention whether the report is one of several from a study. If a new report is in line with the original aims of the study, this is usually indicated by referring to an earlier publication and by briefly restating the salient features of the study. However, the aims of a study may also evolve over time. Researchers often use data for purposes for which they were not originally intended, including, for example, official vital statistics that were collected primarily for administrative purposes, items in questionnaires that originally were only included for completeness, or blood samples that were collected for another purpose. For example, the Physicians’ Health Study, a randomized controlled trial of aspirin and carotene, was later used to demonstrate that a point mutation in the factor V gene was associated with an increased risk of venous thrombosis, but not of myocardial infarction or stroke [35]. The secondary use of existing data is a creative part of observational research and does not necessarily make results less credible or less important. However, briefly restating the original aims might help readers understand the context of the research and possible limitations in the data."
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-4_study_design-2",
    "href": "guidelines/strobe-case-control/index.html#sec-4_study_design-2",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Examples",
    "text": "Examples\n\nWe used a case-crossover design, a variation of a case-control design that is appropriate when a brief exposure (driver’s phone use) causes a transient rise in the risk of a rare outcome (a crash). We compared a driver’s use of a mobile phone at the estimated time of a crash with the same driver’s use during another suitable time period. Because drivers are their own controls, the design controls for characteristics of the driver that may affect the risk of a crash but do not change over a short period of time. As it is important that risks during control periods and crash trips are similar, we compared phone activity during the hazard interval (time immediately before the crash) with phone activity during control intervals (equivalent times during which participants were driving but did not crash) in the previous week\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-5_setting-1",
    "href": "guidelines/strobe-case-control/index.html#sec-5_setting-1",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Read More",
    "text": "Read More\nReaders need information on setting and locations to assess the context and generalisability of a study’s results. Exposures such as environmental factors and therapies can change over time. Also, study methods may evolve over time. Knowing when a study took place and over what period participants were recruited and followed up places the study in historical context and is important for the interpretation of results.\nInformation about setting includes recruitment sites or sources (e.g., electoral roll, outpatient clinic, cancer registry, or tertiary care centre). Information about location may refer to the countries, towns, hospitals or practices where the investigation took place. We advise stating dates rather than only describing the length of time periods. There may be different sets of dates for exposure, disease occurrence, recruitment, beginning and end of follow-up, and data collection. Of note, nearly 80% of 132 reports in oncology journals that used survival analysis included the starting and ending dates for accrual of patients, but only 24% also reported the date on which follow-up ended [37]."
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-5_setting-2",
    "href": "guidelines/strobe-case-control/index.html#sec-5_setting-2",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Examples",
    "text": "Examples\n\nThe Pasitos Cohort Study recruited pregnant women from Women, Infant and Child (WIC) clinics in Socorro and San Elizario, El Paso County, Texas and maternal-child clinics of the Mexican Social Security Institute (IMSS) in Ciudad Juarez, Mexico from April 1998 to October 2000. At baseline, prior to the birth of the enrolled cohort children, staff interviewed mothers regarding the household environment. In this ongoing cohort study, we target follow-up exams at 6-month intervals beginning at age 6 months\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-6a_eligibility_criteria-1",
    "href": "guidelines/strobe-case-control/index.html#sec-6a_eligibility_criteria-1",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Read More",
    "text": "Read More\nDetailed descriptions of the study participants help readers understand the applicability of the results. Investigators usually restrict a study population by defining clinical, demographic and other characteristics of eligible participants. Typical eligibility criteria relate to age, gender, diagnosis and comorbid conditions. Despite their importance, eligibility criteria often are not reported adequately. In a survey of observational stroke research, 17 of 49 reports (35%) did not specify eligibility criteria [5].\nEligibility criteria may be presented as inclusion and exclusion criteria, although this distinction is not always necessary or useful. Regardless, we advise authors to report all eligibility criteria and also to describe the group from which the study population was selected (e.g., the general population of a region or country), and the method of recruitment (e.g., referral or self-selection through advertisements).\nKnowing details about follow-up procedures, including whether procedures minimized non-response and loss to follow-up and whether the procedures were similar for all participants, informs judgments about the validity of results. For example, in a study that used IgM antibodies to detect acute infections, readers needed to know the interval between blood tests for IgM antibodies so that they could judge whether some infections likely were missed because the interval between blood tests was too long [41]. In other studies where follow-up procedures differed between exposed and unexposed groups, readers might recognize substantial bias due to unequal ascertainment of events or differences in non-response or loss to follow-up [42]. Accordingly, we advise that researchers describe the methods used for following participants and whether those methods were the same for all participants, and that they describe the completeness of ascertainment of variables (see also item 14).\nIn case-control studies, the choice of cases and controls is crucial to interpreting the results, and the method of their selection has major implications for study validity. In general, controls should reflect the population from which the cases arose. Various methods are used to sample controls, all with advantages and disadvantages: for cases that arise from a general population, population roster sampling, random digit dialling, neighbourhood or friend controls are used. Neighbourhood or friend controls may present intrinsic matching on exposure [17]. Controls with other diseases may have advantages over population-based controls, in particular for hospital-based cases, because they better reflect the catchment population of a hospital, have greater comparability of recall and ease of recruitment. However, they can present problems if the exposure of interest affects the risk of developing or being hospitalized for the control condition(s) [43,44]. To remedy this problem often a mixture of the best defensible control diseases is used [45]."
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-6a_eligibility_criteria-2",
    "href": "guidelines/strobe-case-control/index.html#sec-6a_eligibility_criteria-2",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Examples",
    "text": "Examples\n\nCutaneous melanoma cases diagnosed in 1999 and 2000 were ascertained through the Iowa Cancer Registry (…). Controls, also identified through the Iowa Cancer Registry, were colorectal cancer patients diagnosed during the same time. Colorectal cancer controls were selected because they are common and have a relatively long survival, and because arsenic exposure has not been conclusively linked to the incidence of colorectal cancer\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-6b_eligibility_criteria-1",
    "href": "guidelines/strobe-case-control/index.html#sec-6b_eligibility_criteria-1",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Read More",
    "text": "Read More\nMatching is much more common in case-control studies, but occasionally, investigators use matching in cohort studies to make groups comparable at the start of follow-up. Matching in cohort studies makes groups directly comparable for potential confounders and presents fewer intricacies than with case-control studies. For example, it is not necessary to take the matching into account for the estimation of the relative risk [48]. Because matching in cohort studies may increase statistical precision investigators might allow for the matching in their analyses and thus obtain narrower confidence intervals.\nIn case-control studies matching is done to increase a study’s efficiency by ensuring similarity in the distribution of variables between cases and controls, in particular the distribution of potential confounding variables [48,49]. Because matching can be done in various ways, with one or more controls per case, the rationale for the choice of matching variables and the details of the method used should be described. Commonly used forms of matching are frequency matching (also called group matching) and individual matching. In frequency matching, investigators choose controls so that the distribution of matching variables becomes identical or similar to that of cases. Individual matching involves matching one or several controls to each case. Although intuitively appealing and sometimes useful, matching in case-control studies has a number of disadvantages, is not always appropriate, and needs to be taken into account in the analysis"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-6b_eligibility_criteria-2",
    "href": "guidelines/strobe-case-control/index.html#sec-6b_eligibility_criteria-2",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Examples",
    "text": "Examples\n\nWe aimed to select five controls for every case from among individuals in the study population who had no diagnosis of autism or other pervasive developmental disorders (PDD) recorded in their general practice record and who were alive and registered with a participating practice on the date of the PDD diagnosis in the case. Controls were individually matched to cases by year of birth (up to 1 year older or younger), sex, and general practice. For each of 300 cases, five controls could be identified who met all the matching criteria. For the remaining 994, one or more controls was excluded…\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-7-1",
    "href": "guidelines/strobe-case-control/index.html#sec-7-1",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Read More",
    "text": "Read More\nAuthors should define all variables considered for and included in the analysis, including outcomes, exposures, predictors, potential confounders and potential effect modifiers. Disease outcomes require adequately detailed description of the diagnostic criteria. This applies to criteria for cases in a case-control study, disease events during follow-up in a cohort study and prevalent disease in a cross-sectional study. Clear definitions and steps taken to adhere to them are particularly important for any disease condition of primary interest in the study.\nFor some studies, determinant or predictor may be appropriate terms for exposure variables and outcomes may be called endpoints. In multivariable models, authors sometimes use dependent variable for an outcome and independent variable or explanatory variable for exposure and confounding variables. The latter is not precise as it does not distinguish exposures from confounders.\nIf many variables have been measured and included in exploratory analyses in an early discovery phase, consider providing a list with details on each variable in an appendix, additional table or separate publication. Of note, the International Journal of Epidemiology recently launched a new section with cohort profiles, that includes detailed information on what was measured at different points in time in particular studies [56,57]. Finally, we advise that authors declare all candidate variables considered for statistical analysis, rather than selectively reporting only those included in the final models (see also item 16a) [58,59]"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-7-2",
    "href": "guidelines/strobe-case-control/index.html#sec-7-2",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Examples",
    "text": "Examples\n\nOnly major congenital malformations were included in the analyses. Minor anomalies were excluded according to the exclusion list of European Registration of Congenital Anomalies (EUROCAT). If a child had more than one major congenital malformation of one organ system, those malformations were treated as one outcome in the analyses by organ system (…) In the statistical analyses, factors considered potential confounders were maternal age at delivery and number of previous parities. Factors considered potential effect modifiers were maternal age at reimbursement for antiepileptic medication and maternal age at delivery\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-8_data_sources__measurement-1",
    "href": "guidelines/strobe-case-control/index.html#sec-8_data_sources__measurement-1",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Read More",
    "text": "Read More\nThe way in which exposures, confounders and outcomes were measured affects the reliability and validity of a study. Measurement error and misclassification of exposures or outcomes can make it more difficult to detect cause-effect relationships, or may produce spurious relationships. Error in measurement of potential confounders can increase the risk of residual confounding [62,63]. It is helpful, therefore, if authors report the findings of any studies of the validity or reliability of assessments or measurements, including details of the reference standard that was used. Rather than simply citing validation studies (as in the first example), we advise that authors give the estimated validity or reliability, which can then be used for measurement error adjustment or sensitivity analyses (see items 12e and 17).\nIn addition, it is important to know if groups being compared differed with respect to the way in which the data were collected. This may be important for laboratory examinations (as in the second example) and other situations. For instance, if an interviewer first questions all the cases and then the controls, or vice versa, bias is possible because of the learning curve; solutions such as randomising the order of interviewing may avoid this problem. Information bias may also arise if the compared groups are not given the same diagnostic tests or if one group receives more tests of the same kind than another (see also item 9)."
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-8_data_sources__measurement-2",
    "href": "guidelines/strobe-case-control/index.html#sec-8_data_sources__measurement-2",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Examples",
    "text": "Examples\n\nSamples pertaining to matched cases and controls were always analyzed together in the same batch and laboratory personnel were unable to distinguish among cases and controls\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-9_bias-1",
    "href": "guidelines/strobe-case-control/index.html#sec-9_bias-1",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Read More",
    "text": "Read More\nBiased studies produce results that differ systematically from the truth (see also Box 3). It is important for a reader to know what measures were taken during the conduct of a study to reduce the potential of bias. Ideally, investigators carefully consider potential sources of bias when they plan their study. At the stage of reporting, we recommend that authors always assess the likelihood of relevant biases. Specifically, the direction and magnitude of bias should be discussed and, if possible, estimated. For instance, in case-control studies information bias can occur, but may be reduced by selecting an appropriate control group, as in the first example [64]. Differences in the medical surveillance of participants were a problem in the second example [65]. Consequently, the authors provide more detail about the additional data they collected to tackle this problem. When investigators have set up quality control programs for data collection to counter a possible drift in measurements of variables in longitudinal studies, or to keep variability at a minimum when multiple observers are used, these should be described.\nUnfortunately, authors often do not address important biases when reporting their results. Among 43 case-control and cohort studies published from 1990 to 1994 that investigated the risk of second cancers in patients with a history of cancer, medical surveillance bias was mentioned in only 5 articles [66]. A survey of reports of mental health research published during 1998 in three psychiatric journals found that only 13% of 392 articles mentioned response bias [67]. A survey of cohort studies in stroke research found that 14 of 49 (28%) articles published from 1999 to 2003 addressed potential selection bias in the recruitment of study participants and 35 (71%) mentioned the possibility that any type of bias may have affected results"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-9_bias-2",
    "href": "guidelines/strobe-case-control/index.html#sec-9_bias-2",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Examples",
    "text": "Examples\n\nIn most case-control studies of suicide, the control group comprises living individuals but we decided to have a control group of people who had died of other causes (…). With a control group of deceased individuals, the sources of information used to assess risk factors are informants who have recently experienced the death of a family member or close associate - and are therefore more comparable to the sources of information in the suicide group than if living controls were used\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-10_study_size-1",
    "href": "guidelines/strobe-case-control/index.html#sec-10_study_size-1",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Read More",
    "text": "Read More\nA study should be large enough to obtain a point estimate with a sufficiently narrow confidence interval to meaningfully answer a research question. Large samples are needed to distinguish a small association from no association. Small studies often provide valuable information, but wide confidence intervals may indicate that they contribute less to current knowledge in comparison with studies providing estimates with narrower confidence intervals. Also, small studies that show interesting or statistically significant associations are published more frequently than small studies that do not have significant findings. While these studies may provide an early signal in the context of discovery, readers should be informed of their potential weaknesses.\nThe importance of sample size determination in observational studies depends on the context. If an analysis is performed on data that were already available for other purposes, the main question is whether the analysis of the data will produce results with sufficient statistical precision to contribute substantially to the literature, and sample size considerations will be informal. Formal, a priori calculation of sample size may be useful when planning a new study. Such calculations are associated with more uncertainty than implied by the single number that is generally produced. For example, estimates of the rate of the event of interest or other assumptions central to calculations are commonly imprecise, if not guesswork. The precision obtained in the final analysis can often not be determined beforehand because it will be reduced by inclusion of confounding variables in multivariable analyses, the degree of precision with which key variables can be measured, and the exclusion of some individuals.\nFew epidemiological studies explain or report deliberations about sample size. We encourage investigators to report pertinent formal sample size calculations if they were done. In other situations they should indicate the considerations that determined the study size (e.g., a fixed available sample, as in the first example above). If the observational study was stopped early when statistical significance was achieved, readers should be told. Do not bother readers with post hoc justifications for study size or retrospective power calculations. From the point of view of the reader, confidence intervals indicate the statistical precision that was ultimately obtained. It should be realized that confidence intervals reflect statistical uncertainty only, and not all uncertainty that may be present in a study (see item 20)."
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-10_study_size-2",
    "href": "guidelines/strobe-case-control/index.html#sec-10_study_size-2",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Examples",
    "text": "Examples\n\nA survey of postnatal depression in the region had documented a prevalence of 19.8%. Assuming depression in mothers with normal weight children to be 20% and an odds ratio of 3 for depression in mothers with a malnourished child we needed 72 case-control sets (one case to one control) with an 80% power and 5% significance\n\n\nThe number of cases in the area during the study period determined the sample size\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-11_quantitative_variables-1",
    "href": "guidelines/strobe-case-control/index.html#sec-11_quantitative_variables-1",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Read More",
    "text": "Read More\nInvestigators make choices regarding how to collect and analyse quantitative data about exposures, effect modifiers and confounders. For example, they may group a continuous exposure variable to create a new categorical variable. Grouping choices may have important consequences for later analyses. We advise that authors explain why and how they grouped quantitative data, including the number of categories, the cut-points, and category mean or median values. Whenever data are reported in tabular form, the counts of cases, controls, persons at risk, person-time at risk, etc. should be given for each category. Tables should not consist solely of effect-measure estimates or results of model fitting.\nInvestigators might model an exposure as continuous in order to retain all the information. In making this choice, one needs to consider the nature of the relationship of the exposure to the outcome. As it may be wrong to assume a linear relation automatically, possible departures from linearity should be investigated. Authors could mention alternative models they explored during analyses (e.g., using log transformation, quadratic terms or spline functions). Several methods exist for fitting a non-linear relation between the exposure and outcome. Also, it may be informative to present both continuous and grouped analyses for a quantitative exposure of prime interest.\nIn a recent survey, two thirds of epidemiological publications studied quantitative exposure variables. In 42 of 50 articles (84%) exposures were grouped into several ordered categories, but often without any stated rationale for the choices made. Fifteen articles used linear associations to model continuous exposure but only two reported checking for linearity. In another survey, of the psychological literature, dichotomization was justified in only 22 of 110 articles (20%)"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-11_quantitative_variables-2",
    "href": "guidelines/strobe-case-control/index.html#sec-11_quantitative_variables-2",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Examples",
    "text": "Examples\n\nPatients with a Glasgow Coma Scale less than 8 are considered to be seriously injured. A GCS of 9 or more indicates less serious brain injury. We examined the association of GCS in these two categories with the occurrence of death within 12 months from injury\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-12a_statistical_methods-1",
    "href": "guidelines/strobe-case-control/index.html#sec-12a_statistical_methods-1",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Read More",
    "text": "Read More\nIn general, there is no one correct statistical analysis but, rather, several possibilities that may address the same question, but make different assumptions. Regardless, investigators should pre-determine analyses at least for the primary study objectives in a study protocol. Often additional analyses are needed, either instead of, or as well as, those originally envisaged, and these may sometimes be motivated by the data. When a study is reported, authors should tell readers whether particular analyses were suggested by data inspection. Even though the distinction between pre-specified and exploratory analyses may sometimes be blurred, authors should clarify reasons for particular analyses.\nIf groups being compared are not similar with regard to some characteristics, adjustment should be made for possible confounding variables by stratification or by multivariable regression (see Box 5) [94]. Often, the study design determines which type of regression analysis is chosen. For instance, Cox proportional hazard regression is commonly used in cohort studies [95]. whereas logistic regression is often the method of choice in case-control studies [96,97]. Analysts should fully describe specific procedures for variable selection and not only present results from the final model [98,99]. If model comparisons are made to narrow down a list of potential confounders for inclusion in a final model, this process should be described. It is helpful to tell readers if one or two covariates are responsible for a great deal of the apparent confounding in a data analysis. Other statistical analyses such as imputation procedures, data transformation, and calculations of attributable risks should also be described. Non-standard or novel approaches should be referenced and the statistical software used reported. As a guiding principle, we advise statistical methods be described with enough detail to enable a knowledgeable reader with access to the original data to verify the reported results [100].\nIn an empirical study, only 93 of 169 articles (55%) reporting adjustment for confounding clearly stated how continuous and multi-category variables were entered into the statistical model [101]. Another study found that among 67 articles in which statistical analyses were adjusted for confounders, it was mostly unclear how confounders were chosen"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-12a_statistical_methods-2",
    "href": "guidelines/strobe-case-control/index.html#sec-12a_statistical_methods-2",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Examples",
    "text": "Examples\n\nThe adjusted relative risk was calculated using the Mantel-Haenszel technique, when evaluating if confounding by age or gender was present in the groups compared. The 95% confidence interval (CI) was computed around the adjusted relative risk, using the variance according to Greenland and Robins and Robins et al.\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-12b_statistical_methods-1",
    "href": "guidelines/strobe-case-control/index.html#sec-12b_statistical_methods-1",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Read More",
    "text": "Read More\nAs discussed in detail under item 17, many debate the use and value of analyses restricted to subgroups of the study population [4,104]. Subgroup analyses are nevertheless often done [4]. Readers need to know which subgroup analyses were planned in advance, and which arose while analysing the data. Also, it is important to explain what methods were used to examine whether effects or associations differed across groups (see item 17).\nInteraction relates to the situation when one factor modifies the effect of another (therefore also called effect modification). The joint action of two factors can be characterized in two ways: on an additive scale, in terms of risk differences; or on a multiplicative scale, in terms of relative risk (see Box 8). Many authors and readers may have their own preference about the way interactions should be analysed. Still, they may be interested to know to what extent the joint effect of exposures differs from the separate effects. There is consensus that the additive scale, which uses absolute risks, is more appropriate for public health and clinical decision making [105]. Whatever view is taken, this should be clearly presented to the reader, as is done in the example above [103]. A lay-out presenting separate effects of both exposures as well as their joint effect, each relative to no exposure, might be most informative. It is presented in the example for interaction under item 17, and the calculations on the different scales are explained in Box 8"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-12b_statistical_methods-2",
    "href": "guidelines/strobe-case-control/index.html#sec-12b_statistical_methods-2",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Examples",
    "text": "Examples\n\nSex differences in susceptibility to the 3 lifestyle-related risk factors studied were explored by testing for biological interaction according to Rothman: a new composite variable with 4 categories (a−b−, a−b+, a+b−, and a+b+) was redefined for sex and a dichotomous exposure of interest where a− and b− denote absence of exposure. RR was calculated for each category after adjustment for age. An interaction effect is defined as departure from additivity of absolute effects, and excess RR caused by interaction (RERI) was calculated:\n\n[calculation]\nwhere RR(a+b+) denotes RR among those exposed to both factors where RR(a−b−) is used as reference category (RR = 1.0). Ninety-five percent CIs were calculated as proposed by Hosmer and Lemeshow. RERI of 0 means no interaction\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-12c_statistical_methods-1",
    "href": "guidelines/strobe-case-control/index.html#sec-12c_statistical_methods-1",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Read More",
    "text": "Read More\nMissing data are common in observational research. Questionnaires posted to study participants are not always filled in completely, participants may not attend all follow-up visits and routine data sources and clinical databases are often incomplete. Despite its ubiquity and importance, few papers report in detail on the problem of missing data [5,107]. Investigators may use any of several approaches to address missing data. We describe some strengths and limitations of various approaches in Box 6. We advise that authors report the number of missing values for each variable of interest (exposures, outcomes, confounders) and for each step in the analysis. Authors should give reasons for missing values if possible, and indicate how many individuals were excluded because of missing data when describing the flow of participants through the study (see also item 13). For analyses that account for missing data, authors should describe the nature of the analysis (e.g., multiple imputation) and the assumptions that were made (e.g., missing at random, see Box 6)."
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-12c_statistical_methods-2",
    "href": "guidelines/strobe-case-control/index.html#sec-12c_statistical_methods-2",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Examples",
    "text": "Examples\n\nOur missing data analysis procedures used missing at random (MAR) assumptions. We used the MICE (multivariate imputation by chained equations) method of multiple multivariate imputation in STATA. We independently analysed 10 copies of the data, each with missing values suitably imputed, in the multivariate logistic regression analyses. We averaged estimates of the variables to give a single mean estimate and adjusted standard errors according to Rubin’s rules\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-12d_statistical_methods-1",
    "href": "guidelines/strobe-case-control/index.html#sec-12d_statistical_methods-1",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Read More",
    "text": "Read More\nIn individually matched case-control studies a crude analysis of the odds ratio, ignoring the matching, usually leads to an estimation that is biased towards unity (see Box 2). A matched analysis is therefore often necessary. This can intuitively be understood as a stratified analysis: each case is seen as one stratum with his or her set of matched controls. The analysis rests on considering whether the case is more often exposed than the controls, despite having made them alike regarding the matching variables. Investigators can do such a stratified analysis using the Mantel-Haenszel method on a matched 2 by 2 table. In its simplest form the odds ratio becomes the ratio of pairs that are discordant for the exposure variable. If matching was done for variables like age and sex that are universal attributes, the analysis needs not retain the individual, person-to-person matching: a simple analysis in categories of age and sex is sufficient [50]. For other matching variables, such as neighbourhood, sibship, or friendship, however, each matched set should be considered its own stratum.\nIn individually matched studies, the most widely used method of analysis is conditional logistic regression, in which each case and their controls are considered together. The conditional method is necessary when the number of controls varies among cases, and when, in addition to the matching variables, other variables need to be adjusted for. To allow readers to judge whether the matched design was appropriately taken into account in the analysis, we recommend that authors describe in detail what statistical methods were used to analyse the data. If taking the matching into account does have little effect on the estimates, authors may choose to present an unmatched analysis"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-12d_statistical_methods-2",
    "href": "guidelines/strobe-case-control/index.html#sec-12d_statistical_methods-2",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Examples",
    "text": "Examples\n\nWe used McNemar’s test, paired t test, and conditional logistic regression analysis to compare dementia patients with their matched controls for cardiovascular risk factors, the occurrence of spontaneous cerebral emboli, carotid disease, and venous to arterial circulation shunt\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-12e_statistical_methods-1",
    "href": "guidelines/strobe-case-control/index.html#sec-12e_statistical_methods-1",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Read More",
    "text": "Read More\nSensitivity analyses are useful to investigate whether or not the main results are consistent with those obtained with alternative analysis strategies or assumptions [121]. Issues that may be examined include the criteria for inclusion in analyses, the definitions of exposures or outcomes [122], which confounding variables merit adjustment, the handling of missing data [120,123], possible selection bias or bias from inaccurate or inconsistent measurement of exposure, disease and other variables, and specific analysis choices, such as the treatment of quantitative variables (see item 11). Sophisticated methods are increasingly used to simultaneously model the influence of several biases or assumptions [124–126].\nIn 1959 Cornfield et al. famously showed that a relative risk of 9 for cigarette smoking and lung cancer was extremely unlikely to be due to any conceivable confounder, since the confounder would need to be at least nine times as prevalent in smokers as in non-smokers [127]. This analysis did not rule out the possibility that such a factor was present, but it did identify the prevalence such a factor would need to have. The same approach was recently used to identify plausible confounding factors that could explain the association between childhood leukaemia and living near electric power lines [128]. More generally, sensitivity analyses can be used to identify the degree of confounding, selection bias, or information bias required to distort an association. One important, perhaps under recognised, use of sensitivity analysis is when a study shows little or no association between an exposure and an outcome and it is plausible that confounding or other biases toward the null are present"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-12e_statistical_methods-2",
    "href": "guidelines/strobe-case-control/index.html#sec-12e_statistical_methods-2",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Examples",
    "text": "Examples\n\nBecause we had a relatively higher proportion of missing dead patients with insufficient data (38/148=25.7%) as compared to live patients (15/437=3.4%) (…), it is possible that this might have biased the results. We have, therefore, carried out a sensitivity analysis. We have assumed that the proportion of women using oral contraceptives in the study group applies to the whole (19.1% for dead, and 11.4% for live patients), and then applied two extreme scenarios: either all the exposed missing patients used second generation pills or they all used third-generation pills\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-13a_participants-1",
    "href": "guidelines/strobe-case-control/index.html#sec-13a_participants-1",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Read More",
    "text": "Read More\nDetailed information on the process of recruiting study participants is important for several reasons. Those included in a study often differ in relevant ways from the target population to which results are applied. This may result in estimates of prevalence or incidence that do not reflect the experience of the target population. For example, people who agreed to participate in a postal survey of sexual behaviour attended church less often, had less conservative sexual attitudes and earlier age at first sexual intercourse, and were more likely to smoke cigarettes and drink alcohol than people who refused [130]. These differences suggest that postal surveys may overestimate sexual liberalism and activity in the population. Such response bias (see Box 3) can distort exposure-disease associations if associations differ between those eligible for the study and those included in the study. As another example, the association between young maternal age and leukaemia in offspring, which has been observed in some case-control studies [131,132], was explained by differential participation of young women in case and control groups. Young women with healthy children were less likely to participate than those with unhealthy children [133]. Although low participation does not necessarily compromise the validity of a study, transparent information on participation and reasons for non-participation is essential. Also, as there are no universally agreed definitions for participation, response or follow-up rates, readers need to understand how authors calculated such proportions [134].\nIdeally, investigators should give an account of the numbers of individuals considered at each stage of recruiting study participants, from the choice of a target population to the inclusion of participants’ data in the analysis. Depending on the type of study, this may include the number of individuals considered to be potentially eligible, the number assessed for eligibility, the number found to be eligible, the number included in the study, the number examined, the number followed up and the number included in the analysis. Information on different sampling units may be required, if sampling of study participants is carried out in two or more stages as in the example above (multistage sampling). In case-control studies, we advise that authors describe the flow of participants separately for case and control groups [135]. Controls can sometimes be selected from several sources, including, for example, hospitalised patients and community dwellers. In this case, we recommend a separate account of the numbers of participants for each type of control group. Olson and colleagues proposed useful reporting guidelines for controls recruited through random-digit dialling and other methods [136].\nA recent survey of epidemiological studies published in 10 general epidemiology, public health and medical journals found that some information regarding participation was provided in 47 of 107 case-control studies (59%), 49 of 154 cohort studies (32%), and 51 of 86 cross-sectional studies (59%) [137]. Incomplete or absent reporting of participation and non-participation in epidemiological studies was also documented in two other surveys of the literature [4,5]. Finally, there is evidence that participation in epidemiological studies may have declined in recent decades [137,138], which underscores the need for transparent reporting"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-13a_participants-2",
    "href": "guidelines/strobe-case-control/index.html#sec-13a_participants-2",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Examples",
    "text": "Examples\n\nOf the 105 freestanding bars and taverns sampled, 13 establishments were no longer in business and 9 were located in restaurants, leaving 83 eligible businesses. In 22 cases, the owner could not be reached by telephone despite 6 or more attempts. The owners of 36 bars declined study participation. (…) The 25 participating bars and taverns employed 124 bartenders, with 67 bartenders working at least 1 weekly daytime shift. Fifty-four of the daytime bartenders (81%) completed baseline interviews and spirometry; 53 of these subjects (98%) completed follow-up\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-13b_participants-1",
    "href": "guidelines/strobe-case-control/index.html#sec-13b_participants-1",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Read More",
    "text": "Read More\nExplaining the reasons why people no longer participated in a study or why they were excluded from statistical analyses helps readers judge whether the study population was representative of the target population and whether bias was possibly introduced. For example, in a cross-sectional health survey, non-participation due to reasons unlikely to be related to health status (for example, the letter of invitation was not delivered because of an incorrect address) will affect the precision of estimates but will probably not introduce bias. Conversely, if many individuals opt out of the survey because of illness, or perceived good health, results may underestimate or overestimate the prevalence of ill health in the population"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-13b_participants-2",
    "href": "guidelines/strobe-case-control/index.html#sec-13b_participants-2",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Examples",
    "text": "Examples\n\nThe main reasons for non-participation were the participant was too ill or had died before interview (cases 30%, controls < 1%), nonresponse (cases 2%, controls 21%), refusal (cases 10%, controls 29%), and other reasons (refusal by consultant or general practitioner, non-English speaking, mental impairment) (cases 7%, controls 5%)\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-13c_participants-1",
    "href": "guidelines/strobe-case-control/index.html#sec-13c_participants-1",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Read More",
    "text": "Read More\nAn informative and well-structured flow diagram can readily and transparently convey information that might otherwise require a lengthy description [142], as in the example above. The diagram may usefully include the main results, such as the number of events for the primary outcome. While we recommend the use of a flow diagram, particularly for complex observational studies, we do not propose a specific format for the diagram"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-13c_participants-2",
    "href": "guidelines/strobe-case-control/index.html#sec-13c_participants-2",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Examples",
    "text": "Examples\n\nFlow diagram from Hay et al. https://doi.org/10.1371/journal.pmed.0040297.g001\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-14a_descriptive_data-1",
    "href": "guidelines/strobe-case-control/index.html#sec-14a_descriptive_data-1",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Read More",
    "text": "Read More\nReaders need descriptions of study participants and their exposures to judge the generalisability of the findings. Information about potential confounders, including whether and how they were measured, influences judgments about study validity. We advise authors to summarize continuous variables for each study group by giving the mean and standard deviation, or when the data have an asymmetrical distribution, as is often the case, the median and percentile range (e.g., 25th and 75th percentiles). Variables that make up a small number of ordered categories (such as stages of disease I to IV) should not be presented as continuous variables; it is preferable to give numbers and proportions for each category (see also Box 4). In studies that compare groups, the descriptive characteristics and numbers should be given by group, as in the example above.\nInferential measures such as standard errors and confidence intervals should not be used to describe the variability of characteristics, and significance tests should be avoided in descriptive tables. Also, P values are not an appropriate criterion for selecting which confounders to adjust for in analysis; even small differences in a confounder that has a strong effect on the outcome can be important.\nIn cohort studies, it may be useful to document how an exposure relates to other characteristics and potential confounders. Authors could present this information in a table with columns for participants in two or more exposure categories, which permits to judge the differences in confounders between these categories.\nIn case-control studies potential confounders cannot be judged by comparing cases and controls. Control persons represent the source population and will usually be different from the cases in many respects. For example, in a study of oral contraceptives and myocardial infarction, a sample of young women with infarction more often had risk factors for that disease, such as high serum cholesterol, smoking and a positive family history, than the control group [146]. This does not influence the assessment of the effect of oral contraceptives, as long as the prescription of oral contraceptives was not guided by the presence of these risk factors—e.g., because the risk factors were only established after the event (see also Box 5). In case-control studies the equivalent of comparing exposed and non-exposed for the presence of potential confounders (as is done in cohorts) can be achieved by exploring the source population of the cases: if the control group is large enough and represents the source population, exposed and unexposed controls can be compared for potential confounders [121,147]"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-14a_descriptive_data-2",
    "href": "guidelines/strobe-case-control/index.html#sec-14a_descriptive_data-2",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Examples",
    "text": "Examples\n\nCharacteristics of the Study Base at Enrolment, Castellana G (Italy), 1985–1986\n\nhttps://doi.org/10.1371/journal.pmed.0040297.t002\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-14b_descriptive_data-1",
    "href": "guidelines/strobe-case-control/index.html#sec-14b_descriptive_data-1",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Read More",
    "text": "Read More\nAs missing data may bias or affect generalisability of results, authors should tell readers amounts of missing data for exposures, potential confounders, and other important characteristics of patients (see also item 12c and Box 6). In a cohort study, authors should report the extent of loss to follow-up (with reasons), since incomplete follow-up may bias findings (see also items 12d and 13). We advise authors to use their tables and figures to enumerate amounts of missing data."
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-14b_descriptive_data-2",
    "href": "guidelines/strobe-case-control/index.html#sec-14b_descriptive_data-2",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Examples",
    "text": "Examples\n\nTable. Symptom End Points Used in Survival Analysis\n\nhttps://doi.org/10.1371/journal.pmed.0040297.t003\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-15_outcome_data-1",
    "href": "guidelines/strobe-case-control/index.html#sec-15_outcome_data-1",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Read More",
    "text": "Read More\nBefore addressing the possible association between exposures (risk factors) and outcomes, authors should report relevant descriptive data. It may be possible and meaningful to present measures of association in the same table that presents the descriptive data (see item 14a). In a cohort study with events as outcomes, report the numbers of events for each outcome of interest. Consider reporting the event rate per person-year of follow-up. If the risk of an event changes over follow-up time, present the numbers and rates of events in appropriate intervals of follow-up or as a Kaplan-Meier life table or plot. It might be preferable to show plots as cumulative incidence that go up from 0% rather than down from 100%, especially if the event rate is lower than, say, 30% [153]. Consider presenting such information separately for participants in different exposure categories of interest. If a cohort study is investigating other time-related outcomes (e.g., quantitative disease markers such as blood pressure), present appropriate summary measures (e.g., means and standard deviations) over time, perhaps in a table or figure.\nFor cross-sectional studies, we recommend presenting the same type of information on prevalent outcome events or summary measures. For case-control studies, the focus will be on reporting exposures separately for cases and controls as frequencies or quantitative summaries [154]. For all designs, it may be helpful also to tabulate continuous outcomes or exposures in categories, even if the data are not analysed as such"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-15_outcome_data-2",
    "href": "guidelines/strobe-case-control/index.html#sec-15_outcome_data-2",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Examples",
    "text": "Examples\n\nTable. Exposure among Liver Cirrhosis Cases and Controls https://doi.org/10.1371/journal.pmed.0040297.t006\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-16a_main_results-1",
    "href": "guidelines/strobe-case-control/index.html#sec-16a_main_results-1",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Read More",
    "text": "Read More\nIn many situations, authors may present the results of unadjusted or minimally adjusted analyses and those from fully adjusted analyses. We advise giving the unadjusted analyses together with the main data, for example the number of cases and controls that were exposed or not. This allows the reader to understand the data behind the measures of association (see also item 15). For adjusted analyses, report the number of persons in the analysis, as this number may differ because of missing values in covariates (see also item 12c). Estimates should be given with confidence intervals.\nReaders can compare unadjusted measures of association with those adjusted for potential confounders and judge by how much, and in what direction, they changed. Readers may think that adjusted results equal the causal part of the measure of association, but adjusted results are not necessarily free of random sampling error, selection bias, information bias, or residual confounding (see Box 5). Thus, great care should be exercised when interpreting adjusted results, as the validity of results often depends crucially on complete knowledge of important confounders, their precise measurement, and appropriate specification in the statistical model (see also item 20) [157,158].\nAuthors should explain all potential confounders considered, and the criteria for excluding or including variables in statistical models. Decisions about excluding or including variables should be guided by knowledge, or explicit assumptions, on causal relations. Inappropriate decisions may introduce bias, for example by including variables that are in the causal pathway between exposure and disease (unless the aim is to asses how much of the effect is carried by the intermediary variable). If the decision to include a variable in the model was based on the change in the estimate, it is important to report what change was considered sufficiently important to justify its inclusion. If a backward deletion or forward inclusion strategy was used to select confounders, explain that process and give the significance level for rejecting the null hypothesis of no confounding. Of note, we and others do not advise selecting confounders based solely on statistical significance testing [147,159,160].\nRecent studies of the quality of reporting of epidemiological studies found that confidence intervals were reported in most articles [4]. However, few authors explained their choice of confounding variables [4,5]."
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-16a_main_results-2",
    "href": "guidelines/strobe-case-control/index.html#sec-16a_main_results-2",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Examples",
    "text": "Examples\n\nTable. Relative Rates of Rehospitalisation by Treatment in Patients in Community Care after First Hospitalisation due to Schizophrenia and Schizoaffective Disorder https://doi.org/10.1371/journal.pmed.0040297.t008\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-16b_main_results-1",
    "href": "guidelines/strobe-case-control/index.html#sec-16b_main_results-1",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Read More",
    "text": "Read More\nCategorizing continuous data has several important implications for analysis (see Box 4) and also affects the presentation of results. In tables, outcomes should be given for each exposure category, for example as counts of persons at risk, person-time at risk, if relevant separately for each group (e.g., cases and controls). Details of the categories used may aid comparison of studies and meta-analysis. If data were grouped using conventional cut-points, such as body mass index thresholds [162], group boundaries (i.e., range of values) can be derived easily, except for the highest and lowest categories. If quantile-derived categories are used, the category boundaries cannot be inferred from the data. As a minimum, authors should report the category boundaries; it is helpful also to report the range of the data and the mean or median values within categories."
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-16b_main_results-2",
    "href": "guidelines/strobe-case-control/index.html#sec-16b_main_results-2",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Examples",
    "text": "Examples\n\nTable. Polychlorinated Biphenyls in Cord Serum\n\nhttps://doi.org/10.1371/journal.pmed.0040297.t005\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-16c_main_results-1",
    "href": "guidelines/strobe-case-control/index.html#sec-16c_main_results-1",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Read More",
    "text": "Read More\nThe results from studies examining the association between an exposure and a disease are commonly reported in relative terms, as ratios of risks, rates or odds (see Box 8). Relative measures capture the strength of the association between an exposure and disease. If the relative risk is a long way from 1 it is less likely that the association is due to confounding [164,165]. Relative effects or associations tend to be more consistent across studies and populations than absolute measures, but what often tends to be the case may be irrelevant in a particular instance. For example, similar relative risks were obtained for the classic cardiovascular risk factors for men living in Northern Ireland, France, the USA and Germany, despite the fact that the underlying risk of coronary heart disease varies substantially between these countries [166,167]. In contrast, in a study of hypertension as a risk factor for cardiovascular disease mortality, the data were more compatible with a constant rate difference than with a constant rate ratio [168].\nWidely used statistical models, including logistic [169] and proportional hazards (Cox) regression [170] are based on ratio measures. In these models, only departures from constancy of ratio effect measures are easily discerned. Nevertheless, measures which assess departures from additivity of risk differences, such as the Relative Excess Risk from Interaction (RERI, see item 12b and Box 8), can be estimated in models based on ratio measures.\nIn many circumstances, the absolute risk associated with an exposure is of greater interest than the relative risk. For example, if the focus is on adverse effects of a drug, one will want to know the number of additional cases per unit time of use (e.g., days, weeks, or years). The example gives the additional number of breast cancer cases per 1000 women who used hormone-replacement therapy for 10 years [163]. Measures such as the attributable risk or population attributable fraction may be useful to gauge how much disease can be prevented if the exposure is eliminated. They should preferably be presented together with a measure of statistical uncertainty (e.g., confidence intervals as in the example). Authors should be aware of the strong assumptions made in this context, including a causal relationship between a risk factor and disease (also see Box 7) [171]. Because of the semantic ambiguity and complexities involved, authors should report in detail what methods were used to calculate attributable risks, ideally giving the formulae used [172].\nA recent survey of abstracts of 222 articles published in leading medical journals found that in 62% of abstracts of randomised trials including a ratio measure absolute risks were given, but only in 21% of abstracts of cohort studies [173]. A free text search of Medline 1966 to 1997 showed that 619 items mentioned attributable risks in the title or abstract, compared to 18,955 using relative risk or odds ratio, for a ratio of 1 to 31 [174]."
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-16c_main_results-2",
    "href": "guidelines/strobe-case-control/index.html#sec-16c_main_results-2",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Examples",
    "text": "Examples\n\n10 years’ use of HRT [hormone replacement therapy] is estimated to result in five (95% CI 3–7) additional breast cancers per 1000 users of oestrogen-only preparations and 19 (15–23) additional cancers per 1000 users of oestrogen-progestagen combinations\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-17_other_analyses-1",
    "href": "guidelines/strobe-case-control/index.html#sec-17_other_analyses-1",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Read More",
    "text": "Read More\nIn addition to the main analysis other analyses are often done in observational studies. They may address specific subgroups, the potential interaction between risk factors, the calculation of attributable risks, or use alternative definitions of study variables in sensitivity analyses.\nThere is debate about the dangers associated with subgroup analyses, and multiplicity of analyses in general [4,104]. In our opinion, there is too great a tendency to look for evidence of subgroup-specific associations, or effect-measure modification, when overall results appear to suggest little or no effect. On the other hand, there is value in exploring whether an overall association appears consistent across several, preferably pre-specified subgroups especially when a study is large enough to have sufficient data in each subgroup. A second area of debate is about interesting subgroups that arose during the data analysis. They might be important findings, but might also arise by chance. Some argue that it is neither possible nor necessary to inform the reader about all subgroup analyses done as future analyses of other data will tell to what extent the early exciting findings stand the test of time [9]. We advise authors to report which analyses were planned, and which were not (see also items 4, 12b and 20). This will allow readers to judge the implications of multiplicity, taking into account the study’s position on the continuum from discovery to verification or refutation.\nA third area of debate is how joint effects and interactions between risk factors should be evaluated: on additive or multiplicative scales, or should the scale be determined by the statistical model that fits best (see also item 12b and Box 8)? A sensible approach is to report the separate effect of each exposure as well as the joint effect—if possible in a table, as in the first example above [183], or in the study by Martinelli et al. [185]. Such a table gives the reader sufficient information to evaluate additive as well as multiplicative interaction (how these calculations are done is shown in Box 8). Confidence intervals for separate and joint effects may help the reader to judge the strength of the data. In addition, confidence intervals around measures of interaction, such as the Relative Excess Risk from Interaction (RERI) relate to tests of interaction or homogeneity tests. One recurrent problem is that authors use comparisons of P-values across subgroups, which lead to erroneous claims about an effect modifier. For instance, a statistically significant association in one category (e.g., men), but not in the other (e.g., women) does not in itself provide evidence of effect modification. Similarly, the confidence intervals for each point estimate are sometimes inappropriately used to infer that there is no interaction when intervals overlap. A more valid inference is achieved by directly evaluating whether the magnitude of an association differs across subgroups.\nSensitivity analyses are helpful to investigate the influence of choices made in the statistical analysis, or to investigate the robustness of the findings to missing data or possible biases (see also item 12b). Judgement is needed regarding the level of reporting of such analyses. If many sensitivity analyses were performed, it may be impractical to present detailed findings for them all. It may sometimes be sufficient to report that sensitivity analyses were carried out and that they were consistent with the main results presented. Detailed presentation is more appropriate if the issue investigated is of major concern, or if effect estimates vary considerably [59,186].\nPocock and colleagues found that 43 out of 73 articles reporting observational studies contained subgroup analyses. The majority claimed differences across groups but only eight articles reported a formal evaluation of interaction (see item 12b) [4]."
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-17_other_analyses-2",
    "href": "guidelines/strobe-case-control/index.html#sec-17_other_analyses-2",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Examples",
    "text": "Examples\n\nTable. Analysis of Oral Contraceptive Use, Presence of Factor V Leiden Allele, and Risk for Venous Thromboembolism\n\nhttps://doi.org/10.1371/journal.pmed.0040297.t009\n\nTable. Sensitivity of the Rate Ratio for Cardiovascular Outcome to an Unmeasured Confounder https://doi.org/10.1371/journal.pmed.0040297.t010\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-18_key_results-1",
    "href": "guidelines/strobe-case-control/index.html#sec-18_key_results-1",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Read More",
    "text": "Read More\nIt is good practice to begin the discussion with a short summary of the main findings of the study. The short summary reminds readers of the main findings and may help them assess whether the subsequent interpretation and implications offered by the authors are supported by the findings"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-18_key_results-2",
    "href": "guidelines/strobe-case-control/index.html#sec-18_key_results-2",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Examples",
    "text": "Examples\n\nWe hypothesized that ethnic minority status would be associated with higher levels of cardiovascular disease (CVD) risk factors, but that the associations would be explained substantially by socioeconomic status (SES). Our hypothesis was not confirmed. After adjustment for age and SES, highly significant differences in body mass index, blood pressure, diabetes, and physical inactivity remained between white women and both black and Mexican American women. In addition, we found large differences in CVD risk factors by SES, a finding that illustrates the high-risk status of both ethnic minority women as well as white women with low SES\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-19_limitations-1",
    "href": "guidelines/strobe-case-control/index.html#sec-19_limitations-1",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Read More",
    "text": "Read More\nThe identification and discussion of the limitations of a study are an essential part of scientific reporting. It is important not only to identify the sources of bias and confounding that could have affected results, but also to discuss the relative importance of different biases, including the likely direction and magnitude of any potential bias (see also item 9 and Box 3).\nAuthors should also discuss any imprecision of the results. Imprecision may arise in connection with several aspects of a study, including the study size (item 10) and the measurement of exposures, confounders and outcomes (item 8). The inability to precisely measure true values of an exposure tends to result in bias towards unity: the less precisely a risk factor is measured, the greater the bias. This effect has been described as attenuation [201,202], or more recently as regression dilution bias [203]. However, when correlated risk factors are measured with different degrees of imprecision, the adjusted relative risk associated with them can be biased towards or away from unity [204–206].\nWhen discussing limitations, authors may compare the study being presented with other studies in the literature in terms of validity, generalisability and precision. In this approach, each study can be viewed as contribution to the literature, not as a stand-alone basis for inference and action [207]. Surprisingly, the discussion of important limitations of a study is sometimes omitted from published reports. A survey of authors who had published original research articles in The Lancet found that important weaknesses of the study were reported by the investigators in the survey questionnaires, but not in the published article"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-19_limitations-2",
    "href": "guidelines/strobe-case-control/index.html#sec-19_limitations-2",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Examples",
    "text": "Examples\n\nSince the prevalence of counseling increases with increasing levels of obesity, our estimates may overestimate the true prevalence. Telephone surveys also may overestimate the true prevalence of counseling. Although persons without telephones have similar levels of overweight as persons with telephones, persons without telephones tend to be less educated, a factor associated with lower levels of counseling in our study. Also, of concern is the potential bias caused by those who refused to participate as well as those who refused to respond to questions about weight. Furthermore, because data were collected cross-sectionally, we cannot infer that counseling preceded a patient’s attempt to lose weight\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-20_interpretation-1",
    "href": "guidelines/strobe-case-control/index.html#sec-20_interpretation-1",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Read More",
    "text": "Read More\nThe heart of the discussion section is the interpretation of a study’s results. Over-interpretation is common and human: even when we try hard to give an objective assessment, reviewers often rightly point out that we went too far in some respects. When interpreting results, authors should consider the nature of the study on the discovery to verification continuum and potential sources of bias, including loss to follow-up and non-participation (see also items 9, 12 and 19). Due consideration should be given to confounding (item 16a), the results of relevant sensitivity analyses, and to the issue of multiplicity and subgroup analyses (item 17). Authors should also consider residual confounding due to unmeasured variables or imprecise measurement of confounders. For example, socioeconomic status (SES) is associated with many health outcomes and often differs between groups being compared. Variables used to measure SES (income, education, or occupation) are surrogates for other undefined and unmeasured exposures, and the true confounder will by definition be measured with error [208]. Authors should address the real range of uncertainty in estimates, which is larger than the statistical uncertainty reflected in confidence intervals. The latter do not take into account other uncertainties that arise from a study’s design, implementation, and methods of measurement [209].\nTo guide thinking and conclusions about causality, some may find criteria proposed by Bradford Hill in 1965 helpful [164]. How strong is the association with the exposure? Did it precede the onset of disease? Is the association consistently observed in different studies and settings? Is there supporting evidence from experimental studies, including laboratory and animal studies? How specific is the exposure’s putative effect, and is there a dose-response relationship? Is the association biologically plausible? These criteria should not, however, be applied mechanically. For example, some have argued that relative risks below 2 or 3 should be ignored [210,211]. This is a reversal of the point by Cornfield et al. about the strength of large relative risks (see item 12b) [127]. Although a causal effect is more likely with a relative risk of 9, it does not follow that one below 3 is necessarily spurious. For instance, the small increase in the risk of childhood leukaemia after intrauterine irradiation is credible because it concerns an adverse effect of a medical procedure for which no alternative explanations are obvious [212]. Moreover, the carcinogenic effects of radiation are well established. The doubling in the risk of ovarian cancer associated with eating 2 to 4 eggs per week is not immediately credible, since dietary habits are associated with a large number of lifestyle factors as well as SES [213]. In contrast, the credibility of much debated epidemiologic findings of a difference in thrombosis risk between different types of oral contraceptives was greatly enhanced by the differences in coagulation found in a randomised cross-over trial [214]. A discussion of the existing external evidence, from different types of studies, should always be included, but may be particularly important for studies reporting small increases in risk. Further, authors should put their results in context with similar studies and explain how the new study affects the existing body of evidence, ideally by referring to a systematic review."
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-20_interpretation-2",
    "href": "guidelines/strobe-case-control/index.html#sec-20_interpretation-2",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Examples",
    "text": "Examples\n\nAny explanation for an association between death from myocardial infarction and use of second generation oral contraceptives must be conjectural. There is no published evidence to suggest a direct biologic mechanism, and there are no other epidemiologic studies with relevant results. (…) The increase in absolute risk is very small and probably applies predominantly to smokers. Due to the lack of corroborative evidence, and because the analysis is based on relatively small numbers, more evidence on the subject is needed. We would not recommend any change in prescribing practice on the strength of these results\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-21_generalisability-1",
    "href": "guidelines/strobe-case-control/index.html#sec-21_generalisability-1",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Read More",
    "text": "Read More\nGeneralisability, also called external validity or applicability, is the extent to which the results of a study can be applied to other circumstances [216]. There is no external validity per se; the term is meaningful only with regard to clearly specified conditions [217]. Can results be applied to an individual, groups or populations that differ from those enrolled in the study with regard to age, sex, ethnicity, severity of disease, and co-morbid conditions? Are the nature and level of exposures comparable, and the definitions of outcomes relevant to another setting or population? Are data that were collected in longitudinal studies many years ago still relevant today? Are results from health services research in one country applicable to health systems in other countries?\nThe question of whether the results of a study have external validity is often a matter of judgment that depends on the study setting, the characteristics of the participants, the exposures examined, and the outcomes assessed. Thus, it is crucial that authors provide readers with adequate information about the setting and locations, eligibility criteria, the exposures and how they were measured, the definition of outcomes, and the period of recruitment and follow-up. The degree of non-participation and the proportion of unexposed participants in whom the outcome develops are also relevant. Knowledge of the absolute risk and prevalence of the exposure, which will often vary across populations, are helpful when applying results to other settings and populations (see Box 7)"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-21_generalisability-2",
    "href": "guidelines/strobe-case-control/index.html#sec-21_generalisability-2",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Examples",
    "text": "Examples\n\nHow applicable are our estimates to other HIV-1-infected patients? This is an important question because the accuracy of prognostic models tends to be lower when applied to data other than those used to develop them. We addressed this issue by penalising model complexity, and by choosing models that generalised best to cohorts omitted from the estimation procedure. Our database included patients from many countries from Europe and North America, who were treated in different settings. The range of patients was broad: men and women, from teenagers to elderly people were included, and the major exposure categories were well represented. The severity of immunodeficiency at baseline ranged from not measureable to very severe, and viral load from undetectable to extremely high\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-case-control/index.html#sec-22_funding-1",
    "href": "guidelines/strobe-case-control/index.html#sec-22_funding-1",
    "title": "The STROBE case-control guideline for writing a Case-control study.",
    "section": "Read More",
    "text": "Read More\nSome journals require authors to disclose the presence or absence of financial and other conflicts of interest [100,218]. Several investigations show strong associations between the source of funding and the conclusions of research articles [219–222]. The conclusions in randomised trials recommended the experimental drug as the drug of choice much more often (odds ratio 5.3) if the trial was funded by for-profit organisations, even after adjustment for the effect size [223]. Other studies document the influence of the tobacco and telecommunication industries on the research they funded [224–227]. There are also examples of undue influence when the sponsor is governmental or a non-profit organisation.\nAuthors or funders may have conflicts of interest that influence any of the following: the design of the study [228]; choice of exposures [228,229], outcomes [230], statistical methods [231], and selective publication of outcomes [230] and studies [232]. Consequently, the role of the funders should be described in detail: in what part of the study they took direct responsibility (e.g., design, data collection, analysis, drafting of manuscript, decision to publish) [100]. Other sources of undue influence include employers (e.g., university administrators for academic researchers and government supervisors, especially political appointees, for government researchers), advisory committees, litigants, and special interest groups"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#about-this-guideline",
    "href": "guidelines/strobe-cohort/index.html#about-this-guideline",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "About this guideline",
    "text": "About this guideline\n\nThis guideline is relevant to studies reporting cohort studies"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#download-resources",
    "href": "guidelines/strobe-cohort/index.html#download-resources",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Download Resources",
    "text": "Download Resources\nReporting guidelines are best used early:\n\nUse a template to write quickly and avoid the blank page\nUse a checklist to double-check and reassure others that your work is described transparently\n\n\n\n\nFor drafting\n\nTemplate\n\n\n\nFor checking\n\nChecklist\n\n\n\nFor planning\n\nTo-do list"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#guidance",
    "href": "guidelines/strobe-cohort/index.html#guidance",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Guidance",
    "text": "Guidance\nApprox. 2 min read\n\nTitle and abstract\n\n\n1a. Title\n\n\n\n\n\n\n\nIndicate the study’s design with a commonly used term in the title or the abstract\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nReaders should be able to easily identify the design that was used from the title or abstract. An explicit, commonly used term for the study design also helps ensure correct indexing of articles in electronic databases\n\n\nExamples\n\nLeukaemia incidence among workers in the shoe and boot manufacturing industry: a case-control study\n\nBack to top\n\n\n\n\n\n\n1b. Abstract\n\n\n\n\n\n\n\nProvide in the abstract an informative and balanced summary of what was done and what was found\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe abstract provides key information that enables readers to understand a study and decide whether to read the article. Typical components include a statement of the research question, a short description of methods and results, and a conclusion. Abstracts should summarize key details of studies and should only present information that is provided in the article. We advise presenting key results in a numerical form that includes numbers of participants, estimates of associations and appropriate measures of variability and uncertainty (e.g., odds ratios with confidence intervals). We regard it insufficient to state only that an exposure is or is not significantly associated with an outcome.\nA series of headings pertaining to the background, design, conduct, and analysis of a study may help readers acquire the essential information rapidly. Many journals require such structured abstracts, which tend to be of higher quality and more readily informative than unstructured summaries.\n\n\nExamples\n\nBackground: The expected survival of HIV-infected patients is of major public health interest.\n\nObjective: To estimate survival time and age-specific mortality rates of an HIV-infected population compared with that of the general population.\nDesign: Population-based cohort study.\nSetting: All HIV-infected persons receiving care in Denmark from 1995 to 2005.\nPatients: Each member of the nationwide Danish HIV Cohort Study was matched with as many as 99 persons from the general population according to sex, date of birth, and municipality of residence.\nMeasurements: The authors computed Kaplan-Meier life tables with age as the time scale to estimate survival from age 25 years. Patients with HIV infection and corresponding persons from the general population were observed from the date of the patient’s HIV diagnosis until death, emigration, or 1 May 2005.\nResults: 3990 HIV-infected patients and 379,872 persons from the general population were included in the study, yielding 22,744 (median, 5.8 y/person) and 2,689,287 (median, 8.4 years/person) person-years of observation. Three percent of participants were lost to follow-up. From age 25 years, the median survival was 19.9 years (95% CI, 18.5 to 21.3) among patients with HIV infection and 51.1 years (CI, 50.9 to 51.5) among the general population. For HIV-infected patients, survival increased to 32.5 years (CI, 29.4 to 34.7) during the 2000 to 2005 period. In the subgroup that excluded persons with known hepatitis C coinfection (16%), median survival was 38.9 years (CI, 35.4 to 40.1) during this same period. The relative mortality rates for patients with HIV infection compared with those for the general population decreased with increasing age, whereas the excess mortality rate increased with increasing age.\nLimitations: The observed mortality rates are assumed to apply beyond the current maximum observation time of 10 years.\nConclusions: The estimated median survival is more than 35 years for a young person diagnosed with HIV infection in the late highly active antiretroviral therapy era. However, an ongoing effort is still needed to further reduce mortality rates for these persons compared with the general population\nBack to top\n\n\n\n\n\n\nIntroduction\n\n\n2. Background / rationale\n\n\n\n\n\n\n\nExplain the scientific background and rationale for the investigation being reported\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe scientific background of the study provides important context for readers. It sets the stage for the study and describes its focus. It gives an overview of what is known on a topic and what gaps in current knowledge are addressed by the study. Background material should note recent pertinent studies and any systematic reviews of pertinent studies.\n\n\nExamples\n\nConcerns about the rising prevalence of obesity in children and adolescents have focused on the well documented associations between childhood obesity and increased cardiovascular risk1 and mortality in adulthood.2 Childhood obesity has considerable social and psychological consequences within childhood and adolescence,3 yet little is known about social, socioeconomic, and psychological consequences in adult life.\n\nA recent systematic review found no longitudinal studies on the outcomes of childhood obesity other than physical health outcomes3 and only two longitudinal studies of the socioeconomic effects of obesity in adolescence. Gortmaker et al found that US women who had been obese in late adolescence in 1981 were less likely to be married and had lower incomes seven years later than women who had not been overweight, while men who had been overweight were less likely to be married.4 Sargent et al found that UK women, but not men, who had been obese at 16 years in 1974 earned 7.4% less than their non-obese peers at age 23.5\nThe study of adult outcomes of childhood obesity is difficult because obesity often continues into adult life and therefore poorer socioeconomic and educational outcomes may actually reflect confounding by adult obesity. Yet identifying outcomes related to obesity confined childhood is important in determining whether people who are obese in childhood and who later lose weight remain at risk for adult adversity and inequalities.\nWe used longitudinal data from the 1970 British birth cohort to examine the adult socioeconomic, educational, social, and psychological outcomes of childhood obesity. We hypothesised that obesity limited to childhood has fewer adverse adult outcomes than obesity that persists into adult life\nBack to top\n\n\n\n\n\n\n3. Objectives\n\n\n\n\n\n\n\nState specific objectives, including any prespecified hypotheses\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nObjectives are the detailed aims of the study. Well crafted objectives specify populations, exposures and outcomes, and parameters that will be estimated. They may be formulated as specific hypotheses or as questions that the study was designed to address. In some situations objectives may be less specific, for example, in early discovery phases. Regardless, the report should clearly reflect the investigators’ intentions. For example, if important subgroups or additional analyses were not the original aim of the study but arose during data analysis, they should be described accordingly (see also items 4, 17 and 20).\n\n\nExamples\n\nOur primary objectives were to 1) determine the prevalence of domestic violence among female patients presenting to four community-based, primary care, adult medicine practices that serve patients of diverse socioeconomic background and 2) identify demographic and clinical differences between currently abused patients and patients not currently being abused\n\nBack to top\n\n\n\n\n\n\nMethods\n\n\n4. Study design\n\n\n\n\n\n\n\nPresent key elements of study design early in the paper\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nWe advise presenting key elements of study design early in the methods section (or at the end of the introduction) so that readers can understand the basics of the study. For example, authors should indicate that the study was a cohort study, which followed people over a particular time period, and describe the group of persons that comprised the cohort and their exposure status. Similarly, if the investigation used a case-control design, the cases and controls and their source population should be described. If the study was a cross-sectional survey, the population and the point in time at which the cross-section was taken should be mentioned. When a study is a variant of the three main study types, there is an additional need for clarity. For instance, for a case-crossover study, one of the variants of the case-control design, a succinct description of the principles was given in the example above [28].\nWe recommend that authors refrain from simply calling a study prospective or retrospective because these terms are ill defined [29]. One usage sees cohort and prospective as synonymous and reserves the word retrospective for case-control studies [30]. A second usage distinguishes prospective and retrospective cohort studies according to the timing of data collection relative to when the idea for the study was developed [31]. A third usage distinguishes prospective and retrospective case-control studies depending on whether the data about the exposure of interest existed when cases were selected [32]. Some advise against using these terms [33], or adopting the alternatives concurrent and historical for describing cohort studies [34]. In STROBE, we do not use the words prospective and retrospective, nor alternatives such as concurrent and historical. We recommend that, whenever authors use these words, they define what they mean. Most importantly, we recommend that authors describe exactly how and when data collection took place.\nThe first part of the methods section might also be the place to mention whether the report is one of several from a study. If a new report is in line with the original aims of the study, this is usually indicated by referring to an earlier publication and by briefly restating the salient features of the study. However, the aims of a study may also evolve over time. Researchers often use data for purposes for which they were not originally intended, including, for example, official vital statistics that were collected primarily for administrative purposes, items in questionnaires that originally were only included for completeness, or blood samples that were collected for another purpose. For example, the Physicians’ Health Study, a randomized controlled trial of aspirin and carotene, was later used to demonstrate that a point mutation in the factor V gene was associated with an increased risk of venous thrombosis, but not of myocardial infarction or stroke [35]. The secondary use of existing data is a creative part of observational research and does not necessarily make results less credible or less important. However, briefly restating the original aims might help readers understand the context of the research and possible limitations in the data.\n\n\nExamples\n\nWe used a case-crossover design, a variation of a case-control design that is appropriate when a brief exposure (driver’s phone use) causes a transient rise in the risk of a rare outcome (a crash). We compared a driver’s use of a mobile phone at the estimated time of a crash with the same driver’s use during another suitable time period. Because drivers are their own controls, the design controls for characteristics of the driver that may affect the risk of a crash but do not change over a short period of time. As it is important that risks during control periods and crash trips are similar, we compared phone activity during the hazard interval (time immediately before the crash) with phone activity during control intervals (equivalent times during which participants were driving but did not crash) in the previous week\n\nBack to top\n\n\n\n\n\n\n5. Setting\n\n\n\n\n\n\n\nDescribe the setting, locations, and relevant dates, including periods of recruitment, exposure, follow-up, and data collection\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nReaders need information on setting and locations to assess the context and generalisability of a study’s results. Exposures such as environmental factors and therapies can change over time. Also, study methods may evolve over time. Knowing when a study took place and over what period participants were recruited and followed up places the study in historical context and is important for the interpretation of results.\nInformation about setting includes recruitment sites or sources (e.g., electoral roll, outpatient clinic, cancer registry, or tertiary care centre). Information about location may refer to the countries, towns, hospitals or practices where the investigation took place. We advise stating dates rather than only describing the length of time periods. There may be different sets of dates for exposure, disease occurrence, recruitment, beginning and end of follow-up, and data collection. Of note, nearly 80% of 132 reports in oncology journals that used survival analysis included the starting and ending dates for accrual of patients, but only 24% also reported the date on which follow-up ended [37].\n\n\nExamples\n\nThe Pasitos Cohort Study recruited pregnant women from Women, Infant and Child (WIC) clinics in Socorro and San Elizario, El Paso County, Texas and maternal-child clinics of the Mexican Social Security Institute (IMSS) in Ciudad Juarez, Mexico from April 1998 to October 2000. At baseline, prior to the birth of the enrolled cohort children, staff interviewed mothers regarding the household environment. In this ongoing cohort study, we target follow-up exams at 6-month intervals beginning at age 6 months\n\nBack to top\n\n\n\n\n\n\n6a. Eligibility criteria\n\n\n\n\n\n\n\nGive the eligibility criteria, and the sources and methods of selection of participants. Describe methods of follow-up.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nDetailed descriptions of the study participants help readers understand the applicability of the results. Investigators usually restrict a study population by defining clinical, demographic and other characteristics of eligible participants. Typical eligibility criteria relate to age, gender, diagnosis and comorbid conditions. Despite their importance, eligibility criteria often are not reported adequately. In a survey of observational stroke research, 17 of 49 reports (35%) did not specify eligibility criteria [5].\nEligibility criteria may be presented as inclusion and exclusion criteria, although this distinction is not always necessary or useful. Regardless, we advise authors to report all eligibility criteria and also to describe the group from which the study population was selected (e.g., the general population of a region or country), and the method of recruitment (e.g., referral or self-selection through advertisements).\nKnowing details about follow-up procedures, including whether procedures minimized non-response and loss to follow-up and whether the procedures were similar for all participants, informs judgments about the validity of results. For example, in a study that used IgM antibodies to detect acute infections, readers needed to know the interval between blood tests for IgM antibodies so that they could judge whether some infections likely were missed because the interval between blood tests was too long [41]. In other studies where follow-up procedures differed between exposed and unexposed groups, readers might recognize substantial bias due to unequal ascertainment of events or differences in non-response or loss to follow-up [42]. Accordingly, we advise that researchers describe the methods used for following participants and whether those methods were the same for all participants, and that they describe the completeness of ascertainment of variables (see also item 14).\nIn case-control studies, the choice of cases and controls is crucial to interpreting the results, and the method of their selection has major implications for study validity. In general, controls should reflect the population from which the cases arose. Various methods are used to sample controls, all with advantages and disadvantages: for cases that arise from a general population, population roster sampling, random digit dialling, neighbourhood or friend controls are used. Neighbourhood or friend controls may present intrinsic matching on exposure [17]. Controls with other diseases may have advantages over population-based controls, in particular for hospital-based cases, because they better reflect the catchment population of a hospital, have greater comparability of recall and ease of recruitment. However, they can present problems if the exposure of interest affects the risk of developing or being hospitalized for the control condition(s) [43,44]. To remedy this problem often a mixture of the best defensible control diseases is used [45].\n\n\nExamples\n\nParticipants in the Iowa Women’s Health Study were a random sample of all women ages 55 to 69 years derived from the state of Iowa automobile driver’s license list in 1985, which represented approximately 94% of Iowa women in that age group. (…) Follow-up questionnaires were mailed in October 1987 and August 1989 to assess vital status and address changes. (…) Incident cancers, except for nonmelanoma skin cancers, were ascertained by the State Health Registry of Iowa (…). The Iowa Women’s Health Study cohort was matched to the registry with combinations of first, last, and maiden names, zip code, birthdate, and social security number\n\nBack to top\n\n\n\n\n\n\n6b. Eligibility criteria\n\n\n\n\n\n\n\nFor matched studies, give matching criteria and number of exposed and unexposed\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nMatching is much more common in case-control studies, but occasionally, investigators use matching in cohort studies to make groups comparable at the start of follow-up. Matching in cohort studies makes groups directly comparable for potential confounders and presents fewer intricacies than with case-control studies. For example, it is not necessary to take the matching into account for the estimation of the relative risk [48]. Because matching in cohort studies may increase statistical precision investigators might allow for the matching in their analyses and thus obtain narrower confidence intervals.\nIn case-control studies matching is done to increase a study’s efficiency by ensuring similarity in the distribution of variables between cases and controls, in particular the distribution of potential confounding variables [48,49]. Because matching can be done in various ways, with one or more controls per case, the rationale for the choice of matching variables and the details of the method used should be described. Commonly used forms of matching are frequency matching (also called group matching) and individual matching. In frequency matching, investigators choose controls so that the distribution of matching variables becomes identical or similar to that of cases. Individual matching involves matching one or several controls to each case. Although intuitively appealing and sometimes useful, matching in case-control studies has a number of disadvantages, is not always appropriate, and needs to be taken into account in the analysis\n\n\nExamples\n\nFor each patient who initially received a statin, we used propensity-based matching to identify one control who did not receive a statin according to the following protocol. First, propensity scores were calculated for each patient in the entire cohort on the basis of an extensive list of factors potentially related to the use of statins or the risk of sepsis. Second, each statin user was matched to a smaller pool of non-statin-users by sex, age (plus or minus 1 year), and index date (plus or minus 3 months). Third, we selected the control with the closest propensity score (within 0.2 SD) to each statin user in a 1:1 fashion and discarded the remaining controls\n\nBack to top\n\n\n\n\n\n\n7. Variables\n\n\n\n\n\n\n\nClearly define all outcomes, exposures, predictors, potential confounders, and effect modifiers. Give diagnostic criteria, if applicable\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAuthors should define all variables considered for and included in the analysis, including outcomes, exposures, predictors, potential confounders and potential effect modifiers. Disease outcomes require adequately detailed description of the diagnostic criteria. This applies to criteria for cases in a case-control study, disease events during follow-up in a cohort study and prevalent disease in a cross-sectional study. Clear definitions and steps taken to adhere to them are particularly important for any disease condition of primary interest in the study.\nFor some studies, determinant or predictor may be appropriate terms for exposure variables and outcomes may be called endpoints. In multivariable models, authors sometimes use dependent variable for an outcome and independent variable or explanatory variable for exposure and confounding variables. The latter is not precise as it does not distinguish exposures from confounders.\nIf many variables have been measured and included in exploratory analyses in an early discovery phase, consider providing a list with details on each variable in an appendix, additional table or separate publication. Of note, the International Journal of Epidemiology recently launched a new section with cohort profiles, that includes detailed information on what was measured at different points in time in particular studies [56,57]. Finally, we advise that authors declare all candidate variables considered for statistical analysis, rather than selectively reporting only those included in the final models (see also item 16a) [58,59]\n\n\nExamples\n\nOnly major congenital malformations were included in the analyses. Minor anomalies were excluded according to the exclusion list of European Registration of Congenital Anomalies (EUROCAT). If a child had more than one major congenital malformation of one organ system, those malformations were treated as one outcome in the analyses by organ system (…) In the statistical analyses, factors considered potential confounders were maternal age at delivery and number of previous parities. Factors considered potential effect modifiers were maternal age at reimbursement for antiepileptic medication and maternal age at delivery\n\nBack to top\n\n\n\n\n\n\n8. Data sources / measurement\n\n\n\n\n\n\n\nFor each variable of interest give sources of data and details of methods of assessment (measurement). Describe comparability of assessment methods if there is more than one group. Give information separately for for exposed and unexposed groups if applicable.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe way in which exposures, confounders and outcomes were measured affects the reliability and validity of a study. Measurement error and misclassification of exposures or outcomes can make it more difficult to detect cause-effect relationships, or may produce spurious relationships. Error in measurement of potential confounders can increase the risk of residual confounding [62,63]. It is helpful, therefore, if authors report the findings of any studies of the validity or reliability of assessments or measurements, including details of the reference standard that was used. Rather than simply citing validation studies (as in the first example), we advise that authors give the estimated validity or reliability, which can then be used for measurement error adjustment or sensitivity analyses (see items 12e and 17).\nIn addition, it is important to know if groups being compared differed with respect to the way in which the data were collected. This may be important for laboratory examinations (as in the second example) and other situations. For instance, if an interviewer first questions all the cases and then the controls, or vice versa, bias is possible because of the learning curve; solutions such as randomising the order of interviewing may avoid this problem. Information bias may also arise if the compared groups are not given the same diagnostic tests or if one group receives more tests of the same kind than another (see also item 9).\n\n\nExamples\n\nTotal caffeine intake was calculated primarily using US Department of Agriculture food composition sources. In these calculations, it was assumed that the content of caffeine was 137 mg per cup of coffee, 47 mg per cup of tea, 46 mg per can or bottle of cola beverage, and 7 mg per serving of chocolate candy. This method of measuring (caffeine) intake was shown to be valid in both the NHS I cohort and a similar cohort study of male health professionals (…) Self-reported diagnosis of hypertension was found to be reliable in the NHS I cohort\n\n\nSamples pertaining to matched cases and controls were always analyzed together in the same batch and laboratory personnel were unable to distinguish among cases and controls\n\nBack to top\n\n\n\n\n\n\n9. Bias\n\n\n\n\n\n\n\nDescribe any efforts to address potential sources of bias\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nBiased studies produce results that differ systematically from the truth (see also Box 3). It is important for a reader to know what measures were taken during the conduct of a study to reduce the potential of bias. Ideally, investigators carefully consider potential sources of bias when they plan their study. At the stage of reporting, we recommend that authors always assess the likelihood of relevant biases. Specifically, the direction and magnitude of bias should be discussed and, if possible, estimated. For instance, in case-control studies information bias can occur, but may be reduced by selecting an appropriate control group, as in the first example [64]. Differences in the medical surveillance of participants were a problem in the second example [65]. Consequently, the authors provide more detail about the additional data they collected to tackle this problem. When investigators have set up quality control programs for data collection to counter a possible drift in measurements of variables in longitudinal studies, or to keep variability at a minimum when multiple observers are used, these should be described.\nUnfortunately, authors often do not address important biases when reporting their results. Among 43 case-control and cohort studies published from 1990 to 1994 that investigated the risk of second cancers in patients with a history of cancer, medical surveillance bias was mentioned in only 5 articles [66]. A survey of reports of mental health research published during 1998 in three psychiatric journals found that only 13% of 392 articles mentioned response bias [67]. A survey of cohort studies in stroke research found that 14 of 49 (28%) articles published from 1999 to 2003 addressed potential selection bias in the recruitment of study participants and 35 (71%) mentioned the possibility that any type of bias may have affected results\n\n\nExamples\n\nDetection bias could influence the association between Type 2 diabetes mellitus (T2DM) and primary open-angle glaucoma (POAG) if women with T2DM were under closer ophthalmic surveillance than women without this condition. We compared the mean number of eye examinations reported by women with and without diabetes. We also recalculated the relative risk for POAG with additional control for covariates associated with more careful ocular surveillance (a self-report of cataract, macular degeneration, number of eye examinations, and number of physical examinations)ave recently experienced the death of a family member or close associate - and are therefore more comparable to the sources of information in the suicide group than if living controls were used\n\nBack to top\n\n\n\n\n\n\n10. Study size\n\n\n\n\n\n\n\nExplain how the study size was arrived at\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nA study should be large enough to obtain a point estimate with a sufficiently narrow confidence interval to meaningfully answer a research question. Large samples are needed to distinguish a small association from no association. Small studies often provide valuable information, but wide confidence intervals may indicate that they contribute less to current knowledge in comparison with studies providing estimates with narrower confidence intervals. Also, small studies that show interesting or statistically significant associations are published more frequently than small studies that do not have significant findings. While these studies may provide an early signal in the context of discovery, readers should be informed of their potential weaknesses.\nThe importance of sample size determination in observational studies depends on the context. If an analysis is performed on data that were already available for other purposes, the main question is whether the analysis of the data will produce results with sufficient statistical precision to contribute substantially to the literature, and sample size considerations will be informal. Formal, a priori calculation of sample size may be useful when planning a new study. Such calculations are associated with more uncertainty than implied by the single number that is generally produced. For example, estimates of the rate of the event of interest or other assumptions central to calculations are commonly imprecise, if not guesswork. The precision obtained in the final analysis can often not be determined beforehand because it will be reduced by inclusion of confounding variables in multivariable analyses, the degree of precision with which key variables can be measured, and the exclusion of some individuals.\nFew epidemiological studies explain or report deliberations about sample size. We encourage investigators to report pertinent formal sample size calculations if they were done. In other situations they should indicate the considerations that determined the study size (e.g., a fixed available sample, as in the first example above). If the observational study was stopped early when statistical significance was achieved, readers should be told. Do not bother readers with post hoc justifications for study size or retrospective power calculations. From the point of view of the reader, confidence intervals indicate the statistical precision that was ultimately obtained. It should be realized that confidence intervals reflect statistical uncertainty only, and not all uncertainty that may be present in a study (see item 20).\n\n\n\n\n\n\n11. Quantitative variables\n\n\n\n\n\n\n\nExplain how quantitative variables were handled in the analyses. If applicable, describe which groupings were chosen, and why\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nInvestigators make choices regarding how to collect and analyse quantitative data about exposures, effect modifiers and confounders. For example, they may group a continuous exposure variable to create a new categorical variable. Grouping choices may have important consequences for later analyses. We advise that authors explain why and how they grouped quantitative data, including the number of categories, the cut-points, and category mean or median values. Whenever data are reported in tabular form, the counts of cases, controls, persons at risk, person-time at risk, etc. should be given for each category. Tables should not consist solely of effect-measure estimates or results of model fitting.\nInvestigators might model an exposure as continuous in order to retain all the information. In making this choice, one needs to consider the nature of the relationship of the exposure to the outcome. As it may be wrong to assume a linear relation automatically, possible departures from linearity should be investigated. Authors could mention alternative models they explored during analyses (e.g., using log transformation, quadratic terms or spline functions). Several methods exist for fitting a non-linear relation between the exposure and outcome. Also, it may be informative to present both continuous and grouped analyses for a quantitative exposure of prime interest.\nIn a recent survey, two thirds of epidemiological publications studied quantitative exposure variables. In 42 of 50 articles (84%) exposures were grouped into several ordered categories, but often without any stated rationale for the choices made. Fifteen articles used linear associations to model continuous exposure but only two reported checking for linearity. In another survey, of the psychological literature, dichotomization was justified in only 22 of 110 articles (20%)\n\n\nExamples\n\nPatients with a Glasgow Coma Scale less than 8 are considered to be seriously injured. A GCS of 9 or more indicates less serious brain injury. We examined the association of GCS in these two categories with the occurrence of death within 12 months from injury\n\nBack to top\n\n\n\n\n\n\n12a. Statistical methods\n\n\n\n\n\n\n\nDescribe all statistical methods, including those used to control for confounding\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nIn general, there is no one correct statistical analysis but, rather, several possibilities that may address the same question, but make different assumptions. Regardless, investigators should pre-determine analyses at least for the primary study objectives in a study protocol. Often additional analyses are needed, either instead of, or as well as, those originally envisaged, and these may sometimes be motivated by the data. When a study is reported, authors should tell readers whether particular analyses were suggested by data inspection. Even though the distinction between pre-specified and exploratory analyses may sometimes be blurred, authors should clarify reasons for particular analyses.\nIf groups being compared are not similar with regard to some characteristics, adjustment should be made for possible confounding variables by stratification or by multivariable regression (see Box 5) [94]. Often, the study design determines which type of regression analysis is chosen. For instance, Cox proportional hazard regression is commonly used in cohort studies [95]. whereas logistic regression is often the method of choice in case-control studies [96,97]. Analysts should fully describe specific procedures for variable selection and not only present results from the final model [98,99]. If model comparisons are made to narrow down a list of potential confounders for inclusion in a final model, this process should be described. It is helpful to tell readers if one or two covariates are responsible for a great deal of the apparent confounding in a data analysis. Other statistical analyses such as imputation procedures, data transformation, and calculations of attributable risks should also be described. Non-standard or novel approaches should be referenced and the statistical software used reported. As a guiding principle, we advise statistical methods be described with enough detail to enable a knowledgeable reader with access to the original data to verify the reported results [100].\nIn an empirical study, only 93 of 169 articles (55%) reporting adjustment for confounding clearly stated how continuous and multi-category variables were entered into the statistical model [101]. Another study found that among 67 articles in which statistical analyses were adjusted for confounders, it was mostly unclear how confounders were chosen\n\n\nExamples\n\nThe adjusted relative risk was calculated using the Mantel-Haenszel technique, when evaluating if confounding by age or gender was present in the groups compared. The 95% confidence interval (CI) was computed around the adjusted relative risk, using the variance according to Greenland and Robins and Robins et al.\n\nBack to top\n\n\n\n\n\n\n12b. Statistical methods\n\n\n\n\n\n\n\nDescribe any methods used to examine subgroups and interactions\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAs discussed in detail under item 17, many debate the use and value of analyses restricted to subgroups of the study population [4,104]. Subgroup analyses are nevertheless often done [4]. Readers need to know which subgroup analyses were planned in advance, and which arose while analysing the data. Also, it is important to explain what methods were used to examine whether effects or associations differed across groups (see item 17).\nInteraction relates to the situation when one factor modifies the effect of another (therefore also called effect modification). The joint action of two factors can be characterized in two ways: on an additive scale, in terms of risk differences; or on a multiplicative scale, in terms of relative risk (see Box 8). Many authors and readers may have their own preference about the way interactions should be analysed. Still, they may be interested to know to what extent the joint effect of exposures differs from the separate effects. There is consensus that the additive scale, which uses absolute risks, is more appropriate for public health and clinical decision making [105]. Whatever view is taken, this should be clearly presented to the reader, as is done in the example above [103]. A lay-out presenting separate effects of both exposures as well as their joint effect, each relative to no exposure, might be most informative. It is presented in the example for interaction under item 17, and the calculations on the different scales are explained in Box 8\n\n\nExamples\n\nSex differences in susceptibility to the 3 lifestyle-related risk factors studied were explored by testing for biological interaction according to Rothman: a new composite variable with 4 categories (a−b−, a−b+, a+b−, and a+b+) was redefined for sex and a dichotomous exposure of interest where a− and b− denote absence of exposure. RR was calculated for each category after adjustment for age. An interaction effect is defined as departure from additivity of absolute effects, and excess RR caused by interaction (RERI) was calculated:\n\n[calculation]\nwhere RR(a+b+) denotes RR among those exposed to both factors where RR(a−b−) is used as reference category (RR = 1.0). Ninety-five percent CIs were calculated as proposed by Hosmer and Lemeshow. RERI of 0 means no interaction\nBack to top\n\n\n\n\n\n\n12c. Statistical methods\n\n\n\n\n\n\n\nExplain how missing data were addressed\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nMissing data are common in observational research. Questionnaires posted to study participants are not always filled in completely, participants may not attend all follow-up visits and routine data sources and clinical databases are often incomplete. Despite its ubiquity and importance, few papers report in detail on the problem of missing data [5,107]. Investigators may use any of several approaches to address missing data. We describe some strengths and limitations of various approaches in Box 6. We advise that authors report the number of missing values for each variable of interest (exposures, outcomes, confounders) and for each step in the analysis. Authors should give reasons for missing values if possible, and indicate how many individuals were excluded because of missing data when describing the flow of participants through the study (see also item 13). For analyses that account for missing data, authors should describe the nature of the analysis (e.g., multiple imputation) and the assumptions that were made (e.g., missing at random, see Box 6).\n\n\nExamples\n\nOur missing data analysis procedures used missing at random (MAR) assumptions. We used the MICE (multivariate imputation by chained equations) method of multiple multivariate imputation in STATA. We independently analysed 10 copies of the data, each with missing values suitably imputed, in the multivariate logistic regression analyses. We averaged estimates of the variables to give a single mean estimate and adjusted standard errors according to Rubin’s rules\n\nBack to top\n\n\n\n\n\n\n12d. Statistical methods\n\n\n\n\n\n\n\nIf applicable, explain how loss to follow-up was addressed\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nCohort studies are analysed using life table methods or other approaches that are based on the person-time of follow-up and time to developing the disease of interest. Among individuals who remain free of the disease at the end of their observation period, the amount of follow-up time is assumed to be unrelated to the probability of developing the outcome. This will be the case if follow-up ends on a fixed date or at a particular age. Loss to follow-up occurs when participants withdraw from a study before that date. This may hamper the validity of a study if loss to follow-up occurs selectively in exposed individuals, or in persons at high risk of developing the disease (informative censoring). In the example above, patients lost to follow-up in treatment programmes with no active follow-up had fewer CD4 helper cells than those remaining under observation and were therefore at higher risk of dying [116].\nIt is important to distinguish persons who reach the end of the study from those lost to follow-up. Unfortunately, statistical software usually does not distinguish between the two situations: in both cases follow-up time is automatically truncated (censored) at the end of the observation period. Investigators therefore need to decide, ideally at the stage of planning the study, how they will deal with loss to follow-up. When few patients are lost, investigators may either exclude individuals with incomplete follow-up, or treat them as if they withdrew alive at either the date of loss to follow-up or the end of the study. We advise authors to report how many patients were lost to follow-up and what censoring strategies they used\n\n\nExamples\n\nIn treatment programmes with active follow-up, those lost to follow-up and those followed-up at 1 year had similar baseline CD4 cell counts (median 115 cells per μL and 123 cells per μL), whereas patients lost to follow-up in programmes with no active follow-up procedures had considerably lower CD4 cell counts than those followed-up (median 64 cells per μL and 123 cells per μL). (…) Treatment programmes with passive follow-up were excluded from subsequent analyses\n\nBack to top\n\n\n\n\n\n\n12e. Statistical methods\n\n\n\n\n\n\n\nDescribe any sensitivity analyses\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nSensitivity analyses are useful to investigate whether or not the main results are consistent with those obtained with alternative analysis strategies or assumptions [121]. Issues that may be examined include the criteria for inclusion in analyses, the definitions of exposures or outcomes [122], which confounding variables merit adjustment, the handling of missing data [120,123], possible selection bias or bias from inaccurate or inconsistent measurement of exposure, disease and other variables, and specific analysis choices, such as the treatment of quantitative variables (see item 11). Sophisticated methods are increasingly used to simultaneously model the influence of several biases or assumptions [124–126].\nIn 1959 Cornfield et al. famously showed that a relative risk of 9 for cigarette smoking and lung cancer was extremely unlikely to be due to any conceivable confounder, since the confounder would need to be at least nine times as prevalent in smokers as in non-smokers [127]. This analysis did not rule out the possibility that such a factor was present, but it did identify the prevalence such a factor would need to have. The same approach was recently used to identify plausible confounding factors that could explain the association between childhood leukaemia and living near electric power lines [128]. More generally, sensitivity analyses can be used to identify the degree of confounding, selection bias, or information bias required to distort an association. One important, perhaps under recognised, use of sensitivity analysis is when a study shows little or no association between an exposure and an outcome and it is plausible that confounding or other biases toward the null are present\n\n\nExamples\n\nBecause we had a relatively higher proportion of missing dead patients with insufficient data (38/148=25.7%) as compared to live patients (15/437=3.4%) (…), it is possible that this might have biased the results. We have, therefore, carried out a sensitivity analysis. We have assumed that the proportion of women using oral contraceptives in the study group applies to the whole (19.1% for dead, and 11.4% for live patients), and then applied two extreme scenarios: either all the exposed missing patients used second generation pills or they all used third-generation pills\n\nBack to top\n\n\n\n\n\n\nResults\n\n\n13a. Participants\n\n\n\n\n\n\n\nReport numbers of individuals at each stage of study—eg numbers potentially eligible, examined for eligibility, confirmed eligible, included in the study, completing follow-up, and analysed. Give information separately for for exposed and unexposed groups if applicable.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nDetailed information on the process of recruiting study participants is important for several reasons. Those included in a study often differ in relevant ways from the target population to which results are applied. This may result in estimates of prevalence or incidence that do not reflect the experience of the target population. For example, people who agreed to participate in a postal survey of sexual behaviour attended church less often, had less conservative sexual attitudes and earlier age at first sexual intercourse, and were more likely to smoke cigarettes and drink alcohol than people who refused [130]. These differences suggest that postal surveys may overestimate sexual liberalism and activity in the population. Such response bias (see Box 3) can distort exposure-disease associations if associations differ between those eligible for the study and those included in the study. As another example, the association between young maternal age and leukaemia in offspring, which has been observed in some case-control studies [131,132], was explained by differential participation of young women in case and control groups. Young women with healthy children were less likely to participate than those with unhealthy children [133]. Although low participation does not necessarily compromise the validity of a study, transparent information on participation and reasons for non-participation is essential. Also, as there are no universally agreed definitions for participation, response or follow-up rates, readers need to understand how authors calculated such proportions [134].\nIdeally, investigators should give an account of the numbers of individuals considered at each stage of recruiting study participants, from the choice of a target population to the inclusion of participants’ data in the analysis. Depending on the type of study, this may include the number of individuals considered to be potentially eligible, the number assessed for eligibility, the number found to be eligible, the number included in the study, the number examined, the number followed up and the number included in the analysis. Information on different sampling units may be required, if sampling of study participants is carried out in two or more stages as in the example above (multistage sampling). In case-control studies, we advise that authors describe the flow of participants separately for case and control groups [135]. Controls can sometimes be selected from several sources, including, for example, hospitalised patients and community dwellers. In this case, we recommend a separate account of the numbers of participants for each type of control group. Olson and colleagues proposed useful reporting guidelines for controls recruited through random-digit dialling and other methods [136].\nA recent survey of epidemiological studies published in 10 general epidemiology, public health and medical journals found that some information regarding participation was provided in 47 of 107 case-control studies (59%), 49 of 154 cohort studies (32%), and 51 of 86 cross-sectional studies (59%) [137]. Incomplete or absent reporting of participation and non-participation in epidemiological studies was also documented in two other surveys of the literature [4,5]. Finally, there is evidence that participation in epidemiological studies may have declined in recent decades [137,138], which underscores the need for transparent reporting\n\n\nExamples\n\nOf the 105 freestanding bars and taverns sampled, 13 establishments were no longer in business and 9 were located in restaurants, leaving 83 eligible businesses. In 22 cases, the owner could not be reached by telephone despite 6 or more attempts. The owners of 36 bars declined study participation. (…) The 25 participating bars and taverns employed 124 bartenders, with 67 bartenders working at least 1 weekly daytime shift. Fifty-four of the daytime bartenders (81%) completed baseline interviews and spirometry; 53 of these subjects (98%) completed follow-up\n\nBack to top\n\n\n\n\n\n\n13b. Participants\n\n\n\n\n\n\n\nGive reasons for non-participation at each stage\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nExplaining the reasons why people no longer participated in a study or why they were excluded from statistical analyses helps readers judge whether the study population was representative of the target population and whether bias was possibly introduced. For example, in a cross-sectional health survey, non-participation due to reasons unlikely to be related to health status (for example, the letter of invitation was not delivered because of an incorrect address) will affect the precision of estimates but will probably not introduce bias. Conversely, if many individuals opt out of the survey because of illness, or perceived good health, results may underestimate or overestimate the prevalence of ill health in the population\n\n\nExamples\n\nThe main reasons for non-participation were the participant was too ill or had died before interview (cases 30%, controls < 1%), nonresponse (cases 2%, controls 21%), refusal (cases 10%, controls 29%), and other reasons (refusal by consultant or general practitioner, non-English speaking, mental impairment) (cases 7%, controls 5%)\n\nBack to top\n\n\n\n\n\n\n13c. Participants\n\n\n\n\n\n\n\nConsider use of a flow diagram\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAn informative and well-structured flow diagram can readily and transparently convey information that might otherwise require a lengthy description [142], as in the example above. The diagram may usefully include the main results, such as the number of events for the primary outcome. While we recommend the use of a flow diagram, particularly for complex observational studies, we do not propose a specific format for the diagram\n\n\nExamples\n\nFlow diagram from Hay et al. https://doi.org/10.1371/journal.pmed.0040297.g001\n\nBack to top\n\n\n\n\n\n\n14a. Descriptive data\n\n\n\n\n\n\n\nGive characteristics of study participants (eg demographic, clinical, social) and information on exposures and potential confounders. Give information separately for exposed and unexposed groups if applicable.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nReaders need descriptions of study participants and their exposures to judge the generalisability of the findings. Information about potential confounders, including whether and how they were measured, influences judgments about study validity. We advise authors to summarize continuous variables for each study group by giving the mean and standard deviation, or when the data have an asymmetrical distribution, as is often the case, the median and percentile range (e.g., 25th and 75th percentiles). Variables that make up a small number of ordered categories (such as stages of disease I to IV) should not be presented as continuous variables; it is preferable to give numbers and proportions for each category (see also Box 4). In studies that compare groups, the descriptive characteristics and numbers should be given by group, as in the example above.\nInferential measures such as standard errors and confidence intervals should not be used to describe the variability of characteristics, and significance tests should be avoided in descriptive tables. Also, P values are not an appropriate criterion for selecting which confounders to adjust for in analysis; even small differences in a confounder that has a strong effect on the outcome can be important.\nIn cohort studies, it may be useful to document how an exposure relates to other characteristics and potential confounders. Authors could present this information in a table with columns for participants in two or more exposure categories, which permits to judge the differences in confounders between these categories.\nIn case-control studies potential confounders cannot be judged by comparing cases and controls. Control persons represent the source population and will usually be different from the cases in many respects. For example, in a study of oral contraceptives and myocardial infarction, a sample of young women with infarction more often had risk factors for that disease, such as high serum cholesterol, smoking and a positive family history, than the control group [146]. This does not influence the assessment of the effect of oral contraceptives, as long as the prescription of oral contraceptives was not guided by the presence of these risk factors—e.g., because the risk factors were only established after the event (see also Box 5). In case-control studies the equivalent of comparing exposed and non-exposed for the presence of potential confounders (as is done in cohorts) can be achieved by exploring the source population of the cases: if the control group is large enough and represents the source population, exposed and unexposed controls can be compared for potential confounders [121,147].\n\n\nExamples\n\nCharacteristics of the Study Base at Enrolment, Castellana G (Italy), 1985–1986\n\nhttps://doi.org/10.1371/journal.pmed.0040297.t002\nBack to top\n\n\n\n\n\n\n14b. Descriptive data\n\n\n\n\n\n\n\nIndicate number of participants with missing data for each variable of interest\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAs missing data may bias or affect generalisability of results, authors should tell readers amounts of missing data for exposures, potential confounders, and other important characteristics of patients (see also item 12c and Box 6). In a cohort study, authors should report the extent of loss to follow-up (with reasons), since incomplete follow-up may bias findings (see also items 12d and 13). We advise authors to use their tables and figures to enumerate amounts of missing data.\n\n\nExamples\n\nTable. Symptom End Points Used in Survival Analysis\n\nhttps://doi.org/10.1371/journal.pmed.0040297.t003\nBack to top\n\n\n\n\n\n\n14c. Descriptive data\n\n\n\n\n\n\n\nSummarise follow-up time (eg, average and total amount)\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nReaders need to know the duration and extent of follow-up for the available outcome data. Authors can present a summary of the average follow-up with either the mean or median follow-up time or both. The mean allows a reader to calculate the total number of person-years by multiplying it with the number of study participants. Authors also may present minimum and maximum times or percentiles of the distribution to show readers the spread of follow-up times. They may report total person-years of follow-up or some indication of the proportion of potential data that was captured [148]. All such information may be presented separately for participants in two or more exposure categories. Almost half of 132 articles in cancer journals (mostly cohort studies) did not give any summary of length of follow-up\n\n\nExamples\n\nDuring the 4366 person-years of follow-up (median 5.4, maximum 8.3 years), 265 subjects were diagnosed as having dementia, including 202 with Alzheimer’s disease\n\nBack to top\n\n\n\n\n\n\n15. Outcome data\n\n\n\n\n\n\n\nReport numbers of outcome events or summary measures over time. Give information separately for exposed and unexposed groups if applicable.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nBefore addressing the possible association between exposures (risk factors) and outcomes, authors should report relevant descriptive data. It may be possible and meaningful to present measures of association in the same table that presents the descriptive data (see item 14a). In a cohort study with events as outcomes, report the numbers of events for each outcome of interest. Consider reporting the event rate per person-year of follow-up. If the risk of an event changes over follow-up time, present the numbers and rates of events in appropriate intervals of follow-up or as a Kaplan-Meier life table or plot. It might be preferable to show plots as cumulative incidence that go up from 0% rather than down from 100%, especially if the event rate is lower than, say, 30% [153]. Consider presenting such information separately for participants in different exposure categories of interest. If a cohort study is investigating other time-related outcomes (e.g., quantitative disease markers such as blood pressure), present appropriate summary measures (e.g., means and standard deviations) over time, perhaps in a table or figure.\nFor cross-sectional studies, we recommend presenting the same type of information on prevalent outcome events or summary measures. For case-control studies, the focus will be on reporting exposures separately for cases and controls as frequencies or quantitative summaries [154]. For all designs, it may be helpful also to tabulate continuous outcomes or exposures in categories, even if the data are not analysed as such\n\n\nExamples\n\nTable. Rates of HIV-1 Seroconversion by Selected Sociodemographic Variables: 1990–1993 https://doi.org/10.1371/journal.pmed.0040297.t004\n\nBack to top\n\n\n\n\n\n\n16a. Main results\n\n\n\n\n\n\n\nGive unadjusted estimates and, if applicable, confounder-adjusted estimates and their precision (eg, 95% confidence interval). Make clear which confounders were adjusted for and why they were included\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nIn many situations, authors may present the results of unadjusted or minimally adjusted analyses and those from fully adjusted analyses. We advise giving the unadjusted analyses together with the main data, for example the number of cases and controls that were exposed or not. This allows the reader to understand the data behind the measures of association (see also item 15). For adjusted analyses, report the number of persons in the analysis, as this number may differ because of missing values in covariates (see also item 12c). Estimates should be given with confidence intervals.\nReaders can compare unadjusted measures of association with those adjusted for potential confounders and judge by how much, and in what direction, they changed. Readers may think that adjusted results equal the causal part of the measure of association, but adjusted results are not necessarily free of random sampling error, selection bias, information bias, or residual confounding (see Box 5). Thus, great care should be exercised when interpreting adjusted results, as the validity of results often depends crucially on complete knowledge of important confounders, their precise measurement, and appropriate specification in the statistical model (see also item 20) [157,158].\nAuthors should explain all potential confounders considered, and the criteria for excluding or including variables in statistical models. Decisions about excluding or including variables should be guided by knowledge, or explicit assumptions, on causal relations. Inappropriate decisions may introduce bias, for example by including variables that are in the causal pathway between exposure and disease (unless the aim is to asses how much of the effect is carried by the intermediary variable). If the decision to include a variable in the model was based on the change in the estimate, it is important to report what change was considered sufficiently important to justify its inclusion. If a backward deletion or forward inclusion strategy was used to select confounders, explain that process and give the significance level for rejecting the null hypothesis of no confounding. Of note, we and others do not advise selecting confounders based solely on statistical significance testing [147,159,160].\nRecent studies of the quality of reporting of epidemiological studies found that confidence intervals were reported in most articles [4]. However, few authors explained their choice of confounding variables [4,5].\n\n\nExamples\n\nTable. Relative Rates of Rehospitalisation by Treatment in Patients in Community Care after First Hospitalisation due to Schizophrenia and Schizoaffective Disorder https://doi.org/10.1371/journal.pmed.0040297.t008\n\nBack to top\n\n\n\n\n\n\n16b. Main results\n\n\n\n\n\n\n\nReport category boundaries when continuous variables were categorized\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nCategorizing continuous data has several important implications for analysis (see Box 4) and also affects the presentation of results. In tables, outcomes should be given for each exposure category, for example as counts of persons at risk, person-time at risk, if relevant separately for each group (e.g., cases and controls). Details of the categories used may aid comparison of studies and meta-analysis. If data were grouped using conventional cut-points, such as body mass index thresholds [162], group boundaries (i.e., range of values) can be derived easily, except for the highest and lowest categories. If quantile-derived categories are used, the category boundaries cannot be inferred from the data. As a minimum, authors should report the category boundaries; it is helpful also to report the range of the data and the mean or median values within categories.\n\n\nExamples\n\nTable. Polychlorinated Biphenyls in Cord Serum\n\nhttps://doi.org/10.1371/journal.pmed.0040297.t005\nBack to top\n\n\n\n\n\n\n16c. Main results\n\n\n\n\n\n\n\nIf relevant, consider translating estimates of relative risk into absolute risk for a meaningful time period\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe results from studies examining the association between an exposure and a disease are commonly reported in relative terms, as ratios of risks, rates or odds (see Box 8). Relative measures capture the strength of the association between an exposure and disease. If the relative risk is a long way from 1 it is less likely that the association is due to confounding [164,165]. Relative effects or associations tend to be more consistent across studies and populations than absolute measures, but what often tends to be the case may be irrelevant in a particular instance. For example, similar relative risks were obtained for the classic cardiovascular risk factors for men living in Northern Ireland, France, the USA and Germany, despite the fact that the underlying risk of coronary heart disease varies substantially between these countries [166,167]. In contrast, in a study of hypertension as a risk factor for cardiovascular disease mortality, the data were more compatible with a constant rate difference than with a constant rate ratio [168].\nWidely used statistical models, including logistic [169] and proportional hazards (Cox) regression [170] are based on ratio measures. In these models, only departures from constancy of ratio effect measures are easily discerned. Nevertheless, measures which assess departures from additivity of risk differences, such as the Relative Excess Risk from Interaction (RERI, see item 12b and Box 8), can be estimated in models based on ratio measures.\nIn many circumstances, the absolute risk associated with an exposure is of greater interest than the relative risk. For example, if the focus is on adverse effects of a drug, one will want to know the number of additional cases per unit time of use (e.g., days, weeks, or years). The example gives the additional number of breast cancer cases per 1000 women who used hormone-replacement therapy for 10 years [163]. Measures such as the attributable risk or population attributable fraction may be useful to gauge how much disease can be prevented if the exposure is eliminated. They should preferably be presented together with a measure of statistical uncertainty (e.g., confidence intervals as in the example). Authors should be aware of the strong assumptions made in this context, including a causal relationship between a risk factor and disease (also see Box 7) [171]. Because of the semantic ambiguity and complexities involved, authors should report in detail what methods were used to calculate attributable risks, ideally giving the formulae used [172].\nA recent survey of abstracts of 222 articles published in leading medical journals found that in 62% of abstracts of randomised trials including a ratio measure absolute risks were given, but only in 21% of abstracts of cohort studies [173]. A free text search of Medline 1966 to 1997 showed that 619 items mentioned attributable risks in the title or abstract, compared to 18,955 using relative risk or odds ratio, for a ratio of 1 to 31 [174].\n\n\nExamples\n\n10 years’ use of HRT [hormone replacement therapy] is estimated to result in five (95% CI 3–7) additional breast cancers per 1000 users of oestrogen-only preparations and 19 (15–23) additional cancers per 1000 users of oestrogen-progestagen combinations\n\nBack to top\n\n\n\n\n\n\n17. Other analyses\n\n\n\n\n\n\n\nReport other analyses done—e.g., analyses of subgroups and interactions, and sensitivity analyses\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nIn addition to the main analysis other analyses are often done in observational studies. They may address specific subgroups, the potential interaction between risk factors, the calculation of attributable risks, or use alternative definitions of study variables in sensitivity analyses.\nThere is debate about the dangers associated with subgroup analyses, and multiplicity of analyses in general [4,104]. In our opinion, there is too great a tendency to look for evidence of subgroup-specific associations, or effect-measure modification, when overall results appear to suggest little or no effect. On the other hand, there is value in exploring whether an overall association appears consistent across several, preferably pre-specified subgroups especially when a study is large enough to have sufficient data in each subgroup. A second area of debate is about interesting subgroups that arose during the data analysis. They might be important findings, but might also arise by chance. Some argue that it is neither possible nor necessary to inform the reader about all subgroup analyses done as future analyses of other data will tell to what extent the early exciting findings stand the test of time [9]. We advise authors to report which analyses were planned, and which were not (see also items 4, 12b and 20). This will allow readers to judge the implications of multiplicity, taking into account the study’s position on the continuum from discovery to verification or refutation.\nA third area of debate is how joint effects and interactions between risk factors should be evaluated: on additive or multiplicative scales, or should the scale be determined by the statistical model that fits best (see also item 12b and Box 8)? A sensible approach is to report the separate effect of each exposure as well as the joint effect—if possible in a table, as in the first example above [183], or in the study by Martinelli et al. [185]. Such a table gives the reader sufficient information to evaluate additive as well as multiplicative interaction (how these calculations are done is shown in Box 8). Confidence intervals for separate and joint effects may help the reader to judge the strength of the data. In addition, confidence intervals around measures of interaction, such as the Relative Excess Risk from Interaction (RERI) relate to tests of interaction or homogeneity tests. One recurrent problem is that authors use comparisons of P-values across subgroups, which lead to erroneous claims about an effect modifier. For instance, a statistically significant association in one category (e.g., men), but not in the other (e.g., women) does not in itself provide evidence of effect modification. Similarly, the confidence intervals for each point estimate are sometimes inappropriately used to infer that there is no interaction when intervals overlap. A more valid inference is achieved by directly evaluating whether the magnitude of an association differs across subgroups.\nSensitivity analyses are helpful to investigate the influence of choices made in the statistical analysis, or to investigate the robustness of the findings to missing data or possible biases (see also item 12b). Judgement is needed regarding the level of reporting of such analyses. If many sensitivity analyses were performed, it may be impractical to present detailed findings for them all. It may sometimes be sufficient to report that sensitivity analyses were carried out and that they were consistent with the main results presented. Detailed presentation is more appropriate if the issue investigated is of major concern, or if effect estimates vary considerably [59,186].\nPocock and colleagues found that 43 out of 73 articles reporting observational studies contained subgroup analyses. The majority claimed differences across groups but only eight articles reported a formal evaluation of interaction (see item 12b) [4].\n\n\nExamples\n\nTable. Analysis of Oral Contraceptive Use, Presence of Factor V Leiden Allele, and Risk for Venous Thromboembolism\n\nhttps://doi.org/10.1371/journal.pmed.0040297.t009\n\nTable. Sensitivity of the Rate Ratio for Cardiovascular Outcome to an Unmeasured Confounder https://doi.org/10.1371/journal.pmed.0040297.t010\n\nBack to top\n\n\n\n\n\n\nDiscussion\n\n\n18. Key results\n\n\n\n\n\n\n\nSummarise key results with reference to study objectives\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nIt is good practice to begin the discussion with a short summary of the main findings of the study. The short summary reminds readers of the main findings and may help them assess whether the subsequent interpretation and implications offered by the authors are supported by the findings\n\n\nExamples\n\nWe hypothesized that ethnic minority status would be associated with higher levels of cardiovascular disease (CVD) risk factors, but that the associations would be explained substantially by socioeconomic status (SES). Our hypothesis was not confirmed. After adjustment for age and SES, highly significant differences in body mass index, blood pressure, diabetes, and physical inactivity remained between white women and both black and Mexican American women. In addition, we found large differences in CVD risk factors by SES, a finding that illustrates the high-risk status of both ethnic minority women as well as white women with low SES\n\nBack to top\n\n\n\n\n\n\n19. Limitations\n\n\n\n\n\n\n\nDiscuss limitations of the study, taking into account sources of potential bias or imprecision. Discuss both direction and magnitude of any potential bias.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe identification and discussion of the limitations of a study are an essential part of scientific reporting. It is important not only to identify the sources of bias and confounding that could have affected results, but also to discuss the relative importance of different biases, including the likely direction and magnitude of any potential bias (see also item 9 and Box 3).\nAuthors should also discuss any imprecision of the results. Imprecision may arise in connection with several aspects of a study, including the study size (item 10) and the measurement of exposures, confounders and outcomes (item 8). The inability to precisely measure true values of an exposure tends to result in bias towards unity: the less precisely a risk factor is measured, the greater the bias. This effect has been described as attenuation [201,202], or more recently as regression dilution bias [203]. However, when correlated risk factors are measured with different degrees of imprecision, the adjusted relative risk associated with them can be biased towards or away from unity [204–206].\nWhen discussing limitations, authors may compare the study being presented with other studies in the literature in terms of validity, generalisability and precision. In this approach, each study can be viewed as contribution to the literature, not as a stand-alone basis for inference and action [207]. Surprisingly, the discussion of important limitations of a study is sometimes omitted from published reports. A survey of authors who had published original research articles in The Lancet found that important weaknesses of the study were reported by the investigators in the survey questionnaires, but not in the published article\n\n\nExamples\n\nSince the prevalence of counseling increases with increasing levels of obesity, our estimates may overestimate the true prevalence. Telephone surveys also may overestimate the true prevalence of counseling. Although persons without telephones have similar levels of overweight as persons with telephones, persons without telephones tend to be less educated, a factor associated with lower levels of counseling in our study. Also, of concern is the potential bias caused by those who refused to participate as well as those who refused to respond to questions about weight. Furthermore, because data were collected cross-sectionally, we cannot infer that counseling preceded a patient’s attempt to lose weight\n\nBack to top\n\n\n\n\n\n\n20. Interpretation\n\n\n\n\n\n\n\nGive a cautious overall interpretation considering objectives, limitations, multiplicity of analyses, results from similar studies, and other relevant evidence.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe heart of the discussion section is the interpretation of a study’s results. Over-interpretation is common and human: even when we try hard to give an objective assessment, reviewers often rightly point out that we went too far in some respects. When interpreting results, authors should consider the nature of the study on the discovery to verification continuum and potential sources of bias, including loss to follow-up and non-participation (see also items 9, 12 and 19). Due consideration should be given to confounding (item 16a), the results of relevant sensitivity analyses, and to the issue of multiplicity and subgroup analyses (item 17). Authors should also consider residual confounding due to unmeasured variables or imprecise measurement of confounders. For example, socioeconomic status (SES) is associated with many health outcomes and often differs between groups being compared. Variables used to measure SES (income, education, or occupation) are surrogates for other undefined and unmeasured exposures, and the true confounder will by definition be measured with error [208]. Authors should address the real range of uncertainty in estimates, which is larger than the statistical uncertainty reflected in confidence intervals. The latter do not take into account other uncertainties that arise from a study’s design, implementation, and methods of measurement [209].\nTo guide thinking and conclusions about causality, some may find criteria proposed by Bradford Hill in 1965 helpful [164]. How strong is the association with the exposure? Did it precede the onset of disease? Is the association consistently observed in different studies and settings? Is there supporting evidence from experimental studies, including laboratory and animal studies? How specific is the exposure’s putative effect, and is there a dose-response relationship? Is the association biologically plausible? These criteria should not, however, be applied mechanically. For example, some have argued that relative risks below 2 or 3 should be ignored [210,211]. This is a reversal of the point by Cornfield et al. about the strength of large relative risks (see item 12b) [127]. Although a causal effect is more likely with a relative risk of 9, it does not follow that one below 3 is necessarily spurious. For instance, the small increase in the risk of childhood leukaemia after intrauterine irradiation is credible because it concerns an adverse effect of a medical procedure for which no alternative explanations are obvious [212]. Moreover, the carcinogenic effects of radiation are well established. The doubling in the risk of ovarian cancer associated with eating 2 to 4 eggs per week is not immediately credible, since dietary habits are associated with a large number of lifestyle factors as well as SES [213]. In contrast, the credibility of much debated epidemiologic findings of a difference in thrombosis risk between different types of oral contraceptives was greatly enhanced by the differences in coagulation found in a randomised cross-over trial [214]. A discussion of the existing external evidence, from different types of studies, should always be included, but may be particularly important for studies reporting small increases in risk. Further, authors should put their results in context with similar studies and explain how the new study affects the existing body of evidence, ideally by referring to a systematic review.\n\n\nExamples\n\nAny explanation for an association between death from myocardial infarction and use of second generation oral contraceptives must be conjectural. There is no published evidence to suggest a direct biologic mechanism, and there are no other epidemiologic studies with relevant results. (…) The increase in absolute risk is very small and probably applies predominantly to smokers. Due to the lack of corroborative evidence, and because the analysis is based on relatively small numbers, more evidence on the subject is needed. We would not recommend any change in prescribing practice on the strength of these results\n\nBack to top\n\n\n\n\n\n\n21. Generalisability\n\n\n\n\n\n\n\nDiscuss the generalisability (external validity) of the study results\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nGeneralisability, also called external validity or applicability, is the extent to which the results of a study can be applied to other circumstances [216]. There is no external validity per se; the term is meaningful only with regard to clearly specified conditions [217]. Can results be applied to an individual, groups or populations that differ from those enrolled in the study with regard to age, sex, ethnicity, severity of disease, and co-morbid conditions? Are the nature and level of exposures comparable, and the definitions of outcomes relevant to another setting or population? Are data that were collected in longitudinal studies many years ago still relevant today? Are results from health services research in one country applicable to health systems in other countries?\nThe question of whether the results of a study have external validity is often a matter of judgment that depends on the study setting, the characteristics of the participants, the exposures examined, and the outcomes assessed. Thus, it is crucial that authors provide readers with adequate information about the setting and locations, eligibility criteria, the exposures and how they were measured, the definition of outcomes, and the period of recruitment and follow-up. The degree of non-participation and the proportion of unexposed participants in whom the outcome develops are also relevant. Knowledge of the absolute risk and prevalence of the exposure, which will often vary across populations, are helpful when applying results to other settings and populations (see Box 7)\n\n\nExamples\n\nHow applicable are our estimates to other HIV-1-infected patients? This is an important question because the accuracy of prognostic models tends to be lower when applied to data other than those used to develop them. We addressed this issue by penalising model complexity, and by choosing models that generalised best to cohorts omitted from the estimation procedure. Our database included patients from many countries from Europe and North America, who were treated in different settings. The range of patients was broad: men and women, from teenagers to elderly people were included, and the major exposure categories were well represented. The severity of immunodeficiency at baseline ranged from not measureable to very severe, and viral load from undetectable to extremely high\n\nBack to top\n\n\n\n\n\n\nOther Information\n\n\n22. Funding\n\n\n\n\n\n\n\nGive the source of funding and the role of the funders for the present study and, if applicable, for the original study on which the present article is based\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nSome journals require authors to disclose the presence or absence of financial and other conflicts of interest [100,218]. Several investigations show strong associations between the source of funding and the conclusions of research articles [219–222]. The conclusions in randomised trials recommended the experimental drug as the drug of choice much more often (odds ratio 5.3) if the trial was funded by for-profit organisations, even after adjustment for the effect size [223]. Other studies document the influence of the tobacco and telecommunication industries on the research they funded [224–227]. There are also examples of undue influence when the sponsor is governmental or a non-profit organisation.\nAuthors or funders may have conflicts of interest that influence any of the following: the design of the study [228]; choice of exposures [228,229], outcomes [230], statistical methods [231], and selective publication of outcomes [230] and studies [232]. Consequently, the role of the funders should be described in detail: in what part of the study they took direct responsibility (e.g., design, data collection, analysis, drafting of manuscript, decision to publish) [100]. Other sources of undue influence include employers (e.g., university administrators for academic researchers and government supervisors, especially political appointees, for government researchers), advisory committees, litigants, and special interest groups"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-1a_title-1",
    "href": "guidelines/strobe-cohort/index.html#sec-1a_title-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nReaders should be able to easily identify the design that was used from the title or abstract. An explicit, commonly used term for the study design also helps ensure correct indexing of articles in electronic databases"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-1a_title-2",
    "href": "guidelines/strobe-cohort/index.html#sec-1a_title-2",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Examples",
    "text": "Examples\n\nLeukaemia incidence among workers in the shoe and boot manufacturing industry: a case-control study\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-1b_abstract-1",
    "href": "guidelines/strobe-cohort/index.html#sec-1b_abstract-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nThe abstract provides key information that enables readers to understand a study and decide whether to read the article. Typical components include a statement of the research question, a short description of methods and results, and a conclusion. Abstracts should summarize key details of studies and should only present information that is provided in the article. We advise presenting key results in a numerical form that includes numbers of participants, estimates of associations and appropriate measures of variability and uncertainty (e.g., odds ratios with confidence intervals). We regard it insufficient to state only that an exposure is or is not significantly associated with an outcome.\nA series of headings pertaining to the background, design, conduct, and analysis of a study may help readers acquire the essential information rapidly. Many journals require such structured abstracts, which tend to be of higher quality and more readily informative than unstructured summaries."
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-1b_abstract-2",
    "href": "guidelines/strobe-cohort/index.html#sec-1b_abstract-2",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Examples",
    "text": "Examples\n\nBackground: The expected survival of HIV-infected patients is of major public health interest.\n\nObjective: To estimate survival time and age-specific mortality rates of an HIV-infected population compared with that of the general population.\nDesign: Population-based cohort study.\nSetting: All HIV-infected persons receiving care in Denmark from 1995 to 2005.\nPatients: Each member of the nationwide Danish HIV Cohort Study was matched with as many as 99 persons from the general population according to sex, date of birth, and municipality of residence.\nMeasurements: The authors computed Kaplan-Meier life tables with age as the time scale to estimate survival from age 25 years. Patients with HIV infection and corresponding persons from the general population were observed from the date of the patient’s HIV diagnosis until death, emigration, or 1 May 2005.\nResults: 3990 HIV-infected patients and 379,872 persons from the general population were included in the study, yielding 22,744 (median, 5.8 y/person) and 2,689,287 (median, 8.4 years/person) person-years of observation. Three percent of participants were lost to follow-up. From age 25 years, the median survival was 19.9 years (95% CI, 18.5 to 21.3) among patients with HIV infection and 51.1 years (CI, 50.9 to 51.5) among the general population. For HIV-infected patients, survival increased to 32.5 years (CI, 29.4 to 34.7) during the 2000 to 2005 period. In the subgroup that excluded persons with known hepatitis C coinfection (16%), median survival was 38.9 years (CI, 35.4 to 40.1) during this same period. The relative mortality rates for patients with HIV infection compared with those for the general population decreased with increasing age, whereas the excess mortality rate increased with increasing age.\nLimitations: The observed mortality rates are assumed to apply beyond the current maximum observation time of 10 years.\nConclusions: The estimated median survival is more than 35 years for a young person diagnosed with HIV infection in the late highly active antiretroviral therapy era. However, an ongoing effort is still needed to further reduce mortality rates for these persons compared with the general population\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-2_background__rationale-1",
    "href": "guidelines/strobe-cohort/index.html#sec-2_background__rationale-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nThe scientific background of the study provides important context for readers. It sets the stage for the study and describes its focus. It gives an overview of what is known on a topic and what gaps in current knowledge are addressed by the study. Background material should note recent pertinent studies and any systematic reviews of pertinent studies."
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-2_background__rationale-2",
    "href": "guidelines/strobe-cohort/index.html#sec-2_background__rationale-2",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Examples",
    "text": "Examples\n\nConcerns about the rising prevalence of obesity in children and adolescents have focused on the well documented associations between childhood obesity and increased cardiovascular risk1 and mortality in adulthood.2 Childhood obesity has considerable social and psychological consequences within childhood and adolescence,3 yet little is known about social, socioeconomic, and psychological consequences in adult life.\n\nA recent systematic review found no longitudinal studies on the outcomes of childhood obesity other than physical health outcomes3 and only two longitudinal studies of the socioeconomic effects of obesity in adolescence. Gortmaker et al found that US women who had been obese in late adolescence in 1981 were less likely to be married and had lower incomes seven years later than women who had not been overweight, while men who had been overweight were less likely to be married.4 Sargent et al found that UK women, but not men, who had been obese at 16 years in 1974 earned 7.4% less than their non-obese peers at age 23.5\nThe study of adult outcomes of childhood obesity is difficult because obesity often continues into adult life and therefore poorer socioeconomic and educational outcomes may actually reflect confounding by adult obesity. Yet identifying outcomes related to obesity confined childhood is important in determining whether people who are obese in childhood and who later lose weight remain at risk for adult adversity and inequalities.\nWe used longitudinal data from the 1970 British birth cohort to examine the adult socioeconomic, educational, social, and psychological outcomes of childhood obesity. We hypothesised that obesity limited to childhood has fewer adverse adult outcomes than obesity that persists into adult life\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-3_objectives-1",
    "href": "guidelines/strobe-cohort/index.html#sec-3_objectives-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nObjectives are the detailed aims of the study. Well crafted objectives specify populations, exposures and outcomes, and parameters that will be estimated. They may be formulated as specific hypotheses or as questions that the study was designed to address. In some situations objectives may be less specific, for example, in early discovery phases. Regardless, the report should clearly reflect the investigators’ intentions. For example, if important subgroups or additional analyses were not the original aim of the study but arose during data analysis, they should be described accordingly (see also items 4, 17 and 20)."
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-3_objectives-2",
    "href": "guidelines/strobe-cohort/index.html#sec-3_objectives-2",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Examples",
    "text": "Examples\n\nOur primary objectives were to 1) determine the prevalence of domestic violence among female patients presenting to four community-based, primary care, adult medicine practices that serve patients of diverse socioeconomic background and 2) identify demographic and clinical differences between currently abused patients and patients not currently being abused\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-4_study_design-1",
    "href": "guidelines/strobe-cohort/index.html#sec-4_study_design-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nWe advise presenting key elements of study design early in the methods section (or at the end of the introduction) so that readers can understand the basics of the study. For example, authors should indicate that the study was a cohort study, which followed people over a particular time period, and describe the group of persons that comprised the cohort and their exposure status. Similarly, if the investigation used a case-control design, the cases and controls and their source population should be described. If the study was a cross-sectional survey, the population and the point in time at which the cross-section was taken should be mentioned. When a study is a variant of the three main study types, there is an additional need for clarity. For instance, for a case-crossover study, one of the variants of the case-control design, a succinct description of the principles was given in the example above [28].\nWe recommend that authors refrain from simply calling a study prospective or retrospective because these terms are ill defined [29]. One usage sees cohort and prospective as synonymous and reserves the word retrospective for case-control studies [30]. A second usage distinguishes prospective and retrospective cohort studies according to the timing of data collection relative to when the idea for the study was developed [31]. A third usage distinguishes prospective and retrospective case-control studies depending on whether the data about the exposure of interest existed when cases were selected [32]. Some advise against using these terms [33], or adopting the alternatives concurrent and historical for describing cohort studies [34]. In STROBE, we do not use the words prospective and retrospective, nor alternatives such as concurrent and historical. We recommend that, whenever authors use these words, they define what they mean. Most importantly, we recommend that authors describe exactly how and when data collection took place.\nThe first part of the methods section might also be the place to mention whether the report is one of several from a study. If a new report is in line with the original aims of the study, this is usually indicated by referring to an earlier publication and by briefly restating the salient features of the study. However, the aims of a study may also evolve over time. Researchers often use data for purposes for which they were not originally intended, including, for example, official vital statistics that were collected primarily for administrative purposes, items in questionnaires that originally were only included for completeness, or blood samples that were collected for another purpose. For example, the Physicians’ Health Study, a randomized controlled trial of aspirin and carotene, was later used to demonstrate that a point mutation in the factor V gene was associated with an increased risk of venous thrombosis, but not of myocardial infarction or stroke [35]. The secondary use of existing data is a creative part of observational research and does not necessarily make results less credible or less important. However, briefly restating the original aims might help readers understand the context of the research and possible limitations in the data."
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-4_study_design-2",
    "href": "guidelines/strobe-cohort/index.html#sec-4_study_design-2",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Examples",
    "text": "Examples\n\nWe used a case-crossover design, a variation of a case-control design that is appropriate when a brief exposure (driver’s phone use) causes a transient rise in the risk of a rare outcome (a crash). We compared a driver’s use of a mobile phone at the estimated time of a crash with the same driver’s use during another suitable time period. Because drivers are their own controls, the design controls for characteristics of the driver that may affect the risk of a crash but do not change over a short period of time. As it is important that risks during control periods and crash trips are similar, we compared phone activity during the hazard interval (time immediately before the crash) with phone activity during control intervals (equivalent times during which participants were driving but did not crash) in the previous week\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-5_setting-1",
    "href": "guidelines/strobe-cohort/index.html#sec-5_setting-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nReaders need information on setting and locations to assess the context and generalisability of a study’s results. Exposures such as environmental factors and therapies can change over time. Also, study methods may evolve over time. Knowing when a study took place and over what period participants were recruited and followed up places the study in historical context and is important for the interpretation of results.\nInformation about setting includes recruitment sites or sources (e.g., electoral roll, outpatient clinic, cancer registry, or tertiary care centre). Information about location may refer to the countries, towns, hospitals or practices where the investigation took place. We advise stating dates rather than only describing the length of time periods. There may be different sets of dates for exposure, disease occurrence, recruitment, beginning and end of follow-up, and data collection. Of note, nearly 80% of 132 reports in oncology journals that used survival analysis included the starting and ending dates for accrual of patients, but only 24% also reported the date on which follow-up ended [37]."
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-5_setting-2",
    "href": "guidelines/strobe-cohort/index.html#sec-5_setting-2",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Examples",
    "text": "Examples\n\nThe Pasitos Cohort Study recruited pregnant women from Women, Infant and Child (WIC) clinics in Socorro and San Elizario, El Paso County, Texas and maternal-child clinics of the Mexican Social Security Institute (IMSS) in Ciudad Juarez, Mexico from April 1998 to October 2000. At baseline, prior to the birth of the enrolled cohort children, staff interviewed mothers regarding the household environment. In this ongoing cohort study, we target follow-up exams at 6-month intervals beginning at age 6 months\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-6a_eligibility_criteria-1",
    "href": "guidelines/strobe-cohort/index.html#sec-6a_eligibility_criteria-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nDetailed descriptions of the study participants help readers understand the applicability of the results. Investigators usually restrict a study population by defining clinical, demographic and other characteristics of eligible participants. Typical eligibility criteria relate to age, gender, diagnosis and comorbid conditions. Despite their importance, eligibility criteria often are not reported adequately. In a survey of observational stroke research, 17 of 49 reports (35%) did not specify eligibility criteria [5].\nEligibility criteria may be presented as inclusion and exclusion criteria, although this distinction is not always necessary or useful. Regardless, we advise authors to report all eligibility criteria and also to describe the group from which the study population was selected (e.g., the general population of a region or country), and the method of recruitment (e.g., referral or self-selection through advertisements).\nKnowing details about follow-up procedures, including whether procedures minimized non-response and loss to follow-up and whether the procedures were similar for all participants, informs judgments about the validity of results. For example, in a study that used IgM antibodies to detect acute infections, readers needed to know the interval between blood tests for IgM antibodies so that they could judge whether some infections likely were missed because the interval between blood tests was too long [41]. In other studies where follow-up procedures differed between exposed and unexposed groups, readers might recognize substantial bias due to unequal ascertainment of events or differences in non-response or loss to follow-up [42]. Accordingly, we advise that researchers describe the methods used for following participants and whether those methods were the same for all participants, and that they describe the completeness of ascertainment of variables (see also item 14).\nIn case-control studies, the choice of cases and controls is crucial to interpreting the results, and the method of their selection has major implications for study validity. In general, controls should reflect the population from which the cases arose. Various methods are used to sample controls, all with advantages and disadvantages: for cases that arise from a general population, population roster sampling, random digit dialling, neighbourhood or friend controls are used. Neighbourhood or friend controls may present intrinsic matching on exposure [17]. Controls with other diseases may have advantages over population-based controls, in particular for hospital-based cases, because they better reflect the catchment population of a hospital, have greater comparability of recall and ease of recruitment. However, they can present problems if the exposure of interest affects the risk of developing or being hospitalized for the control condition(s) [43,44]. To remedy this problem often a mixture of the best defensible control diseases is used [45]."
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-6a_eligibility_criteria-2",
    "href": "guidelines/strobe-cohort/index.html#sec-6a_eligibility_criteria-2",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Examples",
    "text": "Examples\n\nParticipants in the Iowa Women’s Health Study were a random sample of all women ages 55 to 69 years derived from the state of Iowa automobile driver’s license list in 1985, which represented approximately 94% of Iowa women in that age group. (…) Follow-up questionnaires were mailed in October 1987 and August 1989 to assess vital status and address changes. (…) Incident cancers, except for nonmelanoma skin cancers, were ascertained by the State Health Registry of Iowa (…). The Iowa Women’s Health Study cohort was matched to the registry with combinations of first, last, and maiden names, zip code, birthdate, and social security number\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-6b_eligibility_criteria-1",
    "href": "guidelines/strobe-cohort/index.html#sec-6b_eligibility_criteria-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nMatching is much more common in case-control studies, but occasionally, investigators use matching in cohort studies to make groups comparable at the start of follow-up. Matching in cohort studies makes groups directly comparable for potential confounders and presents fewer intricacies than with case-control studies. For example, it is not necessary to take the matching into account for the estimation of the relative risk [48]. Because matching in cohort studies may increase statistical precision investigators might allow for the matching in their analyses and thus obtain narrower confidence intervals.\nIn case-control studies matching is done to increase a study’s efficiency by ensuring similarity in the distribution of variables between cases and controls, in particular the distribution of potential confounding variables [48,49]. Because matching can be done in various ways, with one or more controls per case, the rationale for the choice of matching variables and the details of the method used should be described. Commonly used forms of matching are frequency matching (also called group matching) and individual matching. In frequency matching, investigators choose controls so that the distribution of matching variables becomes identical or similar to that of cases. Individual matching involves matching one or several controls to each case. Although intuitively appealing and sometimes useful, matching in case-control studies has a number of disadvantages, is not always appropriate, and needs to be taken into account in the analysis"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-6b_eligibility_criteria-2",
    "href": "guidelines/strobe-cohort/index.html#sec-6b_eligibility_criteria-2",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Examples",
    "text": "Examples\n\nFor each patient who initially received a statin, we used propensity-based matching to identify one control who did not receive a statin according to the following protocol. First, propensity scores were calculated for each patient in the entire cohort on the basis of an extensive list of factors potentially related to the use of statins or the risk of sepsis. Second, each statin user was matched to a smaller pool of non-statin-users by sex, age (plus or minus 1 year), and index date (plus or minus 3 months). Third, we selected the control with the closest propensity score (within 0.2 SD) to each statin user in a 1:1 fashion and discarded the remaining controls\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-7_variables-1",
    "href": "guidelines/strobe-cohort/index.html#sec-7_variables-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nAuthors should define all variables considered for and included in the analysis, including outcomes, exposures, predictors, potential confounders and potential effect modifiers. Disease outcomes require adequately detailed description of the diagnostic criteria. This applies to criteria for cases in a case-control study, disease events during follow-up in a cohort study and prevalent disease in a cross-sectional study. Clear definitions and steps taken to adhere to them are particularly important for any disease condition of primary interest in the study.\nFor some studies, determinant or predictor may be appropriate terms for exposure variables and outcomes may be called endpoints. In multivariable models, authors sometimes use dependent variable for an outcome and independent variable or explanatory variable for exposure and confounding variables. The latter is not precise as it does not distinguish exposures from confounders.\nIf many variables have been measured and included in exploratory analyses in an early discovery phase, consider providing a list with details on each variable in an appendix, additional table or separate publication. Of note, the International Journal of Epidemiology recently launched a new section with cohort profiles, that includes detailed information on what was measured at different points in time in particular studies [56,57]. Finally, we advise that authors declare all candidate variables considered for statistical analysis, rather than selectively reporting only those included in the final models (see also item 16a) [58,59]"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-7_variables-2",
    "href": "guidelines/strobe-cohort/index.html#sec-7_variables-2",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Examples",
    "text": "Examples\n\nOnly major congenital malformations were included in the analyses. Minor anomalies were excluded according to the exclusion list of European Registration of Congenital Anomalies (EUROCAT). If a child had more than one major congenital malformation of one organ system, those malformations were treated as one outcome in the analyses by organ system (…) In the statistical analyses, factors considered potential confounders were maternal age at delivery and number of previous parities. Factors considered potential effect modifiers were maternal age at reimbursement for antiepileptic medication and maternal age at delivery\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-8_data_sources__measurement-1",
    "href": "guidelines/strobe-cohort/index.html#sec-8_data_sources__measurement-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nThe way in which exposures, confounders and outcomes were measured affects the reliability and validity of a study. Measurement error and misclassification of exposures or outcomes can make it more difficult to detect cause-effect relationships, or may produce spurious relationships. Error in measurement of potential confounders can increase the risk of residual confounding [62,63]. It is helpful, therefore, if authors report the findings of any studies of the validity or reliability of assessments or measurements, including details of the reference standard that was used. Rather than simply citing validation studies (as in the first example), we advise that authors give the estimated validity or reliability, which can then be used for measurement error adjustment or sensitivity analyses (see items 12e and 17).\nIn addition, it is important to know if groups being compared differed with respect to the way in which the data were collected. This may be important for laboratory examinations (as in the second example) and other situations. For instance, if an interviewer first questions all the cases and then the controls, or vice versa, bias is possible because of the learning curve; solutions such as randomising the order of interviewing may avoid this problem. Information bias may also arise if the compared groups are not given the same diagnostic tests or if one group receives more tests of the same kind than another (see also item 9)."
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-8_data_sources__measurement-2",
    "href": "guidelines/strobe-cohort/index.html#sec-8_data_sources__measurement-2",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Examples",
    "text": "Examples\n\nTotal caffeine intake was calculated primarily using US Department of Agriculture food composition sources. In these calculations, it was assumed that the content of caffeine was 137 mg per cup of coffee, 47 mg per cup of tea, 46 mg per can or bottle of cola beverage, and 7 mg per serving of chocolate candy. This method of measuring (caffeine) intake was shown to be valid in both the NHS I cohort and a similar cohort study of male health professionals (…) Self-reported diagnosis of hypertension was found to be reliable in the NHS I cohort\n\n\nSamples pertaining to matched cases and controls were always analyzed together in the same batch and laboratory personnel were unable to distinguish among cases and controls\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-9_bias-1",
    "href": "guidelines/strobe-cohort/index.html#sec-9_bias-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nBiased studies produce results that differ systematically from the truth (see also Box 3). It is important for a reader to know what measures were taken during the conduct of a study to reduce the potential of bias. Ideally, investigators carefully consider potential sources of bias when they plan their study. At the stage of reporting, we recommend that authors always assess the likelihood of relevant biases. Specifically, the direction and magnitude of bias should be discussed and, if possible, estimated. For instance, in case-control studies information bias can occur, but may be reduced by selecting an appropriate control group, as in the first example [64]. Differences in the medical surveillance of participants were a problem in the second example [65]. Consequently, the authors provide more detail about the additional data they collected to tackle this problem. When investigators have set up quality control programs for data collection to counter a possible drift in measurements of variables in longitudinal studies, or to keep variability at a minimum when multiple observers are used, these should be described.\nUnfortunately, authors often do not address important biases when reporting their results. Among 43 case-control and cohort studies published from 1990 to 1994 that investigated the risk of second cancers in patients with a history of cancer, medical surveillance bias was mentioned in only 5 articles [66]. A survey of reports of mental health research published during 1998 in three psychiatric journals found that only 13% of 392 articles mentioned response bias [67]. A survey of cohort studies in stroke research found that 14 of 49 (28%) articles published from 1999 to 2003 addressed potential selection bias in the recruitment of study participants and 35 (71%) mentioned the possibility that any type of bias may have affected results"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-9_bias-2",
    "href": "guidelines/strobe-cohort/index.html#sec-9_bias-2",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Examples",
    "text": "Examples\n\nDetection bias could influence the association between Type 2 diabetes mellitus (T2DM) and primary open-angle glaucoma (POAG) if women with T2DM were under closer ophthalmic surveillance than women without this condition. We compared the mean number of eye examinations reported by women with and without diabetes. We also recalculated the relative risk for POAG with additional control for covariates associated with more careful ocular surveillance (a self-report of cataract, macular degeneration, number of eye examinations, and number of physical examinations)ave recently experienced the death of a family member or close associate - and are therefore more comparable to the sources of information in the suicide group than if living controls were used\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-10_study_size-1",
    "href": "guidelines/strobe-cohort/index.html#sec-10_study_size-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nA study should be large enough to obtain a point estimate with a sufficiently narrow confidence interval to meaningfully answer a research question. Large samples are needed to distinguish a small association from no association. Small studies often provide valuable information, but wide confidence intervals may indicate that they contribute less to current knowledge in comparison with studies providing estimates with narrower confidence intervals. Also, small studies that show interesting or statistically significant associations are published more frequently than small studies that do not have significant findings. While these studies may provide an early signal in the context of discovery, readers should be informed of their potential weaknesses.\nThe importance of sample size determination in observational studies depends on the context. If an analysis is performed on data that were already available for other purposes, the main question is whether the analysis of the data will produce results with sufficient statistical precision to contribute substantially to the literature, and sample size considerations will be informal. Formal, a priori calculation of sample size may be useful when planning a new study. Such calculations are associated with more uncertainty than implied by the single number that is generally produced. For example, estimates of the rate of the event of interest or other assumptions central to calculations are commonly imprecise, if not guesswork. The precision obtained in the final analysis can often not be determined beforehand because it will be reduced by inclusion of confounding variables in multivariable analyses, the degree of precision with which key variables can be measured, and the exclusion of some individuals.\nFew epidemiological studies explain or report deliberations about sample size. We encourage investigators to report pertinent formal sample size calculations if they were done. In other situations they should indicate the considerations that determined the study size (e.g., a fixed available sample, as in the first example above). If the observational study was stopped early when statistical significance was achieved, readers should be told. Do not bother readers with post hoc justifications for study size or retrospective power calculations. From the point of view of the reader, confidence intervals indicate the statistical precision that was ultimately obtained. It should be realized that confidence intervals reflect statistical uncertainty only, and not all uncertainty that may be present in a study (see item 20)."
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-11_quantitative_variables-1",
    "href": "guidelines/strobe-cohort/index.html#sec-11_quantitative_variables-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nInvestigators make choices regarding how to collect and analyse quantitative data about exposures, effect modifiers and confounders. For example, they may group a continuous exposure variable to create a new categorical variable. Grouping choices may have important consequences for later analyses. We advise that authors explain why and how they grouped quantitative data, including the number of categories, the cut-points, and category mean or median values. Whenever data are reported in tabular form, the counts of cases, controls, persons at risk, person-time at risk, etc. should be given for each category. Tables should not consist solely of effect-measure estimates or results of model fitting.\nInvestigators might model an exposure as continuous in order to retain all the information. In making this choice, one needs to consider the nature of the relationship of the exposure to the outcome. As it may be wrong to assume a linear relation automatically, possible departures from linearity should be investigated. Authors could mention alternative models they explored during analyses (e.g., using log transformation, quadratic terms or spline functions). Several methods exist for fitting a non-linear relation between the exposure and outcome. Also, it may be informative to present both continuous and grouped analyses for a quantitative exposure of prime interest.\nIn a recent survey, two thirds of epidemiological publications studied quantitative exposure variables. In 42 of 50 articles (84%) exposures were grouped into several ordered categories, but often without any stated rationale for the choices made. Fifteen articles used linear associations to model continuous exposure but only two reported checking for linearity. In another survey, of the psychological literature, dichotomization was justified in only 22 of 110 articles (20%)"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-11_quantitative_variables-2",
    "href": "guidelines/strobe-cohort/index.html#sec-11_quantitative_variables-2",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Examples",
    "text": "Examples\n\nPatients with a Glasgow Coma Scale less than 8 are considered to be seriously injured. A GCS of 9 or more indicates less serious brain injury. We examined the association of GCS in these two categories with the occurrence of death within 12 months from injury\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-12a_statistical_methods-1",
    "href": "guidelines/strobe-cohort/index.html#sec-12a_statistical_methods-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nIn general, there is no one correct statistical analysis but, rather, several possibilities that may address the same question, but make different assumptions. Regardless, investigators should pre-determine analyses at least for the primary study objectives in a study protocol. Often additional analyses are needed, either instead of, or as well as, those originally envisaged, and these may sometimes be motivated by the data. When a study is reported, authors should tell readers whether particular analyses were suggested by data inspection. Even though the distinction between pre-specified and exploratory analyses may sometimes be blurred, authors should clarify reasons for particular analyses.\nIf groups being compared are not similar with regard to some characteristics, adjustment should be made for possible confounding variables by stratification or by multivariable regression (see Box 5) [94]. Often, the study design determines which type of regression analysis is chosen. For instance, Cox proportional hazard regression is commonly used in cohort studies [95]. whereas logistic regression is often the method of choice in case-control studies [96,97]. Analysts should fully describe specific procedures for variable selection and not only present results from the final model [98,99]. If model comparisons are made to narrow down a list of potential confounders for inclusion in a final model, this process should be described. It is helpful to tell readers if one or two covariates are responsible for a great deal of the apparent confounding in a data analysis. Other statistical analyses such as imputation procedures, data transformation, and calculations of attributable risks should also be described. Non-standard or novel approaches should be referenced and the statistical software used reported. As a guiding principle, we advise statistical methods be described with enough detail to enable a knowledgeable reader with access to the original data to verify the reported results [100].\nIn an empirical study, only 93 of 169 articles (55%) reporting adjustment for confounding clearly stated how continuous and multi-category variables were entered into the statistical model [101]. Another study found that among 67 articles in which statistical analyses were adjusted for confounders, it was mostly unclear how confounders were chosen"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-12a_statistical_methods-2",
    "href": "guidelines/strobe-cohort/index.html#sec-12a_statistical_methods-2",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Examples",
    "text": "Examples\n\nThe adjusted relative risk was calculated using the Mantel-Haenszel technique, when evaluating if confounding by age or gender was present in the groups compared. The 95% confidence interval (CI) was computed around the adjusted relative risk, using the variance according to Greenland and Robins and Robins et al.\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-12b_statistical_methods-1",
    "href": "guidelines/strobe-cohort/index.html#sec-12b_statistical_methods-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nAs discussed in detail under item 17, many debate the use and value of analyses restricted to subgroups of the study population [4,104]. Subgroup analyses are nevertheless often done [4]. Readers need to know which subgroup analyses were planned in advance, and which arose while analysing the data. Also, it is important to explain what methods were used to examine whether effects or associations differed across groups (see item 17).\nInteraction relates to the situation when one factor modifies the effect of another (therefore also called effect modification). The joint action of two factors can be characterized in two ways: on an additive scale, in terms of risk differences; or on a multiplicative scale, in terms of relative risk (see Box 8). Many authors and readers may have their own preference about the way interactions should be analysed. Still, they may be interested to know to what extent the joint effect of exposures differs from the separate effects. There is consensus that the additive scale, which uses absolute risks, is more appropriate for public health and clinical decision making [105]. Whatever view is taken, this should be clearly presented to the reader, as is done in the example above [103]. A lay-out presenting separate effects of both exposures as well as their joint effect, each relative to no exposure, might be most informative. It is presented in the example for interaction under item 17, and the calculations on the different scales are explained in Box 8"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-12b_statistical_methods-2",
    "href": "guidelines/strobe-cohort/index.html#sec-12b_statistical_methods-2",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Examples",
    "text": "Examples\n\nSex differences in susceptibility to the 3 lifestyle-related risk factors studied were explored by testing for biological interaction according to Rothman: a new composite variable with 4 categories (a−b−, a−b+, a+b−, and a+b+) was redefined for sex and a dichotomous exposure of interest where a− and b− denote absence of exposure. RR was calculated for each category after adjustment for age. An interaction effect is defined as departure from additivity of absolute effects, and excess RR caused by interaction (RERI) was calculated:\n\n[calculation]\nwhere RR(a+b+) denotes RR among those exposed to both factors where RR(a−b−) is used as reference category (RR = 1.0). Ninety-five percent CIs were calculated as proposed by Hosmer and Lemeshow. RERI of 0 means no interaction\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-12c_statistical_methods-1",
    "href": "guidelines/strobe-cohort/index.html#sec-12c_statistical_methods-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nMissing data are common in observational research. Questionnaires posted to study participants are not always filled in completely, participants may not attend all follow-up visits and routine data sources and clinical databases are often incomplete. Despite its ubiquity and importance, few papers report in detail on the problem of missing data [5,107]. Investigators may use any of several approaches to address missing data. We describe some strengths and limitations of various approaches in Box 6. We advise that authors report the number of missing values for each variable of interest (exposures, outcomes, confounders) and for each step in the analysis. Authors should give reasons for missing values if possible, and indicate how many individuals were excluded because of missing data when describing the flow of participants through the study (see also item 13). For analyses that account for missing data, authors should describe the nature of the analysis (e.g., multiple imputation) and the assumptions that were made (e.g., missing at random, see Box 6)."
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-12c_statistical_methods-2",
    "href": "guidelines/strobe-cohort/index.html#sec-12c_statistical_methods-2",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Examples",
    "text": "Examples\n\nOur missing data analysis procedures used missing at random (MAR) assumptions. We used the MICE (multivariate imputation by chained equations) method of multiple multivariate imputation in STATA. We independently analysed 10 copies of the data, each with missing values suitably imputed, in the multivariate logistic regression analyses. We averaged estimates of the variables to give a single mean estimate and adjusted standard errors according to Rubin’s rules\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-12d_statistical_methods-1",
    "href": "guidelines/strobe-cohort/index.html#sec-12d_statistical_methods-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nCohort studies are analysed using life table methods or other approaches that are based on the person-time of follow-up and time to developing the disease of interest. Among individuals who remain free of the disease at the end of their observation period, the amount of follow-up time is assumed to be unrelated to the probability of developing the outcome. This will be the case if follow-up ends on a fixed date or at a particular age. Loss to follow-up occurs when participants withdraw from a study before that date. This may hamper the validity of a study if loss to follow-up occurs selectively in exposed individuals, or in persons at high risk of developing the disease (informative censoring). In the example above, patients lost to follow-up in treatment programmes with no active follow-up had fewer CD4 helper cells than those remaining under observation and were therefore at higher risk of dying [116].\nIt is important to distinguish persons who reach the end of the study from those lost to follow-up. Unfortunately, statistical software usually does not distinguish between the two situations: in both cases follow-up time is automatically truncated (censored) at the end of the observation period. Investigators therefore need to decide, ideally at the stage of planning the study, how they will deal with loss to follow-up. When few patients are lost, investigators may either exclude individuals with incomplete follow-up, or treat them as if they withdrew alive at either the date of loss to follow-up or the end of the study. We advise authors to report how many patients were lost to follow-up and what censoring strategies they used"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-12d_statistical_methods-2",
    "href": "guidelines/strobe-cohort/index.html#sec-12d_statistical_methods-2",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Examples",
    "text": "Examples\n\nIn treatment programmes with active follow-up, those lost to follow-up and those followed-up at 1 year had similar baseline CD4 cell counts (median 115 cells per μL and 123 cells per μL), whereas patients lost to follow-up in programmes with no active follow-up procedures had considerably lower CD4 cell counts than those followed-up (median 64 cells per μL and 123 cells per μL). (…) Treatment programmes with passive follow-up were excluded from subsequent analyses\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-12e_statistical_methods-1",
    "href": "guidelines/strobe-cohort/index.html#sec-12e_statistical_methods-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nSensitivity analyses are useful to investigate whether or not the main results are consistent with those obtained with alternative analysis strategies or assumptions [121]. Issues that may be examined include the criteria for inclusion in analyses, the definitions of exposures or outcomes [122], which confounding variables merit adjustment, the handling of missing data [120,123], possible selection bias or bias from inaccurate or inconsistent measurement of exposure, disease and other variables, and specific analysis choices, such as the treatment of quantitative variables (see item 11). Sophisticated methods are increasingly used to simultaneously model the influence of several biases or assumptions [124–126].\nIn 1959 Cornfield et al. famously showed that a relative risk of 9 for cigarette smoking and lung cancer was extremely unlikely to be due to any conceivable confounder, since the confounder would need to be at least nine times as prevalent in smokers as in non-smokers [127]. This analysis did not rule out the possibility that such a factor was present, but it did identify the prevalence such a factor would need to have. The same approach was recently used to identify plausible confounding factors that could explain the association between childhood leukaemia and living near electric power lines [128]. More generally, sensitivity analyses can be used to identify the degree of confounding, selection bias, or information bias required to distort an association. One important, perhaps under recognised, use of sensitivity analysis is when a study shows little or no association between an exposure and an outcome and it is plausible that confounding or other biases toward the null are present"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-12e_statistical_methods-2",
    "href": "guidelines/strobe-cohort/index.html#sec-12e_statistical_methods-2",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Examples",
    "text": "Examples\n\nBecause we had a relatively higher proportion of missing dead patients with insufficient data (38/148=25.7%) as compared to live patients (15/437=3.4%) (…), it is possible that this might have biased the results. We have, therefore, carried out a sensitivity analysis. We have assumed that the proportion of women using oral contraceptives in the study group applies to the whole (19.1% for dead, and 11.4% for live patients), and then applied two extreme scenarios: either all the exposed missing patients used second generation pills or they all used third-generation pills\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-13a_participants-1",
    "href": "guidelines/strobe-cohort/index.html#sec-13a_participants-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nDetailed information on the process of recruiting study participants is important for several reasons. Those included in a study often differ in relevant ways from the target population to which results are applied. This may result in estimates of prevalence or incidence that do not reflect the experience of the target population. For example, people who agreed to participate in a postal survey of sexual behaviour attended church less often, had less conservative sexual attitudes and earlier age at first sexual intercourse, and were more likely to smoke cigarettes and drink alcohol than people who refused [130]. These differences suggest that postal surveys may overestimate sexual liberalism and activity in the population. Such response bias (see Box 3) can distort exposure-disease associations if associations differ between those eligible for the study and those included in the study. As another example, the association between young maternal age and leukaemia in offspring, which has been observed in some case-control studies [131,132], was explained by differential participation of young women in case and control groups. Young women with healthy children were less likely to participate than those with unhealthy children [133]. Although low participation does not necessarily compromise the validity of a study, transparent information on participation and reasons for non-participation is essential. Also, as there are no universally agreed definitions for participation, response or follow-up rates, readers need to understand how authors calculated such proportions [134].\nIdeally, investigators should give an account of the numbers of individuals considered at each stage of recruiting study participants, from the choice of a target population to the inclusion of participants’ data in the analysis. Depending on the type of study, this may include the number of individuals considered to be potentially eligible, the number assessed for eligibility, the number found to be eligible, the number included in the study, the number examined, the number followed up and the number included in the analysis. Information on different sampling units may be required, if sampling of study participants is carried out in two or more stages as in the example above (multistage sampling). In case-control studies, we advise that authors describe the flow of participants separately for case and control groups [135]. Controls can sometimes be selected from several sources, including, for example, hospitalised patients and community dwellers. In this case, we recommend a separate account of the numbers of participants for each type of control group. Olson and colleagues proposed useful reporting guidelines for controls recruited through random-digit dialling and other methods [136].\nA recent survey of epidemiological studies published in 10 general epidemiology, public health and medical journals found that some information regarding participation was provided in 47 of 107 case-control studies (59%), 49 of 154 cohort studies (32%), and 51 of 86 cross-sectional studies (59%) [137]. Incomplete or absent reporting of participation and non-participation in epidemiological studies was also documented in two other surveys of the literature [4,5]. Finally, there is evidence that participation in epidemiological studies may have declined in recent decades [137,138], which underscores the need for transparent reporting"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-13a_participants-2",
    "href": "guidelines/strobe-cohort/index.html#sec-13a_participants-2",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Examples",
    "text": "Examples\n\nOf the 105 freestanding bars and taverns sampled, 13 establishments were no longer in business and 9 were located in restaurants, leaving 83 eligible businesses. In 22 cases, the owner could not be reached by telephone despite 6 or more attempts. The owners of 36 bars declined study participation. (…) The 25 participating bars and taverns employed 124 bartenders, with 67 bartenders working at least 1 weekly daytime shift. Fifty-four of the daytime bartenders (81%) completed baseline interviews and spirometry; 53 of these subjects (98%) completed follow-up\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-13b_participants-1",
    "href": "guidelines/strobe-cohort/index.html#sec-13b_participants-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nExplaining the reasons why people no longer participated in a study or why they were excluded from statistical analyses helps readers judge whether the study population was representative of the target population and whether bias was possibly introduced. For example, in a cross-sectional health survey, non-participation due to reasons unlikely to be related to health status (for example, the letter of invitation was not delivered because of an incorrect address) will affect the precision of estimates but will probably not introduce bias. Conversely, if many individuals opt out of the survey because of illness, or perceived good health, results may underestimate or overestimate the prevalence of ill health in the population"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-13b_participants-2",
    "href": "guidelines/strobe-cohort/index.html#sec-13b_participants-2",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Examples",
    "text": "Examples\n\nThe main reasons for non-participation were the participant was too ill or had died before interview (cases 30%, controls < 1%), nonresponse (cases 2%, controls 21%), refusal (cases 10%, controls 29%), and other reasons (refusal by consultant or general practitioner, non-English speaking, mental impairment) (cases 7%, controls 5%)\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-13c_participants-1",
    "href": "guidelines/strobe-cohort/index.html#sec-13c_participants-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nAn informative and well-structured flow diagram can readily and transparently convey information that might otherwise require a lengthy description [142], as in the example above. The diagram may usefully include the main results, such as the number of events for the primary outcome. While we recommend the use of a flow diagram, particularly for complex observational studies, we do not propose a specific format for the diagram"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-13c_participants-2",
    "href": "guidelines/strobe-cohort/index.html#sec-13c_participants-2",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Examples",
    "text": "Examples\n\nFlow diagram from Hay et al. https://doi.org/10.1371/journal.pmed.0040297.g001\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-14a_descriptive_data-1",
    "href": "guidelines/strobe-cohort/index.html#sec-14a_descriptive_data-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nReaders need descriptions of study participants and their exposures to judge the generalisability of the findings. Information about potential confounders, including whether and how they were measured, influences judgments about study validity. We advise authors to summarize continuous variables for each study group by giving the mean and standard deviation, or when the data have an asymmetrical distribution, as is often the case, the median and percentile range (e.g., 25th and 75th percentiles). Variables that make up a small number of ordered categories (such as stages of disease I to IV) should not be presented as continuous variables; it is preferable to give numbers and proportions for each category (see also Box 4). In studies that compare groups, the descriptive characteristics and numbers should be given by group, as in the example above.\nInferential measures such as standard errors and confidence intervals should not be used to describe the variability of characteristics, and significance tests should be avoided in descriptive tables. Also, P values are not an appropriate criterion for selecting which confounders to adjust for in analysis; even small differences in a confounder that has a strong effect on the outcome can be important.\nIn cohort studies, it may be useful to document how an exposure relates to other characteristics and potential confounders. Authors could present this information in a table with columns for participants in two or more exposure categories, which permits to judge the differences in confounders between these categories.\nIn case-control studies potential confounders cannot be judged by comparing cases and controls. Control persons represent the source population and will usually be different from the cases in many respects. For example, in a study of oral contraceptives and myocardial infarction, a sample of young women with infarction more often had risk factors for that disease, such as high serum cholesterol, smoking and a positive family history, than the control group [146]. This does not influence the assessment of the effect of oral contraceptives, as long as the prescription of oral contraceptives was not guided by the presence of these risk factors—e.g., because the risk factors were only established after the event (see also Box 5). In case-control studies the equivalent of comparing exposed and non-exposed for the presence of potential confounders (as is done in cohorts) can be achieved by exploring the source population of the cases: if the control group is large enough and represents the source population, exposed and unexposed controls can be compared for potential confounders [121,147]."
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-14a_descriptive_data-2",
    "href": "guidelines/strobe-cohort/index.html#sec-14a_descriptive_data-2",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Examples",
    "text": "Examples\n\nCharacteristics of the Study Base at Enrolment, Castellana G (Italy), 1985–1986\n\nhttps://doi.org/10.1371/journal.pmed.0040297.t002\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-14b_descriptive_data-1",
    "href": "guidelines/strobe-cohort/index.html#sec-14b_descriptive_data-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nAs missing data may bias or affect generalisability of results, authors should tell readers amounts of missing data for exposures, potential confounders, and other important characteristics of patients (see also item 12c and Box 6). In a cohort study, authors should report the extent of loss to follow-up (with reasons), since incomplete follow-up may bias findings (see also items 12d and 13). We advise authors to use their tables and figures to enumerate amounts of missing data."
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-14b_descriptive_data-2",
    "href": "guidelines/strobe-cohort/index.html#sec-14b_descriptive_data-2",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Examples",
    "text": "Examples\n\nTable. Symptom End Points Used in Survival Analysis\n\nhttps://doi.org/10.1371/journal.pmed.0040297.t003\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-14c_descriptive_data-1",
    "href": "guidelines/strobe-cohort/index.html#sec-14c_descriptive_data-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nReaders need to know the duration and extent of follow-up for the available outcome data. Authors can present a summary of the average follow-up with either the mean or median follow-up time or both. The mean allows a reader to calculate the total number of person-years by multiplying it with the number of study participants. Authors also may present minimum and maximum times or percentiles of the distribution to show readers the spread of follow-up times. They may report total person-years of follow-up or some indication of the proportion of potential data that was captured [148]. All such information may be presented separately for participants in two or more exposure categories. Almost half of 132 articles in cancer journals (mostly cohort studies) did not give any summary of length of follow-up"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-14c_descriptive_data-2",
    "href": "guidelines/strobe-cohort/index.html#sec-14c_descriptive_data-2",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Examples",
    "text": "Examples\n\nDuring the 4366 person-years of follow-up (median 5.4, maximum 8.3 years), 265 subjects were diagnosed as having dementia, including 202 with Alzheimer’s disease\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-15_outcome_data-1",
    "href": "guidelines/strobe-cohort/index.html#sec-15_outcome_data-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nBefore addressing the possible association between exposures (risk factors) and outcomes, authors should report relevant descriptive data. It may be possible and meaningful to present measures of association in the same table that presents the descriptive data (see item 14a). In a cohort study with events as outcomes, report the numbers of events for each outcome of interest. Consider reporting the event rate per person-year of follow-up. If the risk of an event changes over follow-up time, present the numbers and rates of events in appropriate intervals of follow-up or as a Kaplan-Meier life table or plot. It might be preferable to show plots as cumulative incidence that go up from 0% rather than down from 100%, especially if the event rate is lower than, say, 30% [153]. Consider presenting such information separately for participants in different exposure categories of interest. If a cohort study is investigating other time-related outcomes (e.g., quantitative disease markers such as blood pressure), present appropriate summary measures (e.g., means and standard deviations) over time, perhaps in a table or figure.\nFor cross-sectional studies, we recommend presenting the same type of information on prevalent outcome events or summary measures. For case-control studies, the focus will be on reporting exposures separately for cases and controls as frequencies or quantitative summaries [154]. For all designs, it may be helpful also to tabulate continuous outcomes or exposures in categories, even if the data are not analysed as such"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-15_outcome_data-2",
    "href": "guidelines/strobe-cohort/index.html#sec-15_outcome_data-2",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Examples",
    "text": "Examples\n\nTable. Rates of HIV-1 Seroconversion by Selected Sociodemographic Variables: 1990–1993 https://doi.org/10.1371/journal.pmed.0040297.t004\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-16a_main_results-1",
    "href": "guidelines/strobe-cohort/index.html#sec-16a_main_results-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nIn many situations, authors may present the results of unadjusted or minimally adjusted analyses and those from fully adjusted analyses. We advise giving the unadjusted analyses together with the main data, for example the number of cases and controls that were exposed or not. This allows the reader to understand the data behind the measures of association (see also item 15). For adjusted analyses, report the number of persons in the analysis, as this number may differ because of missing values in covariates (see also item 12c). Estimates should be given with confidence intervals.\nReaders can compare unadjusted measures of association with those adjusted for potential confounders and judge by how much, and in what direction, they changed. Readers may think that adjusted results equal the causal part of the measure of association, but adjusted results are not necessarily free of random sampling error, selection bias, information bias, or residual confounding (see Box 5). Thus, great care should be exercised when interpreting adjusted results, as the validity of results often depends crucially on complete knowledge of important confounders, their precise measurement, and appropriate specification in the statistical model (see also item 20) [157,158].\nAuthors should explain all potential confounders considered, and the criteria for excluding or including variables in statistical models. Decisions about excluding or including variables should be guided by knowledge, or explicit assumptions, on causal relations. Inappropriate decisions may introduce bias, for example by including variables that are in the causal pathway between exposure and disease (unless the aim is to asses how much of the effect is carried by the intermediary variable). If the decision to include a variable in the model was based on the change in the estimate, it is important to report what change was considered sufficiently important to justify its inclusion. If a backward deletion or forward inclusion strategy was used to select confounders, explain that process and give the significance level for rejecting the null hypothesis of no confounding. Of note, we and others do not advise selecting confounders based solely on statistical significance testing [147,159,160].\nRecent studies of the quality of reporting of epidemiological studies found that confidence intervals were reported in most articles [4]. However, few authors explained their choice of confounding variables [4,5]."
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-16a_main_results-2",
    "href": "guidelines/strobe-cohort/index.html#sec-16a_main_results-2",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Examples",
    "text": "Examples\n\nTable. Relative Rates of Rehospitalisation by Treatment in Patients in Community Care after First Hospitalisation due to Schizophrenia and Schizoaffective Disorder https://doi.org/10.1371/journal.pmed.0040297.t008\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-16b_main_results-1",
    "href": "guidelines/strobe-cohort/index.html#sec-16b_main_results-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nCategorizing continuous data has several important implications for analysis (see Box 4) and also affects the presentation of results. In tables, outcomes should be given for each exposure category, for example as counts of persons at risk, person-time at risk, if relevant separately for each group (e.g., cases and controls). Details of the categories used may aid comparison of studies and meta-analysis. If data were grouped using conventional cut-points, such as body mass index thresholds [162], group boundaries (i.e., range of values) can be derived easily, except for the highest and lowest categories. If quantile-derived categories are used, the category boundaries cannot be inferred from the data. As a minimum, authors should report the category boundaries; it is helpful also to report the range of the data and the mean or median values within categories."
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-16b_main_results-2",
    "href": "guidelines/strobe-cohort/index.html#sec-16b_main_results-2",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Examples",
    "text": "Examples\n\nTable. Polychlorinated Biphenyls in Cord Serum\n\nhttps://doi.org/10.1371/journal.pmed.0040297.t005\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-16c_main_results-1",
    "href": "guidelines/strobe-cohort/index.html#sec-16c_main_results-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nThe results from studies examining the association between an exposure and a disease are commonly reported in relative terms, as ratios of risks, rates or odds (see Box 8). Relative measures capture the strength of the association between an exposure and disease. If the relative risk is a long way from 1 it is less likely that the association is due to confounding [164,165]. Relative effects or associations tend to be more consistent across studies and populations than absolute measures, but what often tends to be the case may be irrelevant in a particular instance. For example, similar relative risks were obtained for the classic cardiovascular risk factors for men living in Northern Ireland, France, the USA and Germany, despite the fact that the underlying risk of coronary heart disease varies substantially between these countries [166,167]. In contrast, in a study of hypertension as a risk factor for cardiovascular disease mortality, the data were more compatible with a constant rate difference than with a constant rate ratio [168].\nWidely used statistical models, including logistic [169] and proportional hazards (Cox) regression [170] are based on ratio measures. In these models, only departures from constancy of ratio effect measures are easily discerned. Nevertheless, measures which assess departures from additivity of risk differences, such as the Relative Excess Risk from Interaction (RERI, see item 12b and Box 8), can be estimated in models based on ratio measures.\nIn many circumstances, the absolute risk associated with an exposure is of greater interest than the relative risk. For example, if the focus is on adverse effects of a drug, one will want to know the number of additional cases per unit time of use (e.g., days, weeks, or years). The example gives the additional number of breast cancer cases per 1000 women who used hormone-replacement therapy for 10 years [163]. Measures such as the attributable risk or population attributable fraction may be useful to gauge how much disease can be prevented if the exposure is eliminated. They should preferably be presented together with a measure of statistical uncertainty (e.g., confidence intervals as in the example). Authors should be aware of the strong assumptions made in this context, including a causal relationship between a risk factor and disease (also see Box 7) [171]. Because of the semantic ambiguity and complexities involved, authors should report in detail what methods were used to calculate attributable risks, ideally giving the formulae used [172].\nA recent survey of abstracts of 222 articles published in leading medical journals found that in 62% of abstracts of randomised trials including a ratio measure absolute risks were given, but only in 21% of abstracts of cohort studies [173]. A free text search of Medline 1966 to 1997 showed that 619 items mentioned attributable risks in the title or abstract, compared to 18,955 using relative risk or odds ratio, for a ratio of 1 to 31 [174]."
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-16c_main_results-2",
    "href": "guidelines/strobe-cohort/index.html#sec-16c_main_results-2",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Examples",
    "text": "Examples\n\n10 years’ use of HRT [hormone replacement therapy] is estimated to result in five (95% CI 3–7) additional breast cancers per 1000 users of oestrogen-only preparations and 19 (15–23) additional cancers per 1000 users of oestrogen-progestagen combinations\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-17_other_analyses-1",
    "href": "guidelines/strobe-cohort/index.html#sec-17_other_analyses-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nIn addition to the main analysis other analyses are often done in observational studies. They may address specific subgroups, the potential interaction between risk factors, the calculation of attributable risks, or use alternative definitions of study variables in sensitivity analyses.\nThere is debate about the dangers associated with subgroup analyses, and multiplicity of analyses in general [4,104]. In our opinion, there is too great a tendency to look for evidence of subgroup-specific associations, or effect-measure modification, when overall results appear to suggest little or no effect. On the other hand, there is value in exploring whether an overall association appears consistent across several, preferably pre-specified subgroups especially when a study is large enough to have sufficient data in each subgroup. A second area of debate is about interesting subgroups that arose during the data analysis. They might be important findings, but might also arise by chance. Some argue that it is neither possible nor necessary to inform the reader about all subgroup analyses done as future analyses of other data will tell to what extent the early exciting findings stand the test of time [9]. We advise authors to report which analyses were planned, and which were not (see also items 4, 12b and 20). This will allow readers to judge the implications of multiplicity, taking into account the study’s position on the continuum from discovery to verification or refutation.\nA third area of debate is how joint effects and interactions between risk factors should be evaluated: on additive or multiplicative scales, or should the scale be determined by the statistical model that fits best (see also item 12b and Box 8)? A sensible approach is to report the separate effect of each exposure as well as the joint effect—if possible in a table, as in the first example above [183], or in the study by Martinelli et al. [185]. Such a table gives the reader sufficient information to evaluate additive as well as multiplicative interaction (how these calculations are done is shown in Box 8). Confidence intervals for separate and joint effects may help the reader to judge the strength of the data. In addition, confidence intervals around measures of interaction, such as the Relative Excess Risk from Interaction (RERI) relate to tests of interaction or homogeneity tests. One recurrent problem is that authors use comparisons of P-values across subgroups, which lead to erroneous claims about an effect modifier. For instance, a statistically significant association in one category (e.g., men), but not in the other (e.g., women) does not in itself provide evidence of effect modification. Similarly, the confidence intervals for each point estimate are sometimes inappropriately used to infer that there is no interaction when intervals overlap. A more valid inference is achieved by directly evaluating whether the magnitude of an association differs across subgroups.\nSensitivity analyses are helpful to investigate the influence of choices made in the statistical analysis, or to investigate the robustness of the findings to missing data or possible biases (see also item 12b). Judgement is needed regarding the level of reporting of such analyses. If many sensitivity analyses were performed, it may be impractical to present detailed findings for them all. It may sometimes be sufficient to report that sensitivity analyses were carried out and that they were consistent with the main results presented. Detailed presentation is more appropriate if the issue investigated is of major concern, or if effect estimates vary considerably [59,186].\nPocock and colleagues found that 43 out of 73 articles reporting observational studies contained subgroup analyses. The majority claimed differences across groups but only eight articles reported a formal evaluation of interaction (see item 12b) [4]."
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-17_other_analyses-2",
    "href": "guidelines/strobe-cohort/index.html#sec-17_other_analyses-2",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Examples",
    "text": "Examples\n\nTable. Analysis of Oral Contraceptive Use, Presence of Factor V Leiden Allele, and Risk for Venous Thromboembolism\n\nhttps://doi.org/10.1371/journal.pmed.0040297.t009\n\nTable. Sensitivity of the Rate Ratio for Cardiovascular Outcome to an Unmeasured Confounder https://doi.org/10.1371/journal.pmed.0040297.t010\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-18_key_results-1",
    "href": "guidelines/strobe-cohort/index.html#sec-18_key_results-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nIt is good practice to begin the discussion with a short summary of the main findings of the study. The short summary reminds readers of the main findings and may help them assess whether the subsequent interpretation and implications offered by the authors are supported by the findings"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-18_key_results-2",
    "href": "guidelines/strobe-cohort/index.html#sec-18_key_results-2",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Examples",
    "text": "Examples\n\nWe hypothesized that ethnic minority status would be associated with higher levels of cardiovascular disease (CVD) risk factors, but that the associations would be explained substantially by socioeconomic status (SES). Our hypothesis was not confirmed. After adjustment for age and SES, highly significant differences in body mass index, blood pressure, diabetes, and physical inactivity remained between white women and both black and Mexican American women. In addition, we found large differences in CVD risk factors by SES, a finding that illustrates the high-risk status of both ethnic minority women as well as white women with low SES\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-19_limitations-1",
    "href": "guidelines/strobe-cohort/index.html#sec-19_limitations-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nThe identification and discussion of the limitations of a study are an essential part of scientific reporting. It is important not only to identify the sources of bias and confounding that could have affected results, but also to discuss the relative importance of different biases, including the likely direction and magnitude of any potential bias (see also item 9 and Box 3).\nAuthors should also discuss any imprecision of the results. Imprecision may arise in connection with several aspects of a study, including the study size (item 10) and the measurement of exposures, confounders and outcomes (item 8). The inability to precisely measure true values of an exposure tends to result in bias towards unity: the less precisely a risk factor is measured, the greater the bias. This effect has been described as attenuation [201,202], or more recently as regression dilution bias [203]. However, when correlated risk factors are measured with different degrees of imprecision, the adjusted relative risk associated with them can be biased towards or away from unity [204–206].\nWhen discussing limitations, authors may compare the study being presented with other studies in the literature in terms of validity, generalisability and precision. In this approach, each study can be viewed as contribution to the literature, not as a stand-alone basis for inference and action [207]. Surprisingly, the discussion of important limitations of a study is sometimes omitted from published reports. A survey of authors who had published original research articles in The Lancet found that important weaknesses of the study were reported by the investigators in the survey questionnaires, but not in the published article"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-19_limitations-2",
    "href": "guidelines/strobe-cohort/index.html#sec-19_limitations-2",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Examples",
    "text": "Examples\n\nSince the prevalence of counseling increases with increasing levels of obesity, our estimates may overestimate the true prevalence. Telephone surveys also may overestimate the true prevalence of counseling. Although persons without telephones have similar levels of overweight as persons with telephones, persons without telephones tend to be less educated, a factor associated with lower levels of counseling in our study. Also, of concern is the potential bias caused by those who refused to participate as well as those who refused to respond to questions about weight. Furthermore, because data were collected cross-sectionally, we cannot infer that counseling preceded a patient’s attempt to lose weight\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-20_interpretation-1",
    "href": "guidelines/strobe-cohort/index.html#sec-20_interpretation-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nThe heart of the discussion section is the interpretation of a study’s results. Over-interpretation is common and human: even when we try hard to give an objective assessment, reviewers often rightly point out that we went too far in some respects. When interpreting results, authors should consider the nature of the study on the discovery to verification continuum and potential sources of bias, including loss to follow-up and non-participation (see also items 9, 12 and 19). Due consideration should be given to confounding (item 16a), the results of relevant sensitivity analyses, and to the issue of multiplicity and subgroup analyses (item 17). Authors should also consider residual confounding due to unmeasured variables or imprecise measurement of confounders. For example, socioeconomic status (SES) is associated with many health outcomes and often differs between groups being compared. Variables used to measure SES (income, education, or occupation) are surrogates for other undefined and unmeasured exposures, and the true confounder will by definition be measured with error [208]. Authors should address the real range of uncertainty in estimates, which is larger than the statistical uncertainty reflected in confidence intervals. The latter do not take into account other uncertainties that arise from a study’s design, implementation, and methods of measurement [209].\nTo guide thinking and conclusions about causality, some may find criteria proposed by Bradford Hill in 1965 helpful [164]. How strong is the association with the exposure? Did it precede the onset of disease? Is the association consistently observed in different studies and settings? Is there supporting evidence from experimental studies, including laboratory and animal studies? How specific is the exposure’s putative effect, and is there a dose-response relationship? Is the association biologically plausible? These criteria should not, however, be applied mechanically. For example, some have argued that relative risks below 2 or 3 should be ignored [210,211]. This is a reversal of the point by Cornfield et al. about the strength of large relative risks (see item 12b) [127]. Although a causal effect is more likely with a relative risk of 9, it does not follow that one below 3 is necessarily spurious. For instance, the small increase in the risk of childhood leukaemia after intrauterine irradiation is credible because it concerns an adverse effect of a medical procedure for which no alternative explanations are obvious [212]. Moreover, the carcinogenic effects of radiation are well established. The doubling in the risk of ovarian cancer associated with eating 2 to 4 eggs per week is not immediately credible, since dietary habits are associated with a large number of lifestyle factors as well as SES [213]. In contrast, the credibility of much debated epidemiologic findings of a difference in thrombosis risk between different types of oral contraceptives was greatly enhanced by the differences in coagulation found in a randomised cross-over trial [214]. A discussion of the existing external evidence, from different types of studies, should always be included, but may be particularly important for studies reporting small increases in risk. Further, authors should put their results in context with similar studies and explain how the new study affects the existing body of evidence, ideally by referring to a systematic review."
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-20_interpretation-2",
    "href": "guidelines/strobe-cohort/index.html#sec-20_interpretation-2",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Examples",
    "text": "Examples\n\nAny explanation for an association between death from myocardial infarction and use of second generation oral contraceptives must be conjectural. There is no published evidence to suggest a direct biologic mechanism, and there are no other epidemiologic studies with relevant results. (…) The increase in absolute risk is very small and probably applies predominantly to smokers. Due to the lack of corroborative evidence, and because the analysis is based on relatively small numbers, more evidence on the subject is needed. We would not recommend any change in prescribing practice on the strength of these results\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-21_generalisability-1",
    "href": "guidelines/strobe-cohort/index.html#sec-21_generalisability-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nGeneralisability, also called external validity or applicability, is the extent to which the results of a study can be applied to other circumstances [216]. There is no external validity per se; the term is meaningful only with regard to clearly specified conditions [217]. Can results be applied to an individual, groups or populations that differ from those enrolled in the study with regard to age, sex, ethnicity, severity of disease, and co-morbid conditions? Are the nature and level of exposures comparable, and the definitions of outcomes relevant to another setting or population? Are data that were collected in longitudinal studies many years ago still relevant today? Are results from health services research in one country applicable to health systems in other countries?\nThe question of whether the results of a study have external validity is often a matter of judgment that depends on the study setting, the characteristics of the participants, the exposures examined, and the outcomes assessed. Thus, it is crucial that authors provide readers with adequate information about the setting and locations, eligibility criteria, the exposures and how they were measured, the definition of outcomes, and the period of recruitment and follow-up. The degree of non-participation and the proportion of unexposed participants in whom the outcome develops are also relevant. Knowledge of the absolute risk and prevalence of the exposure, which will often vary across populations, are helpful when applying results to other settings and populations (see Box 7)"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-21_generalisability-2",
    "href": "guidelines/strobe-cohort/index.html#sec-21_generalisability-2",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Examples",
    "text": "Examples\n\nHow applicable are our estimates to other HIV-1-infected patients? This is an important question because the accuracy of prognostic models tends to be lower when applied to data other than those used to develop them. We addressed this issue by penalising model complexity, and by choosing models that generalised best to cohorts omitted from the estimation procedure. Our database included patients from many countries from Europe and North America, who were treated in different settings. The range of patients was broad: men and women, from teenagers to elderly people were included, and the major exposure categories were well represented. The severity of immunodeficiency at baseline ranged from not measureable to very severe, and viral load from undetectable to extremely high\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cohort/index.html#sec-22_funding-1",
    "href": "guidelines/strobe-cohort/index.html#sec-22_funding-1",
    "title": "The STROBE cohort guideline for writing a Cohort study.",
    "section": "Read More",
    "text": "Read More\nSome journals require authors to disclose the presence or absence of financial and other conflicts of interest [100,218]. Several investigations show strong associations between the source of funding and the conclusions of research articles [219–222]. The conclusions in randomised trials recommended the experimental drug as the drug of choice much more often (odds ratio 5.3) if the trial was funded by for-profit organisations, even after adjustment for the effect size [223]. Other studies document the influence of the tobacco and telecommunication industries on the research they funded [224–227]. There are also examples of undue influence when the sponsor is governmental or a non-profit organisation.\nAuthors or funders may have conflicts of interest that influence any of the following: the design of the study [228]; choice of exposures [228,229], outcomes [230], statistical methods [231], and selective publication of outcomes [230] and studies [232]. Consequently, the role of the funders should be described in detail: in what part of the study they took direct responsibility (e.g., design, data collection, analysis, drafting of manuscript, decision to publish) [100]. Other sources of undue influence include employers (e.g., university administrators for academic researchers and government supervisors, especially political appointees, for government researchers), advisory committees, litigants, and special interest groups"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#about-this-guideline",
    "href": "guidelines/strobe-cross-sectional/index.html#about-this-guideline",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "About this guideline",
    "text": "About this guideline\n\nThis guideline is relevant to studies reporting cross-sectional studies"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#download-resources",
    "href": "guidelines/strobe-cross-sectional/index.html#download-resources",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Download Resources",
    "text": "Download Resources\nReporting guidelines are best used early:\n\nUse a template to write quickly and avoid the blank page\nUse a checklist to double-check and reassure others that your work is described transparently\n\n\n\n\nFor drafting\n\nTemplate\n\n\n\nFor checking\n\nChecklist\n\n\n\nFor planning\n\nTo-do list"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#guidance",
    "href": "guidelines/strobe-cross-sectional/index.html#guidance",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Guidance",
    "text": "Guidance\nApprox. 2 min read\n\nTitle and abstract\n\n\n1a. Title\n\n\n\n\n\n\n\nIndicate the study’s design with a commonly used term in the title or the abstract\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nReaders should be able to easily identify the design that was used from the title or abstract. An explicit, commonly used term for the study design also helps ensure correct indexing of articles in electronic databases\n\n\nExamples\n\nLeukaemia incidence among workers in the shoe and boot manufacturing industry: a case-control study\n\nBack to top\n\n\n\n\n\n\n1b. Abstract\n\n\n\n\n\n\n\nProvide in the abstract an informative and balanced summary of what was done and what was found\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe abstract provides key information that enables readers to understand a study and decide whether to read the article. Typical components include a statement of the research question, a short description of methods and results, and a conclusion. Abstracts should summarize key details of studies and should only present information that is provided in the article. We advise presenting key results in a numerical form that includes numbers of participants, estimates of associations and appropriate measures of variability and uncertainty (e.g., odds ratios with confidence intervals). We regard it insufficient to state only that an exposure is or is not significantly associated with an outcome.\nA series of headings pertaining to the background, design, conduct, and analysis of a study may help readers acquire the essential information rapidly. Many journals require such structured abstracts, which tend to be of higher quality and more readily informative than unstructured summaries.\n\n\nExamples\n\nBackground: The expected survival of HIV-infected patients is of major public health interest.\n\nObjective: To estimate survival time and age-specific mortality rates of an HIV-infected population compared with that of the general population.\nDesign: Population-based cohort study.\nSetting: All HIV-infected persons receiving care in Denmark from 1995 to 2005.\nPatients: Each member of the nationwide Danish HIV Cohort Study was matched with as many as 99 persons from the general population according to sex, date of birth, and municipality of residence.\nMeasurements: The authors computed Kaplan-Meier life tables with age as the time scale to estimate survival from age 25 years. Patients with HIV infection and corresponding persons from the general population were observed from the date of the patient’s HIV diagnosis until death, emigration, or 1 May 2005.\nResults: 3990 HIV-infected patients and 379,872 persons from the general population were included in the study, yielding 22,744 (median, 5.8 y/person) and 2,689,287 (median, 8.4 years/person) person-years of observation. Three percent of participants were lost to follow-up. From age 25 years, the median survival was 19.9 years (95% CI, 18.5 to 21.3) among patients with HIV infection and 51.1 years (CI, 50.9 to 51.5) among the general population. For HIV-infected patients, survival increased to 32.5 years (CI, 29.4 to 34.7) during the 2000 to 2005 period. In the subgroup that excluded persons with known hepatitis C coinfection (16%), median survival was 38.9 years (CI, 35.4 to 40.1) during this same period. The relative mortality rates for patients with HIV infection compared with those for the general population decreased with increasing age, whereas the excess mortality rate increased with increasing age.\nLimitations: The observed mortality rates are assumed to apply beyond the current maximum observation time of 10 years.\nConclusions: The estimated median survival is more than 35 years for a young person diagnosed with HIV infection in the late highly active antiretroviral therapy era. However, an ongoing effort is still needed to further reduce mortality rates for these persons compared with the general population\nBack to top\n\n\n\n\n\n\nIntroduction\n\n\n2. Background / rationale\n\n\n\n\n\n\n\nExplain the scientific background and rationale for the investigation being reported\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe scientific background of the study provides important context for readers. It sets the stage for the study and describes its focus. It gives an overview of what is known on a topic and what gaps in current knowledge are addressed by the study. Background material should note recent pertinent studies and any systematic reviews of pertinent studies.\n\n\nExamples\n\nConcerns about the rising prevalence of obesity in children and adolescents have focused on the well documented associations between childhood obesity and increased cardiovascular risk1 and mortality in adulthood.2 Childhood obesity has considerable social and psychological consequences within childhood and adolescence,3 yet little is known about social, socioeconomic, and psychological consequences in adult life.\n\nA recent systematic review found no longitudinal studies on the outcomes of childhood obesity other than physical health outcomes3 and only two longitudinal studies of the socioeconomic effects of obesity in adolescence. Gortmaker et al found that US women who had been obese in late adolescence in 1981 were less likely to be married and had lower incomes seven years later than women who had not been overweight, while men who had been overweight were less likely to be married.4 Sargent et al found that UK women, but not men, who had been obese at 16 years in 1974 earned 7.4% less than their non-obese peers at age 23.5\nThe study of adult outcomes of childhood obesity is difficult because obesity often continues into adult life and therefore poorer socioeconomic and educational outcomes may actually reflect confounding by adult obesity. Yet identifying outcomes related to obesity confined childhood is important in determining whether people who are obese in childhood and who later lose weight remain at risk for adult adversity and inequalities.\nWe used longitudinal data from the 1970 British birth cohort to examine the adult socioeconomic, educational, social, and psychological outcomes of childhood obesity. We hypothesised that obesity limited to childhood has fewer adverse adult outcomes than obesity that persists into adult life\nBack to top\n\n\n\n\n\n\n3. Objectives\n\n\n\n\n\n\n\nState specific objectives, including any prespecified hypotheses\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nObjectives are the detailed aims of the study. Well crafted objectives specify populations, exposures and outcomes, and parameters that will be estimated. They may be formulated as specific hypotheses or as questions that the study was designed to address. In some situations objectives may be less specific, for example, in early discovery phases. Regardless, the report should clearly reflect the investigators’ intentions. For example, if important subgroups or additional analyses were not the original aim of the study but arose during data analysis, they should be described accordingly (see also items 4, 17 and 20).\n\n\nExamples\n\nOur primary objectives were to 1) determine the prevalence of domestic violence among female patients presenting to four community-based, primary care, adult medicine practices that serve patients of diverse socioeconomic background and 2) identify demographic and clinical differences between currently abused patients and patients not currently being abused\n\nBack to top\n\n\n\n\n\n\nMethods\n\n\n4. Study design\n\n\n\n\n\n\n\nPresent key elements of study design early in the paper\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nWe advise presenting key elements of study design early in the methods section (or at the end of the introduction) so that readers can understand the basics of the study. For example, authors should indicate that the study was a cohort study, which followed people over a particular time period, and describe the group of persons that comprised the cohort and their exposure status. Similarly, if the investigation used a case-control design, the cases and controls and their source population should be described. If the study was a cross-sectional survey, the population and the point in time at which the cross-section was taken should be mentioned. When a study is a variant of the three main study types, there is an additional need for clarity. For instance, for a case-crossover study, one of the variants of the case-control design, a succinct description of the principles was given in the example above [28].\nWe recommend that authors refrain from simply calling a study prospective or retrospective because these terms are ill defined [29]. One usage sees cohort and prospective as synonymous and reserves the word retrospective for case-control studies [30]. A second usage distinguishes prospective and retrospective cohort studies according to the timing of data collection relative to when the idea for the study was developed [31]. A third usage distinguishes prospective and retrospective case-control studies depending on whether the data about the exposure of interest existed when cases were selected [32]. Some advise against using these terms [33], or adopting the alternatives concurrent and historical for describing cohort studies [34]. In STROBE, we do not use the words prospective and retrospective, nor alternatives such as concurrent and historical. We recommend that, whenever authors use these words, they define what they mean. Most importantly, we recommend that authors describe exactly how and when data collection took place.\nThe first part of the methods section might also be the place to mention whether the report is one of several from a study. If a new report is in line with the original aims of the study, this is usually indicated by referring to an earlier publication and by briefly restating the salient features of the study. However, the aims of a study may also evolve over time. Researchers often use data for purposes for which they were not originally intended, including, for example, official vital statistics that were collected primarily for administrative purposes, items in questionnaires that originally were only included for completeness, or blood samples that were collected for another purpose. For example, the Physicians’ Health Study, a randomized controlled trial of aspirin and carotene, was later used to demonstrate that a point mutation in the factor V gene was associated with an increased risk of venous thrombosis, but not of myocardial infarction or stroke [35]. The secondary use of existing data is a creative part of observational research and does not necessarily make results less credible or less important. However, briefly restating the original aims might help readers understand the context of the research and possible limitations in the data.\n\n\nExamples\n\nWe used a case-crossover design, a variation of a case-control design that is appropriate when a brief exposure (driver’s phone use) causes a transient rise in the risk of a rare outcome (a crash). We compared a driver’s use of a mobile phone at the estimated time of a crash with the same driver’s use during another suitable time period. Because drivers are their own controls, the design controls for characteristics of the driver that may affect the risk of a crash but do not change over a short period of time. As it is important that risks during control periods and crash trips are similar, we compared phone activity during the hazard interval (time immediately before the crash) with phone activity during control intervals (equivalent times during which participants were driving but did not crash) in the previous week\n\nBack to top\n\n\n\n\n\n\n5. Setting\n\n\n\n\n\n\n\nDescribe the setting, locations, and relevant dates, including periods of recruitment, exposure, follow-up, and data collection\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nReaders need information on setting and locations to assess the context and generalisability of a study’s results. Exposures such as environmental factors and therapies can change over time. Also, study methods may evolve over time. Knowing when a study took place and over what period participants were recruited and followed up places the study in historical context and is important for the interpretation of results.\nInformation about setting includes recruitment sites or sources (e.g., electoral roll, outpatient clinic, cancer registry, or tertiary care centre). Information about location may refer to the countries, towns, hospitals or practices where the investigation took place. We advise stating dates rather than only describing the length of time periods. There may be different sets of dates for exposure, disease occurrence, recruitment, beginning and end of follow-up, and data collection. Of note, nearly 80% of 132 reports in oncology journals that used survival analysis included the starting and ending dates for accrual of patients, but only 24% also reported the date on which follow-up ended [37].\n\n\nExamples\n\nThe Pasitos Cohort Study recruited pregnant women from Women, Infant and Child (WIC) clinics in Socorro and San Elizario, El Paso County, Texas and maternal-child clinics of the Mexican Social Security Institute (IMSS) in Ciudad Juarez, Mexico from April 1998 to October 2000. At baseline, prior to the birth of the enrolled cohort children, staff interviewed mothers regarding the household environment. In this ongoing cohort study, we target follow-up exams at 6-month intervals beginning at age 6 months\n\nBack to top\n\n\n\n\n\n\n6a. Eligibility criteria\n\n\n\n\n\n\n\nGive the eligibility criteria, and the sources and methods of selection of participants.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nDetailed descriptions of the study participants help readers understand the applicability of the results. Investigators usually restrict a study population by defining clinical, demographic and other characteristics of eligible participants. Typical eligibility criteria relate to age, gender, diagnosis and comorbid conditions. Despite their importance, eligibility criteria often are not reported adequately. In a survey of observational stroke research, 17 of 49 reports (35%) did not specify eligibility criteria [5].\nEligibility criteria may be presented as inclusion and exclusion criteria, although this distinction is not always necessary or useful. Regardless, we advise authors to report all eligibility criteria and also to describe the group from which the study population was selected (e.g., the general population of a region or country), and the method of recruitment (e.g., referral or self-selection through advertisements).\nKnowing details about follow-up procedures, including whether procedures minimized non-response and loss to follow-up and whether the procedures were similar for all participants, informs judgments about the validity of results. For example, in a study that used IgM antibodies to detect acute infections, readers needed to know the interval between blood tests for IgM antibodies so that they could judge whether some infections likely were missed because the interval between blood tests was too long [41]. In other studies where follow-up procedures differed between exposed and unexposed groups, readers might recognize substantial bias due to unequal ascertainment of events or differences in non-response or loss to follow-up [42]. Accordingly, we advise that researchers describe the methods used for following participants and whether those methods were the same for all participants, and that they describe the completeness of ascertainment of variables (see also item 14).\nIn case-control studies, the choice of cases and controls is crucial to interpreting the results, and the method of their selection has major implications for study validity. In general, controls should reflect the population from which the cases arose. Various methods are used to sample controls, all with advantages and disadvantages: for cases that arise from a general population, population roster sampling, random digit dialling, neighbourhood or friend controls are used. Neighbourhood or friend controls may present intrinsic matching on exposure [17]. Controls with other diseases may have advantages over population-based controls, in particular for hospital-based cases, because they better reflect the catchment population of a hospital, have greater comparability of recall and ease of recruitment. However, they can present problems if the exposure of interest affects the risk of developing or being hospitalized for the control condition(s) [43,44]. To remedy this problem often a mixture of the best defensible control diseases is used [45].\n\n\nExamples\n\nWe retrospectively identified patients with a principal diagnosis of myocardial infarction (code 410) according to the International Classification of Diseases, 9th Revision, Clinical Modification, from codes designating discharge diagnoses, excluding the codes with a fifth digit of 2, which designates a subsequent episode of care (…) A random sample of the entire Medicare cohort with myocardial infarction from February 1994 to July 1995 was selected (…) To be eligible, patients had to present to the hospital after at least 30 minutes but less than 12 hours of chest pain and had to have ST-segment elevation of at least 1 mm on two contiguous leads on the initial electrocardiogram\n\nBack to top\n\n\n\n\n\n\n7.\n\n\n\n\n\n\n\nClearly define all outcomes, exposures, predictors, potential confounders, and effect modifiers. Give diagnostic criteria, if applicable\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAuthors should define all variables considered for and included in the analysis, including outcomes, exposures, predictors, potential confounders and potential effect modifiers. Disease outcomes require adequately detailed description of the diagnostic criteria. This applies to criteria for cases in a case-control study, disease events during follow-up in a cohort study and prevalent disease in a cross-sectional study. Clear definitions and steps taken to adhere to them are particularly important for any disease condition of primary interest in the study.\nFor some studies, determinant or predictor may be appropriate terms for exposure variables and outcomes may be called endpoints. In multivariable models, authors sometimes use dependent variable for an outcome and independent variable or explanatory variable for exposure and confounding variables. The latter is not precise as it does not distinguish exposures from confounders.\nIf many variables have been measured and included in exploratory analyses in an early discovery phase, consider providing a list with details on each variable in an appendix, additional table or separate publication. Of note, the International Journal of Epidemiology recently launched a new section with cohort profiles, that includes detailed information on what was measured at different points in time in particular studies [56,57]. Finally, we advise that authors declare all candidate variables considered for statistical analysis, rather than selectively reporting only those included in the final models (see also item 16a) [58,59]\n\n\nExamples\n\nOnly major congenital malformations were included in the analyses. Minor anomalies were excluded according to the exclusion list of European Registration of Congenital Anomalies (EUROCAT). If a child had more than one major congenital malformation of one organ system, those malformations were treated as one outcome in the analyses by organ system (…) In the statistical analyses, factors considered potential confounders were maternal age at delivery and number of previous parities. Factors considered potential effect modifiers were maternal age at reimbursement for antiepileptic medication and maternal age at delivery\n\nBack to top\n\n\n\n\n\n\n8. Data sources / measurement\n\n\n\n\n\n\n\nFor each variable of interest give sources of data and details of methods of assessment (measurement). Describe comparability of assessment methods if there is more than one group. Give information separately for for exposed and unexposed groups if applicable.\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nThe way in which exposures, confounders and outcomes were measured affects the reliability and validity of a study. Measurement error and misclassification of exposures or outcomes can make it more difficult to detect cause-effect relationships, or may produce spurious relationships. Error in measurement of potential confounders can increase the risk of residual confounding [62,63]. It is helpful, therefore, if authors report the findings of any studies of the validity or reliability of assessments or measurements, including details of the reference standard that was used. Rather than simply citing validation studies (as in the first example), we advise that authors give the estimated validity or reliability, which can then be used for measurement error adjustment or sensitivity analyses (see items 12e and 17).\nIn addition, it is important to know if groups being compared differed with respect to the way in which the data were collected. This may be important for laboratory examinations (as in the second example) and other situations. For instance, if an interviewer first questions all the cases and then the controls, or vice versa, bias is possible because of the learning curve; solutions such as randomising the order of interviewing may avoid this problem. Information bias may also arise if the compared groups are not given the same diagnostic tests or if one group receives more tests of the same kind than another (see also item 9).\n\n\n\n\n\n\n9. Bias\n\n\n\n\n\n\n\nDescribe any efforts to address potential sources of bias\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nBiased studies produce results that differ systematically from the truth (see also Box 3). It is important for a reader to know what measures were taken during the conduct of a study to reduce the potential of bias. Ideally, investigators carefully consider potential sources of bias when they plan their study. At the stage of reporting, we recommend that authors always assess the likelihood of relevant biases. Specifically, the direction and magnitude of bias should be discussed and, if possible, estimated. For instance, in case-control studies information bias can occur, but may be reduced by selecting an appropriate control group, as in the first example [64]. Differences in the medical surveillance of participants were a problem in the second example [65]. Consequently, the authors provide more detail about the additional data they collected to tackle this problem. When investigators have set up quality control programs for data collection to counter a possible drift in measurements of variables in longitudinal studies, or to keep variability at a minimum when multiple observers are used, these should be described.\nUnfortunately, authors often do not address important biases when reporting their results. Among 43 case-control and cohort studies published from 1990 to 1994 that investigated the risk of second cancers in patients with a history of cancer, medical surveillance bias was mentioned in only 5 articles [66]. A survey of reports of mental health research published during 1998 in three psychiatric journals found that only 13% of 392 articles mentioned response bias [67]. A survey of cohort studies in stroke research found that 14 of 49 (28%) articles published from 1999 to 2003 addressed potential selection bias in the recruitment of study participants and 35 (71%) mentioned the possibility that any type of bias may have affected results\n\n\n\n\n\n\n10. Study size\n\n\n\n\n\n\n\nExplain how the study size was arrived at\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nA study should be large enough to obtain a point estimate with a sufficiently narrow confidence interval to meaningfully answer a research question. Large samples are needed to distinguish a small association from no association. Small studies often provide valuable information, but wide confidence intervals may indicate that they contribute less to current knowledge in comparison with studies providing estimates with narrower confidence intervals. Also, small studies that show interesting or statistically significant associations are published more frequently than small studies that do not have significant findings. While these studies may provide an early signal in the context of discovery, readers should be informed of their potential weaknesses.\nThe importance of sample size determination in observational studies depends on the context. If an analysis is performed on data that were already available for other purposes, the main question is whether the analysis of the data will produce results with sufficient statistical precision to contribute substantially to the literature, and sample size considerations will be informal. Formal, a priori calculation of sample size may be useful when planning a new study. Such calculations are associated with more uncertainty than implied by the single number that is generally produced. For example, estimates of the rate of the event of interest or other assumptions central to calculations are commonly imprecise, if not guesswork. The precision obtained in the final analysis can often not be determined beforehand because it will be reduced by inclusion of confounding variables in multivariable analyses, the degree of precision with which key variables can be measured, and the exclusion of some individuals.\nFew epidemiological studies explain or report deliberations about sample size. We encourage investigators to report pertinent formal sample size calculations if they were done. In other situations they should indicate the considerations that determined the study size (e.g., a fixed available sample, as in the first example above). If the observational study was stopped early when statistical significance was achieved, readers should be told. Do not bother readers with post hoc justifications for study size or retrospective power calculations. From the point of view of the reader, confidence intervals indicate the statistical precision that was ultimately obtained. It should be realized that confidence intervals reflect statistical uncertainty only, and not all uncertainty that may be present in a study (see item 20).\n\n\n\n\n\n\n11. Quantitative variables\n\n\n\n\n\n\n\nExplain how quantitative variables were handled in the analyses. If applicable, describe which groupings were chosen, and why\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nInvestigators make choices regarding how to collect and analyse quantitative data about exposures, effect modifiers and confounders. For example, they may group a continuous exposure variable to create a new categorical variable. Grouping choices may have important consequences for later analyses. We advise that authors explain why and how they grouped quantitative data, including the number of categories, the cut-points, and category mean or median values. Whenever data are reported in tabular form, the counts of cases, controls, persons at risk, person-time at risk, etc. should be given for each category. Tables should not consist solely of effect-measure estimates or results of model fitting.\nInvestigators might model an exposure as continuous in order to retain all the information. In making this choice, one needs to consider the nature of the relationship of the exposure to the outcome. As it may be wrong to assume a linear relation automatically, possible departures from linearity should be investigated. Authors could mention alternative models they explored during analyses (e.g., using log transformation, quadratic terms or spline functions). Several methods exist for fitting a non-linear relation between the exposure and outcome. Also, it may be informative to present both continuous and grouped analyses for a quantitative exposure of prime interest.\nIn a recent survey, two thirds of epidemiological publications studied quantitative exposure variables. In 42 of 50 articles (84%) exposures were grouped into several ordered categories, but often without any stated rationale for the choices made. Fifteen articles used linear associations to model continuous exposure but only two reported checking for linearity. In another survey, of the psychological literature, dichotomization was justified in only 22 of 110 articles (20%)\n\n\nExamples\n\nPatients with a Glasgow Coma Scale less than 8 are considered to be seriously injured. A GCS of 9 or more indicates less serious brain injury. We examined the association of GCS in these two categories with the occurrence of death within 12 months from injury\n\nBack to top\n\n\n\n\n\n\n12a. Statistical methods\n\n\n\n\n\n\n\nDescribe all statistical methods, including those used to control for confounding\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nIn general, there is no one correct statistical analysis but, rather, several possibilities that may address the same question, but make different assumptions. Regardless, investigators should pre-determine analyses at least for the primary study objectives in a study protocol. Often additional analyses are needed, either instead of, or as well as, those originally envisaged, and these may sometimes be motivated by the data. When a study is reported, authors should tell readers whether particular analyses were suggested by data inspection. Even though the distinction between pre-specified and exploratory analyses may sometimes be blurred, authors should clarify reasons for particular analyses.\nIf groups being compared are not similar with regard to some characteristics, adjustment should be made for possible confounding variables by stratification or by multivariable regression (see Box 5) [94]. Often, the study design determines which type of regression analysis is chosen. For instance, Cox proportional hazard regression is commonly used in cohort studies [95]. whereas logistic regression is often the method of choice in case-control studies [96,97]. Analysts should fully describe specific procedures for variable selection and not only present results from the final model [98,99]. If model comparisons are made to narrow down a list of potential confounders for inclusion in a final model, this process should be described. It is helpful to tell readers if one or two covariates are responsible for a great deal of the apparent confounding in a data analysis. Other statistical analyses such as imputation procedures, data transformation, and calculations of attributable risks should also be described. Non-standard or novel approaches should be referenced and the statistical software used reported. As a guiding principle, we advise statistical methods be described with enough detail to enable a knowledgeable reader with access to the original data to verify the reported results [100].\nIn an empirical study, only 93 of 169 articles (55%) reporting adjustment for confounding clearly stated how continuous and multi-category variables were entered into the statistical model [101]. Another study found that among 67 articles in which statistical analyses were adjusted for confounders, it was mostly unclear how confounders were chosen\n\n\nExamples\n\nThe adjusted relative risk was calculated using the Mantel-Haenszel technique, when evaluating if confounding by age or gender was present in the groups compared. The 95% confidence interval (CI) was computed around the adjusted relative risk, using the variance according to Greenland and Robins and Robins et al.\n\nBack to top\n\n\n\n\n\n\n12b. Statistical methods\n\n\n\n\n\n\n\nDescribe any methods used to examine subgroups and interactions\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAs discussed in detail under item 17, many debate the use and value of analyses restricted to subgroups of the study population [4,104]. Subgroup analyses are nevertheless often done [4]. Readers need to know which subgroup analyses were planned in advance, and which arose while analysing the data. Also, it is important to explain what methods were used to examine whether effects or associations differed across groups (see item 17).\nInteraction relates to the situation when one factor modifies the effect of another (therefore also called effect modification). The joint action of two factors can be characterized in two ways: on an additive scale, in terms of risk differences; or on a multiplicative scale, in terms of relative risk (see Box 8). Many authors and readers may have their own preference about the way interactions should be analysed. Still, they may be interested to know to what extent the joint effect of exposures differs from the separate effects. There is consensus that the additive scale, which uses absolute risks, is more appropriate for public health and clinical decision making [105]. Whatever view is taken, this should be clearly presented to the reader, as is done in the example above [103]. A lay-out presenting separate effects of both exposures as well as their joint effect, each relative to no exposure, might be most informative. It is presented in the example for interaction under item 17, and the calculations on the different scales are explained in Box 8\n\n\nExamples\n\nSex differences in susceptibility to the 3 lifestyle-related risk factors studied were explored by testing for biological interaction according to Rothman: a new composite variable with 4 categories (a−b−, a−b+, a+b−, and a+b+) was redefined for sex and a dichotomous exposure of interest where a− and b− denote absence of exposure. RR was calculated for each category after adjustment for age. An interaction effect is defined as departure from additivity of absolute effects, and excess RR caused by interaction (RERI) was calculated:\n\n[calculation]\nwhere RR(a+b+) denotes RR among those exposed to both factors where RR(a−b−) is used as reference category (RR = 1.0). Ninety-five percent CIs were calculated as proposed by Hosmer and Lemeshow. RERI of 0 means no interaction\nBack to top\n\n\n\n\n\n\n12c. Statistical methods\n\n\n\n\n\n\n\nExplain how missing data were addressed\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nMissing data are common in observational research. Questionnaires posted to study participants are not always filled in completely, participants may not attend all follow-up visits and routine data sources and clinical databases are often incomplete. Despite its ubiquity and importance, few papers report in detail on the problem of missing data [5,107]. Investigators may use any of several approaches to address missing data. We describe some strengths and limitations of various approaches in Box 6. We advise that authors report the number of missing values for each variable of interest (exposures, outcomes, confounders) and for each step in the analysis. Authors should give reasons for missing values if possible, and indicate how many individuals were excluded because of missing data when describing the flow of participants through the study (see also item 13). For analyses that account for missing data, authors should describe the nature of the analysis (e.g., multiple imputation) and the assumptions that were made (e.g., missing at random, see Box 6).\n\n\nExamples\n\nOur missing data analysis procedures used missing at random (MAR) assumptions. We used the MICE (multivariate imputation by chained equations) method of multiple multivariate imputation in STATA. We independently analysed 10 copies of the data, each with missing values suitably imputed, in the multivariate logistic regression analyses. We averaged estimates of the variables to give a single mean estimate and adjusted standard errors according to Rubin’s rules\n\nBack to top\n\n\n\n\n\n\n12d. Statistical methods\n\n\n\n\n\n\n\nIf applicable, describe analytical methods taking account of sampling strategy\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nMost cross-sectional studies use a pre-specified sampling strategy to select participants from a source population. Sampling may be more complex than taking a simple random sample, however. It may include several stages and clustering of participants (e.g., in districts or villages). Proportionate stratification may ensure that subgroups with a specific characteristic are correctly represented. Disproportionate stratification may be useful to over-sample a subgroup of particular interest.\nAn estimate of association derived from a complex sample may be more or less precise than that derived from a simple random sample. Measures of precision such as standard error or confidence interval should be corrected using the design effect, a ratio measure that describes how much precision is gained or lost if a more complex sampling strategy is used instead of simple random sampling [119]. Most complex sampling techniques lead to a decrease of precision, resulting in a design effect greater than 1.\nWe advise that authors clearly state the method used to adjust for complex sampling strategies so that readers may understand how the chosen sampling method influenced the precision of the obtained estimates. For instance, with clustered sampling, the implicit trade-off between easier data collection and loss of precision is transparent if the design effect is reported. In the example, the calculated design effects of 1.9 for men indicates that the actual sample size would need to be 1.9 times greater than with simple random sampling for the resulting estimates to have equal precision\n\n\nExamples\n\nThe standard errors (SE) were calculated using the Taylor expansion method to estimate the sampling errors of estimators based on the complex sample design. (…) The overall design effect for diastolic blood pressure was found to be 1.9 for men and 1.8 for women and, for systolic blood pressure, it was 1.9 for men and 2.0 for women\n\nBack to top\n\n\n\n\n\n\n12e. Statistical methods\n\n\n\n\n\n\n\nDescribe any sensitivity analyses\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nSensitivity analyses are useful to investigate whether or not the main results are consistent with those obtained with alternative analysis strategies or assumptions [121]. Issues that may be examined include the criteria for inclusion in analyses, the definitions of exposures or outcomes [122], which confounding variables merit adjustment, the handling of missing data [120,123], possible selection bias or bias from inaccurate or inconsistent measurement of exposure, disease and other variables, and specific analysis choices, such as the treatment of quantitative variables (see item 11). Sophisticated methods are increasingly used to simultaneously model the influence of several biases or assumptions [124–126].\nIn 1959 Cornfield et al. famously showed that a relative risk of 9 for cigarette smoking and lung cancer was extremely unlikely to be due to any conceivable confounder, since the confounder would need to be at least nine times as prevalent in smokers as in non-smokers [127]. This analysis did not rule out the possibility that such a factor was present, but it did identify the prevalence such a factor would need to have. The same approach was recently used to identify plausible confounding factors that could explain the association between childhood leukaemia and living near electric power lines [128]. More generally, sensitivity analyses can be used to identify the degree of confounding, selection bias, or information bias required to distort an association. One important, perhaps under recognised, use of sensitivity analysis is when a study shows little or no association between an exposure and an outcome and it is plausible that confounding or other biases toward the null are present\n\n\nExamples\n\nBecause we had a relatively higher proportion of missing dead patients with insufficient data (38/148=25.7%) as compared to live patients (15/437=3.4%) (…), it is possible that this might have biased the results. We have, therefore, carried out a sensitivity analysis. We have assumed that the proportion of women using oral contraceptives in the study group applies to the whole (19.1% for dead, and 11.4% for live patients), and then applied two extreme scenarios: either all the exposed missing patients used second generation pills or they all used third-generation pills\n\nBack to top\n\n\n\n\n\n\nResults\n\n\n13a. Participants\n\n\n\n\n\n\n\nReport numbers of individuals at each stage of study—eg numbers potentially eligible, examined for eligibility, confirmed eligible, included in the study, completing follow-up, and analysed. Give information separately for for exposed and unexposed groups if applicable.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nDetailed information on the process of recruiting study participants is important for several reasons. Those included in a study often differ in relevant ways from the target population to which results are applied. This may result in estimates of prevalence or incidence that do not reflect the experience of the target population. For example, people who agreed to participate in a postal survey of sexual behaviour attended church less often, had less conservative sexual attitudes and earlier age at first sexual intercourse, and were more likely to smoke cigarettes and drink alcohol than people who refused [130]. These differences suggest that postal surveys may overestimate sexual liberalism and activity in the population. Such response bias (see Box 3) can distort exposure-disease associations if associations differ between those eligible for the study and those included in the study. As another example, the association between young maternal age and leukaemia in offspring, which has been observed in some case-control studies [131,132], was explained by differential participation of young women in case and control groups. Young women with healthy children were less likely to participate than those with unhealthy children [133]. Although low participation does not necessarily compromise the validity of a study, transparent information on participation and reasons for non-participation is essential. Also, as there are no universally agreed definitions for participation, response or follow-up rates, readers need to understand how authors calculated such proportions [134].\nIdeally, investigators should give an account of the numbers of individuals considered at each stage of recruiting study participants, from the choice of a target population to the inclusion of participants’ data in the analysis. Depending on the type of study, this may include the number of individuals considered to be potentially eligible, the number assessed for eligibility, the number found to be eligible, the number included in the study, the number examined, the number followed up and the number included in the analysis. Information on different sampling units may be required, if sampling of study participants is carried out in two or more stages as in the example above (multistage sampling). In case-control studies, we advise that authors describe the flow of participants separately for case and control groups [135]. Controls can sometimes be selected from several sources, including, for example, hospitalised patients and community dwellers. In this case, we recommend a separate account of the numbers of participants for each type of control group. Olson and colleagues proposed useful reporting guidelines for controls recruited through random-digit dialling and other methods [136].\nA recent survey of epidemiological studies published in 10 general epidemiology, public health and medical journals found that some information regarding participation was provided in 47 of 107 case-control studies (59%), 49 of 154 cohort studies (32%), and 51 of 86 cross-sectional studies (59%) [137]. Incomplete or absent reporting of participation and non-participation in epidemiological studies was also documented in two other surveys of the literature [4,5]. Finally, there is evidence that participation in epidemiological studies may have declined in recent decades [137,138], which underscores the need for transparent reporting\n\n\nExamples\n\nOf the 105 freestanding bars and taverns sampled, 13 establishments were no longer in business and 9 were located in restaurants, leaving 83 eligible businesses. In 22 cases, the owner could not be reached by telephone despite 6 or more attempts. The owners of 36 bars declined study participation. (…) The 25 participating bars and taverns employed 124 bartenders, with 67 bartenders working at least 1 weekly daytime shift. Fifty-four of the daytime bartenders (81%) completed baseline interviews and spirometry; 53 of these subjects (98%) completed follow-up\n\nBack to top\n\n\n\n\n\n\n13b. Participants\n\n\n\n\n\n\n\nGive reasons for non-participation at each stage\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nExplaining the reasons why people no longer participated in a study or why they were excluded from statistical analyses helps readers judge whether the study population was representative of the target population and whether bias was possibly introduced. For example, in a cross-sectional health survey, non-participation due to reasons unlikely to be related to health status (for example, the letter of invitation was not delivered because of an incorrect address) will affect the precision of estimates but will probably not introduce bias. Conversely, if many individuals opt out of the survey because of illness, or perceived good health, results may underestimate or overestimate the prevalence of ill health in the population\n\n\nExamples\n\nThe main reasons for non-participation were the participant was too ill or had died before interview (cases 30%, controls < 1%), nonresponse (cases 2%, controls 21%), refusal (cases 10%, controls 29%), and other reasons (refusal by consultant or general practitioner, non-English speaking, mental impairment) (cases 7%, controls 5%)\n\nBack to top\n\n\n\n\n\n\n13c. Participants\n\n\n\n\n\n\n\nConsider use of a flow diagram\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAn informative and well-structured flow diagram can readily and transparently convey information that might otherwise require a lengthy description [142], as in the example above. The diagram may usefully include the main results, such as the number of events for the primary outcome. While we recommend the use of a flow diagram, particularly for complex observational studies, we do not propose a specific format for the diagram\n\n\nExamples\n\nFlow diagram from Hay et al. https://doi.org/10.1371/journal.pmed.0040297.g001\n\nBack to top\n\n\n\n\n\n\n14a. Descriptive data\n\n\n\n\n\n\n\nGive characteristics of study participants (eg demographic, clinical, social) and information on exposures and potential confounders. Give information separately for exposed and unexposed groups if applicable.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nReaders need descriptions of study participants and their exposures to judge the generalisability of the findings. Information about potential confounders, including whether and how they were measured, influences judgments about study validity. We advise authors to summarize continuous variables for each study group by giving the mean and standard deviation, or when the data have an asymmetrical distribution, as is often the case, the median and percentile range (e.g., 25th and 75th percentiles). Variables that make up a small number of ordered categories (such as stages of disease I to IV) should not be presented as continuous variables; it is preferable to give numbers and proportions for each category (see also Box 4). In studies that compare groups, the descriptive characteristics and numbers should be given by group, as in the example above.\nInferential measures such as standard errors and confidence intervals should not be used to describe the variability of characteristics, and significance tests should be avoided in descriptive tables. Also, P values are not an appropriate criterion for selecting which confounders to adjust for in analysis; even small differences in a confounder that has a strong effect on the outcome can be important.\nIn cohort studies, it may be useful to document how an exposure relates to other characteristics and potential confounders. Authors could present this information in a table with columns for participants in two or more exposure categories, which permits to judge the differences in confounders between these categories.\nIn case-control studies potential confounders cannot be judged by comparing cases and controls. Control persons represent the source population and will usually be different from the cases in many respects. For example, in a study of oral contraceptives and myocardial infarction, a sample of young women with infarction more often had risk factors for that disease, such as high serum cholesterol, smoking and a positive family history, than the control group [146]. This does not influence the assessment of the effect of oral contraceptives, as long as the prescription of oral contraceptives was not guided by the presence of these risk factors—e.g., because the risk factors were only established after the event (see also Box 5). In case-control studies the equivalent of comparing exposed and non-exposed for the presence of potential confounders (as is done in cohorts) can be achieved by exploring the source population of the cases: if the control group is large enough and represents the source population, exposed and unexposed controls can be compared for potential confounders [121,147]\n\n\nExamples\n\nCharacteristics of the Study Base at Enrolment, Castellana G (Italy), 1985–1986\n\nhttps://doi.org/10.1371/journal.pmed.0040297.t002\nBack to top\n\n\n\n\n\n\n14b. Descriptive data\n\n\n\n\n\n\n\nIndicate number of participants with missing data for each variable of interest\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAs missing data may bias or affect generalisability of results, authors should tell readers amounts of missing data for exposures, potential confounders, and other important characteristics of patients (see also item 12c and Box 6). In a cohort study, authors should report the extent of loss to follow-up (with reasons), since incomplete follow-up may bias findings (see also items 12d and 13). We advise authors to use their tables and figures to enumerate amounts of missing data.\n\n\nExamples\n\nTable. Symptom End Points Used in Survival Analysis\n\nhttps://doi.org/10.1371/journal.pmed.0040297.t003\nBack to top\n\n\n\n\n\n\n15. Outcome data\n\n\n\n\n\n\n\nReport numbers of outcome events or summary measures. Give information separately for exposed and unexposed groups if applicable.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nBefore addressing the possible association between exposures (risk factors) and outcomes, authors should report relevant descriptive data. It may be possible and meaningful to present measures of association in the same table that presents the descriptive data (see item 14a). In a cohort study with events as outcomes, report the numbers of events for each outcome of interest. Consider reporting the event rate per person-year of follow-up. If the risk of an event changes over follow-up time, present the numbers and rates of events in appropriate intervals of follow-up or as a Kaplan-Meier life table or plot. It might be preferable to show plots as cumulative incidence that go up from 0% rather than down from 100%, especially if the event rate is lower than, say, 30% [153]. Consider presenting such information separately for participants in different exposure categories of interest. If a cohort study is investigating other time-related outcomes (e.g., quantitative disease markers such as blood pressure), present appropriate summary measures (e.g., means and standard deviations) over time, perhaps in a table or figure.\nFor cross-sectional studies, we recommend presenting the same type of information on prevalent outcome events or summary measures. For case-control studies, the focus will be on reporting exposures separately for cases and controls as frequencies or quantitative summaries [154]. For all designs, it may be helpful also to tabulate continuous outcomes or exposures in categories, even if the data are not analysed as such\n\n\nExamples\n\nTable. Prevalence of Current Asthma and Diagnosed Hay Fever by Average Alternaria alternata Antigen Level in the Household https://doi.org/10.1371/journal.pmed.0040297.t007\n\nBack to top\n\n\n\n\n\n\n16a. Main results\n\n\n\n\n\n\n\nGive unadjusted estimates and, if applicable, confounder-adjusted estimates and their precision (eg, 95% confidence interval). Make clear which confounders were adjusted for and why they were included\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nIn many situations, authors may present the results of unadjusted or minimally adjusted analyses and those from fully adjusted analyses. We advise giving the unadjusted analyses together with the main data, for example the number of cases and controls that were exposed or not. This allows the reader to understand the data behind the measures of association (see also item 15). For adjusted analyses, report the number of persons in the analysis, as this number may differ because of missing values in covariates (see also item 12c). Estimates should be given with confidence intervals.\nReaders can compare unadjusted measures of association with those adjusted for potential confounders and judge by how much, and in what direction, they changed. Readers may think that adjusted results equal the causal part of the measure of association, but adjusted results are not necessarily free of random sampling error, selection bias, information bias, or residual confounding (see Box 5). Thus, great care should be exercised when interpreting adjusted results, as the validity of results often depends crucially on complete knowledge of important confounders, their precise measurement, and appropriate specification in the statistical model (see also item 20) [157,158].\nAuthors should explain all potential confounders considered, and the criteria for excluding or including variables in statistical models. Decisions about excluding or including variables should be guided by knowledge, or explicit assumptions, on causal relations. Inappropriate decisions may introduce bias, for example by including variables that are in the causal pathway between exposure and disease (unless the aim is to asses how much of the effect is carried by the intermediary variable). If the decision to include a variable in the model was based on the change in the estimate, it is important to report what change was considered sufficiently important to justify its inclusion. If a backward deletion or forward inclusion strategy was used to select confounders, explain that process and give the significance level for rejecting the null hypothesis of no confounding. Of note, we and others do not advise selecting confounders based solely on statistical significance testing [147,159,160].\nRecent studies of the quality of reporting of epidemiological studies found that confidence intervals were reported in most articles [4]. However, few authors explained their choice of confounding variables [4,5].\n\n\nExamples\n\nTable. Relative Rates of Rehospitalisation by Treatment in Patients in Community Care after First Hospitalisation due to Schizophrenia and Schizoaffective Disorder https://doi.org/10.1371/journal.pmed.0040297.t008\n\nBack to top\n\n\n\n\n\n\n16b. Main results\n\n\n\n\n\n\n\nReport category boundaries when continuous variables were categorized\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nCategorizing continuous data has several important implications for analysis (see Box 4) and also affects the presentation of results. In tables, outcomes should be given for each exposure category, for example as counts of persons at risk, person-time at risk, if relevant separately for each group (e.g., cases and controls). Details of the categories used may aid comparison of studies and meta-analysis. If data were grouped using conventional cut-points, such as body mass index thresholds [162], group boundaries (i.e., range of values) can be derived easily, except for the highest and lowest categories. If quantile-derived categories are used, the category boundaries cannot be inferred from the data. As a minimum, authors should report the category boundaries; it is helpful also to report the range of the data and the mean or median values within categories.\n\n\nExamples\n\nTable. Polychlorinated Biphenyls in Cord Serum\n\nhttps://doi.org/10.1371/journal.pmed.0040297.t005\nBack to top\n\n\n\n\n\n\n16c. Main results\n\n\n\n\n\n\n\nIf relevant, consider translating estimates of relative risk into absolute risk for a meaningful time period\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe results from studies examining the association between an exposure and a disease are commonly reported in relative terms, as ratios of risks, rates or odds (see Box 8). Relative measures capture the strength of the association between an exposure and disease. If the relative risk is a long way from 1 it is less likely that the association is due to confounding [164,165]. Relative effects or associations tend to be more consistent across studies and populations than absolute measures, but what often tends to be the case may be irrelevant in a particular instance. For example, similar relative risks were obtained for the classic cardiovascular risk factors for men living in Northern Ireland, France, the USA and Germany, despite the fact that the underlying risk of coronary heart disease varies substantially between these countries [166,167]. In contrast, in a study of hypertension as a risk factor for cardiovascular disease mortality, the data were more compatible with a constant rate difference than with a constant rate ratio [168].\nWidely used statistical models, including logistic [169] and proportional hazards (Cox) regression [170] are based on ratio measures. In these models, only departures from constancy of ratio effect measures are easily discerned. Nevertheless, measures which assess departures from additivity of risk differences, such as the Relative Excess Risk from Interaction (RERI, see item 12b and Box 8), can be estimated in models based on ratio measures.\nIn many circumstances, the absolute risk associated with an exposure is of greater interest than the relative risk. For example, if the focus is on adverse effects of a drug, one will want to know the number of additional cases per unit time of use (e.g., days, weeks, or years). The example gives the additional number of breast cancer cases per 1000 women who used hormone-replacement therapy for 10 years [163]. Measures such as the attributable risk or population attributable fraction may be useful to gauge how much disease can be prevented if the exposure is eliminated. They should preferably be presented together with a measure of statistical uncertainty (e.g., confidence intervals as in the example). Authors should be aware of the strong assumptions made in this context, including a causal relationship between a risk factor and disease (also see Box 7) [171]. Because of the semantic ambiguity and complexities involved, authors should report in detail what methods were used to calculate attributable risks, ideally giving the formulae used [172].\nA recent survey of abstracts of 222 articles published in leading medical journals found that in 62% of abstracts of randomised trials including a ratio measure absolute risks were given, but only in 21% of abstracts of cohort studies [173]. A free text search of Medline 1966 to 1997 showed that 619 items mentioned attributable risks in the title or abstract, compared to 18,955 using relative risk or odds ratio, for a ratio of 1 to 31 [174].\n\n\nExamples\n\n10 years’ use of HRT [hormone replacement therapy] is estimated to result in five (95% CI 3–7) additional breast cancers per 1000 users of oestrogen-only preparations and 19 (15–23) additional cancers per 1000 users of oestrogen-progestagen combinations\n\nBack to top\n\n\n\n\n\n\n17. Other analyses\n\n\n\n\n\n\n\nReport other analyses done—e.g., analyses of subgroups and interactions, and sensitivity analyses\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nIn addition to the main analysis other analyses are often done in observational studies. They may address specific subgroups, the potential interaction between risk factors, the calculation of attributable risks, or use alternative definitions of study variables in sensitivity analyses.\nThere is debate about the dangers associated with subgroup analyses, and multiplicity of analyses in general [4,104]. In our opinion, there is too great a tendency to look for evidence of subgroup-specific associations, or effect-measure modification, when overall results appear to suggest little or no effect. On the other hand, there is value in exploring whether an overall association appears consistent across several, preferably pre-specified subgroups especially when a study is large enough to have sufficient data in each subgroup. A second area of debate is about interesting subgroups that arose during the data analysis. They might be important findings, but might also arise by chance. Some argue that it is neither possible nor necessary to inform the reader about all subgroup analyses done as future analyses of other data will tell to what extent the early exciting findings stand the test of time [9]. We advise authors to report which analyses were planned, and which were not (see also items 4, 12b and 20). This will allow readers to judge the implications of multiplicity, taking into account the study’s position on the continuum from discovery to verification or refutation.\nA third area of debate is how joint effects and interactions between risk factors should be evaluated: on additive or multiplicative scales, or should the scale be determined by the statistical model that fits best (see also item 12b and Box 8)? A sensible approach is to report the separate effect of each exposure as well as the joint effect—if possible in a table, as in the first example above [183], or in the study by Martinelli et al. [185]. Such a table gives the reader sufficient information to evaluate additive as well as multiplicative interaction (how these calculations are done is shown in Box 8). Confidence intervals for separate and joint effects may help the reader to judge the strength of the data. In addition, confidence intervals around measures of interaction, such as the Relative Excess Risk from Interaction (RERI) relate to tests of interaction or homogeneity tests. One recurrent problem is that authors use comparisons of P-values across subgroups, which lead to erroneous claims about an effect modifier. For instance, a statistically significant association in one category (e.g., men), but not in the other (e.g., women) does not in itself provide evidence of effect modification. Similarly, the confidence intervals for each point estimate are sometimes inappropriately used to infer that there is no interaction when intervals overlap. A more valid inference is achieved by directly evaluating whether the magnitude of an association differs across subgroups.\nSensitivity analyses are helpful to investigate the influence of choices made in the statistical analysis, or to investigate the robustness of the findings to missing data or possible biases (see also item 12b). Judgement is needed regarding the level of reporting of such analyses. If many sensitivity analyses were performed, it may be impractical to present detailed findings for them all. It may sometimes be sufficient to report that sensitivity analyses were carried out and that they were consistent with the main results presented. Detailed presentation is more appropriate if the issue investigated is of major concern, or if effect estimates vary considerably [59,186].\nPocock and colleagues found that 43 out of 73 articles reporting observational studies contained subgroup analyses. The majority claimed differences across groups but only eight articles reported a formal evaluation of interaction (see item 12b) [4].\n\n\nExamples\n\nTable. Analysis of Oral Contraceptive Use, Presence of Factor V Leiden Allele, and Risk for Venous Thromboembolism\n\nhttps://doi.org/10.1371/journal.pmed.0040297.t009\n\nTable. Sensitivity of the Rate Ratio for Cardiovascular Outcome to an Unmeasured Confounder https://doi.org/10.1371/journal.pmed.0040297.t010\n\nBack to top\n\n\n\n\n\n\nDiscussion\n\n\n18. Key results\n\n\n\n\n\n\n\nSummarise key results with reference to study objectives\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nIt is good practice to begin the discussion with a short summary of the main findings of the study. The short summary reminds readers of the main findings and may help them assess whether the subsequent interpretation and implications offered by the authors are supported by the findings\n\n\nExamples\n\nWe hypothesized that ethnic minority status would be associated with higher levels of cardiovascular disease (CVD) risk factors, but that the associations would be explained substantially by socioeconomic status (SES). Our hypothesis was not confirmed. After adjustment for age and SES, highly significant differences in body mass index, blood pressure, diabetes, and physical inactivity remained between white women and both black and Mexican American women. In addition, we found large differences in CVD risk factors by SES, a finding that illustrates the high-risk status of both ethnic minority women as well as white women with low SES\n\nBack to top\n\n\n\n\n\n\n19. Limitations\n\n\n\n\n\n\n\nDiscuss limitations of the study, taking into account sources of potential bias or imprecision. Discuss both direction and magnitude of any potential bias.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe identification and discussion of the limitations of a study are an essential part of scientific reporting. It is important not only to identify the sources of bias and confounding that could have affected results, but also to discuss the relative importance of different biases, including the likely direction and magnitude of any potential bias (see also item 9 and Box 3).\nAuthors should also discuss any imprecision of the results. Imprecision may arise in connection with several aspects of a study, including the study size (item 10) and the measurement of exposures, confounders and outcomes (item 8). The inability to precisely measure true values of an exposure tends to result in bias towards unity: the less precisely a risk factor is measured, the greater the bias. This effect has been described as attenuation [201,202], or more recently as regression dilution bias [203]. However, when correlated risk factors are measured with different degrees of imprecision, the adjusted relative risk associated with them can be biased towards or away from unity [204–206].\nWhen discussing limitations, authors may compare the study being presented with other studies in the literature in terms of validity, generalisability and precision. In this approach, each study can be viewed as contribution to the literature, not as a stand-alone basis for inference and action [207]. Surprisingly, the discussion of important limitations of a study is sometimes omitted from published reports. A survey of authors who had published original research articles in The Lancet found that important weaknesses of the study were reported by the investigators in the survey questionnaires, but not in the published article\n\n\nExamples\n\nSince the prevalence of counseling increases with increasing levels of obesity, our estimates may overestimate the true prevalence. Telephone surveys also may overestimate the true prevalence of counseling. Although persons without telephones have similar levels of overweight as persons with telephones, persons without telephones tend to be less educated, a factor associated with lower levels of counseling in our study. Also, of concern is the potential bias caused by those who refused to participate as well as those who refused to respond to questions about weight. Furthermore, because data were collected cross-sectionally, we cannot infer that counseling preceded a patient’s attempt to lose weight\n\nBack to top\n\n\n\n\n\n\n20. Interpretation\n\n\n\n\n\n\n\nGive a cautious overall interpretation considering objectives, limitations, multiplicity of analyses, results from similar studies, and other relevant evidence.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe heart of the discussion section is the interpretation of a study’s results. Over-interpretation is common and human: even when we try hard to give an objective assessment, reviewers often rightly point out that we went too far in some respects. When interpreting results, authors should consider the nature of the study on the discovery to verification continuum and potential sources of bias, including loss to follow-up and non-participation (see also items 9, 12 and 19). Due consideration should be given to confounding (item 16a), the results of relevant sensitivity analyses, and to the issue of multiplicity and subgroup analyses (item 17). Authors should also consider residual confounding due to unmeasured variables or imprecise measurement of confounders. For example, socioeconomic status (SES) is associated with many health outcomes and often differs between groups being compared. Variables used to measure SES (income, education, or occupation) are surrogates for other undefined and unmeasured exposures, and the true confounder will by definition be measured with error [208]. Authors should address the real range of uncertainty in estimates, which is larger than the statistical uncertainty reflected in confidence intervals. The latter do not take into account other uncertainties that arise from a study’s design, implementation, and methods of measurement [209].\nTo guide thinking and conclusions about causality, some may find criteria proposed by Bradford Hill in 1965 helpful [164]. How strong is the association with the exposure? Did it precede the onset of disease? Is the association consistently observed in different studies and settings? Is there supporting evidence from experimental studies, including laboratory and animal studies? How specific is the exposure’s putative effect, and is there a dose-response relationship? Is the association biologically plausible? These criteria should not, however, be applied mechanically. For example, some have argued that relative risks below 2 or 3 should be ignored [210,211]. This is a reversal of the point by Cornfield et al. about the strength of large relative risks (see item 12b) [127]. Although a causal effect is more likely with a relative risk of 9, it does not follow that one below 3 is necessarily spurious. For instance, the small increase in the risk of childhood leukaemia after intrauterine irradiation is credible because it concerns an adverse effect of a medical procedure for which no alternative explanations are obvious [212]. Moreover, the carcinogenic effects of radiation are well established. The doubling in the risk of ovarian cancer associated with eating 2 to 4 eggs per week is not immediately credible, since dietary habits are associated with a large number of lifestyle factors as well as SES [213]. In contrast, the credibility of much debated epidemiologic findings of a difference in thrombosis risk between different types of oral contraceptives was greatly enhanced by the differences in coagulation found in a randomised cross-over trial [214]. A discussion of the existing external evidence, from different types of studies, should always be included, but may be particularly important for studies reporting small increases in risk. Further, authors should put their results in context with similar studies and explain how the new study affects the existing body of evidence, ideally by referring to a systematic review.\n\n\nExamples\n\nAny explanation for an association between death from myocardial infarction and use of second generation oral contraceptives must be conjectural. There is no published evidence to suggest a direct biologic mechanism, and there are no other epidemiologic studies with relevant results. (…) The increase in absolute risk is very small and probably applies predominantly to smokers. Due to the lack of corroborative evidence, and because the analysis is based on relatively small numbers, more evidence on the subject is needed. We would not recommend any change in prescribing practice on the strength of these results\n\nBack to top\n\n\n\n\n\n\n21. Generalisability\n\n\n\n\n\n\n\nDiscuss the generalisability (external validity) of the study results\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nGeneralisability, also called external validity or applicability, is the extent to which the results of a study can be applied to other circumstances [216]. There is no external validity per se; the term is meaningful only with regard to clearly specified conditions [217]. Can results be applied to an individual, groups or populations that differ from those enrolled in the study with regard to age, sex, ethnicity, severity of disease, and co-morbid conditions? Are the nature and level of exposures comparable, and the definitions of outcomes relevant to another setting or population? Are data that were collected in longitudinal studies many years ago still relevant today? Are results from health services research in one country applicable to health systems in other countries?\nThe question of whether the results of a study have external validity is often a matter of judgment that depends on the study setting, the characteristics of the participants, the exposures examined, and the outcomes assessed. Thus, it is crucial that authors provide readers with adequate information about the setting and locations, eligibility criteria, the exposures and how they were measured, the definition of outcomes, and the period of recruitment and follow-up. The degree of non-participation and the proportion of unexposed participants in whom the outcome develops are also relevant. Knowledge of the absolute risk and prevalence of the exposure, which will often vary across populations, are helpful when applying results to other settings and populations (see Box 7)\n\n\nExamples\n\nHow applicable are our estimates to other HIV-1-infected patients? This is an important question because the accuracy of prognostic models tends to be lower when applied to data other than those used to develop them. We addressed this issue by penalising model complexity, and by choosing models that generalised best to cohorts omitted from the estimation procedure. Our database included patients from many countries from Europe and North America, who were treated in different settings. The range of patients was broad: men and women, from teenagers to elderly people were included, and the major exposure categories were well represented. The severity of immunodeficiency at baseline ranged from not measureable to very severe, and viral load from undetectable to extremely high\n\nBack to top\n\n\n\n\n\n\nOther Information\n\n\n22. Funding\n\n\n\n\n\n\n\nGive the source of funding and the role of the funders for the present study and, if applicable, for the original study on which the present article is based\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nSome journals require authors to disclose the presence or absence of financial and other conflicts of interest [100,218]. Several investigations show strong associations between the source of funding and the conclusions of research articles [219–222]. The conclusions in randomised trials recommended the experimental drug as the drug of choice much more often (odds ratio 5.3) if the trial was funded by for-profit organisations, even after adjustment for the effect size [223]. Other studies document the influence of the tobacco and telecommunication industries on the research they funded [224–227]. There are also examples of undue influence when the sponsor is governmental or a non-profit organisation.\nAuthors or funders may have conflicts of interest that influence any of the following: the design of the study [228]; choice of exposures [228,229], outcomes [230], statistical methods [231], and selective publication of outcomes [230] and studies [232]. Consequently, the role of the funders should be described in detail: in what part of the study they took direct responsibility (e.g., design, data collection, analysis, drafting of manuscript, decision to publish) [100]. Other sources of undue influence include employers (e.g., university administrators for academic researchers and government supervisors, especially political appointees, for government researchers), advisory committees, litigants, and special interest groups"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-1a_title-1",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-1a_title-1",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Read More",
    "text": "Read More\nReaders should be able to easily identify the design that was used from the title or abstract. An explicit, commonly used term for the study design also helps ensure correct indexing of articles in electronic databases"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-1a_title-2",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-1a_title-2",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Examples",
    "text": "Examples\n\nLeukaemia incidence among workers in the shoe and boot manufacturing industry: a case-control study\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-1b_abstract-1",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-1b_abstract-1",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Read More",
    "text": "Read More\nThe abstract provides key information that enables readers to understand a study and decide whether to read the article. Typical components include a statement of the research question, a short description of methods and results, and a conclusion. Abstracts should summarize key details of studies and should only present information that is provided in the article. We advise presenting key results in a numerical form that includes numbers of participants, estimates of associations and appropriate measures of variability and uncertainty (e.g., odds ratios with confidence intervals). We regard it insufficient to state only that an exposure is or is not significantly associated with an outcome.\nA series of headings pertaining to the background, design, conduct, and analysis of a study may help readers acquire the essential information rapidly. Many journals require such structured abstracts, which tend to be of higher quality and more readily informative than unstructured summaries."
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-1b_abstract-2",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-1b_abstract-2",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Examples",
    "text": "Examples\n\nBackground: The expected survival of HIV-infected patients is of major public health interest.\n\nObjective: To estimate survival time and age-specific mortality rates of an HIV-infected population compared with that of the general population.\nDesign: Population-based cohort study.\nSetting: All HIV-infected persons receiving care in Denmark from 1995 to 2005.\nPatients: Each member of the nationwide Danish HIV Cohort Study was matched with as many as 99 persons from the general population according to sex, date of birth, and municipality of residence.\nMeasurements: The authors computed Kaplan-Meier life tables with age as the time scale to estimate survival from age 25 years. Patients with HIV infection and corresponding persons from the general population were observed from the date of the patient’s HIV diagnosis until death, emigration, or 1 May 2005.\nResults: 3990 HIV-infected patients and 379,872 persons from the general population were included in the study, yielding 22,744 (median, 5.8 y/person) and 2,689,287 (median, 8.4 years/person) person-years of observation. Three percent of participants were lost to follow-up. From age 25 years, the median survival was 19.9 years (95% CI, 18.5 to 21.3) among patients with HIV infection and 51.1 years (CI, 50.9 to 51.5) among the general population. For HIV-infected patients, survival increased to 32.5 years (CI, 29.4 to 34.7) during the 2000 to 2005 period. In the subgroup that excluded persons with known hepatitis C coinfection (16%), median survival was 38.9 years (CI, 35.4 to 40.1) during this same period. The relative mortality rates for patients with HIV infection compared with those for the general population decreased with increasing age, whereas the excess mortality rate increased with increasing age.\nLimitations: The observed mortality rates are assumed to apply beyond the current maximum observation time of 10 years.\nConclusions: The estimated median survival is more than 35 years for a young person diagnosed with HIV infection in the late highly active antiretroviral therapy era. However, an ongoing effort is still needed to further reduce mortality rates for these persons compared with the general population\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-2_background__rationale-1",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-2_background__rationale-1",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Read More",
    "text": "Read More\nThe scientific background of the study provides important context for readers. It sets the stage for the study and describes its focus. It gives an overview of what is known on a topic and what gaps in current knowledge are addressed by the study. Background material should note recent pertinent studies and any systematic reviews of pertinent studies."
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-2_background__rationale-2",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-2_background__rationale-2",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Examples",
    "text": "Examples\n\nConcerns about the rising prevalence of obesity in children and adolescents have focused on the well documented associations between childhood obesity and increased cardiovascular risk1 and mortality in adulthood.2 Childhood obesity has considerable social and psychological consequences within childhood and adolescence,3 yet little is known about social, socioeconomic, and psychological consequences in adult life.\n\nA recent systematic review found no longitudinal studies on the outcomes of childhood obesity other than physical health outcomes3 and only two longitudinal studies of the socioeconomic effects of obesity in adolescence. Gortmaker et al found that US women who had been obese in late adolescence in 1981 were less likely to be married and had lower incomes seven years later than women who had not been overweight, while men who had been overweight were less likely to be married.4 Sargent et al found that UK women, but not men, who had been obese at 16 years in 1974 earned 7.4% less than their non-obese peers at age 23.5\nThe study of adult outcomes of childhood obesity is difficult because obesity often continues into adult life and therefore poorer socioeconomic and educational outcomes may actually reflect confounding by adult obesity. Yet identifying outcomes related to obesity confined childhood is important in determining whether people who are obese in childhood and who later lose weight remain at risk for adult adversity and inequalities.\nWe used longitudinal data from the 1970 British birth cohort to examine the adult socioeconomic, educational, social, and psychological outcomes of childhood obesity. We hypothesised that obesity limited to childhood has fewer adverse adult outcomes than obesity that persists into adult life\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-3_objectives-1",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-3_objectives-1",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Read More",
    "text": "Read More\nObjectives are the detailed aims of the study. Well crafted objectives specify populations, exposures and outcomes, and parameters that will be estimated. They may be formulated as specific hypotheses or as questions that the study was designed to address. In some situations objectives may be less specific, for example, in early discovery phases. Regardless, the report should clearly reflect the investigators’ intentions. For example, if important subgroups or additional analyses were not the original aim of the study but arose during data analysis, they should be described accordingly (see also items 4, 17 and 20)."
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-3_objectives-2",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-3_objectives-2",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Examples",
    "text": "Examples\n\nOur primary objectives were to 1) determine the prevalence of domestic violence among female patients presenting to four community-based, primary care, adult medicine practices that serve patients of diverse socioeconomic background and 2) identify demographic and clinical differences between currently abused patients and patients not currently being abused\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-4_study_design-1",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-4_study_design-1",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Read More",
    "text": "Read More\nWe advise presenting key elements of study design early in the methods section (or at the end of the introduction) so that readers can understand the basics of the study. For example, authors should indicate that the study was a cohort study, which followed people over a particular time period, and describe the group of persons that comprised the cohort and their exposure status. Similarly, if the investigation used a case-control design, the cases and controls and their source population should be described. If the study was a cross-sectional survey, the population and the point in time at which the cross-section was taken should be mentioned. When a study is a variant of the three main study types, there is an additional need for clarity. For instance, for a case-crossover study, one of the variants of the case-control design, a succinct description of the principles was given in the example above [28].\nWe recommend that authors refrain from simply calling a study prospective or retrospective because these terms are ill defined [29]. One usage sees cohort and prospective as synonymous and reserves the word retrospective for case-control studies [30]. A second usage distinguishes prospective and retrospective cohort studies according to the timing of data collection relative to when the idea for the study was developed [31]. A third usage distinguishes prospective and retrospective case-control studies depending on whether the data about the exposure of interest existed when cases were selected [32]. Some advise against using these terms [33], or adopting the alternatives concurrent and historical for describing cohort studies [34]. In STROBE, we do not use the words prospective and retrospective, nor alternatives such as concurrent and historical. We recommend that, whenever authors use these words, they define what they mean. Most importantly, we recommend that authors describe exactly how and when data collection took place.\nThe first part of the methods section might also be the place to mention whether the report is one of several from a study. If a new report is in line with the original aims of the study, this is usually indicated by referring to an earlier publication and by briefly restating the salient features of the study. However, the aims of a study may also evolve over time. Researchers often use data for purposes for which they were not originally intended, including, for example, official vital statistics that were collected primarily for administrative purposes, items in questionnaires that originally were only included for completeness, or blood samples that were collected for another purpose. For example, the Physicians’ Health Study, a randomized controlled trial of aspirin and carotene, was later used to demonstrate that a point mutation in the factor V gene was associated with an increased risk of venous thrombosis, but not of myocardial infarction or stroke [35]. The secondary use of existing data is a creative part of observational research and does not necessarily make results less credible or less important. However, briefly restating the original aims might help readers understand the context of the research and possible limitations in the data."
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-4_study_design-2",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-4_study_design-2",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Examples",
    "text": "Examples\n\nWe used a case-crossover design, a variation of a case-control design that is appropriate when a brief exposure (driver’s phone use) causes a transient rise in the risk of a rare outcome (a crash). We compared a driver’s use of a mobile phone at the estimated time of a crash with the same driver’s use during another suitable time period. Because drivers are their own controls, the design controls for characteristics of the driver that may affect the risk of a crash but do not change over a short period of time. As it is important that risks during control periods and crash trips are similar, we compared phone activity during the hazard interval (time immediately before the crash) with phone activity during control intervals (equivalent times during which participants were driving but did not crash) in the previous week\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-5_setting-1",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-5_setting-1",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Read More",
    "text": "Read More\nReaders need information on setting and locations to assess the context and generalisability of a study’s results. Exposures such as environmental factors and therapies can change over time. Also, study methods may evolve over time. Knowing when a study took place and over what period participants were recruited and followed up places the study in historical context and is important for the interpretation of results.\nInformation about setting includes recruitment sites or sources (e.g., electoral roll, outpatient clinic, cancer registry, or tertiary care centre). Information about location may refer to the countries, towns, hospitals or practices where the investigation took place. We advise stating dates rather than only describing the length of time periods. There may be different sets of dates for exposure, disease occurrence, recruitment, beginning and end of follow-up, and data collection. Of note, nearly 80% of 132 reports in oncology journals that used survival analysis included the starting and ending dates for accrual of patients, but only 24% also reported the date on which follow-up ended [37]."
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-5_setting-2",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-5_setting-2",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Examples",
    "text": "Examples\n\nThe Pasitos Cohort Study recruited pregnant women from Women, Infant and Child (WIC) clinics in Socorro and San Elizario, El Paso County, Texas and maternal-child clinics of the Mexican Social Security Institute (IMSS) in Ciudad Juarez, Mexico from April 1998 to October 2000. At baseline, prior to the birth of the enrolled cohort children, staff interviewed mothers regarding the household environment. In this ongoing cohort study, we target follow-up exams at 6-month intervals beginning at age 6 months\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-6a_eligibility_criteria-1",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-6a_eligibility_criteria-1",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Read More",
    "text": "Read More\nDetailed descriptions of the study participants help readers understand the applicability of the results. Investigators usually restrict a study population by defining clinical, demographic and other characteristics of eligible participants. Typical eligibility criteria relate to age, gender, diagnosis and comorbid conditions. Despite their importance, eligibility criteria often are not reported adequately. In a survey of observational stroke research, 17 of 49 reports (35%) did not specify eligibility criteria [5].\nEligibility criteria may be presented as inclusion and exclusion criteria, although this distinction is not always necessary or useful. Regardless, we advise authors to report all eligibility criteria and also to describe the group from which the study population was selected (e.g., the general population of a region or country), and the method of recruitment (e.g., referral or self-selection through advertisements).\nKnowing details about follow-up procedures, including whether procedures minimized non-response and loss to follow-up and whether the procedures were similar for all participants, informs judgments about the validity of results. For example, in a study that used IgM antibodies to detect acute infections, readers needed to know the interval between blood tests for IgM antibodies so that they could judge whether some infections likely were missed because the interval between blood tests was too long [41]. In other studies where follow-up procedures differed between exposed and unexposed groups, readers might recognize substantial bias due to unequal ascertainment of events or differences in non-response or loss to follow-up [42]. Accordingly, we advise that researchers describe the methods used for following participants and whether those methods were the same for all participants, and that they describe the completeness of ascertainment of variables (see also item 14).\nIn case-control studies, the choice of cases and controls is crucial to interpreting the results, and the method of their selection has major implications for study validity. In general, controls should reflect the population from which the cases arose. Various methods are used to sample controls, all with advantages and disadvantages: for cases that arise from a general population, population roster sampling, random digit dialling, neighbourhood or friend controls are used. Neighbourhood or friend controls may present intrinsic matching on exposure [17]. Controls with other diseases may have advantages over population-based controls, in particular for hospital-based cases, because they better reflect the catchment population of a hospital, have greater comparability of recall and ease of recruitment. However, they can present problems if the exposure of interest affects the risk of developing or being hospitalized for the control condition(s) [43,44]. To remedy this problem often a mixture of the best defensible control diseases is used [45]."
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-6a_eligibility_criteria-2",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-6a_eligibility_criteria-2",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Examples",
    "text": "Examples\n\nWe retrospectively identified patients with a principal diagnosis of myocardial infarction (code 410) according to the International Classification of Diseases, 9th Revision, Clinical Modification, from codes designating discharge diagnoses, excluding the codes with a fifth digit of 2, which designates a subsequent episode of care (…) A random sample of the entire Medicare cohort with myocardial infarction from February 1994 to July 1995 was selected (…) To be eligible, patients had to present to the hospital after at least 30 minutes but less than 12 hours of chest pain and had to have ST-segment elevation of at least 1 mm on two contiguous leads on the initial electrocardiogram\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-7-1",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-7-1",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Read More",
    "text": "Read More\nAuthors should define all variables considered for and included in the analysis, including outcomes, exposures, predictors, potential confounders and potential effect modifiers. Disease outcomes require adequately detailed description of the diagnostic criteria. This applies to criteria for cases in a case-control study, disease events during follow-up in a cohort study and prevalent disease in a cross-sectional study. Clear definitions and steps taken to adhere to them are particularly important for any disease condition of primary interest in the study.\nFor some studies, determinant or predictor may be appropriate terms for exposure variables and outcomes may be called endpoints. In multivariable models, authors sometimes use dependent variable for an outcome and independent variable or explanatory variable for exposure and confounding variables. The latter is not precise as it does not distinguish exposures from confounders.\nIf many variables have been measured and included in exploratory analyses in an early discovery phase, consider providing a list with details on each variable in an appendix, additional table or separate publication. Of note, the International Journal of Epidemiology recently launched a new section with cohort profiles, that includes detailed information on what was measured at different points in time in particular studies [56,57]. Finally, we advise that authors declare all candidate variables considered for statistical analysis, rather than selectively reporting only those included in the final models (see also item 16a) [58,59]"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-7-2",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-7-2",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Examples",
    "text": "Examples\n\nOnly major congenital malformations were included in the analyses. Minor anomalies were excluded according to the exclusion list of European Registration of Congenital Anomalies (EUROCAT). If a child had more than one major congenital malformation of one organ system, those malformations were treated as one outcome in the analyses by organ system (…) In the statistical analyses, factors considered potential confounders were maternal age at delivery and number of previous parities. Factors considered potential effect modifiers were maternal age at reimbursement for antiepileptic medication and maternal age at delivery\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-8_data_sources__measurement-1",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-8_data_sources__measurement-1",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Read More",
    "text": "Read More\nThe way in which exposures, confounders and outcomes were measured affects the reliability and validity of a study. Measurement error and misclassification of exposures or outcomes can make it more difficult to detect cause-effect relationships, or may produce spurious relationships. Error in measurement of potential confounders can increase the risk of residual confounding [62,63]. It is helpful, therefore, if authors report the findings of any studies of the validity or reliability of assessments or measurements, including details of the reference standard that was used. Rather than simply citing validation studies (as in the first example), we advise that authors give the estimated validity or reliability, which can then be used for measurement error adjustment or sensitivity analyses (see items 12e and 17).\nIn addition, it is important to know if groups being compared differed with respect to the way in which the data were collected. This may be important for laboratory examinations (as in the second example) and other situations. For instance, if an interviewer first questions all the cases and then the controls, or vice versa, bias is possible because of the learning curve; solutions such as randomising the order of interviewing may avoid this problem. Information bias may also arise if the compared groups are not given the same diagnostic tests or if one group receives more tests of the same kind than another (see also item 9)."
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-9_bias-1",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-9_bias-1",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Read More",
    "text": "Read More\nBiased studies produce results that differ systematically from the truth (see also Box 3). It is important for a reader to know what measures were taken during the conduct of a study to reduce the potential of bias. Ideally, investigators carefully consider potential sources of bias when they plan their study. At the stage of reporting, we recommend that authors always assess the likelihood of relevant biases. Specifically, the direction and magnitude of bias should be discussed and, if possible, estimated. For instance, in case-control studies information bias can occur, but may be reduced by selecting an appropriate control group, as in the first example [64]. Differences in the medical surveillance of participants were a problem in the second example [65]. Consequently, the authors provide more detail about the additional data they collected to tackle this problem. When investigators have set up quality control programs for data collection to counter a possible drift in measurements of variables in longitudinal studies, or to keep variability at a minimum when multiple observers are used, these should be described.\nUnfortunately, authors often do not address important biases when reporting their results. Among 43 case-control and cohort studies published from 1990 to 1994 that investigated the risk of second cancers in patients with a history of cancer, medical surveillance bias was mentioned in only 5 articles [66]. A survey of reports of mental health research published during 1998 in three psychiatric journals found that only 13% of 392 articles mentioned response bias [67]. A survey of cohort studies in stroke research found that 14 of 49 (28%) articles published from 1999 to 2003 addressed potential selection bias in the recruitment of study participants and 35 (71%) mentioned the possibility that any type of bias may have affected results"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-10_study_size-1",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-10_study_size-1",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Read More",
    "text": "Read More\nA study should be large enough to obtain a point estimate with a sufficiently narrow confidence interval to meaningfully answer a research question. Large samples are needed to distinguish a small association from no association. Small studies often provide valuable information, but wide confidence intervals may indicate that they contribute less to current knowledge in comparison with studies providing estimates with narrower confidence intervals. Also, small studies that show interesting or statistically significant associations are published more frequently than small studies that do not have significant findings. While these studies may provide an early signal in the context of discovery, readers should be informed of their potential weaknesses.\nThe importance of sample size determination in observational studies depends on the context. If an analysis is performed on data that were already available for other purposes, the main question is whether the analysis of the data will produce results with sufficient statistical precision to contribute substantially to the literature, and sample size considerations will be informal. Formal, a priori calculation of sample size may be useful when planning a new study. Such calculations are associated with more uncertainty than implied by the single number that is generally produced. For example, estimates of the rate of the event of interest or other assumptions central to calculations are commonly imprecise, if not guesswork. The precision obtained in the final analysis can often not be determined beforehand because it will be reduced by inclusion of confounding variables in multivariable analyses, the degree of precision with which key variables can be measured, and the exclusion of some individuals.\nFew epidemiological studies explain or report deliberations about sample size. We encourage investigators to report pertinent formal sample size calculations if they were done. In other situations they should indicate the considerations that determined the study size (e.g., a fixed available sample, as in the first example above). If the observational study was stopped early when statistical significance was achieved, readers should be told. Do not bother readers with post hoc justifications for study size or retrospective power calculations. From the point of view of the reader, confidence intervals indicate the statistical precision that was ultimately obtained. It should be realized that confidence intervals reflect statistical uncertainty only, and not all uncertainty that may be present in a study (see item 20)."
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-11_quantitative_variables-1",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-11_quantitative_variables-1",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Read More",
    "text": "Read More\nInvestigators make choices regarding how to collect and analyse quantitative data about exposures, effect modifiers and confounders. For example, they may group a continuous exposure variable to create a new categorical variable. Grouping choices may have important consequences for later analyses. We advise that authors explain why and how they grouped quantitative data, including the number of categories, the cut-points, and category mean or median values. Whenever data are reported in tabular form, the counts of cases, controls, persons at risk, person-time at risk, etc. should be given for each category. Tables should not consist solely of effect-measure estimates or results of model fitting.\nInvestigators might model an exposure as continuous in order to retain all the information. In making this choice, one needs to consider the nature of the relationship of the exposure to the outcome. As it may be wrong to assume a linear relation automatically, possible departures from linearity should be investigated. Authors could mention alternative models they explored during analyses (e.g., using log transformation, quadratic terms or spline functions). Several methods exist for fitting a non-linear relation between the exposure and outcome. Also, it may be informative to present both continuous and grouped analyses for a quantitative exposure of prime interest.\nIn a recent survey, two thirds of epidemiological publications studied quantitative exposure variables. In 42 of 50 articles (84%) exposures were grouped into several ordered categories, but often without any stated rationale for the choices made. Fifteen articles used linear associations to model continuous exposure but only two reported checking for linearity. In another survey, of the psychological literature, dichotomization was justified in only 22 of 110 articles (20%)"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-11_quantitative_variables-2",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-11_quantitative_variables-2",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Examples",
    "text": "Examples\n\nPatients with a Glasgow Coma Scale less than 8 are considered to be seriously injured. A GCS of 9 or more indicates less serious brain injury. We examined the association of GCS in these two categories with the occurrence of death within 12 months from injury\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-12a_statistical_methods-1",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-12a_statistical_methods-1",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Read More",
    "text": "Read More\nIn general, there is no one correct statistical analysis but, rather, several possibilities that may address the same question, but make different assumptions. Regardless, investigators should pre-determine analyses at least for the primary study objectives in a study protocol. Often additional analyses are needed, either instead of, or as well as, those originally envisaged, and these may sometimes be motivated by the data. When a study is reported, authors should tell readers whether particular analyses were suggested by data inspection. Even though the distinction between pre-specified and exploratory analyses may sometimes be blurred, authors should clarify reasons for particular analyses.\nIf groups being compared are not similar with regard to some characteristics, adjustment should be made for possible confounding variables by stratification or by multivariable regression (see Box 5) [94]. Often, the study design determines which type of regression analysis is chosen. For instance, Cox proportional hazard regression is commonly used in cohort studies [95]. whereas logistic regression is often the method of choice in case-control studies [96,97]. Analysts should fully describe specific procedures for variable selection and not only present results from the final model [98,99]. If model comparisons are made to narrow down a list of potential confounders for inclusion in a final model, this process should be described. It is helpful to tell readers if one or two covariates are responsible for a great deal of the apparent confounding in a data analysis. Other statistical analyses such as imputation procedures, data transformation, and calculations of attributable risks should also be described. Non-standard or novel approaches should be referenced and the statistical software used reported. As a guiding principle, we advise statistical methods be described with enough detail to enable a knowledgeable reader with access to the original data to verify the reported results [100].\nIn an empirical study, only 93 of 169 articles (55%) reporting adjustment for confounding clearly stated how continuous and multi-category variables were entered into the statistical model [101]. Another study found that among 67 articles in which statistical analyses were adjusted for confounders, it was mostly unclear how confounders were chosen"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-12a_statistical_methods-2",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-12a_statistical_methods-2",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Examples",
    "text": "Examples\n\nThe adjusted relative risk was calculated using the Mantel-Haenszel technique, when evaluating if confounding by age or gender was present in the groups compared. The 95% confidence interval (CI) was computed around the adjusted relative risk, using the variance according to Greenland and Robins and Robins et al.\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-12b_statistical_methods-1",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-12b_statistical_methods-1",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Read More",
    "text": "Read More\nAs discussed in detail under item 17, many debate the use and value of analyses restricted to subgroups of the study population [4,104]. Subgroup analyses are nevertheless often done [4]. Readers need to know which subgroup analyses were planned in advance, and which arose while analysing the data. Also, it is important to explain what methods were used to examine whether effects or associations differed across groups (see item 17).\nInteraction relates to the situation when one factor modifies the effect of another (therefore also called effect modification). The joint action of two factors can be characterized in two ways: on an additive scale, in terms of risk differences; or on a multiplicative scale, in terms of relative risk (see Box 8). Many authors and readers may have their own preference about the way interactions should be analysed. Still, they may be interested to know to what extent the joint effect of exposures differs from the separate effects. There is consensus that the additive scale, which uses absolute risks, is more appropriate for public health and clinical decision making [105]. Whatever view is taken, this should be clearly presented to the reader, as is done in the example above [103]. A lay-out presenting separate effects of both exposures as well as their joint effect, each relative to no exposure, might be most informative. It is presented in the example for interaction under item 17, and the calculations on the different scales are explained in Box 8"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-12b_statistical_methods-2",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-12b_statistical_methods-2",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Examples",
    "text": "Examples\n\nSex differences in susceptibility to the 3 lifestyle-related risk factors studied were explored by testing for biological interaction according to Rothman: a new composite variable with 4 categories (a−b−, a−b+, a+b−, and a+b+) was redefined for sex and a dichotomous exposure of interest where a− and b− denote absence of exposure. RR was calculated for each category after adjustment for age. An interaction effect is defined as departure from additivity of absolute effects, and excess RR caused by interaction (RERI) was calculated:\n\n[calculation]\nwhere RR(a+b+) denotes RR among those exposed to both factors where RR(a−b−) is used as reference category (RR = 1.0). Ninety-five percent CIs were calculated as proposed by Hosmer and Lemeshow. RERI of 0 means no interaction\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-12c_statistical_methods-1",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-12c_statistical_methods-1",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Read More",
    "text": "Read More\nMissing data are common in observational research. Questionnaires posted to study participants are not always filled in completely, participants may not attend all follow-up visits and routine data sources and clinical databases are often incomplete. Despite its ubiquity and importance, few papers report in detail on the problem of missing data [5,107]. Investigators may use any of several approaches to address missing data. We describe some strengths and limitations of various approaches in Box 6. We advise that authors report the number of missing values for each variable of interest (exposures, outcomes, confounders) and for each step in the analysis. Authors should give reasons for missing values if possible, and indicate how many individuals were excluded because of missing data when describing the flow of participants through the study (see also item 13). For analyses that account for missing data, authors should describe the nature of the analysis (e.g., multiple imputation) and the assumptions that were made (e.g., missing at random, see Box 6)."
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-12c_statistical_methods-2",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-12c_statistical_methods-2",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Examples",
    "text": "Examples\n\nOur missing data analysis procedures used missing at random (MAR) assumptions. We used the MICE (multivariate imputation by chained equations) method of multiple multivariate imputation in STATA. We independently analysed 10 copies of the data, each with missing values suitably imputed, in the multivariate logistic regression analyses. We averaged estimates of the variables to give a single mean estimate and adjusted standard errors according to Rubin’s rules\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-12d_statistical_methods-1",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-12d_statistical_methods-1",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Read More",
    "text": "Read More\nMost cross-sectional studies use a pre-specified sampling strategy to select participants from a source population. Sampling may be more complex than taking a simple random sample, however. It may include several stages and clustering of participants (e.g., in districts or villages). Proportionate stratification may ensure that subgroups with a specific characteristic are correctly represented. Disproportionate stratification may be useful to over-sample a subgroup of particular interest.\nAn estimate of association derived from a complex sample may be more or less precise than that derived from a simple random sample. Measures of precision such as standard error or confidence interval should be corrected using the design effect, a ratio measure that describes how much precision is gained or lost if a more complex sampling strategy is used instead of simple random sampling [119]. Most complex sampling techniques lead to a decrease of precision, resulting in a design effect greater than 1.\nWe advise that authors clearly state the method used to adjust for complex sampling strategies so that readers may understand how the chosen sampling method influenced the precision of the obtained estimates. For instance, with clustered sampling, the implicit trade-off between easier data collection and loss of precision is transparent if the design effect is reported. In the example, the calculated design effects of 1.9 for men indicates that the actual sample size would need to be 1.9 times greater than with simple random sampling for the resulting estimates to have equal precision"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-12d_statistical_methods-2",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-12d_statistical_methods-2",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Examples",
    "text": "Examples\n\nThe standard errors (SE) were calculated using the Taylor expansion method to estimate the sampling errors of estimators based on the complex sample design. (…) The overall design effect for diastolic blood pressure was found to be 1.9 for men and 1.8 for women and, for systolic blood pressure, it was 1.9 for men and 2.0 for women\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-12e_statistical_methods-1",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-12e_statistical_methods-1",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Read More",
    "text": "Read More\nSensitivity analyses are useful to investigate whether or not the main results are consistent with those obtained with alternative analysis strategies or assumptions [121]. Issues that may be examined include the criteria for inclusion in analyses, the definitions of exposures or outcomes [122], which confounding variables merit adjustment, the handling of missing data [120,123], possible selection bias or bias from inaccurate or inconsistent measurement of exposure, disease and other variables, and specific analysis choices, such as the treatment of quantitative variables (see item 11). Sophisticated methods are increasingly used to simultaneously model the influence of several biases or assumptions [124–126].\nIn 1959 Cornfield et al. famously showed that a relative risk of 9 for cigarette smoking and lung cancer was extremely unlikely to be due to any conceivable confounder, since the confounder would need to be at least nine times as prevalent in smokers as in non-smokers [127]. This analysis did not rule out the possibility that such a factor was present, but it did identify the prevalence such a factor would need to have. The same approach was recently used to identify plausible confounding factors that could explain the association between childhood leukaemia and living near electric power lines [128]. More generally, sensitivity analyses can be used to identify the degree of confounding, selection bias, or information bias required to distort an association. One important, perhaps under recognised, use of sensitivity analysis is when a study shows little or no association between an exposure and an outcome and it is plausible that confounding or other biases toward the null are present"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-12e_statistical_methods-2",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-12e_statistical_methods-2",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Examples",
    "text": "Examples\n\nBecause we had a relatively higher proportion of missing dead patients with insufficient data (38/148=25.7%) as compared to live patients (15/437=3.4%) (…), it is possible that this might have biased the results. We have, therefore, carried out a sensitivity analysis. We have assumed that the proportion of women using oral contraceptives in the study group applies to the whole (19.1% for dead, and 11.4% for live patients), and then applied two extreme scenarios: either all the exposed missing patients used second generation pills or they all used third-generation pills\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-13a_participants-1",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-13a_participants-1",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Read More",
    "text": "Read More\nDetailed information on the process of recruiting study participants is important for several reasons. Those included in a study often differ in relevant ways from the target population to which results are applied. This may result in estimates of prevalence or incidence that do not reflect the experience of the target population. For example, people who agreed to participate in a postal survey of sexual behaviour attended church less often, had less conservative sexual attitudes and earlier age at first sexual intercourse, and were more likely to smoke cigarettes and drink alcohol than people who refused [130]. These differences suggest that postal surveys may overestimate sexual liberalism and activity in the population. Such response bias (see Box 3) can distort exposure-disease associations if associations differ between those eligible for the study and those included in the study. As another example, the association between young maternal age and leukaemia in offspring, which has been observed in some case-control studies [131,132], was explained by differential participation of young women in case and control groups. Young women with healthy children were less likely to participate than those with unhealthy children [133]. Although low participation does not necessarily compromise the validity of a study, transparent information on participation and reasons for non-participation is essential. Also, as there are no universally agreed definitions for participation, response or follow-up rates, readers need to understand how authors calculated such proportions [134].\nIdeally, investigators should give an account of the numbers of individuals considered at each stage of recruiting study participants, from the choice of a target population to the inclusion of participants’ data in the analysis. Depending on the type of study, this may include the number of individuals considered to be potentially eligible, the number assessed for eligibility, the number found to be eligible, the number included in the study, the number examined, the number followed up and the number included in the analysis. Information on different sampling units may be required, if sampling of study participants is carried out in two or more stages as in the example above (multistage sampling). In case-control studies, we advise that authors describe the flow of participants separately for case and control groups [135]. Controls can sometimes be selected from several sources, including, for example, hospitalised patients and community dwellers. In this case, we recommend a separate account of the numbers of participants for each type of control group. Olson and colleagues proposed useful reporting guidelines for controls recruited through random-digit dialling and other methods [136].\nA recent survey of epidemiological studies published in 10 general epidemiology, public health and medical journals found that some information regarding participation was provided in 47 of 107 case-control studies (59%), 49 of 154 cohort studies (32%), and 51 of 86 cross-sectional studies (59%) [137]. Incomplete or absent reporting of participation and non-participation in epidemiological studies was also documented in two other surveys of the literature [4,5]. Finally, there is evidence that participation in epidemiological studies may have declined in recent decades [137,138], which underscores the need for transparent reporting"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-13a_participants-2",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-13a_participants-2",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Examples",
    "text": "Examples\n\nOf the 105 freestanding bars and taverns sampled, 13 establishments were no longer in business and 9 were located in restaurants, leaving 83 eligible businesses. In 22 cases, the owner could not be reached by telephone despite 6 or more attempts. The owners of 36 bars declined study participation. (…) The 25 participating bars and taverns employed 124 bartenders, with 67 bartenders working at least 1 weekly daytime shift. Fifty-four of the daytime bartenders (81%) completed baseline interviews and spirometry; 53 of these subjects (98%) completed follow-up\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-13b_participants-1",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-13b_participants-1",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Read More",
    "text": "Read More\nExplaining the reasons why people no longer participated in a study or why they were excluded from statistical analyses helps readers judge whether the study population was representative of the target population and whether bias was possibly introduced. For example, in a cross-sectional health survey, non-participation due to reasons unlikely to be related to health status (for example, the letter of invitation was not delivered because of an incorrect address) will affect the precision of estimates but will probably not introduce bias. Conversely, if many individuals opt out of the survey because of illness, or perceived good health, results may underestimate or overestimate the prevalence of ill health in the population"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-13b_participants-2",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-13b_participants-2",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Examples",
    "text": "Examples\n\nThe main reasons for non-participation were the participant was too ill or had died before interview (cases 30%, controls < 1%), nonresponse (cases 2%, controls 21%), refusal (cases 10%, controls 29%), and other reasons (refusal by consultant or general practitioner, non-English speaking, mental impairment) (cases 7%, controls 5%)\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-13c_participants-1",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-13c_participants-1",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Read More",
    "text": "Read More\nAn informative and well-structured flow diagram can readily and transparently convey information that might otherwise require a lengthy description [142], as in the example above. The diagram may usefully include the main results, such as the number of events for the primary outcome. While we recommend the use of a flow diagram, particularly for complex observational studies, we do not propose a specific format for the diagram"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-13c_participants-2",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-13c_participants-2",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Examples",
    "text": "Examples\n\nFlow diagram from Hay et al. https://doi.org/10.1371/journal.pmed.0040297.g001\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-14a_descriptive_data-1",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-14a_descriptive_data-1",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Read More",
    "text": "Read More\nReaders need descriptions of study participants and their exposures to judge the generalisability of the findings. Information about potential confounders, including whether and how they were measured, influences judgments about study validity. We advise authors to summarize continuous variables for each study group by giving the mean and standard deviation, or when the data have an asymmetrical distribution, as is often the case, the median and percentile range (e.g., 25th and 75th percentiles). Variables that make up a small number of ordered categories (such as stages of disease I to IV) should not be presented as continuous variables; it is preferable to give numbers and proportions for each category (see also Box 4). In studies that compare groups, the descriptive characteristics and numbers should be given by group, as in the example above.\nInferential measures such as standard errors and confidence intervals should not be used to describe the variability of characteristics, and significance tests should be avoided in descriptive tables. Also, P values are not an appropriate criterion for selecting which confounders to adjust for in analysis; even small differences in a confounder that has a strong effect on the outcome can be important.\nIn cohort studies, it may be useful to document how an exposure relates to other characteristics and potential confounders. Authors could present this information in a table with columns for participants in two or more exposure categories, which permits to judge the differences in confounders between these categories.\nIn case-control studies potential confounders cannot be judged by comparing cases and controls. Control persons represent the source population and will usually be different from the cases in many respects. For example, in a study of oral contraceptives and myocardial infarction, a sample of young women with infarction more often had risk factors for that disease, such as high serum cholesterol, smoking and a positive family history, than the control group [146]. This does not influence the assessment of the effect of oral contraceptives, as long as the prescription of oral contraceptives was not guided by the presence of these risk factors—e.g., because the risk factors were only established after the event (see also Box 5). In case-control studies the equivalent of comparing exposed and non-exposed for the presence of potential confounders (as is done in cohorts) can be achieved by exploring the source population of the cases: if the control group is large enough and represents the source population, exposed and unexposed controls can be compared for potential confounders [121,147]"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-14a_descriptive_data-2",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-14a_descriptive_data-2",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Examples",
    "text": "Examples\n\nCharacteristics of the Study Base at Enrolment, Castellana G (Italy), 1985–1986\n\nhttps://doi.org/10.1371/journal.pmed.0040297.t002\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-14b_descriptive_data-1",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-14b_descriptive_data-1",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Read More",
    "text": "Read More\nAs missing data may bias or affect generalisability of results, authors should tell readers amounts of missing data for exposures, potential confounders, and other important characteristics of patients (see also item 12c and Box 6). In a cohort study, authors should report the extent of loss to follow-up (with reasons), since incomplete follow-up may bias findings (see also items 12d and 13). We advise authors to use their tables and figures to enumerate amounts of missing data."
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-14b_descriptive_data-2",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-14b_descriptive_data-2",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Examples",
    "text": "Examples\n\nTable. Symptom End Points Used in Survival Analysis\n\nhttps://doi.org/10.1371/journal.pmed.0040297.t003\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-15_outcome_data-1",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-15_outcome_data-1",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Read More",
    "text": "Read More\nBefore addressing the possible association between exposures (risk factors) and outcomes, authors should report relevant descriptive data. It may be possible and meaningful to present measures of association in the same table that presents the descriptive data (see item 14a). In a cohort study with events as outcomes, report the numbers of events for each outcome of interest. Consider reporting the event rate per person-year of follow-up. If the risk of an event changes over follow-up time, present the numbers and rates of events in appropriate intervals of follow-up or as a Kaplan-Meier life table or plot. It might be preferable to show plots as cumulative incidence that go up from 0% rather than down from 100%, especially if the event rate is lower than, say, 30% [153]. Consider presenting such information separately for participants in different exposure categories of interest. If a cohort study is investigating other time-related outcomes (e.g., quantitative disease markers such as blood pressure), present appropriate summary measures (e.g., means and standard deviations) over time, perhaps in a table or figure.\nFor cross-sectional studies, we recommend presenting the same type of information on prevalent outcome events or summary measures. For case-control studies, the focus will be on reporting exposures separately for cases and controls as frequencies or quantitative summaries [154]. For all designs, it may be helpful also to tabulate continuous outcomes or exposures in categories, even if the data are not analysed as such"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-15_outcome_data-2",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-15_outcome_data-2",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Examples",
    "text": "Examples\n\nTable. Prevalence of Current Asthma and Diagnosed Hay Fever by Average Alternaria alternata Antigen Level in the Household https://doi.org/10.1371/journal.pmed.0040297.t007\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-16a_main_results-1",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-16a_main_results-1",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Read More",
    "text": "Read More\nIn many situations, authors may present the results of unadjusted or minimally adjusted analyses and those from fully adjusted analyses. We advise giving the unadjusted analyses together with the main data, for example the number of cases and controls that were exposed or not. This allows the reader to understand the data behind the measures of association (see also item 15). For adjusted analyses, report the number of persons in the analysis, as this number may differ because of missing values in covariates (see also item 12c). Estimates should be given with confidence intervals.\nReaders can compare unadjusted measures of association with those adjusted for potential confounders and judge by how much, and in what direction, they changed. Readers may think that adjusted results equal the causal part of the measure of association, but adjusted results are not necessarily free of random sampling error, selection bias, information bias, or residual confounding (see Box 5). Thus, great care should be exercised when interpreting adjusted results, as the validity of results often depends crucially on complete knowledge of important confounders, their precise measurement, and appropriate specification in the statistical model (see also item 20) [157,158].\nAuthors should explain all potential confounders considered, and the criteria for excluding or including variables in statistical models. Decisions about excluding or including variables should be guided by knowledge, or explicit assumptions, on causal relations. Inappropriate decisions may introduce bias, for example by including variables that are in the causal pathway between exposure and disease (unless the aim is to asses how much of the effect is carried by the intermediary variable). If the decision to include a variable in the model was based on the change in the estimate, it is important to report what change was considered sufficiently important to justify its inclusion. If a backward deletion or forward inclusion strategy was used to select confounders, explain that process and give the significance level for rejecting the null hypothesis of no confounding. Of note, we and others do not advise selecting confounders based solely on statistical significance testing [147,159,160].\nRecent studies of the quality of reporting of epidemiological studies found that confidence intervals were reported in most articles [4]. However, few authors explained their choice of confounding variables [4,5]."
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-16a_main_results-2",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-16a_main_results-2",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Examples",
    "text": "Examples\n\nTable. Relative Rates of Rehospitalisation by Treatment in Patients in Community Care after First Hospitalisation due to Schizophrenia and Schizoaffective Disorder https://doi.org/10.1371/journal.pmed.0040297.t008\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-16b_main_results-1",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-16b_main_results-1",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Read More",
    "text": "Read More\nCategorizing continuous data has several important implications for analysis (see Box 4) and also affects the presentation of results. In tables, outcomes should be given for each exposure category, for example as counts of persons at risk, person-time at risk, if relevant separately for each group (e.g., cases and controls). Details of the categories used may aid comparison of studies and meta-analysis. If data were grouped using conventional cut-points, such as body mass index thresholds [162], group boundaries (i.e., range of values) can be derived easily, except for the highest and lowest categories. If quantile-derived categories are used, the category boundaries cannot be inferred from the data. As a minimum, authors should report the category boundaries; it is helpful also to report the range of the data and the mean or median values within categories."
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-16b_main_results-2",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-16b_main_results-2",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Examples",
    "text": "Examples\n\nTable. Polychlorinated Biphenyls in Cord Serum\n\nhttps://doi.org/10.1371/journal.pmed.0040297.t005\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-16c_main_results-1",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-16c_main_results-1",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Read More",
    "text": "Read More\nThe results from studies examining the association between an exposure and a disease are commonly reported in relative terms, as ratios of risks, rates or odds (see Box 8). Relative measures capture the strength of the association between an exposure and disease. If the relative risk is a long way from 1 it is less likely that the association is due to confounding [164,165]. Relative effects or associations tend to be more consistent across studies and populations than absolute measures, but what often tends to be the case may be irrelevant in a particular instance. For example, similar relative risks were obtained for the classic cardiovascular risk factors for men living in Northern Ireland, France, the USA and Germany, despite the fact that the underlying risk of coronary heart disease varies substantially between these countries [166,167]. In contrast, in a study of hypertension as a risk factor for cardiovascular disease mortality, the data were more compatible with a constant rate difference than with a constant rate ratio [168].\nWidely used statistical models, including logistic [169] and proportional hazards (Cox) regression [170] are based on ratio measures. In these models, only departures from constancy of ratio effect measures are easily discerned. Nevertheless, measures which assess departures from additivity of risk differences, such as the Relative Excess Risk from Interaction (RERI, see item 12b and Box 8), can be estimated in models based on ratio measures.\nIn many circumstances, the absolute risk associated with an exposure is of greater interest than the relative risk. For example, if the focus is on adverse effects of a drug, one will want to know the number of additional cases per unit time of use (e.g., days, weeks, or years). The example gives the additional number of breast cancer cases per 1000 women who used hormone-replacement therapy for 10 years [163]. Measures such as the attributable risk or population attributable fraction may be useful to gauge how much disease can be prevented if the exposure is eliminated. They should preferably be presented together with a measure of statistical uncertainty (e.g., confidence intervals as in the example). Authors should be aware of the strong assumptions made in this context, including a causal relationship between a risk factor and disease (also see Box 7) [171]. Because of the semantic ambiguity and complexities involved, authors should report in detail what methods were used to calculate attributable risks, ideally giving the formulae used [172].\nA recent survey of abstracts of 222 articles published in leading medical journals found that in 62% of abstracts of randomised trials including a ratio measure absolute risks were given, but only in 21% of abstracts of cohort studies [173]. A free text search of Medline 1966 to 1997 showed that 619 items mentioned attributable risks in the title or abstract, compared to 18,955 using relative risk or odds ratio, for a ratio of 1 to 31 [174]."
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-16c_main_results-2",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-16c_main_results-2",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Examples",
    "text": "Examples\n\n10 years’ use of HRT [hormone replacement therapy] is estimated to result in five (95% CI 3–7) additional breast cancers per 1000 users of oestrogen-only preparations and 19 (15–23) additional cancers per 1000 users of oestrogen-progestagen combinations\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-17_other_analyses-1",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-17_other_analyses-1",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Read More",
    "text": "Read More\nIn addition to the main analysis other analyses are often done in observational studies. They may address specific subgroups, the potential interaction between risk factors, the calculation of attributable risks, or use alternative definitions of study variables in sensitivity analyses.\nThere is debate about the dangers associated with subgroup analyses, and multiplicity of analyses in general [4,104]. In our opinion, there is too great a tendency to look for evidence of subgroup-specific associations, or effect-measure modification, when overall results appear to suggest little or no effect. On the other hand, there is value in exploring whether an overall association appears consistent across several, preferably pre-specified subgroups especially when a study is large enough to have sufficient data in each subgroup. A second area of debate is about interesting subgroups that arose during the data analysis. They might be important findings, but might also arise by chance. Some argue that it is neither possible nor necessary to inform the reader about all subgroup analyses done as future analyses of other data will tell to what extent the early exciting findings stand the test of time [9]. We advise authors to report which analyses were planned, and which were not (see also items 4, 12b and 20). This will allow readers to judge the implications of multiplicity, taking into account the study’s position on the continuum from discovery to verification or refutation.\nA third area of debate is how joint effects and interactions between risk factors should be evaluated: on additive or multiplicative scales, or should the scale be determined by the statistical model that fits best (see also item 12b and Box 8)? A sensible approach is to report the separate effect of each exposure as well as the joint effect—if possible in a table, as in the first example above [183], or in the study by Martinelli et al. [185]. Such a table gives the reader sufficient information to evaluate additive as well as multiplicative interaction (how these calculations are done is shown in Box 8). Confidence intervals for separate and joint effects may help the reader to judge the strength of the data. In addition, confidence intervals around measures of interaction, such as the Relative Excess Risk from Interaction (RERI) relate to tests of interaction or homogeneity tests. One recurrent problem is that authors use comparisons of P-values across subgroups, which lead to erroneous claims about an effect modifier. For instance, a statistically significant association in one category (e.g., men), but not in the other (e.g., women) does not in itself provide evidence of effect modification. Similarly, the confidence intervals for each point estimate are sometimes inappropriately used to infer that there is no interaction when intervals overlap. A more valid inference is achieved by directly evaluating whether the magnitude of an association differs across subgroups.\nSensitivity analyses are helpful to investigate the influence of choices made in the statistical analysis, or to investigate the robustness of the findings to missing data or possible biases (see also item 12b). Judgement is needed regarding the level of reporting of such analyses. If many sensitivity analyses were performed, it may be impractical to present detailed findings for them all. It may sometimes be sufficient to report that sensitivity analyses were carried out and that they were consistent with the main results presented. Detailed presentation is more appropriate if the issue investigated is of major concern, or if effect estimates vary considerably [59,186].\nPocock and colleagues found that 43 out of 73 articles reporting observational studies contained subgroup analyses. The majority claimed differences across groups but only eight articles reported a formal evaluation of interaction (see item 12b) [4]."
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-17_other_analyses-2",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-17_other_analyses-2",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Examples",
    "text": "Examples\n\nTable. Analysis of Oral Contraceptive Use, Presence of Factor V Leiden Allele, and Risk for Venous Thromboembolism\n\nhttps://doi.org/10.1371/journal.pmed.0040297.t009\n\nTable. Sensitivity of the Rate Ratio for Cardiovascular Outcome to an Unmeasured Confounder https://doi.org/10.1371/journal.pmed.0040297.t010\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-18_key_results-1",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-18_key_results-1",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Read More",
    "text": "Read More\nIt is good practice to begin the discussion with a short summary of the main findings of the study. The short summary reminds readers of the main findings and may help them assess whether the subsequent interpretation and implications offered by the authors are supported by the findings"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-18_key_results-2",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-18_key_results-2",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Examples",
    "text": "Examples\n\nWe hypothesized that ethnic minority status would be associated with higher levels of cardiovascular disease (CVD) risk factors, but that the associations would be explained substantially by socioeconomic status (SES). Our hypothesis was not confirmed. After adjustment for age and SES, highly significant differences in body mass index, blood pressure, diabetes, and physical inactivity remained between white women and both black and Mexican American women. In addition, we found large differences in CVD risk factors by SES, a finding that illustrates the high-risk status of both ethnic minority women as well as white women with low SES\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-19_limitations-1",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-19_limitations-1",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Read More",
    "text": "Read More\nThe identification and discussion of the limitations of a study are an essential part of scientific reporting. It is important not only to identify the sources of bias and confounding that could have affected results, but also to discuss the relative importance of different biases, including the likely direction and magnitude of any potential bias (see also item 9 and Box 3).\nAuthors should also discuss any imprecision of the results. Imprecision may arise in connection with several aspects of a study, including the study size (item 10) and the measurement of exposures, confounders and outcomes (item 8). The inability to precisely measure true values of an exposure tends to result in bias towards unity: the less precisely a risk factor is measured, the greater the bias. This effect has been described as attenuation [201,202], or more recently as regression dilution bias [203]. However, when correlated risk factors are measured with different degrees of imprecision, the adjusted relative risk associated with them can be biased towards or away from unity [204–206].\nWhen discussing limitations, authors may compare the study being presented with other studies in the literature in terms of validity, generalisability and precision. In this approach, each study can be viewed as contribution to the literature, not as a stand-alone basis for inference and action [207]. Surprisingly, the discussion of important limitations of a study is sometimes omitted from published reports. A survey of authors who had published original research articles in The Lancet found that important weaknesses of the study were reported by the investigators in the survey questionnaires, but not in the published article"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-19_limitations-2",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-19_limitations-2",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Examples",
    "text": "Examples\n\nSince the prevalence of counseling increases with increasing levels of obesity, our estimates may overestimate the true prevalence. Telephone surveys also may overestimate the true prevalence of counseling. Although persons without telephones have similar levels of overweight as persons with telephones, persons without telephones tend to be less educated, a factor associated with lower levels of counseling in our study. Also, of concern is the potential bias caused by those who refused to participate as well as those who refused to respond to questions about weight. Furthermore, because data were collected cross-sectionally, we cannot infer that counseling preceded a patient’s attempt to lose weight\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-20_interpretation-1",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-20_interpretation-1",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Read More",
    "text": "Read More\nThe heart of the discussion section is the interpretation of a study’s results. Over-interpretation is common and human: even when we try hard to give an objective assessment, reviewers often rightly point out that we went too far in some respects. When interpreting results, authors should consider the nature of the study on the discovery to verification continuum and potential sources of bias, including loss to follow-up and non-participation (see also items 9, 12 and 19). Due consideration should be given to confounding (item 16a), the results of relevant sensitivity analyses, and to the issue of multiplicity and subgroup analyses (item 17). Authors should also consider residual confounding due to unmeasured variables or imprecise measurement of confounders. For example, socioeconomic status (SES) is associated with many health outcomes and often differs between groups being compared. Variables used to measure SES (income, education, or occupation) are surrogates for other undefined and unmeasured exposures, and the true confounder will by definition be measured with error [208]. Authors should address the real range of uncertainty in estimates, which is larger than the statistical uncertainty reflected in confidence intervals. The latter do not take into account other uncertainties that arise from a study’s design, implementation, and methods of measurement [209].\nTo guide thinking and conclusions about causality, some may find criteria proposed by Bradford Hill in 1965 helpful [164]. How strong is the association with the exposure? Did it precede the onset of disease? Is the association consistently observed in different studies and settings? Is there supporting evidence from experimental studies, including laboratory and animal studies? How specific is the exposure’s putative effect, and is there a dose-response relationship? Is the association biologically plausible? These criteria should not, however, be applied mechanically. For example, some have argued that relative risks below 2 or 3 should be ignored [210,211]. This is a reversal of the point by Cornfield et al. about the strength of large relative risks (see item 12b) [127]. Although a causal effect is more likely with a relative risk of 9, it does not follow that one below 3 is necessarily spurious. For instance, the small increase in the risk of childhood leukaemia after intrauterine irradiation is credible because it concerns an adverse effect of a medical procedure for which no alternative explanations are obvious [212]. Moreover, the carcinogenic effects of radiation are well established. The doubling in the risk of ovarian cancer associated with eating 2 to 4 eggs per week is not immediately credible, since dietary habits are associated with a large number of lifestyle factors as well as SES [213]. In contrast, the credibility of much debated epidemiologic findings of a difference in thrombosis risk between different types of oral contraceptives was greatly enhanced by the differences in coagulation found in a randomised cross-over trial [214]. A discussion of the existing external evidence, from different types of studies, should always be included, but may be particularly important for studies reporting small increases in risk. Further, authors should put their results in context with similar studies and explain how the new study affects the existing body of evidence, ideally by referring to a systematic review."
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-20_interpretation-2",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-20_interpretation-2",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Examples",
    "text": "Examples\n\nAny explanation for an association between death from myocardial infarction and use of second generation oral contraceptives must be conjectural. There is no published evidence to suggest a direct biologic mechanism, and there are no other epidemiologic studies with relevant results. (…) The increase in absolute risk is very small and probably applies predominantly to smokers. Due to the lack of corroborative evidence, and because the analysis is based on relatively small numbers, more evidence on the subject is needed. We would not recommend any change in prescribing practice on the strength of these results\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-21_generalisability-1",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-21_generalisability-1",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Read More",
    "text": "Read More\nGeneralisability, also called external validity or applicability, is the extent to which the results of a study can be applied to other circumstances [216]. There is no external validity per se; the term is meaningful only with regard to clearly specified conditions [217]. Can results be applied to an individual, groups or populations that differ from those enrolled in the study with regard to age, sex, ethnicity, severity of disease, and co-morbid conditions? Are the nature and level of exposures comparable, and the definitions of outcomes relevant to another setting or population? Are data that were collected in longitudinal studies many years ago still relevant today? Are results from health services research in one country applicable to health systems in other countries?\nThe question of whether the results of a study have external validity is often a matter of judgment that depends on the study setting, the characteristics of the participants, the exposures examined, and the outcomes assessed. Thus, it is crucial that authors provide readers with adequate information about the setting and locations, eligibility criteria, the exposures and how they were measured, the definition of outcomes, and the period of recruitment and follow-up. The degree of non-participation and the proportion of unexposed participants in whom the outcome develops are also relevant. Knowledge of the absolute risk and prevalence of the exposure, which will often vary across populations, are helpful when applying results to other settings and populations (see Box 7)"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-21_generalisability-2",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-21_generalisability-2",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Examples",
    "text": "Examples\n\nHow applicable are our estimates to other HIV-1-infected patients? This is an important question because the accuracy of prognostic models tends to be lower when applied to data other than those used to develop them. We addressed this issue by penalising model complexity, and by choosing models that generalised best to cohorts omitted from the estimation procedure. Our database included patients from many countries from Europe and North America, who were treated in different settings. The range of patients was broad: men and women, from teenagers to elderly people were included, and the major exposure categories were well represented. The severity of immunodeficiency at baseline ranged from not measureable to very severe, and viral load from undetectable to extremely high\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe-cross-sectional/index.html#sec-22_funding-1",
    "href": "guidelines/strobe-cross-sectional/index.html#sec-22_funding-1",
    "title": "The STROBE cross sectional guideline for writing a Cross sectional study.",
    "section": "Read More",
    "text": "Read More\nSome journals require authors to disclose the presence or absence of financial and other conflicts of interest [100,218]. Several investigations show strong associations between the source of funding and the conclusions of research articles [219–222]. The conclusions in randomised trials recommended the experimental drug as the drug of choice much more often (odds ratio 5.3) if the trial was funded by for-profit organisations, even after adjustment for the effect size [223]. Other studies document the influence of the tobacco and telecommunication industries on the research they funded [224–227]. There are also examples of undue influence when the sponsor is governmental or a non-profit organisation.\nAuthors or funders may have conflicts of interest that influence any of the following: the design of the study [228]; choice of exposures [228,229], outcomes [230], statistical methods [231], and selective publication of outcomes [230] and studies [232]. Consequently, the role of the funders should be described in detail: in what part of the study they took direct responsibility (e.g., design, data collection, analysis, drafting of manuscript, decision to publish) [100]. Other sources of undue influence include employers (e.g., university administrators for academic researchers and government supervisors, especially political appointees, for government researchers), advisory committees, litigants, and special interest groups"
  },
  {
    "objectID": "guidelines/strobe/index.html#about-this-guideline",
    "href": "guidelines/strobe/index.html#about-this-guideline",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "About this guideline",
    "text": "About this guideline\n\nThis advice is relevant to studies where the investigators observe exposures (e.g. smoking status) and outcomes (e.g. cancer) in study participants but do not influence what happens. Observational studies include cohort, case-control, and cross-sectional studies."
  },
  {
    "objectID": "guidelines/strobe/index.html#download-resources",
    "href": "guidelines/strobe/index.html#download-resources",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Download Resources",
    "text": "Download Resources\nReporting guidelines are best used early:\n\nUse a template to write quickly and avoid the blank page\nUse a checklist to double-check and reassure others that your work is described transparently\n\n\n\n\nFor drafting\n\nTemplate\n\n\n\nFor checking\n\nChecklist\n\n\n\nFor planning\n\nTo-do list"
  },
  {
    "objectID": "guidelines/strobe/index.html#guidance",
    "href": "guidelines/strobe/index.html#guidance",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Guidance",
    "text": "Guidance\nApprox. 3 min read\n\nTitle and abstract\n\n\n1a. Title\n\n\n\n\n\n\n\nIndicate the study’s design with a commonly used term in the title or the abstract\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nReaders should be able to easily identify the design that was used from the title or abstract. An explicit, commonly used term for the study design also helps ensure correct indexing of articles in electronic databases.\n\n\nExamples\n\nLeukaemia incidence among workers in the shoe and boot manufacturing industry: a case-control study\n\nBack to top\n\n\n\n\n\n\n1b. Abstract\n\n\n\n\n\n\n\nProvide in the abstract an informative and balanced summary of what was done and what was found\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe abstract provides key information that enables readers to understand a study and decide whether to read the article. Typical components include a statement of the research question, a short description of methods and results, and a conclusion [22]. Abstracts should summarize key details of studies and should only present information that is provided in the article. We advise presenting key results in a numerical form that includes numbers of participants, estimates of associations and appropriate measures of variability and uncertainty (e.g., odds ratios with confidence intervals). We regard it insufficient to state only that an exposure is or is not significantly associated with an outcome.\n\n\nExamples\n\nBackground: The expected survival of HIV-infected patients is of major public health interest.\n\n\nObjective: To estimate survival time and age-specific mortality rates of an HIV-infected population compared with that of the general population.\n\n\nDesign: Population-based cohort study.\n\n\nSetting: All HIV-infected persons receiving care in Denmark from 1995 to 2005.\n\n\nPatients: Each member of the nationwide Danish HIV Cohort Study was matched with as many as 99 persons from the general population according to sex, date of birth, and municipality of residence.\n\n\nMeasurements: The authors computed Kaplan-Meier life tables with age as the time scale to estimate survival from age 25 years. Patients with HIV infection and corresponding persons from the general population were observed from the date of the patient’s HIV diagnosis until death, emigration, or 1 May 2005.\n\n\nResults: 3990 HIV-infected patients and 379,872 persons from the general population were included in the study, yielding 22,744 (median, 5.8 y/person) and 2,689,287 (median, 8.4 years/person) person-years of observation. Three percent of participants were lost to follow-up. From age 25 years, the median survival was 19.9 years (95% CI, 18.5 to 21.3) among patients with HIV infection and 51.1 years (CI, 50.9 to 51.5) among the general population. For HIV-infected patients, survival increased to 32.5 years (CI, 29.4 to 34.7) during the 2000 to 2005 period. In the subgroup that excluded persons with known hepatitis C coinfection (16%), median survival was 38.9 years (CI, 35.4 to 40.1) during this same period. The relative mortality rates for patients with HIV infection compared with those for the general population decreased with increasing age, whereas the excess mortality rate increased with increasing age.\n\n\nLimitations: The observed mortality rates are assumed to apply beyond the current maximum observation time of 10 years.\n\n\nConclusions: The estimated median survival is more than 35 years for a young person diagnosed with HIV infection in the late highly active antiretroviral therapy era. However, an ongoing effort is still needed to further reduce mortality rates for these persons compared with the general population\n\nBack to top\n\n\n\n\n\n\nIntroduction\n\n\n2. Background / rationale\n\n\n\n\n\n\n\nExplain the scientific background and rationale for the investigation being reported\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe scientific background of the study provides important context for readers. It sets the stage for the study and describes its focus. It gives an overview of what is known on a topic and what gaps in current knowledge are addressed by the study. Background material should note recent pertinent studies and any systematic reviews of pertinent studies.\n\n\nExamples\n\nConcerns about the rising prevalence of obesity in children and adolescents have focused on the well documented associations between childhood obesity and increased cardiovascular risk1 and mortality in adulthood.2 Childhood obesity has considerable social and psychological consequences within childhood and adolescence,3 yet little is known about social, socioeconomic, and psychological consequences in adult life.\n\nA recent systematic review found no longitudinal studies on the outcomes of childhood obesity other than physical health outcomes3 and only two longitudinal studies of the socioeconomic effects of obesity in adolescence. Gortmaker et al found that US women who had been obese in late adolescence in 1981 were less likely to be married and had lower incomes seven years later than women who had not been overweight, while men who had been overweight were less likely to be married.4 Sargent et al found that UK women, but not men, who had been obese at 16 years in 1974 earned 7.4% less than their non-obese peers at age 23.5\nThe study of adult outcomes of childhood obesity is difficult because obesity often continues into adult life and therefore poorer socioeconomic and educational outcomes may actually reflect confounding by adult obesity. Yet identifying outcomes related to obesity confined childhood is important in determining whether people who are obese in childhood and who later lose weight remain at risk for adult adversity and inequalities.\nWe used longitudinal data from the 1970 British birth cohort to examine the adult socioeconomic, educational, social, and psychological outcomes of childhood obesity. We hypothesised that obesity limited to childhood has fewer adverse adult outcomes than obesity that persists into adult life\nBack to top\n\n\n\n\n\n\n3. Objectives\n\n\n\n\n\n\n\nState specific objectives, including any prespecified hypotheses\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nObjectives are the detailed aims of the study. Well crafted objectives specify populations, exposures and outcomes, and parameters that will be estimated. They may be formulated as specific hypotheses or as questions that the study was designed to address. In some situations objectives may be less specific, for example, in early discovery phases. Regardless, the report should clearly reflect the investigators’ intentions. For example, if important subgroups or additional analyses were not the original aim of the study but arose during data analysis, they should be described accordingly (see also items 4, 17 and 20).\n\n\nExamples\n\nOur primary objectives were to 1) determine the prevalence of domestic violence among female patients presenting to four community-based, primary care, adult medicine practices that serve patients of diverse socioeconomic background and 2) identify demographic and clinical differences between currently abused patients and patients not currently being abused\n\nBack to top\n\n\n\n\n\n\nMethods\n\n\n4. Study design\n\n\n\n\n\n\n\nPresent key elements of study design early in the paper.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nWe advise presenting key elements of study design early in the methods section (or at the end of the introduction) so that readers can understand the basics of the study. For example, authors should indicate that the study was a cohort study, which followed people over a particular time period, and describe the group of persons that comprised the cohort and their exposure status. Similarly, if the investigation used a case-control design, the cases and controls and their source population should be described. If the study was a cross-sectional survey, the population and the point in time at which the cross-section was taken should be mentioned. When a study is a variant of the three main study types, there is an additional need for clarity. For instance, for a case-crossover study, one of the variants of the case-control design, a succinct description of the principles was given in the example above [28].\nWe recommend that authors refrain from simply calling a study prospective or retrospective because these terms are ill defined [29]. One usage sees cohort and prospective as synonymous and reserves the word retrospective for case-control studies [30]. A second usage distinguishes prospective and retrospective cohort studies according to the timing of data collection relative to when the idea for the study was developed [31]. A third usage distinguishes prospective and retrospective case-control studies depending on whether the data about the exposure of interest existed when cases were selected [32]. Some advise against using these terms [33], or adopting the alternatives concurrent and historical for describing cohort studies [34]. In STROBE, we do not use the words prospective and retrospective, nor alternatives such as concurrent and historical. We recommend that, whenever authors use these words, they define what they mean. Most importantly, we recommend that authors describe exactly how and when data collection took place.\nThe first part of the methods section might also be the place to mention whether the report is one of several from a study. If a new report is in line with the original aims of the study, this is usually indicated by referring to an earlier publication and by briefly restating the salient features of the study. However, the aims of a study may also evolve over time. Researchers often use data for purposes for which they were not originally intended, including, for example, official vital statistics that were collected primarily for administrative purposes, items in questionnaires that originally were only included for completeness, or blood samples that were collected for another purpose. For example, the Physicians’ Health Study, a randomized controlled trial of aspirin and carotene, was later used to demonstrate that a point mutation in the factor V gene was associated with an increased risk of venous thrombosis, but not of myocardial infarction or stroke [35]. The secondary use of existing data is a creative part of observational research and does not necessarily make results less credible or less important. However, briefly restating the original aims might help readers understand the context of the research and possible limitations in the data.\n\n\nExamples\n\nWe used a case-crossover design, a variation of a case-control design that is appropriate when a brief exposure (driver’s phone use) causes a transient rise in the risk of a rare outcome (a crash). We compared a driver’s use of a mobile phone at the estimated time of a crash with the same driver’s use during another suitable time period. Because drivers are their own controls, the design controls for characteristics of the driver that may affect the risk of a crash but do not change over a short period of time. As it is important that risks during control periods and crash trips are similar, we compared phone activity during the hazard interval (time immediately before the crash) with phone activity during control intervals (equivalent times during which participants were driving but did not crash) in the previous week\n\nBack to top\n\n\n\n\n\n\n5. Setting\n\n\n\n\n\n\n\nDescribe the setting, locations, and relevant dates, including periods of recruitment, exposure, follow-up, and data collection.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nReaders need information on setting and locations to assess the context and generalisability of a study’s results. Exposures such as environmental factors and therapies can change over time. Also, study methods may evolve over time. Knowing when a study took place and over what period participants were recruited and followed up places the study in historical context and is important for the interpretation of results.\nInformation about setting includes recruitment sites or sources (e.g., electoral roll, outpatient clinic, cancer registry, or tertiary care centre). Information about location may refer to the countries, towns, hospitals or practices where the investigation took place. We advise stating dates rather than only describing the length of time periods. There may be different sets of dates for exposure, disease occurrence, recruitment, beginning and end of follow-up, and data collection. Of note, nearly 80% of 132 reports in oncology journals that used survival analysis included the starting and ending dates for accrual of patients, but only 24% also reported the date on which follow-up ended [37].\n\n\nExamples\n\nThe Pasitos Cohort Study recruited pregnant women from Women, Infant and Child (WIC) clinics in Socorro and San Elizario, El Paso County, Texas and maternal-child clinics of the Mexican Social Security Institute (IMSS) in Ciudad Juarez, Mexico from April 1998 to October 2000. At baseline, prior to the birth of the enrolled cohort children, staff interviewed mothers regarding the household environment. In this ongoing cohort study, we target follow-up exams at 6-month intervals beginning at age 6 months\n\nBack to top\n\n\n\n\n\n\n6a. Eligibility criteria\n\n\n\n\n\n\n\nCohort study: Give the eligibility criteria, and the sources and methods of selection of participants. Describe methods of follow-up.\nCase-control study: Give the eligibility criteria, and the sources and methods of case ascertainment and control selection. Give the rationale for the choice of cases and controls.\nCross-sectional study: Give the eligibility criteria, and the sources and methods of selection of participants.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nDetailed descriptions of the study participants help readers understand the applicability of the results. Investigators usually restrict a study population by defining clinical, demographic and other characteristics of eligible participants. Typical eligibility criteria relate to age, gender, diagnosis and comorbid conditions. Despite their importance, eligibility criteria often are not reported adequately. In a survey of observational stroke research, 17 of 49 reports (35%) did not specify eligibility criteria [5].\nEligibility criteria may be presented as inclusion and exclusion criteria, although this distinction is not always necessary or useful. Regardless, we advise authors to report all eligibility criteria and also to describe the group from which the study population was selected (e.g., the general population of a region or country), and the method of recruitment (e.g., referral or self-selection through advertisements).\nKnowing details about follow-up procedures, including whether procedures minimized non-response and loss to follow-up and whether the procedures were similar for all participants, informs judgments about the validity of results. For example, in a study that used IgM antibodies to detect acute infections, readers needed to know the interval between blood tests for IgM antibodies so that they could judge whether some infections likely were missed because the interval between blood tests was too long [41]. In other studies where follow-up procedures differed between exposed and unexposed groups, readers might recognize substantial bias due to unequal ascertainment of events or differences in non-response or loss to follow-up [42]. Accordingly, we advise that researchers describe the methods used for following participants and whether those methods were the same for all participants, and that they describe the completeness of ascertainment of variables (see also item 14).\nIn case-control studies, the choice of cases and controls is crucial to interpreting the results, and the method of their selection has major implications for study validity. In general, controls should reflect the population from which the cases arose. Various methods are used to sample controls, all with advantages and disadvantages: for cases that arise from a general population, population roster sampling, random digit dialling, neighbourhood or friend controls are used. Neighbourhood or friend controls may present intrinsic matching on exposure [17]. Controls with other diseases may have advantages over population-based controls, in particular for hospital-based cases, because they better reflect the catchment population of a hospital, have greater comparability of recall and ease of recruitment. However, they can present problems if the exposure of interest affects the risk of developing or being hospitalized for the control condition(s) [43,44]. To remedy this problem often a mixture of the best defensible control diseases is used [45].\n\n\nExamples\n\nCohort study: Participants in the Iowa Women’s Health Study were a random sample of all women ages 55 to 69 years derived from the state of Iowa automobile driver’s license list in 1985, which represented approximately 94% of Iowa women in that age group. (…) Follow-up questionnaires were mailed in October 1987 and August 1989 to assess vital status and address changes. (…) Incident cancers, except for nonmelanoma skin cancers, were ascertained by the State Health Registry of Iowa (…). The Iowa Women’s Health Study cohort was matched to the registry with combinations of first, last, and maiden names, zip code, birthdate, and social security number.\n\n\nCase-control study: Cutaneous melanoma cases diagnosed in 1999 and 2000 were ascertained through the Iowa Cancer Registry (…). Controls, also identified through the Iowa Cancer Registry, were colorectal cancer patients diagnosed during the same time. Colorectal cancer controls were selected because they are common and have a relatively long survival, and because arsenic exposure has not been conclusively linked to the incidence of colorectal cancer.\n\n\n“Cross-sectional study:** We retrospectively identified patients with a principal diagnosis of myocardial infarction (code 410) according to the International Classification of Diseases, 9th Revision, Clinical Modification, from codes designating discharge diagnoses, excluding the codes with a fifth digit of 2, which designates a subsequent episode of care (…) A random sample of the entire Medicare cohort with myocardial infarction from February 1994 to July 1995 was selected (…) To be eligible, patients had to present to the hospital after at least 30 minutes but less than 12 hours of chest pain and had to have ST-segment elevation of at least 1 mm on two contiguous leads on the initial electrocardiogram.\n\n\n\n\nBack to top\n\n\n\n\n\n\n6b. Matching criteria\n\n\n\n\n\n\n\nCohort study: For matched studies, give matching criteria and number of exposed and unexposed.\nCase-control study: For matched studies, give matching criteria and the number of controls per case.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nMatching is much more common in case-control studies, but occasionally, investigators use matching in cohort studies to make groups comparable at the start of follow-up. Matching in cohort studies makes groups directly comparable for potential confounders and presents fewer intricacies than with case-control studies. For example, it is not necessary to take the matching into account for the estimation of the relative risk [48]. Because matching in cohort studies may increase statistical precision investigators might allow for the matching in their analyses and thus obtain narrower confidence intervals.\nIn case-control studies matching is done to increase a study’s efficiency by ensuring similarity in the distribution of variables between cases and controls, in particular the distribution of potential confounding variables [48,49]. Because matching can be done in various ways, with one or more controls per case, the rationale for the choice of matching variables and the details of the method used should be described. Commonly used forms of matching are frequency matching (also called group matching) and individual matching. In frequency matching, investigators choose controls so that the distribution of matching variables becomes identical or similar to that of cases. Individual matching involves matching one or several controls to each case. Although intuitively appealing and sometimes useful, matching in case-control studies has a number of disadvantages, is not always appropriate, and needs to be taken into account in the analysis (see Box 2).\nEven apparently simple matching procedures may be poorly reported. For example, authors may state that controls were matched to cases within five years, or using five year age bands. Does this mean that, if a case was 54 years old, the respective control needed to be in the five-year age band 50 to 54, or aged 49 to 59, which is within five years of age 54? If a wide (e.g., 10-year) age band is chosen, there is a danger of residual confounding by age (see also Box 4), for example because controls may then be younger than cases on average.\n\n\nExamples\n\nCohort study: For each patient who initially received a statin, we used propensity-based matching to identify one control who did not receive a statin according to the following protocol. First, propensity scores were calculated for each patient in the entire cohort on the basis of an extensive list of factors potentially related to the use of statins or the risk of sepsis. Second, each statin user was matched to a smaller pool of non-statin-users by sex, age (plus or minus 1 year), and index date (plus or minus 3 months). Third, we selected the control with the closest propensity score (within 0.2 SD) to each statin user in a 1:1 fashion and discarded the remaining controls.\n\n\nCase-control study: We aimed to select five controls for every case from among individuals in the study population who had no diagnosis of autism or other pervasive developmental disorders (PDD) recorded in their general practice record and who were alive and registered with a participating practice on the date of the PDD diagnosis in the case. Controls were individually matched to cases by year of birth (up to 1 year older or younger), sex, and general practice. For each of 300 cases, five controls could be identified who met all the matching criteria. For the remaining 994, one or more controls was excluded…\n\nBack to top\n\n\n\n\n\n\n7. Variables\n\n\n\n\n\n\n\nClearly define all outcomes, exposures, predictors, potential confounders, and effect modifiers. Give diagnostic criteria, if applicable\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAuthors should define all variables considered for and included in the analysis, including outcomes, exposures, predictors, potential confounders and potential effect modifiers. Disease outcomes require adequately detailed description of the diagnostic criteria. This applies to criteria for cases in a case-control study, disease events during follow-up in a cohort study and prevalent disease in a cross-sectional study. Clear definitions and steps taken to adhere to them are particularly important for any disease condition of primary interest in the study.\nFor some studies, determinant or predictor may be appropriate terms for exposure variables and outcomes may be called endpoints. In multivariable models, authors sometimes use dependent variable for an outcome and independent variable or explanatory variable for exposure and confounding variables. The latter is not precise as it does not distinguish exposures from confounders.\nIf many variables have been measured and included in exploratory analyses in an early discovery phase, consider providing a list with details on each variable in an appendix, additional table or separate publication. Of note, the International Journal of Epidemiology recently launched a new section with cohort profiles, that includes detailed information on what was measured at different points in time in particular studies [56,57]. Finally, we advise that authors declare all candidate variables considered for statistical analysis, rather than selectively reporting only those included in the final models (see also item 16a) [58,59].\n\n\nExamples\n\nOnly major congenital malformations were included in the analyses. Minor anomalies were excluded according to the exclusion list of European Registration of Congenital Anomalies (EUROCAT). If a child had more than one major congenital malformation of one organ system, those malformations were treated as one outcome in the analyses by organ system (…) In the statistical analyses, factors considered potential confounders were maternal age at delivery and number of previous parities. Factors considered potential effect modifiers were maternal age at reimbursement for antiepileptic medication and maternal age at delivery\n\nBack to top\n\n\n\n\n\n\n8. Data sources / measurement\n\n\n\n\n\n\n\nFor each variable of interest give sources of data and details of methods of assessment (measurement). Describe comparability of assessment methods if there is more than one group\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe way in which exposures, confounders and outcomes were measured affects the reliability and validity of a study. Measurement error and misclassification of exposures or outcomes can make it more difficult to detect cause-effect relationships, or may produce spurious relationships. Error in measurement of potential confounders can increase the risk of residual confounding [62,63]. It is helpful, therefore, if authors report the findings of any studies of the validity or reliability of assessments or measurements, including details of the reference standard that was used. Rather than simply citing validation studies (as in the first example), we advise that authors give the estimated validity or reliability, which can then be used for measurement error adjustment or sensitivity analyses (see items 12e and 17).\nIn addition, it is important to know if groups being compared differed with respect to the way in which the data were collected. This may be important for laboratory examinations (as in the second example) and other situations. For instance, if an interviewer first questions all the cases and then the controls, or vice versa, bias is possible because of the learning curve; solutions such as randomising the order of interviewing may avoid this problem. Information bias may also arise if the compared groups are not given the same diagnostic tests or if one group receives more tests of the same kind than another (see also item 9).\n\n\nExamples\n\nTotal caffeine intake was calculated primarily using US Department of Agriculture food composition sources. In these calculations, it was assumed that the content of caffeine was 137 mg per cup of coffee, 47 mg per cup of tea, 46 mg per can or bottle of cola beverage, and 7 mg per serving of chocolate candy. This method of measuring (caffeine) intake was shown to be valid in both the NHS I cohort and a similar cohort study of male health professionals (…) Self-reported diagnosis of hypertension was found to be reliable in the NHS I cohort\n\n\nSamples pertaining to matched cases and controls were always analyzed together in the same batch and laboratory personnel were unable to distinguish among cases and controls\n\nBack to top\n\n\n\n\n\n\n9. Bias\n\n\n\n\n\n\n\nDescribe any efforts to address potential sources of bias\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nBiased studies produce results that differ systematically from the truth (see also Box 3). It is important for a reader to know what measures were taken during the conduct of a study to reduce the potential of bias. Ideally, investigators carefully consider potential sources of bias when they plan their study. At the stage of reporting, we recommend that authors always assess the likelihood of relevant biases. Specifically, the direction and magnitude of bias should be discussed and, if possible, estimated. For instance, in case-control studies information bias can occur, but may be reduced by selecting an appropriate control group, as in the first example [64]. Differences in the medical surveillance of participants were a problem in the second example [65]. Consequently, the authors provide more detail about the additional data they collected to tackle this problem. When investigators have set up quality control programs for data collection to counter a possible drift in measurements of variables in longitudinal studies, or to keep variability at a minimum when multiple observers are used, these should be described.\nUnfortunately, authors often do not address important biases when reporting their results. Among 43 case-control and cohort studies published from 1990 to 1994 that investigated the risk of second cancers in patients with a history of cancer, medical surveillance bias was mentioned in only 5 articles [66]. A survey of reports of mental health research published during 1998 in three psychiatric journals found that only 13% of 392 articles mentioned response bias [67]. A survey of cohort studies in stroke research found that 14 of 49 (28%) articles published from 1999 to 2003 addressed potential selection bias in the recruitment of study participants and 35 (71%) mentioned the possibility that any type of bias may have affected results [5].\n\n\nExamples\n\nIn most case-control studies of suicide, the control group comprises living individuals but we decided to have a control group of people who had died of other causes (…). With a control group of deceased individuals, the sources of information used to assess risk factors are informants who have recently experienced the death of a family member or close associate - and are therefore more comparable to the sources of information in the suicide group than if living controls were used\n\n\nDetection bias could influence the association between Type 2 diabetes mellitus (T2DM) and primary open-angle glaucoma (POAG) if women with T2DM were under closer ophthalmic surveillance than women without this condition. We compared the mean number of eye examinations reported by women with and without diabetes. We also recalculated the relative risk for POAG with additional control for covariates associated with more careful ocular surveillance (a self-report of cataract, macular degeneration, number of eye examinations, and number of physical examinations)\n\nBack to top\n\n\n\n\n\n\n10. Study size\n\n\n\n\n\n\n\nExplain how the study size was arrived at\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nA study should be large enough to obtain a point estimate with a sufficiently narrow confidence interval to meaningfully answer a research question. Large samples are needed to distinguish a small association from no association. Small studies often provide valuable information, but wide confidence intervals may indicate that they contribute less to current knowledge in comparison with studies providing estimates with narrower confidence intervals. Also, small studies that show interesting or statistically significant associations are published more frequently than small studies that do not have significant findings. While these studies may provide an early signal in the context of discovery, readers should be informed of their potential weaknesses.\nThe importance of sample size determination in observational studies depends on the context. If an analysis is performed on data that were already available for other purposes, the main question is whether the analysis of the data will produce results with sufficient statistical precision to contribute substantially to the literature, and sample size considerations will be informal. Formal, a priori calculation of sample size may be useful when planning a new study [75,76]. Such calculations are associated with more uncertainty than implied by the single number that is generally produced. For example, estimates of the rate of the event of interest or other assumptions central to calculations are commonly imprecise, if not guesswork [77]. The precision obtained in the final analysis can often not be determined beforehand because it will be reduced by inclusion of confounding variables in multivariable analyses [78], the degree of precision with which key variables can be measured [79], and the exclusion of some individuals.\nFew epidemiological studies explain or report deliberations about sample size [4,5]. We encourage investigators to report pertinent formal sample size calculations if they were done. In other situations they should indicate the considerations that determined the study size (e.g., a fixed available sample, as in the first example above). If the observational study was stopped early when statistical significance was achieved, readers should be told. Do not bother readers with post hoc justifications for study size or retrospective power calculations [77]. From the point of view of the reader, confidence intervals indicate the statistical precision that was ultimately obtained. It should be realized that confidence intervals reflect statistical uncertainty only, and not all uncertainty that may be present in a study (see item 20).\n\n\nExamples\n\nThe number of cases in the area during the study period determined the sample size\n\n\nA survey of postnatal depression in the region had documented a prevalence of 19.8%. Assuming depression in mothers with normal weight children to be 20% and an odds ratio of 3 for depression in mothers with a malnourished child we needed 72 case-control sets (one case to one control) with an 80% power and 5% significance\n\nBack to top\n\n\n\n\n\n\n11. Quantitative variables\n\n\n\n\n\n\n\nExplain how quantitative variables were handled in the analyses. If applicable, describe which groupings were chosen, and why\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nInvestigators make choices regarding how to collect and analyse quantitative data about exposures, effect modifiers and confounders. For example, they may group a continuous exposure variable to create a new categorical variable (see Box 4). Grouping choices may have important consequences for later analyses [81,82]. We advise that authors explain why and how they grouped quantitative data, including the number of categories, the cut-points, and category mean or median values. Whenever data are reported in tabular form, the counts of cases, controls, persons at risk, person-time at risk, etc. should be given for each category. Tables should not consist solely of effect-measure estimates or results of model fitting.\nInvestigators might model an exposure as continuous in order to retain all the information. In making this choice, one needs to consider the nature of the relationship of the exposure to the outcome. As it may be wrong to assume a linear relation automatically, possible departures from linearity should be investigated. Authors could mention alternative models they explored during analyses (e.g., using log transformation, quadratic terms or spline functions). Several methods exist for fitting a non-linear relation between the exposure and outcome [82–84]. Also, it may be informative to present both continuous and grouped analyses for a quantitative exposure of prime interest.\nIn a recent survey, two thirds of epidemiological publications studied quantitative exposure variables [4]. In 42 of 50 articles (84%) exposures were grouped into several ordered categories, but often without any stated rationale for the choices made. Fifteen articles used linear associations to model continuous exposure but only two reported checking for linearity. In another survey, of the psychological literature, dichotomization was justified in only 22 of 110 articles (20%) [85].\n\n\nExamples\n\nPatients with a Glasgow Coma Scale less than 8 are considered to be seriously injured. A GCS of 9 or more indicates less serious brain injury. We examined the association of GCS in these two categories with the occurrence of death within 12 months from injury\n\nBack to top\n\n\n\n\n\n\n12a. Statistical methods\n\n\n\n\n\n\n\nDescribe all statistical methods, including those used to control for confounding.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nIn general, there is no one correct statistical analysis but, rather, several possibilities that may address the same question, but make different assumptions. Regardless, investigators should pre-determine analyses at least for the primary study objectives in a study protocol. Often additional analyses are needed, either instead of, or as well as, those originally envisaged, and these may sometimes be motivated by the data. When a study is reported, authors should tell readers whether particular analyses were suggested by data inspection. Even though the distinction between pre-specified and exploratory analyses may sometimes be blurred, authors should clarify reasons for particular analyses.\nIf groups being compared are not similar with regard to some characteristics, adjustment should be made for possible confounding variables by stratification or by multivariable regression (see Box 5) [94]. Often, the study design determines which type of regression analysis is chosen. For instance, Cox proportional hazard regression is commonly used in cohort studies [95]. whereas logistic regression is often the method of choice in case-control studies [96,97]. Analysts should fully describe specific procedures for variable selection and not only present results from the final model [98,99]. If model comparisons are made to narrow down a list of potential confounders for inclusion in a final model, this process should be described. It is helpful to tell readers if one or two covariates are responsible for a great deal of the apparent confounding in a data analysis. Other statistical analyses such as imputation procedures, data transformation, and calculations of attributable risks should also be described. Non-standard or novel approaches should be referenced and the statistical software used reported. As a guiding principle, we advise statistical methods be described with enough detail to enable a knowledgeable reader with access to the original data to verify the reported results [100].\nIn an empirical study, only 93 of 169 articles (55%) reporting adjustment for confounding clearly stated how continuous and multi-category variables were entered into the statistical model [101]. Another study found that among 67 articles in which statistical analyses were adjusted for confounders, it was mostly unclear how confounders were chosen [4].\n\n\nExamples\n\nThe adjusted relative risk was calculated using the Mantel-Haenszel technique, when evaluating if confounding by age or gender was present in the groups compared. The 95% confidence interval (CI) was computed around the adjusted relative risk, using the variance according to Greenland and Robins and Robins et al\n\nBack to top\n\n\n\n\n\n\n12b. Statistical methods\n\n\n\n\n\n\n\nDescribe any methods used to examine subgroups and interactions\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAs discussed in detail under item 17, many debate the use and value of analyses restricted to subgroups of the study population [4,104]. Subgroup analyses are nevertheless often done [4]. Readers need to know which subgroup analyses were planned in advance, and which arose while analysing the data. Also, it is important to explain what methods were used to examine whether effects or associations differed across groups (see item 17).\nInteraction relates to the situation when one factor modifies the effect of another (therefore also called effect modification). The joint action of two factors can be characterized in two ways: on an additive scale, in terms of risk differences; or on a multiplicative scale, in terms of relative risk (see Box 8). Many authors and readers may have their own preference about the way interactions should be analysed. Still, they may be interested to know to what extent the joint effect of exposures differs from the separate effects. There is consensus that the additive scale, which uses absolute risks, is more appropriate for public health and clinical decision making [105]. Whatever view is taken, this should be clearly presented to the reader, as is done in the example above [103]. A lay-out presenting separate effects of both exposures as well as their joint effect, each relative to no exposure, might be most informative. It is presented in the example for interaction under item 17, and the calculations on the different scales are explained in Box 8.\n\n\nExamples\n\nSex differences in susceptibility to the 3 lifestyle-related risk factors studied were explored by testing for biological interaction according to Rothman: a new composite variable with 4 categories (a − b − , a − b+, a+b − , and a+b+) was redefined for sex and a dichotomous exposure of interest where a− and b− denote absence of exposure. RR was calculated for each category after adjustment for age. An interaction effect is defined as departure from additivity of absolute effects, and excess RR caused by interaction (RERI) was calculated:\n\n[equation image]\nwhere RR(a+b+) denotes RR among those exposed to both factors where RR(a − b −) is used as reference category (RR = 1.0). Ninety-five percent CIs were calculated as proposed by Hosmer and Lemeshow. RERI of 0 means no interaction\nBack to top\n\n\n\n\n\n\n12c. Statistical methods\n\n\n\n\n\n\n\nExplain how missing data were addressed.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nMissing data are common in observational research. Questionnaires posted to study participants are not always filled in completely, participants may not attend all follow-up visits and routine data sources and clinical databases are often incomplete. Despite its ubiquity and importance, few papers report in detail on the problem of missing data [5,107]. Investigators may use any of several approaches to address missing data. We describe some strengths and limitations of various approaches in Box 6. We advise that authors report the number of missing values for each variable of interest (exposures, outcomes, confounders) and for each step in the analysis. Authors should give reasons for missing values if possible, and indicate how many individuals were excluded because of missing data when describing the flow of participants through the study (see also item 13). For analyses that account for missing data, authors should describe the nature of the analysis (e.g., multiple imputation) and the assumptions that were made (e.g., missing at random, see Box 6).\n\n\nExamples\n\nOur missing data analysis procedures used missing at random (MAR) assumptions. We used the MICE (multivariate imputation by chained equations) method of multiple multivariate imputation in STATA. We independently analysed 10 copies of the data, each with missing values suitably imputed, in the multivariate logistic regression analyses. We averaged estimates of the variables to give a single mean estimate and adjusted standard errors according to Rubin’s rules\n\nBack to top\n\n\n\n\n\n\n17. 12di. Statistical methods\n\n\n\n\n\n\n\nIf applicable, describe how loss to follow-up was addressed\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nCohort studies are analysed using life table methods or other approaches that are based on the person-time of follow-up and time to developing the disease of interest. Among individuals who remain free of the disease at the end of their observation period, the amount of follow-up time is assumed to be unrelated to the probability of developing the outcome. This will be the case if follow-up ends on a fixed date or at a particular age. Loss to follow-up occurs when participants withdraw from a study before that date. This may hamper the validity of a study if loss to follow-up occurs selectively in exposed individuals, or in persons at high risk of developing the disease (informative censoring). In the example above, patients lost to follow-up in treatment programmes with no active follow-up had fewer CD4 helper cells than those remaining under observation and were therefore at higher risk of dying [116].\nIt is important to distinguish persons who reach the end of the study from those lost to follow-up. Unfortunately, statistical software usually does not distinguish between the two situations: in both cases follow-up time is automatically truncated (censored) at the end of the observation period. Investigators therefore need to decide, ideally at the stage of planning the study, how they will deal with loss to follow-up. When few patients are lost, investigators may either exclude individuals with incomplete follow-up, or treat them as if they withdrew alive at either the date of loss to follow-up or the end of the study. We advise authors to report how many patients were lost to follow-up and what censoring strategies they used.\n\n\nExamples\n\nIn treatment programmes with active follow-up, those lost to follow-up and those followed-up at 1 year had similar baseline CD4 cell counts (median 115 cells per μL and 123 cells per μL), whereas patients lost to follow-up in programmes with no active follow-up procedures had considerably lower CD4 cell counts than those followed-up (median 64 cells per μL and 123 cells per μL). (…) Treatment programmes with passive follow-up were excluded from subsequent analyses\n\nBack to top\n\n\n\n\n\n\n18. 12dii. Statistical methods\n\n\n\n\n\n\n\nCase-control study: If applicable, explain how matching of cases and controls was addressed.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nIn individually matched case-control studies a crude analysis of the odds ratio, ignoring the matching, usually leads to an estimation that is biased towards unity (see Box 2). A matched analysis is therefore often necessary. This can intuitively be understood as a stratified analysis: each case is seen as one stratum with his or her set of matched controls. The analysis rests on considering whether the case is more often exposed than the controls, despite having made them alike regarding the matching variables. Investigators can do such a stratified analysis using the Mantel-Haenszel method on a matched 2 by 2 table. In its simplest form the odds ratio becomes the ratio of pairs that are discordant for the exposure variable. If matching was done for variables like age and sex that are universal attributes, the analysis needs not retain the individual, person-to-person matching: a simple analysis in categories of age and sex is sufficient [50]. For other matching variables, such as neighbourhood, sibship, or friendship, however, each matched set should be considered its own stratum.\nIn individually matched studies, the most widely used method of analysis is conditional logistic regression, in which each case and their controls are considered together. The conditional method is necessary when the number of controls varies among cases, and when, in addition to the matching variables, other variables need to be adjusted for. To allow readers to judge whether the matched design was appropriately taken into account in the analysis, we recommend that authors describe in detail what statistical methods were used to analyse the data. If taking the matching into account does have little effect on the estimates, authors may choose to present an unmatched analysis.\n\n\nExamples\n\nWe used McNemar’s test, paired t test, and conditional logistic regression analysis to compare dementia patients with their matched controls for cardiovascular risk factors, the occurrence of spontaneous cerebral emboli, carotid disease, and venous to arterial circulation shunt\n\nBack to top\n\n\n\n\n\n\n19. 12diii. Statistical methods\n\n\n\n\n\n\n\nCross-sectional study: If applicable, describe analytical methods taking account of sampling strategy.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nMost cross-sectional studies use a pre-specified sampling strategy to select participants from a source population. Sampling may be more complex than taking a simple random sample, however. It may include several stages and clustering of participants (e.g., in districts or villages). Proportionate stratification may ensure that subgroups with a specific characteristic are correctly represented. Disproportionate stratification may be useful to over-sample a subgroup of particular interest.\nAn estimate of association derived from a complex sample may be more or less precise than that derived from a simple random sample. Measures of precision such as standard error or confidence interval should be corrected using the design effect, a ratio measure that describes how much precision is gained or lost if a more complex sampling strategy is used instead of simple random sampling [119]. Most complex sampling techniques lead to a decrease of precision, resulting in a design effect greater than 1.\nWe advise that authors clearly state the method used to adjust for complex sampling strategies so that readers may understand how the chosen sampling method influenced the precision of the obtained estimates. For instance, with clustered sampling, the implicit trade-off between easier data collection and loss of precision is transparent if the design effect is reported. In the example, the calculated design effects of 1.9 for men indicates that the actual sample size would need to be 1.9 times greater than with simple random sampling for the resulting estimates to have equal precision.\n\n\nExamples\n\nThe standard errors (SE) were calculated using the Taylor expansion method to estimate the sampling errors of estimators based on the complex sample design. (…) The overall design effect for diastolic blood pressure was found to be 1.9 for men and 1.8 for women and, for systolic blood pressure, it was 1.9 for men and 2.0 for women\n\nBack to top\n\n\n\n\n\n\n12e. Statistical methods\n\n\n\n\n\n\n\nDescribe any sensitivity analyses\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nSensitivity analyses are useful to investigate whether or not the main results are consistent with those obtained with alternative analysis strategies or assumptions [121]. Issues that may be examined include the criteria for inclusion in analyses, the definitions of exposures or outcomes [122], which confounding variables merit adjustment, the handling of missing data [120,123], possible selection bias or bias from inaccurate or inconsistent measurement of exposure, disease and other variables, and specific analysis choices, such as the treatment of quantitative variables (see item 11). Sophisticated methods are increasingly used to simultaneously model the influence of several biases or assumptions [124–126].\nIn 1959 Cornfield et al. famously showed that a relative risk of 9 for cigarette smoking and lung cancer was extremely unlikely to be due to any conceivable confounder, since the confounder would need to be at least nine times as prevalent in smokers as in non-smokers [127]. This analysis did not rule out the possibility that such a factor was present, but it did identify the prevalence such a factor would need to have. The same approach was recently used to identify plausible confounding factors that could explain the association between childhood leukaemia and living near electric power lines [128]. More generally, sensitivity analyses can be used to identify the degree of confounding, selection bias, or information bias required to distort an association. One important, perhaps under recognised, use of sensitivity analysis is when a study shows little or no association between an exposure and an outcome and it is plausible that confounding or other biases toward the null are present.\n\n\nExamples\n\nBecause we had a relatively higher proportion of missing dead patients with insufficient data (38/148=25.7%) as compared to live patients (15/437=3.4%) (…), it is possible that this might have biased the results. We have, therefore, carried out a sensitivity analysis. We have assumed that the proportion of women using oral contraceptives in the study group applies to the whole (19.1% for dead, and 11.4% for live patients), and then applied two extreme scenarios: either all the exposed missing patients used second generation pills or they all used third-generation pills\n\nBack to top\n\n\n\n\n\n\nResults\n\n\n13a. Participants\n\n\n\n\n\n\n\nReport the numbers of individuals at each stage of the study—e.g., numbers potentially eligible, examined for eligibility, confirmed eligible, included in the study, completing follow-up, and analysed; Consider use of a flow diagram.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nDetailed information on the process of recruiting study participants is important for several reasons. Those included in a study often differ in relevant ways from the target population to which results are applied. This may result in estimates of prevalence or incidence that do not reflect the experience of the target population. For example, people who agreed to participate in a postal survey of sexual behaviour attended church less often, had less conservative sexual attitudes and earlier age at first sexual intercourse, and were more likely to smoke cigarettes and drink alcohol than people who refused [130]. These differences suggest that postal surveys may overestimate sexual liberalism and activity in the population. Such response bias (see Box 3) can distort exposure-disease associations if associations differ between those eligible for the study and those included in the study. As another example, the association between young maternal age and leukaemia in offspring, which has been observed in some case-control studies [131,132], was explained by differential participation of young women in case and control groups. Young women with healthy children were less likely to participate than those with unhealthy children [133]. Although low participation does not necessarily compromise the validity of a study, transparent information on participation and reasons for non-participation is essential. Also, as there are no universally agreed definitions for participation, response or follow-up rates, readers need to understand how authors calculated such proportions [134].\nIdeally, investigators should give an account of the numbers of individuals considered at each stage of recruiting study participants, from the choice of a target population to the inclusion of participants’ data in the analysis. Depending on the type of study, this may include the number of individuals considered to be potentially eligible, the number assessed for eligibility, the number found to be eligible, the number included in the study, the number examined, the number followed up and the number included in the analysis. Information on different sampling units may be required, if sampling of study participants is carried out in two or more stages as in the example above (multistage sampling). In case-control studies, we advise that authors describe the flow of participants separately for case and control groups [135]. Controls can sometimes be selected from several sources, including, for example, hospitalised patients and community dwellers. In this case, we recommend a separate account of the numbers of participants for each type of control group. Olson and colleagues proposed useful reporting guidelines for controls recruited through random-digit dialling and other methods [136].\nA recent survey of epidemiological studies published in 10 general epidemiology, public health and medical journals found that some information regarding participation was provided in 47 of 107 case-control studies (59%), 49 of 154 cohort studies (32%), and 51 of 86 cross-sectional studies (59%) [137]. Incomplete or absent reporting of participation and non-participation in epidemiological studies was also documented in two other surveys of the literature [4,5]. Finally, there is evidence that participation in epidemiological studies may have declined in recent decades [137,138], which underscores the need for transparent reporting [139].\n\n\nExamples\n\nOf the 105 freestanding bars and taverns sampled, 13 establishments were no longer in business and 9 were located in restaurants, leaving 83 eligible businesses. In 22 cases, the owner could not be reached by telephone despite 6 or more attempts. The owners of 36 bars declined study participation. (…) The 25 participating bars and taverns employed 124 bartenders, with 67 bartenders working at least 1 weekly daytime shift. Fifty-four of the daytime bartenders (81%) completed baseline interviews and spirometry; 53 of these subjects (98%) completed follow-up\n\n\nFlow diagram from Hay et al (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2020496/figure/pmed-0040297-g001/)\n\nBack to top\n\n\n\n\n\n\n13b. Participants\n\n\n\n\n\n\n\nGive reasons for non-participation at each stage\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nExplaining the reasons why people no longer participated in a study or why they were excluded from statistical analyses helps readers judge whether the study population was representative of the target population and whether bias was possibly introduced. For example, in a cross-sectional health survey, non-participation due to reasons unlikely to be related to health status (for example, the letter of invitation was not delivered because of an incorrect address) will affect the precision of estimates but will probably not introduce bias. Conversely, if many individuals opt out of the survey because of illness, or perceived good health, results may underestimate or overestimate the prevalence of ill health in the population.\n\n\nExamples\n\nThe main reasons for non-participation were the participant was too ill or had died before interview (cases 30%, controls < 1%), nonresponse (cases 2%, controls 21%), refusal (cases 10%, controls 29%), and other reasons (refusal by consultant or general practitioner, non-English speaking, mental impairment) (cases 7%, controls 5%)\n\nBack to top\n\n\n\n\n\n\n13c. Participants\n\n\n\n\n\n\n\nConsider use of a flow diagram.\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nAn informative and well-structured flow diagram can readily and transparently convey information that might otherwise require a lengthy description [142], as in the example above. The diagram may usefully include the main results, such as the number of events for the primary outcome. While we recommend the use of a flow diagram, particularly for complex observational studies, we do not propose a specific format for the diagram.\n\n\n\n\n\n\n14a.\n\n\n\n\n\n\n\nGive characteristics of study participants (e.g., demographic, clinical, social) and information on exposures and potential confounders. Present the information in a table\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nReaders need descriptions of study participants and their exposures to judge the generalisability of the findings. Information about potential confounders, including whether and how they were measured, influences judgments about study validity. We advise authors to summarize continuous variables for each study group by giving the mean and standard deviation, or when the data have an asymmetrical distribution, as is often the case, the median and percentile range (e.g., 25th and 75th percentiles). Variables that make up a small number of ordered categories (such as stages of disease I to IV) should not be presented as continuous variables; it is preferable to give numbers and proportions for each category (see also Box 4). In studies that compare groups, the descriptive characteristics and numbers should be given by group, as in the example above.\nInferential measures such as standard errors and confidence intervals should not be used to describe the variability of characteristics, and significance tests should be avoided in descriptive tables. Also, P values are not an appropriate criterion for selecting which confounders to adjust for in analysis; even small differences in a confounder that has a strong effect on the outcome can be important [144,145].\nIn cohort studies, it may be useful to document how an exposure relates to other characteristics and potential confounders. Authors could present this information in a table with columns for participants in two or more exposure categories, which permits to judge the differences in confounders between these categories.\nIn case-control studies potential confounders cannot be judged by comparing cases and controls. Control persons represent the source population and will usually be different from the cases in many respects. For example, in a study of oral contraceptives and myocardial infarction, a sample of young women with infarction more often had risk factors for that disease, such as high serum cholesterol, smoking and a positive family history, than the control group [146]. This does not influence the assessment of the effect of oral contraceptives, as long as the prescription of oral contraceptives was not guided by the presence of these risk factors—e.g., because the risk factors were only established after the event (see also Box 5). In case-control studies the equivalent of comparing exposed and non-exposed for the presence of potential confounders (as is done in cohorts) can be achieved by exploring the source population of the cases: if the control group is large enough and represents the source population, exposed and unexposed controls can be compared for potential confounders [121,147].\n\n\nExamples\n\nTable: Characteristics of the Study Base at Enrolment, Castellana G (Italy) (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2020496/table/pmed-0040297-t002/(\n\nBack to top\n\n\n\n\n\n\n14b.\n\n\n\n\n\n\n\nIndicate the number of participants with missing data for each variable of interest.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nAs missing data may bias or affect generalisability of results, authors should tell readers amounts of missing data for exposures, potential confounders, and other important characteristics of patients (see also item 12c and Box 6). In a cohort study, authors should report the extent of loss to follow-up (with reasons), since incomplete follow-up may bias findings (see also items 12d and 13) [148]. We advise authors to use their tables and figures to enumerate amounts of missing data.\n\n\nExamples\n\nSymptom End Points Used in Survival Analysis (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2020496/table/pmed-0040297-t003/)\n\nBack to top\n\n\n\n\n\n\n14c.\n\n\n\n\n\n\n\nCohort study: Summarise follow-up time—e.g., average and total amount\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nReaders need to know the duration and extent of follow-up for the available outcome data. Authors can present a summary of the average follow-up with either the mean or median follow-up time or both. The mean allows a reader to calculate the total number of person-years by multiplying it with the number of study participants. Authors also may present minimum and maximum times or percentiles of the distribution to show readers the spread of follow-up times. They may report total person-years of follow-up or some indication of the proportion of potential data that was captured [148]. All such information may be presented separately for participants in two or more exposure categories. Almost half of 132 articles in cancer journals (mostly cohort studies) did not give any summary of length of follow-up [37].\n\n\nExamples\n\nDuring the 4366 person-years of follow-up (median 5.4, maximum 8.3 years), 265 subjects were diagnosed as having dementia, including 202 with Alzheimer’s disease\n\nBack to top\n\n\n\n\n\n\n15. Outcome data\n\n\n\n\n\n\n\nCohort study: Report numbers of outcome events or summary measures over time.\nCase-control study: Report numbers in each exposure category, or summary measures of exposure.\nCross-sectional study: Report numbers of outcome events or summary measures.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nBefore addressing the possible association between exposures (risk factors) and outcomes, authors should report relevant descriptive data. It may be possible and meaningful to present measures of association in the same table that presents the descriptive data (see item 14a). In a cohort study with events as outcomes, report the numbers of events for each outcome of interest. Consider reporting the event rate per person-year of follow-up. If the risk of an event changes over follow-up time, present the numbers and rates of events in appropriate intervals of follow-up or as a Kaplan-Meier life table or plot. It might be preferable to show plots as cumulative incidence that go up from 0% rather than down from 100%, especially if the event rate is lower than, say, 30% [153]. Consider presenting such information separately for participants in different exposure categories of interest. If a cohort study is investigating other time-related outcomes (e.g., quantitative disease markers such as blood pressure), present appropriate summary measures (e.g., means and standard deviations) over time, perhaps in a table or figure.\nFor cross-sectional studies, we recommend presenting the same type of information on prevalent outcome events or summary measures. For case-control studies, the focus will be on reporting exposures separately for cases and controls as frequencies or quantitative summaries [154]. For all designs, it may be helpful also to tabulate continuous outcomes or exposures in categories, even if the data are not analysed as such.\n\n\nExamples\n\nCohort study: Table: Rates of HIV-1 Seroconversion by Selected Sociodemographic Variables: 1990–1993 (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2020496/table/pmed-0040297-t004/)\n\n\nCase-control study: Table: Exposure among Liver Cirrhosis Cases and Controls (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2020496/table/pmed-0040297-t006/)\n\n\nCross-sectional study: Table: Prevalence of Current Asthma and Diagnosed Hay Fever by Average Alternaria alternata Antigen Level in the Household (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2020496/table/pmed-0040297-t007/)\n\nBack to top\n\n\n\n\n\n\n16a. Main results\n\n\n\n\n\n\n\nGive unadjusted estimates and, if applicable, confounder-adjusted estimates and their precision (e.g., 95% confidence intervals). Make clear which confounders were adjusted for and why they were included\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nIn many situations, authors may present the results of unadjusted or minimally adjusted analyses and those from fully adjusted analyses. We advise giving the unadjusted analyses together with the main data, for example the number of cases and controls that were exposed or not. This allows the reader to understand the data behind the measures of association (see also item 15). For adjusted analyses, report the number of persons in the analysis, as this number may differ because of missing values in covariates (see also item 12c). Estimates should be given with confidence intervals.\nReaders can compare unadjusted measures of association with those adjusted for potential confounders and judge by how much, and in what direction, they changed. Readers may think that adjusted results equal the causal part of the measure of association, but adjusted results are not necessarily free of random sampling error, selection bias, information bias, or residual confounding (see Box 5). Thus, great care should be exercised when interpreting adjusted results, as the validity of results often depends crucially on complete knowledge of important confounders, their precise measurement, and appropriate specification in the statistical model (see also item 20) [157,158].\nAuthors should explain all potential confounders considered, and the criteria for excluding or including variables in statistical models. Decisions about excluding or including variables should be guided by knowledge, or explicit assumptions, on causal relations. Inappropriate decisions may introduce bias, for example by including variables that are in the causal pathway between exposure and disease (unless the aim is to asses how much of the effect is carried by the intermediary variable). If the decision to include a variable in the model was based on the change in the estimate, it is important to report what change was considered sufficiently important to justify its inclusion. If a backward deletion or forward inclusion strategy was used to select confounders, explain that process and give the significance level for rejecting the null hypothesis of no confounding. Of note, we and others do not advise selecting confounders based solely on statistical significance testing [147,159,160].\nRecent studies of the quality of reporting of epidemiological studies found that confidence intervals were reported in most articles [4]. However, few authors explained their choice of confounding variables [4,5].\n\n\nExamples\n\nWe initially considered the following variables as potential confounders by Mantel-Haenszel stratified analysis: (…) The variables we included in the final logistic regression models were those (…) that produced a 10% change in the odds ratio after the Mantel-Haenszel adjustment\n\n\nTable: Relative Rates of Rehospitalisation by Treatment in Patients in Community Care after First Hospitalisation due to Schizophrenia and Schizoaffective Disorder (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2020496/table/pmed-0040297-t008/)\n\nBack to top\n\n\n\n\n\n\n16b. Main results\n\n\n\n\n\n\n\nReport category boundaries when continuous variables were categorised\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nCategorizing continuous data has several important implications for analysis (see Box 4) and also affects the presentation of results. In tables, outcomes should be given for each exposure category, for example as counts of persons at risk, person-time at risk, if relevant separately for each group (e.g., cases and controls). Details of the categories used may aid comparison of studies and meta-analysis. If data were grouped using conventional cut-points, such as body mass index thresholds [162], group boundaries (i.e., range of values) can be derived easily, except for the highest and lowest categories. If quantile-derived categories are used, the category boundaries cannot be inferred from the data. As a minimum, authors should report the category boundaries; it is helpful also to report the range of the data and the mean or median values within categories.\n\n\nExamples\n\nTable: Polychlorinated Biphenyls in Cord Serum (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2020496/table/pmed-0040297-t005/)\n\nBack to top\n\n\n\n\n\n\n16c. Main results\n\n\n\n\n\n\n\nIf relevant, consider translating estimates of relative risk into absolute risk for a meaningful time period\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe results from studies examining the association between an exposure and a disease are commonly reported in relative terms, as ratios of risks, rates or odds (see Box 8). Relative measures capture the strength of the association between an exposure and disease. If the relative risk is a long way from 1 it is less likely that the association is due to confounding [164,165]. Relative effects or associations tend to be more consistent across studies and populations than absolute measures, but what often tends to be the case may be irrelevant in a particular instance. For example, similar relative risks were obtained for the classic cardiovascular risk factors for men living in Northern Ireland, France, the USA and Germany, despite the fact that the underlying risk of coronary heart disease varies substantially between these countries [166,167]. In contrast, in a study of hypertension as a risk factor for cardiovascular disease mortality, the data were more compatible with a constant rate difference than with a constant rate ratio [168].\nWidely used statistical models, including logistic [169] and proportional hazards (Cox) regression [170] are based on ratio measures. In these models, only departures from constancy of ratio effect measures are easily discerned. Nevertheless, measures which assess departures from additivity of risk differences, such as the Relative Excess Risk from Interaction (RERI, see item 12b and Box 8), can be estimated in models based on ratio measures.\nIn many circumstances, the absolute risk associated with an exposure is of greater interest than the relative risk. For example, if the focus is on adverse effects of a drug, one will want to know the number of additional cases per unit time of use (e.g., days, weeks, or years). The example gives the additional number of breast cancer cases per 1000 women who used hormone-replacement therapy for 10 years [163]. Measures such as the attributable risk or population attributable fraction may be useful to gauge how much disease can be prevented if the exposure is eliminated. They should preferably be presented together with a measure of statistical uncertainty (e.g., confidence intervals as in the example). Authors should be aware of the strong assumptions made in this context, including a causal relationship between a risk factor and disease (also see Box 7) [171]. Because of the semantic ambiguity and complexities involved, authors should report in detail what methods were used to calculate attributable risks, ideally giving the formulae used [172].\nA recent survey of abstracts of 222 articles published in leading medical journals found that in 62% of abstracts of randomised trials including a ratio measure absolute risks were given, but only in 21% of abstracts of cohort studies [173]. A free text search of Medline 1966 to 1997 showed that 619 items mentioned attributable risks in the title or abstract, compared to 18,955 using relative risk or odds ratio, for a ratio of 1 to 31 [174].\n\n\nExamples\n\n10 years’ use of HRT [hormone replacement therapy] is estimated to result in five (95% CI 3–7) additional breast cancers per 1000 users of oestrogen-only preparations and 19 (15–23) additional cancers per 1000 users of oestrogen-progestagen combinations\n\nBack to top\n\n\n\n\n\n\n17. Other analyses\n\n\n\n\n\n\n\nReport other analyses done—e.g., analyses of subgroups and interactions, and sensitivity analyses\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nIn addition to the main analysis other analyses are often done in observational studies. They may address specific subgroups, the potential interaction between risk factors, the calculation of attributable risks, or use alternative definitions of study variables in sensitivity analyses.\nThere is debate about the dangers associated with subgroup analyses, and multiplicity of analyses in general [4,104]. In our opinion, there is too great a tendency to look for evidence of subgroup-specific associations, or effect-measure modification, when overall results appear to suggest little or no effect. On the other hand, there is value in exploring whether an overall association appears consistent across several, preferably pre-specified subgroups especially when a study is large enough to have sufficient data in each subgroup. A second area of debate is about interesting subgroups that arose during the data analysis. They might be important findings, but might also arise by chance. Some argue that it is neither possible nor necessary to inform the reader about all subgroup analyses done as future analyses of other data will tell to what extent the early exciting findings stand the test of time [9]. We advise authors to report which analyses were planned, and which were not (see also items 4, 12b and 20). This will allow readers to judge the implications of multiplicity, taking into account the study’s position on the continuum from discovery to verification or refutation.\nA third area of debate is how joint effects and interactions between risk factors should be evaluated: on additive or multiplicative scales, or should the scale be determined by the statistical model that fits best (see also item 12b and Box 8)? A sensible approach is to report the separate effect of each exposure as well as the joint effect—if possible in a table, as in the first example above [183], or in the study by Martinelli et al. [185]. Such a table gives the reader sufficient information to evaluate additive as well as multiplicative interaction (how these calculations are done is shown in Box 8). Confidence intervals for separate and joint effects may help the reader to judge the strength of the data. In addition, confidence intervals around measures of interaction, such as the Relative Excess Risk from Interaction (RERI) relate to tests of interaction or homogeneity tests. One recurrent problem is that authors use comparisons of P-values across subgroups, which lead to erroneous claims about an effect modifier. For instance, a statistically significant association in one category (e.g., men), but not in the other (e.g., women) does not in itself provide evidence of effect modification. Similarly, the confidence intervals for each point estimate are sometimes inappropriately used to infer that there is no interaction when intervals overlap. A more valid inference is achieved by directly evaluating whether the magnitude of an association differs across subgroups.\nSensitivity analyses are helpful to investigate the influence of choices made in the statistical analysis, or to investigate the robustness of the findings to missing data or possible biases (see also item 12b). Judgement is needed regarding the level of reporting of such analyses. If many sensitivity analyses were performed, it may be impractical to present detailed findings for them all. It may sometimes be sufficient to report that sensitivity analyses were carried out and that they were consistent with the main results presented. Detailed presentation is more appropriate if the issue investigated is of major concern, or if effect estimates vary considerably [59,186].\nPocock and colleagues found that 43 out of 73 articles reporting observational studies contained subgroup analyses. The majority claimed differences across groups but only eight articles reported a formal evaluation of interaction (see item 12b) [4].\n\n\nExamples\n\nAnalysis of Oral Contraceptive Use, Presence of Factor V Leiden Allele, and Risk for Venous Thromboembolism (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2020496/table/pmed-0040297-t009/)\n\n\nSensitivity of the Rate Ratio for Cardiovascular Outcome to an Unmeasured Confounder (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2020496/table/pmed-0040297-t010/)\n\nBack to top\n\n\n\n\n\n\nDiscussion\n\n\n18. Key results\n\n\n\n\n\n\n\nSummarise key results with reference to study objectives\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nIt is good practice to begin the discussion with a short summary of the main findings of the study. The short summary reminds readers of the main findings and may help them assess whether the subsequent interpretation and implications offered by the authors are supported by the findings.\n\n\nExamples\n\nWe hypothesized that ethnic minority status would be associated with higher levels of cardiovascular disease (CVD) risk factors, but that the associations would be explained substantially by socioeconomic status (SES). Our hypothesis was not confirmed. After adjustment for age and SES, highly significant differences in body mass index, blood pressure, diabetes, and physical inactivity remained between white women and both black and Mexican American women. In addition, we found large differences in CVD risk factors by SES, a finding that illustrates the high-risk status of both ethnic minority women as well as white women with low SES\n\nBack to top\n\n\n\n\n\n\n19. Limitations\n\n\n\n\n\n\n\nDiscuss limitations of the study, taking into account sources of potential bias or imprecision. Discuss both direction and magnitude of any potential bias.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe identification and discussion of the limitations of a study are an essential part of scientific reporting. It is important not only to identify the sources of bias and confounding that could have affected results, but also to discuss the relative importance of different biases, including the likely direction and magnitude of any potential bias (see also item 9 and Box 3).\nAuthors should also discuss any imprecision of the results. Imprecision may arise in connection with several aspects of a study, including the study size (item 10) and the measurement of exposures, confounders and outcomes (item 8). The inability to precisely measure true values of an exposure tends to result in bias towards unity: the less precisely a risk factor is measured, the greater the bias. This effect has been described as attenuation [201,202], or more recently as regression dilution bias [203]. However, when correlated risk factors are measured with different degrees of imprecision, the adjusted relative risk associated with them can be biased towards or away from unity [204–206].\nWhen discussing limitations, authors may compare the study being presented with other studies in the literature in terms of validity, generalisability and precision. In this approach, each study can be viewed as contribution to the literature, not as a stand-alone basis for inference and action [207]. Surprisingly, the discussion of important limitations of a study is sometimes omitted from published reports. A survey of authors who had published original research articles in The Lancet found that important weaknesses of the study were reported by the investigators in the survey questionnaires, but not in the published article [192].\n\n\nExamples\n\nSince the prevalence of counseling increases with increasing levels of obesity, our estimates may overestimate the true prevalence. Telephone surveys also may overestimate the true prevalence of counseling. Although persons without telephones have similar levels of overweight as persons with telephones, persons without telephones tend to be less educated, a factor associated with lower levels of counseling in our study. Also, of concern is the potential bias caused by those who refused to participate as well as those who refused to respond to questions about weight. Furthermore, because data were collected cross-sectionally, we cannot infer that counseling preceded a patient’s attempt to lose weight\n\nBack to top\n\n\n\n\n\n\n20. Interpretation\n\n\n\n\n\n\n\nGive a cautious overall interpretation considering objectives, limitations, multiplicity of analyses, results from similar studies, and other relevant evidence.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nThe heart of the discussion section is the interpretation of a study’s results. Over-interpretation is common and human: even when we try hard to give an objective assessment, reviewers often rightly point out that we went too far in some respects. When interpreting results, authors should consider the nature of the study on the discovery to verification continuum and potential sources of bias, including loss to follow-up and non-participation (see also items 9, 12 and 19). Due consideration should be given to confounding (item 16a), the results of relevant sensitivity analyses, and to the issue of multiplicity and subgroup analyses (item 17). Authors should also consider residual confounding due to unmeasured variables or imprecise measurement of confounders. For example, socioeconomic status (SES) is associated with many health outcomes and often differs between groups being compared. Variables used to measure SES (income, education, or occupation) are surrogates for other undefined and unmeasured exposures, and the true confounder will by definition be measured with error [208]. Authors should address the real range of uncertainty in estimates, which is larger than the statistical uncertainty reflected in confidence intervals. The latter do not take into account other uncertainties that arise from a study’s design, implementation, and methods of measurement [209].\nTo guide thinking and conclusions about causality, some may find criteria proposed by Bradford Hill in 1965 helpful [164]. How strong is the association with the exposure? Did it precede the onset of disease? Is the association consistently observed in different studies and settings? Is there supporting evidence from experimental studies, including laboratory and animal studies? How specific is the exposure’s putative effect, and is there a dose-response relationship? Is the association biologically plausible? These criteria should not, however, be applied mechanically. For example, some have argued that relative risks below 2 or 3 should be ignored [210,211]. This is a reversal of the point by Cornfield et al. about the strength of large relative risks (see item 12b) [127]. Although a causal effect is more likely with a relative risk of 9, it does not follow that one below 3 is necessarily spurious. For instance, the small increase in the risk of childhood leukaemia after intrauterine irradiation is credible because it concerns an adverse effect of a medical procedure for which no alternative explanations are obvious [212]. Moreover, the carcinogenic effects of radiation are well established. The doubling in the risk of ovarian cancer associated with eating 2 to 4 eggs per week is not immediately credible, since dietary habits are associated with a large number of lifestyle factors as well as SES [213]. In contrast, the credibility of much debated epidemiologic findings of a difference in thrombosis risk between different types of oral contraceptives was greatly enhanced by the differences in coagulation found in a randomised cross-over trial [214]. A discussion of the existing external evidence, from different types of studies, should always be included, but may be particularly important for studies reporting small increases in risk. Further, authors should put their results in context with similar studies and explain how the new study affects the existing body of evidence, ideally by referring to a systematic review.\n\n\nExamples\n\nAny explanation for an association between death from myocardial infarction and use of second generation oral contraceptives must be conjectural. There is no published evidence to suggest a direct biologic mechanism, and there are no other epidemiologic studies with relevant results. (…) The increase in absolute risk is very small and probably applies predominantly to smokers. Due to the lack of corroborative evidence, and because the analysis is based on relatively small numbers, more evidence on the subject is needed. We would not recommend any change in prescribing practice on the strength of these results\n\nBack to top\n\n\n\n\n\n\n21. Generisability\n\n\n\n\n\n\n\nDiscuss the generalisability (external validity) of the study results.\n\n\n\n\n\n\nRead more\n\n\n\n\n\nJump to:\n\nRead More\nExamples\n\n\nRead More\nGeneralisability, also called external validity or applicability, is the extent to which the results of a study can be applied to other circumstances [216]. There is no external validity per se; the term is meaningful only with regard to clearly specified conditions [217]. Can results be applied to an individual, groups or populations that differ from those enrolled in the study with regard to age, sex, ethnicity, severity of disease, and co-morbid conditions? Are the nature and level of exposures comparable, and the definitions of outcomes relevant to another setting or population? Are data that were collected in longitudinal studies many years ago still relevant today? Are results from health services research in one country applicable to health systems in other countries?\nThe question of whether the results of a study have external validity is often a matter of judgment that depends on the study setting, the characteristics of the participants, the exposures examined, and the outcomes assessed. Thus, it is crucial that authors provide readers with adequate information about the setting and locations, eligibility criteria, the exposures and how they were measured, the definition of outcomes, and the period of recruitment and follow-up. The degree of non-participation and the proportion of unexposed participants in whom the outcome develops are also relevant. Knowledge of the absolute risk and prevalence of the exposure, which will often vary across populations, are helpful when applying results to other settings and populations (see Box 7).\n\n\nExamples\n\nHow applicable are our estimates to other HIV-1-infected patients? This is an important question because the accuracy of prognostic models tends to be lower when applied to data other than those used to develop them. We addressed this issue by penalising model complexity, and by choosing models that generalised best to cohorts omitted from the estimation procedure. Our database included patients from many countries from Europe and North America, who were treated in different settings. The range of patients was broad: men and women, from teenagers to elderly people were included, and the major exposure categories were well represented. The severity of immunodeficiency at baseline ranged from not measureable to very severe, and viral load from undetectable to extremely high\n\nBack to top\n\n\n\n\n\n\nOther information\n\n\n22. Funding\n\n\n\n\n\n\n\nGive the source of funding and the role of the funders for the present study and, if applicable, for the original study on which the present article is based\n\n\n\n\n\n\nRead more\n\n\n\n\n\n\nRead More\nSome journals require authors to disclose the presence or absence of financial and other conflicts of interest [100,218]. Several investigations show strong associations between the source of funding and the conclusions of research articles [219–222]. The conclusions in randomised trials recommended the experimental drug as the drug of choice much more often (odds ratio 5.3) if the trial was funded by for-profit organisations, even after adjustment for the effect size [223]. Other studies document the influence of the tobacco and telecommunication industries on the research they funded [224–227]. There are also examples of undue influence when the sponsor is governmental or a non-profit organisation.\nAuthors or funders may have conflicts of interest that influence any of the following: the design of the study [228]; choice of exposures [228,229], outcomes [230], statistical methods [231], and selective publication of outcomes [230] and studies [232]. Consequently, the role of the funders should be described in detail: in what part of the study they took direct responsibility (e.g., design, data collection, analysis, drafting of manuscript, decision to publish) [100]. Other sources of undue influence include employers (e.g., university administrators for academic researchers and government supervisors, especially political appointees, for government researchers), advisory committees, litigants, and special interest groups."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-1a_title-1",
    "href": "guidelines/strobe/index.html#sec-1a_title-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nReaders should be able to easily identify the design that was used from the title or abstract. An explicit, commonly used term for the study design also helps ensure correct indexing of articles in electronic databases."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-1a_title-2",
    "href": "guidelines/strobe/index.html#sec-1a_title-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\nLeukaemia incidence among workers in the shoe and boot manufacturing industry: a case-control study\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-1b_abstract-1",
    "href": "guidelines/strobe/index.html#sec-1b_abstract-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nThe abstract provides key information that enables readers to understand a study and decide whether to read the article. Typical components include a statement of the research question, a short description of methods and results, and a conclusion [22]. Abstracts should summarize key details of studies and should only present information that is provided in the article. We advise presenting key results in a numerical form that includes numbers of participants, estimates of associations and appropriate measures of variability and uncertainty (e.g., odds ratios with confidence intervals). We regard it insufficient to state only that an exposure is or is not significantly associated with an outcome."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-1b_abstract-2",
    "href": "guidelines/strobe/index.html#sec-1b_abstract-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\nBackground: The expected survival of HIV-infected patients is of major public health interest.\n\n\nObjective: To estimate survival time and age-specific mortality rates of an HIV-infected population compared with that of the general population.\n\n\nDesign: Population-based cohort study.\n\n\nSetting: All HIV-infected persons receiving care in Denmark from 1995 to 2005.\n\n\nPatients: Each member of the nationwide Danish HIV Cohort Study was matched with as many as 99 persons from the general population according to sex, date of birth, and municipality of residence.\n\n\nMeasurements: The authors computed Kaplan-Meier life tables with age as the time scale to estimate survival from age 25 years. Patients with HIV infection and corresponding persons from the general population were observed from the date of the patient’s HIV diagnosis until death, emigration, or 1 May 2005.\n\n\nResults: 3990 HIV-infected patients and 379,872 persons from the general population were included in the study, yielding 22,744 (median, 5.8 y/person) and 2,689,287 (median, 8.4 years/person) person-years of observation. Three percent of participants were lost to follow-up. From age 25 years, the median survival was 19.9 years (95% CI, 18.5 to 21.3) among patients with HIV infection and 51.1 years (CI, 50.9 to 51.5) among the general population. For HIV-infected patients, survival increased to 32.5 years (CI, 29.4 to 34.7) during the 2000 to 2005 period. In the subgroup that excluded persons with known hepatitis C coinfection (16%), median survival was 38.9 years (CI, 35.4 to 40.1) during this same period. The relative mortality rates for patients with HIV infection compared with those for the general population decreased with increasing age, whereas the excess mortality rate increased with increasing age.\n\n\nLimitations: The observed mortality rates are assumed to apply beyond the current maximum observation time of 10 years.\n\n\nConclusions: The estimated median survival is more than 35 years for a young person diagnosed with HIV infection in the late highly active antiretroviral therapy era. However, an ongoing effort is still needed to further reduce mortality rates for these persons compared with the general population\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-2_background__rationale-1",
    "href": "guidelines/strobe/index.html#sec-2_background__rationale-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nThe scientific background of the study provides important context for readers. It sets the stage for the study and describes its focus. It gives an overview of what is known on a topic and what gaps in current knowledge are addressed by the study. Background material should note recent pertinent studies and any systematic reviews of pertinent studies."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-2_background__rationale-2",
    "href": "guidelines/strobe/index.html#sec-2_background__rationale-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\nConcerns about the rising prevalence of obesity in children and adolescents have focused on the well documented associations between childhood obesity and increased cardiovascular risk1 and mortality in adulthood.2 Childhood obesity has considerable social and psychological consequences within childhood and adolescence,3 yet little is known about social, socioeconomic, and psychological consequences in adult life.\n\nA recent systematic review found no longitudinal studies on the outcomes of childhood obesity other than physical health outcomes3 and only two longitudinal studies of the socioeconomic effects of obesity in adolescence. Gortmaker et al found that US women who had been obese in late adolescence in 1981 were less likely to be married and had lower incomes seven years later than women who had not been overweight, while men who had been overweight were less likely to be married.4 Sargent et al found that UK women, but not men, who had been obese at 16 years in 1974 earned 7.4% less than their non-obese peers at age 23.5\nThe study of adult outcomes of childhood obesity is difficult because obesity often continues into adult life and therefore poorer socioeconomic and educational outcomes may actually reflect confounding by adult obesity. Yet identifying outcomes related to obesity confined childhood is important in determining whether people who are obese in childhood and who later lose weight remain at risk for adult adversity and inequalities.\nWe used longitudinal data from the 1970 British birth cohort to examine the adult socioeconomic, educational, social, and psychological outcomes of childhood obesity. We hypothesised that obesity limited to childhood has fewer adverse adult outcomes than obesity that persists into adult life\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-3_objectives-1",
    "href": "guidelines/strobe/index.html#sec-3_objectives-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nObjectives are the detailed aims of the study. Well crafted objectives specify populations, exposures and outcomes, and parameters that will be estimated. They may be formulated as specific hypotheses or as questions that the study was designed to address. In some situations objectives may be less specific, for example, in early discovery phases. Regardless, the report should clearly reflect the investigators’ intentions. For example, if important subgroups or additional analyses were not the original aim of the study but arose during data analysis, they should be described accordingly (see also items 4, 17 and 20)."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-3_objectives-2",
    "href": "guidelines/strobe/index.html#sec-3_objectives-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\nOur primary objectives were to 1) determine the prevalence of domestic violence among female patients presenting to four community-based, primary care, adult medicine practices that serve patients of diverse socioeconomic background and 2) identify demographic and clinical differences between currently abused patients and patients not currently being abused\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-4_study_design-1",
    "href": "guidelines/strobe/index.html#sec-4_study_design-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nWe advise presenting key elements of study design early in the methods section (or at the end of the introduction) so that readers can understand the basics of the study. For example, authors should indicate that the study was a cohort study, which followed people over a particular time period, and describe the group of persons that comprised the cohort and their exposure status. Similarly, if the investigation used a case-control design, the cases and controls and their source population should be described. If the study was a cross-sectional survey, the population and the point in time at which the cross-section was taken should be mentioned. When a study is a variant of the three main study types, there is an additional need for clarity. For instance, for a case-crossover study, one of the variants of the case-control design, a succinct description of the principles was given in the example above [28].\nWe recommend that authors refrain from simply calling a study prospective or retrospective because these terms are ill defined [29]. One usage sees cohort and prospective as synonymous and reserves the word retrospective for case-control studies [30]. A second usage distinguishes prospective and retrospective cohort studies according to the timing of data collection relative to when the idea for the study was developed [31]. A third usage distinguishes prospective and retrospective case-control studies depending on whether the data about the exposure of interest existed when cases were selected [32]. Some advise against using these terms [33], or adopting the alternatives concurrent and historical for describing cohort studies [34]. In STROBE, we do not use the words prospective and retrospective, nor alternatives such as concurrent and historical. We recommend that, whenever authors use these words, they define what they mean. Most importantly, we recommend that authors describe exactly how and when data collection took place.\nThe first part of the methods section might also be the place to mention whether the report is one of several from a study. If a new report is in line with the original aims of the study, this is usually indicated by referring to an earlier publication and by briefly restating the salient features of the study. However, the aims of a study may also evolve over time. Researchers often use data for purposes for which they were not originally intended, including, for example, official vital statistics that were collected primarily for administrative purposes, items in questionnaires that originally were only included for completeness, or blood samples that were collected for another purpose. For example, the Physicians’ Health Study, a randomized controlled trial of aspirin and carotene, was later used to demonstrate that a point mutation in the factor V gene was associated with an increased risk of venous thrombosis, but not of myocardial infarction or stroke [35]. The secondary use of existing data is a creative part of observational research and does not necessarily make results less credible or less important. However, briefly restating the original aims might help readers understand the context of the research and possible limitations in the data."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-4_study_design-2",
    "href": "guidelines/strobe/index.html#sec-4_study_design-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\nWe used a case-crossover design, a variation of a case-control design that is appropriate when a brief exposure (driver’s phone use) causes a transient rise in the risk of a rare outcome (a crash). We compared a driver’s use of a mobile phone at the estimated time of a crash with the same driver’s use during another suitable time period. Because drivers are their own controls, the design controls for characteristics of the driver that may affect the risk of a crash but do not change over a short period of time. As it is important that risks during control periods and crash trips are similar, we compared phone activity during the hazard interval (time immediately before the crash) with phone activity during control intervals (equivalent times during which participants were driving but did not crash) in the previous week\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-5_setting-1",
    "href": "guidelines/strobe/index.html#sec-5_setting-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nReaders need information on setting and locations to assess the context and generalisability of a study’s results. Exposures such as environmental factors and therapies can change over time. Also, study methods may evolve over time. Knowing when a study took place and over what period participants were recruited and followed up places the study in historical context and is important for the interpretation of results.\nInformation about setting includes recruitment sites or sources (e.g., electoral roll, outpatient clinic, cancer registry, or tertiary care centre). Information about location may refer to the countries, towns, hospitals or practices where the investigation took place. We advise stating dates rather than only describing the length of time periods. There may be different sets of dates for exposure, disease occurrence, recruitment, beginning and end of follow-up, and data collection. Of note, nearly 80% of 132 reports in oncology journals that used survival analysis included the starting and ending dates for accrual of patients, but only 24% also reported the date on which follow-up ended [37]."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-5_setting-2",
    "href": "guidelines/strobe/index.html#sec-5_setting-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\nThe Pasitos Cohort Study recruited pregnant women from Women, Infant and Child (WIC) clinics in Socorro and San Elizario, El Paso County, Texas and maternal-child clinics of the Mexican Social Security Institute (IMSS) in Ciudad Juarez, Mexico from April 1998 to October 2000. At baseline, prior to the birth of the enrolled cohort children, staff interviewed mothers regarding the household environment. In this ongoing cohort study, we target follow-up exams at 6-month intervals beginning at age 6 months\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-6a_eligibility_criteria-1",
    "href": "guidelines/strobe/index.html#sec-6a_eligibility_criteria-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nDetailed descriptions of the study participants help readers understand the applicability of the results. Investigators usually restrict a study population by defining clinical, demographic and other characteristics of eligible participants. Typical eligibility criteria relate to age, gender, diagnosis and comorbid conditions. Despite their importance, eligibility criteria often are not reported adequately. In a survey of observational stroke research, 17 of 49 reports (35%) did not specify eligibility criteria [5].\nEligibility criteria may be presented as inclusion and exclusion criteria, although this distinction is not always necessary or useful. Regardless, we advise authors to report all eligibility criteria and also to describe the group from which the study population was selected (e.g., the general population of a region or country), and the method of recruitment (e.g., referral or self-selection through advertisements).\nKnowing details about follow-up procedures, including whether procedures minimized non-response and loss to follow-up and whether the procedures were similar for all participants, informs judgments about the validity of results. For example, in a study that used IgM antibodies to detect acute infections, readers needed to know the interval between blood tests for IgM antibodies so that they could judge whether some infections likely were missed because the interval between blood tests was too long [41]. In other studies where follow-up procedures differed between exposed and unexposed groups, readers might recognize substantial bias due to unequal ascertainment of events or differences in non-response or loss to follow-up [42]. Accordingly, we advise that researchers describe the methods used for following participants and whether those methods were the same for all participants, and that they describe the completeness of ascertainment of variables (see also item 14).\nIn case-control studies, the choice of cases and controls is crucial to interpreting the results, and the method of their selection has major implications for study validity. In general, controls should reflect the population from which the cases arose. Various methods are used to sample controls, all with advantages and disadvantages: for cases that arise from a general population, population roster sampling, random digit dialling, neighbourhood or friend controls are used. Neighbourhood or friend controls may present intrinsic matching on exposure [17]. Controls with other diseases may have advantages over population-based controls, in particular for hospital-based cases, because they better reflect the catchment population of a hospital, have greater comparability of recall and ease of recruitment. However, they can present problems if the exposure of interest affects the risk of developing or being hospitalized for the control condition(s) [43,44]. To remedy this problem often a mixture of the best defensible control diseases is used [45]."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-6a_eligibility_criteria-2",
    "href": "guidelines/strobe/index.html#sec-6a_eligibility_criteria-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\nCohort study: Participants in the Iowa Women’s Health Study were a random sample of all women ages 55 to 69 years derived from the state of Iowa automobile driver’s license list in 1985, which represented approximately 94% of Iowa women in that age group. (…) Follow-up questionnaires were mailed in October 1987 and August 1989 to assess vital status and address changes. (…) Incident cancers, except for nonmelanoma skin cancers, were ascertained by the State Health Registry of Iowa (…). The Iowa Women’s Health Study cohort was matched to the registry with combinations of first, last, and maiden names, zip code, birthdate, and social security number.\n\n\nCase-control study: Cutaneous melanoma cases diagnosed in 1999 and 2000 were ascertained through the Iowa Cancer Registry (…). Controls, also identified through the Iowa Cancer Registry, were colorectal cancer patients diagnosed during the same time. Colorectal cancer controls were selected because they are common and have a relatively long survival, and because arsenic exposure has not been conclusively linked to the incidence of colorectal cancer.\n\n\n“Cross-sectional study:** We retrospectively identified patients with a principal diagnosis of myocardial infarction (code 410) according to the International Classification of Diseases, 9th Revision, Clinical Modification, from codes designating discharge diagnoses, excluding the codes with a fifth digit of 2, which designates a subsequent episode of care (…) A random sample of the entire Medicare cohort with myocardial infarction from February 1994 to July 1995 was selected (…) To be eligible, patients had to present to the hospital after at least 30 minutes but less than 12 hours of chest pain and had to have ST-segment elevation of at least 1 mm on two contiguous leads on the initial electrocardiogram.\n\n\n\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-6b_matching_criteria-1",
    "href": "guidelines/strobe/index.html#sec-6b_matching_criteria-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nMatching is much more common in case-control studies, but occasionally, investigators use matching in cohort studies to make groups comparable at the start of follow-up. Matching in cohort studies makes groups directly comparable for potential confounders and presents fewer intricacies than with case-control studies. For example, it is not necessary to take the matching into account for the estimation of the relative risk [48]. Because matching in cohort studies may increase statistical precision investigators might allow for the matching in their analyses and thus obtain narrower confidence intervals.\nIn case-control studies matching is done to increase a study’s efficiency by ensuring similarity in the distribution of variables between cases and controls, in particular the distribution of potential confounding variables [48,49]. Because matching can be done in various ways, with one or more controls per case, the rationale for the choice of matching variables and the details of the method used should be described. Commonly used forms of matching are frequency matching (also called group matching) and individual matching. In frequency matching, investigators choose controls so that the distribution of matching variables becomes identical or similar to that of cases. Individual matching involves matching one or several controls to each case. Although intuitively appealing and sometimes useful, matching in case-control studies has a number of disadvantages, is not always appropriate, and needs to be taken into account in the analysis (see Box 2).\nEven apparently simple matching procedures may be poorly reported. For example, authors may state that controls were matched to cases within five years, or using five year age bands. Does this mean that, if a case was 54 years old, the respective control needed to be in the five-year age band 50 to 54, or aged 49 to 59, which is within five years of age 54? If a wide (e.g., 10-year) age band is chosen, there is a danger of residual confounding by age (see also Box 4), for example because controls may then be younger than cases on average."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-6b_matching_criteria-2",
    "href": "guidelines/strobe/index.html#sec-6b_matching_criteria-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\nCohort study: For each patient who initially received a statin, we used propensity-based matching to identify one control who did not receive a statin according to the following protocol. First, propensity scores were calculated for each patient in the entire cohort on the basis of an extensive list of factors potentially related to the use of statins or the risk of sepsis. Second, each statin user was matched to a smaller pool of non-statin-users by sex, age (plus or minus 1 year), and index date (plus or minus 3 months). Third, we selected the control with the closest propensity score (within 0.2 SD) to each statin user in a 1:1 fashion and discarded the remaining controls.\n\n\nCase-control study: We aimed to select five controls for every case from among individuals in the study population who had no diagnosis of autism or other pervasive developmental disorders (PDD) recorded in their general practice record and who were alive and registered with a participating practice on the date of the PDD diagnosis in the case. Controls were individually matched to cases by year of birth (up to 1 year older or younger), sex, and general practice. For each of 300 cases, five controls could be identified who met all the matching criteria. For the remaining 994, one or more controls was excluded…\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-7_variables-1",
    "href": "guidelines/strobe/index.html#sec-7_variables-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nAuthors should define all variables considered for and included in the analysis, including outcomes, exposures, predictors, potential confounders and potential effect modifiers. Disease outcomes require adequately detailed description of the diagnostic criteria. This applies to criteria for cases in a case-control study, disease events during follow-up in a cohort study and prevalent disease in a cross-sectional study. Clear definitions and steps taken to adhere to them are particularly important for any disease condition of primary interest in the study.\nFor some studies, determinant or predictor may be appropriate terms for exposure variables and outcomes may be called endpoints. In multivariable models, authors sometimes use dependent variable for an outcome and independent variable or explanatory variable for exposure and confounding variables. The latter is not precise as it does not distinguish exposures from confounders.\nIf many variables have been measured and included in exploratory analyses in an early discovery phase, consider providing a list with details on each variable in an appendix, additional table or separate publication. Of note, the International Journal of Epidemiology recently launched a new section with cohort profiles, that includes detailed information on what was measured at different points in time in particular studies [56,57]. Finally, we advise that authors declare all candidate variables considered for statistical analysis, rather than selectively reporting only those included in the final models (see also item 16a) [58,59]."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-7_variables-2",
    "href": "guidelines/strobe/index.html#sec-7_variables-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\nOnly major congenital malformations were included in the analyses. Minor anomalies were excluded according to the exclusion list of European Registration of Congenital Anomalies (EUROCAT). If a child had more than one major congenital malformation of one organ system, those malformations were treated as one outcome in the analyses by organ system (…) In the statistical analyses, factors considered potential confounders were maternal age at delivery and number of previous parities. Factors considered potential effect modifiers were maternal age at reimbursement for antiepileptic medication and maternal age at delivery\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-8_data_sources__measurement-1",
    "href": "guidelines/strobe/index.html#sec-8_data_sources__measurement-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nThe way in which exposures, confounders and outcomes were measured affects the reliability and validity of a study. Measurement error and misclassification of exposures or outcomes can make it more difficult to detect cause-effect relationships, or may produce spurious relationships. Error in measurement of potential confounders can increase the risk of residual confounding [62,63]. It is helpful, therefore, if authors report the findings of any studies of the validity or reliability of assessments or measurements, including details of the reference standard that was used. Rather than simply citing validation studies (as in the first example), we advise that authors give the estimated validity or reliability, which can then be used for measurement error adjustment or sensitivity analyses (see items 12e and 17).\nIn addition, it is important to know if groups being compared differed with respect to the way in which the data were collected. This may be important for laboratory examinations (as in the second example) and other situations. For instance, if an interviewer first questions all the cases and then the controls, or vice versa, bias is possible because of the learning curve; solutions such as randomising the order of interviewing may avoid this problem. Information bias may also arise if the compared groups are not given the same diagnostic tests or if one group receives more tests of the same kind than another (see also item 9)."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-8_data_sources__measurement-2",
    "href": "guidelines/strobe/index.html#sec-8_data_sources__measurement-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\nTotal caffeine intake was calculated primarily using US Department of Agriculture food composition sources. In these calculations, it was assumed that the content of caffeine was 137 mg per cup of coffee, 47 mg per cup of tea, 46 mg per can or bottle of cola beverage, and 7 mg per serving of chocolate candy. This method of measuring (caffeine) intake was shown to be valid in both the NHS I cohort and a similar cohort study of male health professionals (…) Self-reported diagnosis of hypertension was found to be reliable in the NHS I cohort\n\n\nSamples pertaining to matched cases and controls were always analyzed together in the same batch and laboratory personnel were unable to distinguish among cases and controls\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-9_bias-1",
    "href": "guidelines/strobe/index.html#sec-9_bias-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nBiased studies produce results that differ systematically from the truth (see also Box 3). It is important for a reader to know what measures were taken during the conduct of a study to reduce the potential of bias. Ideally, investigators carefully consider potential sources of bias when they plan their study. At the stage of reporting, we recommend that authors always assess the likelihood of relevant biases. Specifically, the direction and magnitude of bias should be discussed and, if possible, estimated. For instance, in case-control studies information bias can occur, but may be reduced by selecting an appropriate control group, as in the first example [64]. Differences in the medical surveillance of participants were a problem in the second example [65]. Consequently, the authors provide more detail about the additional data they collected to tackle this problem. When investigators have set up quality control programs for data collection to counter a possible drift in measurements of variables in longitudinal studies, or to keep variability at a minimum when multiple observers are used, these should be described.\nUnfortunately, authors often do not address important biases when reporting their results. Among 43 case-control and cohort studies published from 1990 to 1994 that investigated the risk of second cancers in patients with a history of cancer, medical surveillance bias was mentioned in only 5 articles [66]. A survey of reports of mental health research published during 1998 in three psychiatric journals found that only 13% of 392 articles mentioned response bias [67]. A survey of cohort studies in stroke research found that 14 of 49 (28%) articles published from 1999 to 2003 addressed potential selection bias in the recruitment of study participants and 35 (71%) mentioned the possibility that any type of bias may have affected results [5]."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-9_bias-2",
    "href": "guidelines/strobe/index.html#sec-9_bias-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\nIn most case-control studies of suicide, the control group comprises living individuals but we decided to have a control group of people who had died of other causes (…). With a control group of deceased individuals, the sources of information used to assess risk factors are informants who have recently experienced the death of a family member or close associate - and are therefore more comparable to the sources of information in the suicide group than if living controls were used\n\n\nDetection bias could influence the association between Type 2 diabetes mellitus (T2DM) and primary open-angle glaucoma (POAG) if women with T2DM were under closer ophthalmic surveillance than women without this condition. We compared the mean number of eye examinations reported by women with and without diabetes. We also recalculated the relative risk for POAG with additional control for covariates associated with more careful ocular surveillance (a self-report of cataract, macular degeneration, number of eye examinations, and number of physical examinations)\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-10_study_size-1",
    "href": "guidelines/strobe/index.html#sec-10_study_size-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nA study should be large enough to obtain a point estimate with a sufficiently narrow confidence interval to meaningfully answer a research question. Large samples are needed to distinguish a small association from no association. Small studies often provide valuable information, but wide confidence intervals may indicate that they contribute less to current knowledge in comparison with studies providing estimates with narrower confidence intervals. Also, small studies that show interesting or statistically significant associations are published more frequently than small studies that do not have significant findings. While these studies may provide an early signal in the context of discovery, readers should be informed of their potential weaknesses.\nThe importance of sample size determination in observational studies depends on the context. If an analysis is performed on data that were already available for other purposes, the main question is whether the analysis of the data will produce results with sufficient statistical precision to contribute substantially to the literature, and sample size considerations will be informal. Formal, a priori calculation of sample size may be useful when planning a new study [75,76]. Such calculations are associated with more uncertainty than implied by the single number that is generally produced. For example, estimates of the rate of the event of interest or other assumptions central to calculations are commonly imprecise, if not guesswork [77]. The precision obtained in the final analysis can often not be determined beforehand because it will be reduced by inclusion of confounding variables in multivariable analyses [78], the degree of precision with which key variables can be measured [79], and the exclusion of some individuals.\nFew epidemiological studies explain or report deliberations about sample size [4,5]. We encourage investigators to report pertinent formal sample size calculations if they were done. In other situations they should indicate the considerations that determined the study size (e.g., a fixed available sample, as in the first example above). If the observational study was stopped early when statistical significance was achieved, readers should be told. Do not bother readers with post hoc justifications for study size or retrospective power calculations [77]. From the point of view of the reader, confidence intervals indicate the statistical precision that was ultimately obtained. It should be realized that confidence intervals reflect statistical uncertainty only, and not all uncertainty that may be present in a study (see item 20)."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-10_study_size-2",
    "href": "guidelines/strobe/index.html#sec-10_study_size-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\nThe number of cases in the area during the study period determined the sample size\n\n\nA survey of postnatal depression in the region had documented a prevalence of 19.8%. Assuming depression in mothers with normal weight children to be 20% and an odds ratio of 3 for depression in mothers with a malnourished child we needed 72 case-control sets (one case to one control) with an 80% power and 5% significance\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-11_quantitative_variables-1",
    "href": "guidelines/strobe/index.html#sec-11_quantitative_variables-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nInvestigators make choices regarding how to collect and analyse quantitative data about exposures, effect modifiers and confounders. For example, they may group a continuous exposure variable to create a new categorical variable (see Box 4). Grouping choices may have important consequences for later analyses [81,82]. We advise that authors explain why and how they grouped quantitative data, including the number of categories, the cut-points, and category mean or median values. Whenever data are reported in tabular form, the counts of cases, controls, persons at risk, person-time at risk, etc. should be given for each category. Tables should not consist solely of effect-measure estimates or results of model fitting.\nInvestigators might model an exposure as continuous in order to retain all the information. In making this choice, one needs to consider the nature of the relationship of the exposure to the outcome. As it may be wrong to assume a linear relation automatically, possible departures from linearity should be investigated. Authors could mention alternative models they explored during analyses (e.g., using log transformation, quadratic terms or spline functions). Several methods exist for fitting a non-linear relation between the exposure and outcome [82–84]. Also, it may be informative to present both continuous and grouped analyses for a quantitative exposure of prime interest.\nIn a recent survey, two thirds of epidemiological publications studied quantitative exposure variables [4]. In 42 of 50 articles (84%) exposures were grouped into several ordered categories, but often without any stated rationale for the choices made. Fifteen articles used linear associations to model continuous exposure but only two reported checking for linearity. In another survey, of the psychological literature, dichotomization was justified in only 22 of 110 articles (20%) [85]."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-11_quantitative_variables-2",
    "href": "guidelines/strobe/index.html#sec-11_quantitative_variables-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\nPatients with a Glasgow Coma Scale less than 8 are considered to be seriously injured. A GCS of 9 or more indicates less serious brain injury. We examined the association of GCS in these two categories with the occurrence of death within 12 months from injury\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-12a_statistical_methods-1",
    "href": "guidelines/strobe/index.html#sec-12a_statistical_methods-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nIn general, there is no one correct statistical analysis but, rather, several possibilities that may address the same question, but make different assumptions. Regardless, investigators should pre-determine analyses at least for the primary study objectives in a study protocol. Often additional analyses are needed, either instead of, or as well as, those originally envisaged, and these may sometimes be motivated by the data. When a study is reported, authors should tell readers whether particular analyses were suggested by data inspection. Even though the distinction between pre-specified and exploratory analyses may sometimes be blurred, authors should clarify reasons for particular analyses.\nIf groups being compared are not similar with regard to some characteristics, adjustment should be made for possible confounding variables by stratification or by multivariable regression (see Box 5) [94]. Often, the study design determines which type of regression analysis is chosen. For instance, Cox proportional hazard regression is commonly used in cohort studies [95]. whereas logistic regression is often the method of choice in case-control studies [96,97]. Analysts should fully describe specific procedures for variable selection and not only present results from the final model [98,99]. If model comparisons are made to narrow down a list of potential confounders for inclusion in a final model, this process should be described. It is helpful to tell readers if one or two covariates are responsible for a great deal of the apparent confounding in a data analysis. Other statistical analyses such as imputation procedures, data transformation, and calculations of attributable risks should also be described. Non-standard or novel approaches should be referenced and the statistical software used reported. As a guiding principle, we advise statistical methods be described with enough detail to enable a knowledgeable reader with access to the original data to verify the reported results [100].\nIn an empirical study, only 93 of 169 articles (55%) reporting adjustment for confounding clearly stated how continuous and multi-category variables were entered into the statistical model [101]. Another study found that among 67 articles in which statistical analyses were adjusted for confounders, it was mostly unclear how confounders were chosen [4]."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-12a_statistical_methods-2",
    "href": "guidelines/strobe/index.html#sec-12a_statistical_methods-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\nThe adjusted relative risk was calculated using the Mantel-Haenszel technique, when evaluating if confounding by age or gender was present in the groups compared. The 95% confidence interval (CI) was computed around the adjusted relative risk, using the variance according to Greenland and Robins and Robins et al\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-12b_statistical_methods-1",
    "href": "guidelines/strobe/index.html#sec-12b_statistical_methods-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nAs discussed in detail under item 17, many debate the use and value of analyses restricted to subgroups of the study population [4,104]. Subgroup analyses are nevertheless often done [4]. Readers need to know which subgroup analyses were planned in advance, and which arose while analysing the data. Also, it is important to explain what methods were used to examine whether effects or associations differed across groups (see item 17).\nInteraction relates to the situation when one factor modifies the effect of another (therefore also called effect modification). The joint action of two factors can be characterized in two ways: on an additive scale, in terms of risk differences; or on a multiplicative scale, in terms of relative risk (see Box 8). Many authors and readers may have their own preference about the way interactions should be analysed. Still, they may be interested to know to what extent the joint effect of exposures differs from the separate effects. There is consensus that the additive scale, which uses absolute risks, is more appropriate for public health and clinical decision making [105]. Whatever view is taken, this should be clearly presented to the reader, as is done in the example above [103]. A lay-out presenting separate effects of both exposures as well as their joint effect, each relative to no exposure, might be most informative. It is presented in the example for interaction under item 17, and the calculations on the different scales are explained in Box 8."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-12b_statistical_methods-2",
    "href": "guidelines/strobe/index.html#sec-12b_statistical_methods-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\nSex differences in susceptibility to the 3 lifestyle-related risk factors studied were explored by testing for biological interaction according to Rothman: a new composite variable with 4 categories (a − b − , a − b+, a+b − , and a+b+) was redefined for sex and a dichotomous exposure of interest where a− and b− denote absence of exposure. RR was calculated for each category after adjustment for age. An interaction effect is defined as departure from additivity of absolute effects, and excess RR caused by interaction (RERI) was calculated:\n\n[equation image]\nwhere RR(a+b+) denotes RR among those exposed to both factors where RR(a − b −) is used as reference category (RR = 1.0). Ninety-five percent CIs were calculated as proposed by Hosmer and Lemeshow. RERI of 0 means no interaction\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-12c_statistical_methods-1",
    "href": "guidelines/strobe/index.html#sec-12c_statistical_methods-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nMissing data are common in observational research. Questionnaires posted to study participants are not always filled in completely, participants may not attend all follow-up visits and routine data sources and clinical databases are often incomplete. Despite its ubiquity and importance, few papers report in detail on the problem of missing data [5,107]. Investigators may use any of several approaches to address missing data. We describe some strengths and limitations of various approaches in Box 6. We advise that authors report the number of missing values for each variable of interest (exposures, outcomes, confounders) and for each step in the analysis. Authors should give reasons for missing values if possible, and indicate how many individuals were excluded because of missing data when describing the flow of participants through the study (see also item 13). For analyses that account for missing data, authors should describe the nature of the analysis (e.g., multiple imputation) and the assumptions that were made (e.g., missing at random, see Box 6)."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-12c_statistical_methods-2",
    "href": "guidelines/strobe/index.html#sec-12c_statistical_methods-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\nOur missing data analysis procedures used missing at random (MAR) assumptions. We used the MICE (multivariate imputation by chained equations) method of multiple multivariate imputation in STATA. We independently analysed 10 copies of the data, each with missing values suitably imputed, in the multivariate logistic regression analyses. We averaged estimates of the variables to give a single mean estimate and adjusted standard errors according to Rubin’s rules\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-12di_statistical_methods-1",
    "href": "guidelines/strobe/index.html#sec-12di_statistical_methods-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nCohort studies are analysed using life table methods or other approaches that are based on the person-time of follow-up and time to developing the disease of interest. Among individuals who remain free of the disease at the end of their observation period, the amount of follow-up time is assumed to be unrelated to the probability of developing the outcome. This will be the case if follow-up ends on a fixed date or at a particular age. Loss to follow-up occurs when participants withdraw from a study before that date. This may hamper the validity of a study if loss to follow-up occurs selectively in exposed individuals, or in persons at high risk of developing the disease (informative censoring). In the example above, patients lost to follow-up in treatment programmes with no active follow-up had fewer CD4 helper cells than those remaining under observation and were therefore at higher risk of dying [116].\nIt is important to distinguish persons who reach the end of the study from those lost to follow-up. Unfortunately, statistical software usually does not distinguish between the two situations: in both cases follow-up time is automatically truncated (censored) at the end of the observation period. Investigators therefore need to decide, ideally at the stage of planning the study, how they will deal with loss to follow-up. When few patients are lost, investigators may either exclude individuals with incomplete follow-up, or treat them as if they withdrew alive at either the date of loss to follow-up or the end of the study. We advise authors to report how many patients were lost to follow-up and what censoring strategies they used."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-12di_statistical_methods-2",
    "href": "guidelines/strobe/index.html#sec-12di_statistical_methods-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\nIn treatment programmes with active follow-up, those lost to follow-up and those followed-up at 1 year had similar baseline CD4 cell counts (median 115 cells per μL and 123 cells per μL), whereas patients lost to follow-up in programmes with no active follow-up procedures had considerably lower CD4 cell counts than those followed-up (median 64 cells per μL and 123 cells per μL). (…) Treatment programmes with passive follow-up were excluded from subsequent analyses\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-12dii_statistical_methods-1",
    "href": "guidelines/strobe/index.html#sec-12dii_statistical_methods-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nIn individually matched case-control studies a crude analysis of the odds ratio, ignoring the matching, usually leads to an estimation that is biased towards unity (see Box 2). A matched analysis is therefore often necessary. This can intuitively be understood as a stratified analysis: each case is seen as one stratum with his or her set of matched controls. The analysis rests on considering whether the case is more often exposed than the controls, despite having made them alike regarding the matching variables. Investigators can do such a stratified analysis using the Mantel-Haenszel method on a matched 2 by 2 table. In its simplest form the odds ratio becomes the ratio of pairs that are discordant for the exposure variable. If matching was done for variables like age and sex that are universal attributes, the analysis needs not retain the individual, person-to-person matching: a simple analysis in categories of age and sex is sufficient [50]. For other matching variables, such as neighbourhood, sibship, or friendship, however, each matched set should be considered its own stratum.\nIn individually matched studies, the most widely used method of analysis is conditional logistic regression, in which each case and their controls are considered together. The conditional method is necessary when the number of controls varies among cases, and when, in addition to the matching variables, other variables need to be adjusted for. To allow readers to judge whether the matched design was appropriately taken into account in the analysis, we recommend that authors describe in detail what statistical methods were used to analyse the data. If taking the matching into account does have little effect on the estimates, authors may choose to present an unmatched analysis."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-12dii_statistical_methods-2",
    "href": "guidelines/strobe/index.html#sec-12dii_statistical_methods-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\nWe used McNemar’s test, paired t test, and conditional logistic regression analysis to compare dementia patients with their matched controls for cardiovascular risk factors, the occurrence of spontaneous cerebral emboli, carotid disease, and venous to arterial circulation shunt\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-12diii_statistical_methods-1",
    "href": "guidelines/strobe/index.html#sec-12diii_statistical_methods-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nMost cross-sectional studies use a pre-specified sampling strategy to select participants from a source population. Sampling may be more complex than taking a simple random sample, however. It may include several stages and clustering of participants (e.g., in districts or villages). Proportionate stratification may ensure that subgroups with a specific characteristic are correctly represented. Disproportionate stratification may be useful to over-sample a subgroup of particular interest.\nAn estimate of association derived from a complex sample may be more or less precise than that derived from a simple random sample. Measures of precision such as standard error or confidence interval should be corrected using the design effect, a ratio measure that describes how much precision is gained or lost if a more complex sampling strategy is used instead of simple random sampling [119]. Most complex sampling techniques lead to a decrease of precision, resulting in a design effect greater than 1.\nWe advise that authors clearly state the method used to adjust for complex sampling strategies so that readers may understand how the chosen sampling method influenced the precision of the obtained estimates. For instance, with clustered sampling, the implicit trade-off between easier data collection and loss of precision is transparent if the design effect is reported. In the example, the calculated design effects of 1.9 for men indicates that the actual sample size would need to be 1.9 times greater than with simple random sampling for the resulting estimates to have equal precision."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-12diii_statistical_methods-2",
    "href": "guidelines/strobe/index.html#sec-12diii_statistical_methods-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\nThe standard errors (SE) were calculated using the Taylor expansion method to estimate the sampling errors of estimators based on the complex sample design. (…) The overall design effect for diastolic blood pressure was found to be 1.9 for men and 1.8 for women and, for systolic blood pressure, it was 1.9 for men and 2.0 for women\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-12e_statistical_methods-1",
    "href": "guidelines/strobe/index.html#sec-12e_statistical_methods-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nSensitivity analyses are useful to investigate whether or not the main results are consistent with those obtained with alternative analysis strategies or assumptions [121]. Issues that may be examined include the criteria for inclusion in analyses, the definitions of exposures or outcomes [122], which confounding variables merit adjustment, the handling of missing data [120,123], possible selection bias or bias from inaccurate or inconsistent measurement of exposure, disease and other variables, and specific analysis choices, such as the treatment of quantitative variables (see item 11). Sophisticated methods are increasingly used to simultaneously model the influence of several biases or assumptions [124–126].\nIn 1959 Cornfield et al. famously showed that a relative risk of 9 for cigarette smoking and lung cancer was extremely unlikely to be due to any conceivable confounder, since the confounder would need to be at least nine times as prevalent in smokers as in non-smokers [127]. This analysis did not rule out the possibility that such a factor was present, but it did identify the prevalence such a factor would need to have. The same approach was recently used to identify plausible confounding factors that could explain the association between childhood leukaemia and living near electric power lines [128]. More generally, sensitivity analyses can be used to identify the degree of confounding, selection bias, or information bias required to distort an association. One important, perhaps under recognised, use of sensitivity analysis is when a study shows little or no association between an exposure and an outcome and it is plausible that confounding or other biases toward the null are present."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-12e_statistical_methods-2",
    "href": "guidelines/strobe/index.html#sec-12e_statistical_methods-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\nBecause we had a relatively higher proportion of missing dead patients with insufficient data (38/148=25.7%) as compared to live patients (15/437=3.4%) (…), it is possible that this might have biased the results. We have, therefore, carried out a sensitivity analysis. We have assumed that the proportion of women using oral contraceptives in the study group applies to the whole (19.1% for dead, and 11.4% for live patients), and then applied two extreme scenarios: either all the exposed missing patients used second generation pills or they all used third-generation pills\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-13a_participants-1",
    "href": "guidelines/strobe/index.html#sec-13a_participants-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nDetailed information on the process of recruiting study participants is important for several reasons. Those included in a study often differ in relevant ways from the target population to which results are applied. This may result in estimates of prevalence or incidence that do not reflect the experience of the target population. For example, people who agreed to participate in a postal survey of sexual behaviour attended church less often, had less conservative sexual attitudes and earlier age at first sexual intercourse, and were more likely to smoke cigarettes and drink alcohol than people who refused [130]. These differences suggest that postal surveys may overestimate sexual liberalism and activity in the population. Such response bias (see Box 3) can distort exposure-disease associations if associations differ between those eligible for the study and those included in the study. As another example, the association between young maternal age and leukaemia in offspring, which has been observed in some case-control studies [131,132], was explained by differential participation of young women in case and control groups. Young women with healthy children were less likely to participate than those with unhealthy children [133]. Although low participation does not necessarily compromise the validity of a study, transparent information on participation and reasons for non-participation is essential. Also, as there are no universally agreed definitions for participation, response or follow-up rates, readers need to understand how authors calculated such proportions [134].\nIdeally, investigators should give an account of the numbers of individuals considered at each stage of recruiting study participants, from the choice of a target population to the inclusion of participants’ data in the analysis. Depending on the type of study, this may include the number of individuals considered to be potentially eligible, the number assessed for eligibility, the number found to be eligible, the number included in the study, the number examined, the number followed up and the number included in the analysis. Information on different sampling units may be required, if sampling of study participants is carried out in two or more stages as in the example above (multistage sampling). In case-control studies, we advise that authors describe the flow of participants separately for case and control groups [135]. Controls can sometimes be selected from several sources, including, for example, hospitalised patients and community dwellers. In this case, we recommend a separate account of the numbers of participants for each type of control group. Olson and colleagues proposed useful reporting guidelines for controls recruited through random-digit dialling and other methods [136].\nA recent survey of epidemiological studies published in 10 general epidemiology, public health and medical journals found that some information regarding participation was provided in 47 of 107 case-control studies (59%), 49 of 154 cohort studies (32%), and 51 of 86 cross-sectional studies (59%) [137]. Incomplete or absent reporting of participation and non-participation in epidemiological studies was also documented in two other surveys of the literature [4,5]. Finally, there is evidence that participation in epidemiological studies may have declined in recent decades [137,138], which underscores the need for transparent reporting [139]."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-13a_participants-2",
    "href": "guidelines/strobe/index.html#sec-13a_participants-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\nOf the 105 freestanding bars and taverns sampled, 13 establishments were no longer in business and 9 were located in restaurants, leaving 83 eligible businesses. In 22 cases, the owner could not be reached by telephone despite 6 or more attempts. The owners of 36 bars declined study participation. (…) The 25 participating bars and taverns employed 124 bartenders, with 67 bartenders working at least 1 weekly daytime shift. Fifty-four of the daytime bartenders (81%) completed baseline interviews and spirometry; 53 of these subjects (98%) completed follow-up\n\n\nFlow diagram from Hay et al (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2020496/figure/pmed-0040297-g001/)\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-13b_participants-1",
    "href": "guidelines/strobe/index.html#sec-13b_participants-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nExplaining the reasons why people no longer participated in a study or why they were excluded from statistical analyses helps readers judge whether the study population was representative of the target population and whether bias was possibly introduced. For example, in a cross-sectional health survey, non-participation due to reasons unlikely to be related to health status (for example, the letter of invitation was not delivered because of an incorrect address) will affect the precision of estimates but will probably not introduce bias. Conversely, if many individuals opt out of the survey because of illness, or perceived good health, results may underestimate or overestimate the prevalence of ill health in the population."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-13b_participants-2",
    "href": "guidelines/strobe/index.html#sec-13b_participants-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\nThe main reasons for non-participation were the participant was too ill or had died before interview (cases 30%, controls < 1%), nonresponse (cases 2%, controls 21%), refusal (cases 10%, controls 29%), and other reasons (refusal by consultant or general practitioner, non-English speaking, mental impairment) (cases 7%, controls 5%)\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-13c_participants-1",
    "href": "guidelines/strobe/index.html#sec-13c_participants-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nAn informative and well-structured flow diagram can readily and transparently convey information that might otherwise require a lengthy description [142], as in the example above. The diagram may usefully include the main results, such as the number of events for the primary outcome. While we recommend the use of a flow diagram, particularly for complex observational studies, we do not propose a specific format for the diagram."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-14a_-1",
    "href": "guidelines/strobe/index.html#sec-14a_-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nReaders need descriptions of study participants and their exposures to judge the generalisability of the findings. Information about potential confounders, including whether and how they were measured, influences judgments about study validity. We advise authors to summarize continuous variables for each study group by giving the mean and standard deviation, or when the data have an asymmetrical distribution, as is often the case, the median and percentile range (e.g., 25th and 75th percentiles). Variables that make up a small number of ordered categories (such as stages of disease I to IV) should not be presented as continuous variables; it is preferable to give numbers and proportions for each category (see also Box 4). In studies that compare groups, the descriptive characteristics and numbers should be given by group, as in the example above.\nInferential measures such as standard errors and confidence intervals should not be used to describe the variability of characteristics, and significance tests should be avoided in descriptive tables. Also, P values are not an appropriate criterion for selecting which confounders to adjust for in analysis; even small differences in a confounder that has a strong effect on the outcome can be important [144,145].\nIn cohort studies, it may be useful to document how an exposure relates to other characteristics and potential confounders. Authors could present this information in a table with columns for participants in two or more exposure categories, which permits to judge the differences in confounders between these categories.\nIn case-control studies potential confounders cannot be judged by comparing cases and controls. Control persons represent the source population and will usually be different from the cases in many respects. For example, in a study of oral contraceptives and myocardial infarction, a sample of young women with infarction more often had risk factors for that disease, such as high serum cholesterol, smoking and a positive family history, than the control group [146]. This does not influence the assessment of the effect of oral contraceptives, as long as the prescription of oral contraceptives was not guided by the presence of these risk factors—e.g., because the risk factors were only established after the event (see also Box 5). In case-control studies the equivalent of comparing exposed and non-exposed for the presence of potential confounders (as is done in cohorts) can be achieved by exploring the source population of the cases: if the control group is large enough and represents the source population, exposed and unexposed controls can be compared for potential confounders [121,147]."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-14a_-2",
    "href": "guidelines/strobe/index.html#sec-14a_-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\nTable: Characteristics of the Study Base at Enrolment, Castellana G (Italy) (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2020496/table/pmed-0040297-t002/(\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-14b_-1",
    "href": "guidelines/strobe/index.html#sec-14b_-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nAs missing data may bias or affect generalisability of results, authors should tell readers amounts of missing data for exposures, potential confounders, and other important characteristics of patients (see also item 12c and Box 6). In a cohort study, authors should report the extent of loss to follow-up (with reasons), since incomplete follow-up may bias findings (see also items 12d and 13) [148]. We advise authors to use their tables and figures to enumerate amounts of missing data."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-14b_-2",
    "href": "guidelines/strobe/index.html#sec-14b_-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\nSymptom End Points Used in Survival Analysis (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2020496/table/pmed-0040297-t003/)\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-14c_-1",
    "href": "guidelines/strobe/index.html#sec-14c_-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nReaders need to know the duration and extent of follow-up for the available outcome data. Authors can present a summary of the average follow-up with either the mean or median follow-up time or both. The mean allows a reader to calculate the total number of person-years by multiplying it with the number of study participants. Authors also may present minimum and maximum times or percentiles of the distribution to show readers the spread of follow-up times. They may report total person-years of follow-up or some indication of the proportion of potential data that was captured [148]. All such information may be presented separately for participants in two or more exposure categories. Almost half of 132 articles in cancer journals (mostly cohort studies) did not give any summary of length of follow-up [37]."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-14c_-2",
    "href": "guidelines/strobe/index.html#sec-14c_-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\nDuring the 4366 person-years of follow-up (median 5.4, maximum 8.3 years), 265 subjects were diagnosed as having dementia, including 202 with Alzheimer’s disease\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-15_outcome_data-1",
    "href": "guidelines/strobe/index.html#sec-15_outcome_data-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nBefore addressing the possible association between exposures (risk factors) and outcomes, authors should report relevant descriptive data. It may be possible and meaningful to present measures of association in the same table that presents the descriptive data (see item 14a). In a cohort study with events as outcomes, report the numbers of events for each outcome of interest. Consider reporting the event rate per person-year of follow-up. If the risk of an event changes over follow-up time, present the numbers and rates of events in appropriate intervals of follow-up or as a Kaplan-Meier life table or plot. It might be preferable to show plots as cumulative incidence that go up from 0% rather than down from 100%, especially if the event rate is lower than, say, 30% [153]. Consider presenting such information separately for participants in different exposure categories of interest. If a cohort study is investigating other time-related outcomes (e.g., quantitative disease markers such as blood pressure), present appropriate summary measures (e.g., means and standard deviations) over time, perhaps in a table or figure.\nFor cross-sectional studies, we recommend presenting the same type of information on prevalent outcome events or summary measures. For case-control studies, the focus will be on reporting exposures separately for cases and controls as frequencies or quantitative summaries [154]. For all designs, it may be helpful also to tabulate continuous outcomes or exposures in categories, even if the data are not analysed as such."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-15_outcome_data-2",
    "href": "guidelines/strobe/index.html#sec-15_outcome_data-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\nCohort study: Table: Rates of HIV-1 Seroconversion by Selected Sociodemographic Variables: 1990–1993 (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2020496/table/pmed-0040297-t004/)\n\n\nCase-control study: Table: Exposure among Liver Cirrhosis Cases and Controls (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2020496/table/pmed-0040297-t006/)\n\n\nCross-sectional study: Table: Prevalence of Current Asthma and Diagnosed Hay Fever by Average Alternaria alternata Antigen Level in the Household (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2020496/table/pmed-0040297-t007/)\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-16a_main_results-1",
    "href": "guidelines/strobe/index.html#sec-16a_main_results-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nIn many situations, authors may present the results of unadjusted or minimally adjusted analyses and those from fully adjusted analyses. We advise giving the unadjusted analyses together with the main data, for example the number of cases and controls that were exposed or not. This allows the reader to understand the data behind the measures of association (see also item 15). For adjusted analyses, report the number of persons in the analysis, as this number may differ because of missing values in covariates (see also item 12c). Estimates should be given with confidence intervals.\nReaders can compare unadjusted measures of association with those adjusted for potential confounders and judge by how much, and in what direction, they changed. Readers may think that adjusted results equal the causal part of the measure of association, but adjusted results are not necessarily free of random sampling error, selection bias, information bias, or residual confounding (see Box 5). Thus, great care should be exercised when interpreting adjusted results, as the validity of results often depends crucially on complete knowledge of important confounders, their precise measurement, and appropriate specification in the statistical model (see also item 20) [157,158].\nAuthors should explain all potential confounders considered, and the criteria for excluding or including variables in statistical models. Decisions about excluding or including variables should be guided by knowledge, or explicit assumptions, on causal relations. Inappropriate decisions may introduce bias, for example by including variables that are in the causal pathway between exposure and disease (unless the aim is to asses how much of the effect is carried by the intermediary variable). If the decision to include a variable in the model was based on the change in the estimate, it is important to report what change was considered sufficiently important to justify its inclusion. If a backward deletion or forward inclusion strategy was used to select confounders, explain that process and give the significance level for rejecting the null hypothesis of no confounding. Of note, we and others do not advise selecting confounders based solely on statistical significance testing [147,159,160].\nRecent studies of the quality of reporting of epidemiological studies found that confidence intervals were reported in most articles [4]. However, few authors explained their choice of confounding variables [4,5]."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-16a_main_results-2",
    "href": "guidelines/strobe/index.html#sec-16a_main_results-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\nWe initially considered the following variables as potential confounders by Mantel-Haenszel stratified analysis: (…) The variables we included in the final logistic regression models were those (…) that produced a 10% change in the odds ratio after the Mantel-Haenszel adjustment\n\n\nTable: Relative Rates of Rehospitalisation by Treatment in Patients in Community Care after First Hospitalisation due to Schizophrenia and Schizoaffective Disorder (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2020496/table/pmed-0040297-t008/)\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-16b_main_results-1",
    "href": "guidelines/strobe/index.html#sec-16b_main_results-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nCategorizing continuous data has several important implications for analysis (see Box 4) and also affects the presentation of results. In tables, outcomes should be given for each exposure category, for example as counts of persons at risk, person-time at risk, if relevant separately for each group (e.g., cases and controls). Details of the categories used may aid comparison of studies and meta-analysis. If data were grouped using conventional cut-points, such as body mass index thresholds [162], group boundaries (i.e., range of values) can be derived easily, except for the highest and lowest categories. If quantile-derived categories are used, the category boundaries cannot be inferred from the data. As a minimum, authors should report the category boundaries; it is helpful also to report the range of the data and the mean or median values within categories."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-16b_main_results-2",
    "href": "guidelines/strobe/index.html#sec-16b_main_results-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\nTable: Polychlorinated Biphenyls in Cord Serum (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2020496/table/pmed-0040297-t005/)\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-16c_main_results-1",
    "href": "guidelines/strobe/index.html#sec-16c_main_results-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nThe results from studies examining the association between an exposure and a disease are commonly reported in relative terms, as ratios of risks, rates or odds (see Box 8). Relative measures capture the strength of the association between an exposure and disease. If the relative risk is a long way from 1 it is less likely that the association is due to confounding [164,165]. Relative effects or associations tend to be more consistent across studies and populations than absolute measures, but what often tends to be the case may be irrelevant in a particular instance. For example, similar relative risks were obtained for the classic cardiovascular risk factors for men living in Northern Ireland, France, the USA and Germany, despite the fact that the underlying risk of coronary heart disease varies substantially between these countries [166,167]. In contrast, in a study of hypertension as a risk factor for cardiovascular disease mortality, the data were more compatible with a constant rate difference than with a constant rate ratio [168].\nWidely used statistical models, including logistic [169] and proportional hazards (Cox) regression [170] are based on ratio measures. In these models, only departures from constancy of ratio effect measures are easily discerned. Nevertheless, measures which assess departures from additivity of risk differences, such as the Relative Excess Risk from Interaction (RERI, see item 12b and Box 8), can be estimated in models based on ratio measures.\nIn many circumstances, the absolute risk associated with an exposure is of greater interest than the relative risk. For example, if the focus is on adverse effects of a drug, one will want to know the number of additional cases per unit time of use (e.g., days, weeks, or years). The example gives the additional number of breast cancer cases per 1000 women who used hormone-replacement therapy for 10 years [163]. Measures such as the attributable risk or population attributable fraction may be useful to gauge how much disease can be prevented if the exposure is eliminated. They should preferably be presented together with a measure of statistical uncertainty (e.g., confidence intervals as in the example). Authors should be aware of the strong assumptions made in this context, including a causal relationship between a risk factor and disease (also see Box 7) [171]. Because of the semantic ambiguity and complexities involved, authors should report in detail what methods were used to calculate attributable risks, ideally giving the formulae used [172].\nA recent survey of abstracts of 222 articles published in leading medical journals found that in 62% of abstracts of randomised trials including a ratio measure absolute risks were given, but only in 21% of abstracts of cohort studies [173]. A free text search of Medline 1966 to 1997 showed that 619 items mentioned attributable risks in the title or abstract, compared to 18,955 using relative risk or odds ratio, for a ratio of 1 to 31 [174]."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-16c_main_results-2",
    "href": "guidelines/strobe/index.html#sec-16c_main_results-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\n10 years’ use of HRT [hormone replacement therapy] is estimated to result in five (95% CI 3–7) additional breast cancers per 1000 users of oestrogen-only preparations and 19 (15–23) additional cancers per 1000 users of oestrogen-progestagen combinations\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-17_other_analyses-1",
    "href": "guidelines/strobe/index.html#sec-17_other_analyses-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nIn addition to the main analysis other analyses are often done in observational studies. They may address specific subgroups, the potential interaction between risk factors, the calculation of attributable risks, or use alternative definitions of study variables in sensitivity analyses.\nThere is debate about the dangers associated with subgroup analyses, and multiplicity of analyses in general [4,104]. In our opinion, there is too great a tendency to look for evidence of subgroup-specific associations, or effect-measure modification, when overall results appear to suggest little or no effect. On the other hand, there is value in exploring whether an overall association appears consistent across several, preferably pre-specified subgroups especially when a study is large enough to have sufficient data in each subgroup. A second area of debate is about interesting subgroups that arose during the data analysis. They might be important findings, but might also arise by chance. Some argue that it is neither possible nor necessary to inform the reader about all subgroup analyses done as future analyses of other data will tell to what extent the early exciting findings stand the test of time [9]. We advise authors to report which analyses were planned, and which were not (see also items 4, 12b and 20). This will allow readers to judge the implications of multiplicity, taking into account the study’s position on the continuum from discovery to verification or refutation.\nA third area of debate is how joint effects and interactions between risk factors should be evaluated: on additive or multiplicative scales, or should the scale be determined by the statistical model that fits best (see also item 12b and Box 8)? A sensible approach is to report the separate effect of each exposure as well as the joint effect—if possible in a table, as in the first example above [183], or in the study by Martinelli et al. [185]. Such a table gives the reader sufficient information to evaluate additive as well as multiplicative interaction (how these calculations are done is shown in Box 8). Confidence intervals for separate and joint effects may help the reader to judge the strength of the data. In addition, confidence intervals around measures of interaction, such as the Relative Excess Risk from Interaction (RERI) relate to tests of interaction or homogeneity tests. One recurrent problem is that authors use comparisons of P-values across subgroups, which lead to erroneous claims about an effect modifier. For instance, a statistically significant association in one category (e.g., men), but not in the other (e.g., women) does not in itself provide evidence of effect modification. Similarly, the confidence intervals for each point estimate are sometimes inappropriately used to infer that there is no interaction when intervals overlap. A more valid inference is achieved by directly evaluating whether the magnitude of an association differs across subgroups.\nSensitivity analyses are helpful to investigate the influence of choices made in the statistical analysis, or to investigate the robustness of the findings to missing data or possible biases (see also item 12b). Judgement is needed regarding the level of reporting of such analyses. If many sensitivity analyses were performed, it may be impractical to present detailed findings for them all. It may sometimes be sufficient to report that sensitivity analyses were carried out and that they were consistent with the main results presented. Detailed presentation is more appropriate if the issue investigated is of major concern, or if effect estimates vary considerably [59,186].\nPocock and colleagues found that 43 out of 73 articles reporting observational studies contained subgroup analyses. The majority claimed differences across groups but only eight articles reported a formal evaluation of interaction (see item 12b) [4]."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-17_other_analyses-2",
    "href": "guidelines/strobe/index.html#sec-17_other_analyses-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\nAnalysis of Oral Contraceptive Use, Presence of Factor V Leiden Allele, and Risk for Venous Thromboembolism (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2020496/table/pmed-0040297-t009/)\n\n\nSensitivity of the Rate Ratio for Cardiovascular Outcome to an Unmeasured Confounder (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2020496/table/pmed-0040297-t010/)\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-18_key_results-1",
    "href": "guidelines/strobe/index.html#sec-18_key_results-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nIt is good practice to begin the discussion with a short summary of the main findings of the study. The short summary reminds readers of the main findings and may help them assess whether the subsequent interpretation and implications offered by the authors are supported by the findings."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-18_key_results-2",
    "href": "guidelines/strobe/index.html#sec-18_key_results-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\nWe hypothesized that ethnic minority status would be associated with higher levels of cardiovascular disease (CVD) risk factors, but that the associations would be explained substantially by socioeconomic status (SES). Our hypothesis was not confirmed. After adjustment for age and SES, highly significant differences in body mass index, blood pressure, diabetes, and physical inactivity remained between white women and both black and Mexican American women. In addition, we found large differences in CVD risk factors by SES, a finding that illustrates the high-risk status of both ethnic minority women as well as white women with low SES\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-19_limitations-1",
    "href": "guidelines/strobe/index.html#sec-19_limitations-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nThe identification and discussion of the limitations of a study are an essential part of scientific reporting. It is important not only to identify the sources of bias and confounding that could have affected results, but also to discuss the relative importance of different biases, including the likely direction and magnitude of any potential bias (see also item 9 and Box 3).\nAuthors should also discuss any imprecision of the results. Imprecision may arise in connection with several aspects of a study, including the study size (item 10) and the measurement of exposures, confounders and outcomes (item 8). The inability to precisely measure true values of an exposure tends to result in bias towards unity: the less precisely a risk factor is measured, the greater the bias. This effect has been described as attenuation [201,202], or more recently as regression dilution bias [203]. However, when correlated risk factors are measured with different degrees of imprecision, the adjusted relative risk associated with them can be biased towards or away from unity [204–206].\nWhen discussing limitations, authors may compare the study being presented with other studies in the literature in terms of validity, generalisability and precision. In this approach, each study can be viewed as contribution to the literature, not as a stand-alone basis for inference and action [207]. Surprisingly, the discussion of important limitations of a study is sometimes omitted from published reports. A survey of authors who had published original research articles in The Lancet found that important weaknesses of the study were reported by the investigators in the survey questionnaires, but not in the published article [192]."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-19_limitations-2",
    "href": "guidelines/strobe/index.html#sec-19_limitations-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\nSince the prevalence of counseling increases with increasing levels of obesity, our estimates may overestimate the true prevalence. Telephone surveys also may overestimate the true prevalence of counseling. Although persons without telephones have similar levels of overweight as persons with telephones, persons without telephones tend to be less educated, a factor associated with lower levels of counseling in our study. Also, of concern is the potential bias caused by those who refused to participate as well as those who refused to respond to questions about weight. Furthermore, because data were collected cross-sectionally, we cannot infer that counseling preceded a patient’s attempt to lose weight\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-20_interpretation-1",
    "href": "guidelines/strobe/index.html#sec-20_interpretation-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nThe heart of the discussion section is the interpretation of a study’s results. Over-interpretation is common and human: even when we try hard to give an objective assessment, reviewers often rightly point out that we went too far in some respects. When interpreting results, authors should consider the nature of the study on the discovery to verification continuum and potential sources of bias, including loss to follow-up and non-participation (see also items 9, 12 and 19). Due consideration should be given to confounding (item 16a), the results of relevant sensitivity analyses, and to the issue of multiplicity and subgroup analyses (item 17). Authors should also consider residual confounding due to unmeasured variables or imprecise measurement of confounders. For example, socioeconomic status (SES) is associated with many health outcomes and often differs between groups being compared. Variables used to measure SES (income, education, or occupation) are surrogates for other undefined and unmeasured exposures, and the true confounder will by definition be measured with error [208]. Authors should address the real range of uncertainty in estimates, which is larger than the statistical uncertainty reflected in confidence intervals. The latter do not take into account other uncertainties that arise from a study’s design, implementation, and methods of measurement [209].\nTo guide thinking and conclusions about causality, some may find criteria proposed by Bradford Hill in 1965 helpful [164]. How strong is the association with the exposure? Did it precede the onset of disease? Is the association consistently observed in different studies and settings? Is there supporting evidence from experimental studies, including laboratory and animal studies? How specific is the exposure’s putative effect, and is there a dose-response relationship? Is the association biologically plausible? These criteria should not, however, be applied mechanically. For example, some have argued that relative risks below 2 or 3 should be ignored [210,211]. This is a reversal of the point by Cornfield et al. about the strength of large relative risks (see item 12b) [127]. Although a causal effect is more likely with a relative risk of 9, it does not follow that one below 3 is necessarily spurious. For instance, the small increase in the risk of childhood leukaemia after intrauterine irradiation is credible because it concerns an adverse effect of a medical procedure for which no alternative explanations are obvious [212]. Moreover, the carcinogenic effects of radiation are well established. The doubling in the risk of ovarian cancer associated with eating 2 to 4 eggs per week is not immediately credible, since dietary habits are associated with a large number of lifestyle factors as well as SES [213]. In contrast, the credibility of much debated epidemiologic findings of a difference in thrombosis risk between different types of oral contraceptives was greatly enhanced by the differences in coagulation found in a randomised cross-over trial [214]. A discussion of the existing external evidence, from different types of studies, should always be included, but may be particularly important for studies reporting small increases in risk. Further, authors should put their results in context with similar studies and explain how the new study affects the existing body of evidence, ideally by referring to a systematic review."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-20_interpretation-2",
    "href": "guidelines/strobe/index.html#sec-20_interpretation-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\nAny explanation for an association between death from myocardial infarction and use of second generation oral contraceptives must be conjectural. There is no published evidence to suggest a direct biologic mechanism, and there are no other epidemiologic studies with relevant results. (…) The increase in absolute risk is very small and probably applies predominantly to smokers. Due to the lack of corroborative evidence, and because the analysis is based on relatively small numbers, more evidence on the subject is needed. We would not recommend any change in prescribing practice on the strength of these results\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-21_generisability-1",
    "href": "guidelines/strobe/index.html#sec-21_generisability-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nGeneralisability, also called external validity or applicability, is the extent to which the results of a study can be applied to other circumstances [216]. There is no external validity per se; the term is meaningful only with regard to clearly specified conditions [217]. Can results be applied to an individual, groups or populations that differ from those enrolled in the study with regard to age, sex, ethnicity, severity of disease, and co-morbid conditions? Are the nature and level of exposures comparable, and the definitions of outcomes relevant to another setting or population? Are data that were collected in longitudinal studies many years ago still relevant today? Are results from health services research in one country applicable to health systems in other countries?\nThe question of whether the results of a study have external validity is often a matter of judgment that depends on the study setting, the characteristics of the participants, the exposures examined, and the outcomes assessed. Thus, it is crucial that authors provide readers with adequate information about the setting and locations, eligibility criteria, the exposures and how they were measured, the definition of outcomes, and the period of recruitment and follow-up. The degree of non-participation and the proportion of unexposed participants in whom the outcome develops are also relevant. Knowledge of the absolute risk and prevalence of the exposure, which will often vary across populations, are helpful when applying results to other settings and populations (see Box 7)."
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-21_generisability-2",
    "href": "guidelines/strobe/index.html#sec-21_generisability-2",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Examples",
    "text": "Examples\n\nHow applicable are our estimates to other HIV-1-infected patients? This is an important question because the accuracy of prognostic models tends to be lower when applied to data other than those used to develop them. We addressed this issue by penalising model complexity, and by choosing models that generalised best to cohorts omitted from the estimation procedure. Our database included patients from many countries from Europe and North America, who were treated in different settings. The range of patients was broad: men and women, from teenagers to elderly people were included, and the major exposure categories were well represented. The severity of immunodeficiency at baseline ranged from not measureable to very severe, and viral load from undetectable to extremely high\n\nBack to top"
  },
  {
    "objectID": "guidelines/strobe/index.html#sec-22_funding-1",
    "href": "guidelines/strobe/index.html#sec-22_funding-1",
    "title": "The STROBE guideline for writing a observational study.",
    "section": "Read More",
    "text": "Read More\nSome journals require authors to disclose the presence or absence of financial and other conflicts of interest [100,218]. Several investigations show strong associations between the source of funding and the conclusions of research articles [219–222]. The conclusions in randomised trials recommended the experimental drug as the drug of choice much more often (odds ratio 5.3) if the trial was funded by for-profit organisations, even after adjustment for the effect size [223]. Other studies document the influence of the tobacco and telecommunication industries on the research they funded [224–227]. There are also examples of undue influence when the sponsor is governmental or a non-profit organisation.\nAuthors or funders may have conflicts of interest that influence any of the following: the design of the study [228]; choice of exposures [228,229], outcomes [230], statistical methods [231], and selective publication of outcomes [230] and studies [232]. Consequently, the role of the funders should be described in detail: in what part of the study they took direct responsibility (e.g., design, data collection, analysis, drafting of manuscript, decision to publish) [100]. Other sources of undue influence include employers (e.g., university administrators for academic researchers and government supervisors, especially political appointees, for government researchers), advisory committees, litigants, and special interest groups."
  },
  {
    "objectID": "guidelines/tripod/index.html#about-this-guideline",
    "href": "guidelines/tripod/index.html#about-this-guideline",
    "title": "The TRIPOD guideline for writing a Prediction model development and validation study.",
    "section": "About this guideline",
    "text": "About this guideline\n\nThis guideline is relevant to studies reporting prediction models (both diagnostic and prognostic)"
  },
  {
    "objectID": "guidelines/tripod/index.html#download-resources",
    "href": "guidelines/tripod/index.html#download-resources",
    "title": "The TRIPOD guideline for writing a Prediction model development and validation study.",
    "section": "Download Resources",
    "text": "Download Resources\nReporting guidelines are best used early:\n\nUse a template to write quickly and avoid the blank page\nUse a checklist to double-check and reassure others that your work is described transparently\n\n\n\n\nFor drafting\n\nTemplate\n\n\n\nFor checking\n\nChecklist\n\n\n\nFor planning\n\nTo-do list"
  },
  {
    "objectID": "guidelines/tripod/index.html#guidance",
    "href": "guidelines/tripod/index.html#guidance",
    "title": "The TRIPOD guideline for writing a Prediction model development and validation study.",
    "section": "Guidance",
    "text": "Guidance\nApprox. 3 min read\n\nTitle\n\n\n1. Title\n\n\n\n\n\n\n\nIdentify the study as developing and / or validating a multivariable prediction model, the target population, and the outcome to be predicted.\n\n\nAbstract\n\n\n2. Abstract\n\n\n\n\n\n\n\nProvide a summary of objectives, study design, setting, participants, sample size, predictors, outcome, statistical analysis, results, and conclusions.\n\n\nIntroduction\n\n\n3a. Introduction\n\n\n\n\n\n\n\nExplain the medical context (including whether diagnostic or prognostic) and rationale for developing or validating the multivariable prediction model, including references to existing models.\n\n\n3b. Introduction\n\n\n\n\n\n\n\nSpecify the objectives, including whether the study describes the development or validation of the model or both.\n\n\nMethods\n\n\n4a. Source of data\n\n\n\n\n\n\n\nDescribe the study design or source of data (e.g., randomized trial, cohort, or registry data), separately for the development and validation data sets, if applicable.\n\n\n4b. Source of data\n\n\n\n\n\n\n\nSpecify the key study dates, including start of accrual; end of accrual; and, if applicable, end of follow-up.\n\n\n5a. Participants\n\n\n\n\n\n\n\nSpecify key elements of the study setting (e.g., primary care, secondary care, general population) including number and location of centres.\n\n\n5b. Participants\n\n\n\n\n\n\n\nDescribe eligibility criteria for participants.\n\n\n5c. Participants\n\n\n\n\n\n\n\nGive details of treatments received, if relevant\n\n\n6a. Outcome\n\n\n\n\n\n\n\nClearly define the outcome that is predicted by the prediction model, including how and when assessed.\n\n\n6b. Outcome\n\n\n\n\n\n\n\nReport any actions to blind assessment of the outcome to be predicted.\n\n\n7a. Predictors\n\n\n\n\n\n\n\nClearly define all predictors used in developing or validating the multivariable prediction model, including how and when they were measured\n\n\n7b. Predictors\n\n\n\n\n\n\n\nReport any actions to blind assessment of predictors for the outcome and other predictors.\n\n\n8. Sample size\n\n\n\n\n\n\n\nExplain how the study size was arrived at.\n\n\n9. Missing data\n\n\n\n\n\n\n\nDescribe how missing data were handled (e.g., complete-case analysis, single imputation, multiple imputation) with details of any imputation method.\n\n\n10a. Statistical analysis methods\n\n\n\n\n\n\n\nIf you are developing a prediction model describe how predictors were handled in the analyses.\n\n\n10b. Statistical analysis methods\n\n\n\n\n\n\n\nIf you are developing a prediction model, specify type of model, all model-building procedures (including any predictor selection), and method for internal validation.\n\n\n10c. Statistical analysis methods\n\n\n\n\n\n\n\nIf you are validating a prediction model, describe how the predictions were calculated.\n\n\n10d. Statistical analysis methods\n\n\n\n\n\n\n\nSpecify all measures used to assess model performance and, if relevant, to compare multiple models.\n\n\n10e. Statistical analysis methods\n\n\n\n\n\n\n\nIf you are validating a prediction model, describe any model updating (e.g., recalibration) arising from the validation, if done\n\n\n11. Risk groups\n\n\n\n\n\n\n\nProvide details on how risk groups were created, if done.\n\n\n12. Development vs. validation\n\n\n\n\n\n\n\nFor validation, identify any differences from the development data in setting, eligibility criteria, outcome, and predictors.\n\n\nResults\n\n\n13a. Participants\n\n\n\n\n\n\n\nDescribe the flow of participants through the study, including the number of participants with and without the outcome and, if applicable, a summary of the follow-up time. A diagram may be helpful.\n\n\n13b. Participants\n\n\n\n\n\n\n\nDescribe the characteristics of the participants (basic demographics, clinical features, available predictors), including the number of participants with missing data for predictors and outcome.\n\n\n13c. Participants\n\n\n\n\n\n\n\nFor validation, show a comparison with the development data of the distribution of important variables (demographics, predictors and outcome).\n\n\n14a. Model development\n\n\n\n\n\n\n\nIf developing a model, specify the number of participants and outcome events in each analysis.\n\n\n14b. Model development\n\n\n\n\n\n\n\nIf developing a model, report the unadjusted association, if calculated between each candidate predictor and outcome.\n\n\n15a. Model specification\n\n\n\n\n\n\n\nIf developing a model, present the full prediction model to allow predictions for individuals (i.e., all regression coefficients, and model intercept or baseline survival at a given time point).\n\n\n15b. Model specification\n\n\n\n\n\n\n\nIf developing a prediction model, explain how to the use it.\n\n\n16. Model performance\n\n\n\n\n\n\n\nReport performance measures (with CIs) for the prediction model.\n\n\n17. Model-updating\n\n\n\n\n\n\n\nIf validating a model, report the results from any model updating, if done (i.e., model specification, model performance).\n\n\nDiscussion\n\n\n18. Limitations\n\n\n\n\n\n\n\nDiscuss any limitations of the study (such as nonrepresentative sample, few events per predictor, missing data).\n\n\n19a. Interpretation\n\n\n\n\n\n\n\nFor validation, discuss the results with reference to performance in the development data, and any other validation data\n\n\n19b. Interpretation\n\n\n\n\n\n\n\nGive an overall interpretation of the results, considering objectives, limitations, results from similar studies, and other relevant evidence.\n\n\n20. Implications\n\n\n\n\n\n\n\nDiscuss the potential clinical use of the model and implications for future research\n\n\nOther information\n\n\n21. Supplementary information\n\n\n\n\n\n\n\nProvide information about the availability of supplementary resources, such as study protocol, Web calculator, and data sets.\n\n\n22. Funding\n\n\n\n\n\n\n\nGive the source of funding and the role of the funders for the present study."
  }
]